(* frege: -v -isafety -jobs 6 -slicing -no_carry_constraint -vo lex __asm_ntt_0_1_2_3_32.cl
Parsing Cryptoline file:                [OK]            0.198150 seconds
Checking well-formedness:               [OK]            0.115184 seconds
Transforming to SSA form:               [OK]            0.026615 seconds
Rewriting assignments:                  [OK]            0.025058 seconds
Verifying program safety:               [OK]            3167.382206 seconds
Verifying range assertions:             [OK]            153.081934 seconds
Verifying range specification:          [OK]            2880.165224 seconds
Rewriting value-preserved casting:      [OK]            0.001627 seconds
Verifying algebraic assertions:         [OK]            218.204960 seconds
Verifying algebraic specification:      [OK]            33826.135516 seconds
Verification result:                    [OK]            40245.353313 seconds
                      Round 1 (6013 safety conditions, timeout = 300 seconds)
*)

proc main (
sint32 f000, sint32 f001, sint32 f002, sint32 f003, sint32 f004,
sint32 f005, sint32 f006, sint32 f007, sint32 f008, sint32 f009,
sint32 f010, sint32 f011, sint32 f012, sint32 f013, sint32 f014,
sint32 f015, sint32 f016, sint32 f017, sint32 f018, sint32 f019,
sint32 f020, sint32 f021, sint32 f022, sint32 f023, sint32 f024,
sint32 f025, sint32 f026, sint32 f027, sint32 f028, sint32 f029,
sint32 f030, sint32 f031, sint32 f032, sint32 f033, sint32 f034,
sint32 f035, sint32 f036, sint32 f037, sint32 f038, sint32 f039,
sint32 f040, sint32 f041, sint32 f042, sint32 f043, sint32 f044,
sint32 f045, sint32 f046, sint32 f047, sint32 f048, sint32 f049,
sint32 f050, sint32 f051, sint32 f052, sint32 f053, sint32 f054,
sint32 f055, sint32 f056, sint32 f057, sint32 f058, sint32 f059,
sint32 f060, sint32 f061, sint32 f062, sint32 f063, sint32 f064,
sint32 f065, sint32 f066, sint32 f067, sint32 f068, sint32 f069,
sint32 f070, sint32 f071, sint32 f072, sint32 f073, sint32 f074,
sint32 f075, sint32 f076, sint32 f077, sint32 f078, sint32 f079,
sint32 f080, sint32 f081, sint32 f082, sint32 f083, sint32 f084,
sint32 f085, sint32 f086, sint32 f087, sint32 f088, sint32 f089,
sint32 f090, sint32 f091, sint32 f092, sint32 f093, sint32 f094,
sint32 f095, sint32 f096, sint32 f097, sint32 f098, sint32 f099,
sint32 f100, sint32 f101, sint32 f102, sint32 f103, sint32 f104,
sint32 f105, sint32 f106, sint32 f107, sint32 f108, sint32 f109,
sint32 f110, sint32 f111, sint32 f112, sint32 f113, sint32 f114,
sint32 f115, sint32 f116, sint32 f117, sint32 f118, sint32 f119,
sint32 f120, sint32 f121, sint32 f122, sint32 f123, sint32 f124,
sint32 f125, sint32 f126, sint32 f127, sint32 f128, sint32 f129,
sint32 f130, sint32 f131, sint32 f132, sint32 f133, sint32 f134,
sint32 f135, sint32 f136, sint32 f137, sint32 f138, sint32 f139,
sint32 f140, sint32 f141, sint32 f142, sint32 f143, sint32 f144,
sint32 f145, sint32 f146, sint32 f147, sint32 f148, sint32 f149,
sint32 f150, sint32 f151, sint32 f152, sint32 f153, sint32 f154,
sint32 f155, sint32 f156, sint32 f157, sint32 f158, sint32 f159,
sint32 f160, sint32 f161, sint32 f162, sint32 f163, sint32 f164,
sint32 f165, sint32 f166, sint32 f167, sint32 f168, sint32 f169,
sint32 f170, sint32 f171, sint32 f172, sint32 f173, sint32 f174,
sint32 f175, sint32 f176, sint32 f177, sint32 f178, sint32 f179,
sint32 f180, sint32 f181, sint32 f182, sint32 f183, sint32 f184,
sint32 f185, sint32 f186, sint32 f187, sint32 f188, sint32 f189,
sint32 f190, sint32 f191, sint32 f192, sint32 f193, sint32 f194,
sint32 f195, sint32 f196, sint32 f197, sint32 f198, sint32 f199,
sint32 f200, sint32 f201, sint32 f202, sint32 f203, sint32 f204,
sint32 f205, sint32 f206, sint32 f207, sint32 f208, sint32 f209,
sint32 f210, sint32 f211, sint32 f212, sint32 f213, sint32 f214,
sint32 f215, sint32 f216, sint32 f217, sint32 f218, sint32 f219,
sint32 f220, sint32 f221, sint32 f222, sint32 f223, sint32 f224,
sint32 f225, sint32 f226, sint32 f227, sint32 f228, sint32 f229,
sint32 f230, sint32 f231, sint32 f232, sint32 f233, sint32 f234,
sint32 f235, sint32 f236, sint32 f237, sint32 f238, sint32 f239,
sint32 f240, sint32 f241, sint32 f242, sint32 f243, sint32 f244,
sint32 f245, sint32 f246, sint32 f247, sint32 f248, sint32 f249,
sint32 f250, sint32 f251, sint32 f252, sint32 f253, sint32 f254,
sint32 f255, sint32 f256, sint32 f257, sint32 f258, sint32 f259,
sint32 f260, sint32 f261, sint32 f262, sint32 f263, sint32 f264,
sint32 f265, sint32 f266, sint32 f267, sint32 f268, sint32 f269,
sint32 f270, sint32 f271, sint32 f272, sint32 f273, sint32 f274,
sint32 f275, sint32 f276, sint32 f277, sint32 f278, sint32 f279,
sint32 f280, sint32 f281, sint32 f282, sint32 f283, sint32 f284,
sint32 f285, sint32 f286, sint32 f287, sint32 f288, sint32 f289,
sint32 f290, sint32 f291, sint32 f292, sint32 f293, sint32 f294,
sint32 f295, sint32 f296, sint32 f297, sint32 f298, sint32 f299,
sint32 f300, sint32 f301, sint32 f302, sint32 f303, sint32 f304,
sint32 f305, sint32 f306, sint32 f307, sint32 f308, sint32 f309,
sint32 f310, sint32 f311, sint32 f312, sint32 f313, sint32 f314,
sint32 f315, sint32 f316, sint32 f317, sint32 f318, sint32 f319,
sint32 f320, sint32 f321, sint32 f322, sint32 f323, sint32 f324,
sint32 f325, sint32 f326, sint32 f327, sint32 f328, sint32 f329,
sint32 f330, sint32 f331, sint32 f332, sint32 f333, sint32 f334,
sint32 f335, sint32 f336, sint32 f337, sint32 f338, sint32 f339,
sint32 f340, sint32 f341, sint32 f342, sint32 f343, sint32 f344,
sint32 f345, sint32 f346, sint32 f347, sint32 f348, sint32 f349,
sint32 f350, sint32 f351, sint32 f352, sint32 f353, sint32 f354,
sint32 f355, sint32 f356, sint32 f357, sint32 f358, sint32 f359,
sint32 f360, sint32 f361, sint32 f362, sint32 f363, sint32 f364,
sint32 f365, sint32 f366, sint32 f367, sint32 f368, sint32 f369,
sint32 f370, sint32 f371, sint32 f372, sint32 f373, sint32 f374,
sint32 f375, sint32 f376, sint32 f377, sint32 f378, sint32 f379,
sint32 f380, sint32 f381, sint32 f382, sint32 f383, sint32 f384,
sint32 f385, sint32 f386, sint32 f387, sint32 f388, sint32 f389,
sint32 f390, sint32 f391, sint32 f392, sint32 f393, sint32 f394,
sint32 f395, sint32 f396, sint32 f397, sint32 f398, sint32 f399,
sint32 f400, sint32 f401, sint32 f402, sint32 f403, sint32 f404,
sint32 f405, sint32 f406, sint32 f407, sint32 f408, sint32 f409,
sint32 f410, sint32 f411, sint32 f412, sint32 f413, sint32 f414,
sint32 f415, sint32 f416, sint32 f417, sint32 f418, sint32 f419,
sint32 f420, sint32 f421, sint32 f422, sint32 f423, sint32 f424,
sint32 f425, sint32 f426, sint32 f427, sint32 f428, sint32 f429,
sint32 f430, sint32 f431, sint32 f432, sint32 f433, sint32 f434,
sint32 f435, sint32 f436, sint32 f437, sint32 f438, sint32 f439,
sint32 f440, sint32 f441, sint32 f442, sint32 f443, sint32 f444,
sint32 f445, sint32 f446, sint32 f447, sint32 f448, sint32 f449,
sint32 f450, sint32 f451, sint32 f452, sint32 f453, sint32 f454,
sint32 f455, sint32 f456, sint32 f457, sint32 f458, sint32 f459,
sint32 f460, sint32 f461, sint32 f462, sint32 f463, sint32 f464,
sint32 f465, sint32 f466, sint32 f467, sint32 f468, sint32 f469,
sint32 f470, sint32 f471, sint32 f472, sint32 f473, sint32 f474,
sint32 f475, sint32 f476, sint32 f477, sint32 f478, sint32 f479,
sint32 f480, sint32 f481, sint32 f482, sint32 f483, sint32 f484,
sint32 f485, sint32 f486, sint32 f487, sint32 f488, sint32 f489,
sint32 f490, sint32 f491, sint32 f492, sint32 f493, sint32 f494,
sint32 f495, sint32 f496, sint32 f497, sint32 f498, sint32 f499,
sint32 f500, sint32 f501, sint32 f502, sint32 f503, sint32 f504,
sint32 f505, sint32 f506, sint32 f507, sint32 f508
) =
{
true && and [
(-2048)@32 <=s f000, f000 <s 2048@32, (-2048)@32 <=s f001, f001 <s 2048@32,
(-2048)@32 <=s f002, f002 <s 2048@32, (-2048)@32 <=s f003, f003 <s 2048@32,
(-2048)@32 <=s f004, f004 <s 2048@32, (-2048)@32 <=s f005, f005 <s 2048@32,
(-2048)@32 <=s f006, f006 <s 2048@32, (-2048)@32 <=s f007, f007 <s 2048@32,
(-2048)@32 <=s f008, f008 <s 2048@32, (-2048)@32 <=s f009, f009 <s 2048@32,
(-2048)@32 <=s f010, f010 <s 2048@32, (-2048)@32 <=s f011, f011 <s 2048@32,
(-2048)@32 <=s f012, f012 <s 2048@32, (-2048)@32 <=s f013, f013 <s 2048@32,
(-2048)@32 <=s f014, f014 <s 2048@32, (-2048)@32 <=s f015, f015 <s 2048@32,
(-2048)@32 <=s f016, f016 <s 2048@32, (-2048)@32 <=s f017, f017 <s 2048@32,
(-2048)@32 <=s f018, f018 <s 2048@32, (-2048)@32 <=s f019, f019 <s 2048@32,
(-2048)@32 <=s f020, f020 <s 2048@32, (-2048)@32 <=s f021, f021 <s 2048@32,
(-2048)@32 <=s f022, f022 <s 2048@32, (-2048)@32 <=s f023, f023 <s 2048@32,
(-2048)@32 <=s f024, f024 <s 2048@32, (-2048)@32 <=s f025, f025 <s 2048@32,
(-2048)@32 <=s f026, f026 <s 2048@32, (-2048)@32 <=s f027, f027 <s 2048@32,
(-2048)@32 <=s f028, f028 <s 2048@32, (-2048)@32 <=s f029, f029 <s 2048@32,
(-2048)@32 <=s f030, f030 <s 2048@32, (-2048)@32 <=s f031, f031 <s 2048@32,
(-2048)@32 <=s f032, f032 <s 2048@32, (-2048)@32 <=s f033, f033 <s 2048@32,
(-2048)@32 <=s f034, f034 <s 2048@32, (-2048)@32 <=s f035, f035 <s 2048@32,
(-2048)@32 <=s f036, f036 <s 2048@32, (-2048)@32 <=s f037, f037 <s 2048@32,
(-2048)@32 <=s f038, f038 <s 2048@32, (-2048)@32 <=s f039, f039 <s 2048@32,
(-2048)@32 <=s f040, f040 <s 2048@32, (-2048)@32 <=s f041, f041 <s 2048@32,
(-2048)@32 <=s f042, f042 <s 2048@32, (-2048)@32 <=s f043, f043 <s 2048@32,
(-2048)@32 <=s f044, f044 <s 2048@32, (-2048)@32 <=s f045, f045 <s 2048@32,
(-2048)@32 <=s f046, f046 <s 2048@32, (-2048)@32 <=s f047, f047 <s 2048@32,
(-2048)@32 <=s f048, f048 <s 2048@32, (-2048)@32 <=s f049, f049 <s 2048@32,
(-2048)@32 <=s f050, f050 <s 2048@32, (-2048)@32 <=s f051, f051 <s 2048@32,
(-2048)@32 <=s f052, f052 <s 2048@32, (-2048)@32 <=s f053, f053 <s 2048@32,
(-2048)@32 <=s f054, f054 <s 2048@32, (-2048)@32 <=s f055, f055 <s 2048@32,
(-2048)@32 <=s f056, f056 <s 2048@32, (-2048)@32 <=s f057, f057 <s 2048@32,
(-2048)@32 <=s f058, f058 <s 2048@32, (-2048)@32 <=s f059, f059 <s 2048@32,
(-2048)@32 <=s f060, f060 <s 2048@32, (-2048)@32 <=s f061, f061 <s 2048@32,
(-2048)@32 <=s f062, f062 <s 2048@32, (-2048)@32 <=s f063, f063 <s 2048@32,
(-2048)@32 <=s f064, f064 <s 2048@32, (-2048)@32 <=s f065, f065 <s 2048@32,
(-2048)@32 <=s f066, f066 <s 2048@32, (-2048)@32 <=s f067, f067 <s 2048@32,
(-2048)@32 <=s f068, f068 <s 2048@32, (-2048)@32 <=s f069, f069 <s 2048@32,
(-2048)@32 <=s f070, f070 <s 2048@32, (-2048)@32 <=s f071, f071 <s 2048@32,
(-2048)@32 <=s f072, f072 <s 2048@32, (-2048)@32 <=s f073, f073 <s 2048@32,
(-2048)@32 <=s f074, f074 <s 2048@32, (-2048)@32 <=s f075, f075 <s 2048@32,
(-2048)@32 <=s f076, f076 <s 2048@32, (-2048)@32 <=s f077, f077 <s 2048@32,
(-2048)@32 <=s f078, f078 <s 2048@32, (-2048)@32 <=s f079, f079 <s 2048@32,
(-2048)@32 <=s f080, f080 <s 2048@32, (-2048)@32 <=s f081, f081 <s 2048@32,
(-2048)@32 <=s f082, f082 <s 2048@32, (-2048)@32 <=s f083, f083 <s 2048@32,
(-2048)@32 <=s f084, f084 <s 2048@32, (-2048)@32 <=s f085, f085 <s 2048@32,
(-2048)@32 <=s f086, f086 <s 2048@32, (-2048)@32 <=s f087, f087 <s 2048@32,
(-2048)@32 <=s f088, f088 <s 2048@32, (-2048)@32 <=s f089, f089 <s 2048@32,
(-2048)@32 <=s f090, f090 <s 2048@32, (-2048)@32 <=s f091, f091 <s 2048@32,
(-2048)@32 <=s f092, f092 <s 2048@32, (-2048)@32 <=s f093, f093 <s 2048@32,
(-2048)@32 <=s f094, f094 <s 2048@32, (-2048)@32 <=s f095, f095 <s 2048@32,
(-2048)@32 <=s f096, f096 <s 2048@32, (-2048)@32 <=s f097, f097 <s 2048@32,
(-2048)@32 <=s f098, f098 <s 2048@32, (-2048)@32 <=s f099, f099 <s 2048@32,
(-2048)@32 <=s f100, f100 <s 2048@32, (-2048)@32 <=s f101, f101 <s 2048@32,
(-2048)@32 <=s f102, f102 <s 2048@32, (-2048)@32 <=s f103, f103 <s 2048@32,
(-2048)@32 <=s f104, f104 <s 2048@32, (-2048)@32 <=s f105, f105 <s 2048@32,
(-2048)@32 <=s f106, f106 <s 2048@32, (-2048)@32 <=s f107, f107 <s 2048@32,
(-2048)@32 <=s f108, f108 <s 2048@32, (-2048)@32 <=s f109, f109 <s 2048@32,
(-2048)@32 <=s f110, f110 <s 2048@32, (-2048)@32 <=s f111, f111 <s 2048@32,
(-2048)@32 <=s f112, f112 <s 2048@32, (-2048)@32 <=s f113, f113 <s 2048@32,
(-2048)@32 <=s f114, f114 <s 2048@32, (-2048)@32 <=s f115, f115 <s 2048@32,
(-2048)@32 <=s f116, f116 <s 2048@32, (-2048)@32 <=s f117, f117 <s 2048@32,
(-2048)@32 <=s f118, f118 <s 2048@32, (-2048)@32 <=s f119, f119 <s 2048@32,
(-2048)@32 <=s f120, f120 <s 2048@32, (-2048)@32 <=s f121, f121 <s 2048@32,
(-2048)@32 <=s f122, f122 <s 2048@32, (-2048)@32 <=s f123, f123 <s 2048@32,
(-2048)@32 <=s f124, f124 <s 2048@32, (-2048)@32 <=s f125, f125 <s 2048@32,
(-2048)@32 <=s f126, f126 <s 2048@32, (-2048)@32 <=s f127, f127 <s 2048@32,
(-2048)@32 <=s f128, f128 <s 2048@32, (-2048)@32 <=s f129, f129 <s 2048@32,
(-2048)@32 <=s f130, f130 <s 2048@32, (-2048)@32 <=s f131, f131 <s 2048@32,
(-2048)@32 <=s f132, f132 <s 2048@32, (-2048)@32 <=s f133, f133 <s 2048@32,
(-2048)@32 <=s f134, f134 <s 2048@32, (-2048)@32 <=s f135, f135 <s 2048@32,
(-2048)@32 <=s f136, f136 <s 2048@32, (-2048)@32 <=s f137, f137 <s 2048@32,
(-2048)@32 <=s f138, f138 <s 2048@32, (-2048)@32 <=s f139, f139 <s 2048@32,
(-2048)@32 <=s f140, f140 <s 2048@32, (-2048)@32 <=s f141, f141 <s 2048@32,
(-2048)@32 <=s f142, f142 <s 2048@32, (-2048)@32 <=s f143, f143 <s 2048@32,
(-2048)@32 <=s f144, f144 <s 2048@32, (-2048)@32 <=s f145, f145 <s 2048@32,
(-2048)@32 <=s f146, f146 <s 2048@32, (-2048)@32 <=s f147, f147 <s 2048@32,
(-2048)@32 <=s f148, f148 <s 2048@32, (-2048)@32 <=s f149, f149 <s 2048@32,
(-2048)@32 <=s f150, f150 <s 2048@32, (-2048)@32 <=s f151, f151 <s 2048@32,
(-2048)@32 <=s f152, f152 <s 2048@32, (-2048)@32 <=s f153, f153 <s 2048@32,
(-2048)@32 <=s f154, f154 <s 2048@32, (-2048)@32 <=s f155, f155 <s 2048@32,
(-2048)@32 <=s f156, f156 <s 2048@32, (-2048)@32 <=s f157, f157 <s 2048@32,
(-2048)@32 <=s f158, f158 <s 2048@32, (-2048)@32 <=s f159, f159 <s 2048@32,
(-2048)@32 <=s f160, f160 <s 2048@32, (-2048)@32 <=s f161, f161 <s 2048@32,
(-2048)@32 <=s f162, f162 <s 2048@32, (-2048)@32 <=s f163, f163 <s 2048@32,
(-2048)@32 <=s f164, f164 <s 2048@32, (-2048)@32 <=s f165, f165 <s 2048@32,
(-2048)@32 <=s f166, f166 <s 2048@32, (-2048)@32 <=s f167, f167 <s 2048@32,
(-2048)@32 <=s f168, f168 <s 2048@32, (-2048)@32 <=s f169, f169 <s 2048@32,
(-2048)@32 <=s f170, f170 <s 2048@32, (-2048)@32 <=s f171, f171 <s 2048@32,
(-2048)@32 <=s f172, f172 <s 2048@32, (-2048)@32 <=s f173, f173 <s 2048@32,
(-2048)@32 <=s f174, f174 <s 2048@32, (-2048)@32 <=s f175, f175 <s 2048@32,
(-2048)@32 <=s f176, f176 <s 2048@32, (-2048)@32 <=s f177, f177 <s 2048@32,
(-2048)@32 <=s f178, f178 <s 2048@32, (-2048)@32 <=s f179, f179 <s 2048@32,
(-2048)@32 <=s f180, f180 <s 2048@32, (-2048)@32 <=s f181, f181 <s 2048@32,
(-2048)@32 <=s f182, f182 <s 2048@32, (-2048)@32 <=s f183, f183 <s 2048@32,
(-2048)@32 <=s f184, f184 <s 2048@32, (-2048)@32 <=s f185, f185 <s 2048@32,
(-2048)@32 <=s f186, f186 <s 2048@32, (-2048)@32 <=s f187, f187 <s 2048@32,
(-2048)@32 <=s f188, f188 <s 2048@32, (-2048)@32 <=s f189, f189 <s 2048@32,
(-2048)@32 <=s f190, f190 <s 2048@32, (-2048)@32 <=s f191, f191 <s 2048@32,
(-2048)@32 <=s f192, f192 <s 2048@32, (-2048)@32 <=s f193, f193 <s 2048@32,
(-2048)@32 <=s f194, f194 <s 2048@32, (-2048)@32 <=s f195, f195 <s 2048@32,
(-2048)@32 <=s f196, f196 <s 2048@32, (-2048)@32 <=s f197, f197 <s 2048@32,
(-2048)@32 <=s f198, f198 <s 2048@32, (-2048)@32 <=s f199, f199 <s 2048@32,
(-2048)@32 <=s f200, f200 <s 2048@32, (-2048)@32 <=s f201, f201 <s 2048@32,
(-2048)@32 <=s f202, f202 <s 2048@32, (-2048)@32 <=s f203, f203 <s 2048@32,
(-2048)@32 <=s f204, f204 <s 2048@32, (-2048)@32 <=s f205, f205 <s 2048@32,
(-2048)@32 <=s f206, f206 <s 2048@32, (-2048)@32 <=s f207, f207 <s 2048@32,
(-2048)@32 <=s f208, f208 <s 2048@32, (-2048)@32 <=s f209, f209 <s 2048@32,
(-2048)@32 <=s f210, f210 <s 2048@32, (-2048)@32 <=s f211, f211 <s 2048@32,
(-2048)@32 <=s f212, f212 <s 2048@32, (-2048)@32 <=s f213, f213 <s 2048@32,
(-2048)@32 <=s f214, f214 <s 2048@32, (-2048)@32 <=s f215, f215 <s 2048@32,
(-2048)@32 <=s f216, f216 <s 2048@32, (-2048)@32 <=s f217, f217 <s 2048@32,
(-2048)@32 <=s f218, f218 <s 2048@32, (-2048)@32 <=s f219, f219 <s 2048@32,
(-2048)@32 <=s f220, f220 <s 2048@32, (-2048)@32 <=s f221, f221 <s 2048@32,
(-2048)@32 <=s f222, f222 <s 2048@32, (-2048)@32 <=s f223, f223 <s 2048@32,
(-2048)@32 <=s f224, f224 <s 2048@32, (-2048)@32 <=s f225, f225 <s 2048@32,
(-2048)@32 <=s f226, f226 <s 2048@32, (-2048)@32 <=s f227, f227 <s 2048@32,
(-2048)@32 <=s f228, f228 <s 2048@32, (-2048)@32 <=s f229, f229 <s 2048@32,
(-2048)@32 <=s f230, f230 <s 2048@32, (-2048)@32 <=s f231, f231 <s 2048@32,
(-2048)@32 <=s f232, f232 <s 2048@32, (-2048)@32 <=s f233, f233 <s 2048@32,
(-2048)@32 <=s f234, f234 <s 2048@32, (-2048)@32 <=s f235, f235 <s 2048@32,
(-2048)@32 <=s f236, f236 <s 2048@32, (-2048)@32 <=s f237, f237 <s 2048@32,
(-2048)@32 <=s f238, f238 <s 2048@32, (-2048)@32 <=s f239, f239 <s 2048@32,
(-2048)@32 <=s f240, f240 <s 2048@32, (-2048)@32 <=s f241, f241 <s 2048@32,
(-2048)@32 <=s f242, f242 <s 2048@32, (-2048)@32 <=s f243, f243 <s 2048@32,
(-2048)@32 <=s f244, f244 <s 2048@32, (-2048)@32 <=s f245, f245 <s 2048@32,
(-2048)@32 <=s f246, f246 <s 2048@32, (-2048)@32 <=s f247, f247 <s 2048@32,
(-2048)@32 <=s f248, f248 <s 2048@32, (-2048)@32 <=s f249, f249 <s 2048@32,
(-2048)@32 <=s f250, f250 <s 2048@32, (-2048)@32 <=s f251, f251 <s 2048@32,
(-2048)@32 <=s f252, f252 <s 2048@32, (-2048)@32 <=s f253, f253 <s 2048@32,
(-2048)@32 <=s f254, f254 <s 2048@32, (-2048)@32 <=s f255, f255 <s 2048@32,
(-2048)@32 <=s f256, f256 <s 2048@32, (-2048)@32 <=s f257, f257 <s 2048@32,
(-2048)@32 <=s f258, f258 <s 2048@32, (-2048)@32 <=s f259, f259 <s 2048@32,
(-2048)@32 <=s f260, f260 <s 2048@32, (-2048)@32 <=s f261, f261 <s 2048@32,
(-2048)@32 <=s f262, f262 <s 2048@32, (-2048)@32 <=s f263, f263 <s 2048@32,
(-2048)@32 <=s f264, f264 <s 2048@32, (-2048)@32 <=s f265, f265 <s 2048@32,
(-2048)@32 <=s f266, f266 <s 2048@32, (-2048)@32 <=s f267, f267 <s 2048@32,
(-2048)@32 <=s f268, f268 <s 2048@32, (-2048)@32 <=s f269, f269 <s 2048@32,
(-2048)@32 <=s f270, f270 <s 2048@32, (-2048)@32 <=s f271, f271 <s 2048@32,
(-2048)@32 <=s f272, f272 <s 2048@32, (-2048)@32 <=s f273, f273 <s 2048@32,
(-2048)@32 <=s f274, f274 <s 2048@32, (-2048)@32 <=s f275, f275 <s 2048@32,
(-2048)@32 <=s f276, f276 <s 2048@32, (-2048)@32 <=s f277, f277 <s 2048@32,
(-2048)@32 <=s f278, f278 <s 2048@32, (-2048)@32 <=s f279, f279 <s 2048@32,
(-2048)@32 <=s f280, f280 <s 2048@32, (-2048)@32 <=s f281, f281 <s 2048@32,
(-2048)@32 <=s f282, f282 <s 2048@32, (-2048)@32 <=s f283, f283 <s 2048@32,
(-2048)@32 <=s f284, f284 <s 2048@32, (-2048)@32 <=s f285, f285 <s 2048@32,
(-2048)@32 <=s f286, f286 <s 2048@32, (-2048)@32 <=s f287, f287 <s 2048@32,
(-2048)@32 <=s f288, f288 <s 2048@32, (-2048)@32 <=s f289, f289 <s 2048@32,
(-2048)@32 <=s f290, f290 <s 2048@32, (-2048)@32 <=s f291, f291 <s 2048@32,
(-2048)@32 <=s f292, f292 <s 2048@32, (-2048)@32 <=s f293, f293 <s 2048@32,
(-2048)@32 <=s f294, f294 <s 2048@32, (-2048)@32 <=s f295, f295 <s 2048@32,
(-2048)@32 <=s f296, f296 <s 2048@32, (-2048)@32 <=s f297, f297 <s 2048@32,
(-2048)@32 <=s f298, f298 <s 2048@32, (-2048)@32 <=s f299, f299 <s 2048@32,
(-2048)@32 <=s f300, f300 <s 2048@32, (-2048)@32 <=s f301, f301 <s 2048@32,
(-2048)@32 <=s f302, f302 <s 2048@32, (-2048)@32 <=s f303, f303 <s 2048@32,
(-2048)@32 <=s f304, f304 <s 2048@32, (-2048)@32 <=s f305, f305 <s 2048@32,
(-2048)@32 <=s f306, f306 <s 2048@32, (-2048)@32 <=s f307, f307 <s 2048@32,
(-2048)@32 <=s f308, f308 <s 2048@32, (-2048)@32 <=s f309, f309 <s 2048@32,
(-2048)@32 <=s f310, f310 <s 2048@32, (-2048)@32 <=s f311, f311 <s 2048@32,
(-2048)@32 <=s f312, f312 <s 2048@32, (-2048)@32 <=s f313, f313 <s 2048@32,
(-2048)@32 <=s f314, f314 <s 2048@32, (-2048)@32 <=s f315, f315 <s 2048@32,
(-2048)@32 <=s f316, f316 <s 2048@32, (-2048)@32 <=s f317, f317 <s 2048@32,
(-2048)@32 <=s f318, f318 <s 2048@32, (-2048)@32 <=s f319, f319 <s 2048@32,
(-2048)@32 <=s f320, f320 <s 2048@32, (-2048)@32 <=s f321, f321 <s 2048@32,
(-2048)@32 <=s f322, f322 <s 2048@32, (-2048)@32 <=s f323, f323 <s 2048@32,
(-2048)@32 <=s f324, f324 <s 2048@32, (-2048)@32 <=s f325, f325 <s 2048@32,
(-2048)@32 <=s f326, f326 <s 2048@32, (-2048)@32 <=s f327, f327 <s 2048@32,
(-2048)@32 <=s f328, f328 <s 2048@32, (-2048)@32 <=s f329, f329 <s 2048@32,
(-2048)@32 <=s f330, f330 <s 2048@32, (-2048)@32 <=s f331, f331 <s 2048@32,
(-2048)@32 <=s f332, f332 <s 2048@32, (-2048)@32 <=s f333, f333 <s 2048@32,
(-2048)@32 <=s f334, f334 <s 2048@32, (-2048)@32 <=s f335, f335 <s 2048@32,
(-2048)@32 <=s f336, f336 <s 2048@32, (-2048)@32 <=s f337, f337 <s 2048@32,
(-2048)@32 <=s f338, f338 <s 2048@32, (-2048)@32 <=s f339, f339 <s 2048@32,
(-2048)@32 <=s f340, f340 <s 2048@32, (-2048)@32 <=s f341, f341 <s 2048@32,
(-2048)@32 <=s f342, f342 <s 2048@32, (-2048)@32 <=s f343, f343 <s 2048@32,
(-2048)@32 <=s f344, f344 <s 2048@32, (-2048)@32 <=s f345, f345 <s 2048@32,
(-2048)@32 <=s f346, f346 <s 2048@32, (-2048)@32 <=s f347, f347 <s 2048@32,
(-2048)@32 <=s f348, f348 <s 2048@32, (-2048)@32 <=s f349, f349 <s 2048@32,
(-2048)@32 <=s f350, f350 <s 2048@32, (-2048)@32 <=s f351, f351 <s 2048@32,
(-2048)@32 <=s f352, f352 <s 2048@32, (-2048)@32 <=s f353, f353 <s 2048@32,
(-2048)@32 <=s f354, f354 <s 2048@32, (-2048)@32 <=s f355, f355 <s 2048@32,
(-2048)@32 <=s f356, f356 <s 2048@32, (-2048)@32 <=s f357, f357 <s 2048@32,
(-2048)@32 <=s f358, f358 <s 2048@32, (-2048)@32 <=s f359, f359 <s 2048@32,
(-2048)@32 <=s f360, f360 <s 2048@32, (-2048)@32 <=s f361, f361 <s 2048@32,
(-2048)@32 <=s f362, f362 <s 2048@32, (-2048)@32 <=s f363, f363 <s 2048@32,
(-2048)@32 <=s f364, f364 <s 2048@32, (-2048)@32 <=s f365, f365 <s 2048@32,
(-2048)@32 <=s f366, f366 <s 2048@32, (-2048)@32 <=s f367, f367 <s 2048@32,
(-2048)@32 <=s f368, f368 <s 2048@32, (-2048)@32 <=s f369, f369 <s 2048@32,
(-2048)@32 <=s f370, f370 <s 2048@32, (-2048)@32 <=s f371, f371 <s 2048@32,
(-2048)@32 <=s f372, f372 <s 2048@32, (-2048)@32 <=s f373, f373 <s 2048@32,
(-2048)@32 <=s f374, f374 <s 2048@32, (-2048)@32 <=s f375, f375 <s 2048@32,
(-2048)@32 <=s f376, f376 <s 2048@32, (-2048)@32 <=s f377, f377 <s 2048@32,
(-2048)@32 <=s f378, f378 <s 2048@32, (-2048)@32 <=s f379, f379 <s 2048@32,
(-2048)@32 <=s f380, f380 <s 2048@32, (-2048)@32 <=s f381, f381 <s 2048@32,
(-2048)@32 <=s f382, f382 <s 2048@32, (-2048)@32 <=s f383, f383 <s 2048@32,
(-2048)@32 <=s f384, f384 <s 2048@32, (-2048)@32 <=s f385, f385 <s 2048@32,
(-2048)@32 <=s f386, f386 <s 2048@32, (-2048)@32 <=s f387, f387 <s 2048@32,
(-2048)@32 <=s f388, f388 <s 2048@32, (-2048)@32 <=s f389, f389 <s 2048@32,
(-2048)@32 <=s f390, f390 <s 2048@32, (-2048)@32 <=s f391, f391 <s 2048@32,
(-2048)@32 <=s f392, f392 <s 2048@32, (-2048)@32 <=s f393, f393 <s 2048@32,
(-2048)@32 <=s f394, f394 <s 2048@32, (-2048)@32 <=s f395, f395 <s 2048@32,
(-2048)@32 <=s f396, f396 <s 2048@32, (-2048)@32 <=s f397, f397 <s 2048@32,
(-2048)@32 <=s f398, f398 <s 2048@32, (-2048)@32 <=s f399, f399 <s 2048@32,
(-2048)@32 <=s f400, f400 <s 2048@32, (-2048)@32 <=s f401, f401 <s 2048@32,
(-2048)@32 <=s f402, f402 <s 2048@32, (-2048)@32 <=s f403, f403 <s 2048@32,
(-2048)@32 <=s f404, f404 <s 2048@32, (-2048)@32 <=s f405, f405 <s 2048@32,
(-2048)@32 <=s f406, f406 <s 2048@32, (-2048)@32 <=s f407, f407 <s 2048@32,
(-2048)@32 <=s f408, f408 <s 2048@32, (-2048)@32 <=s f409, f409 <s 2048@32,
(-2048)@32 <=s f410, f410 <s 2048@32, (-2048)@32 <=s f411, f411 <s 2048@32,
(-2048)@32 <=s f412, f412 <s 2048@32, (-2048)@32 <=s f413, f413 <s 2048@32,
(-2048)@32 <=s f414, f414 <s 2048@32, (-2048)@32 <=s f415, f415 <s 2048@32,
(-2048)@32 <=s f416, f416 <s 2048@32, (-2048)@32 <=s f417, f417 <s 2048@32,
(-2048)@32 <=s f418, f418 <s 2048@32, (-2048)@32 <=s f419, f419 <s 2048@32,
(-2048)@32 <=s f420, f420 <s 2048@32, (-2048)@32 <=s f421, f421 <s 2048@32,
(-2048)@32 <=s f422, f422 <s 2048@32, (-2048)@32 <=s f423, f423 <s 2048@32,
(-2048)@32 <=s f424, f424 <s 2048@32, (-2048)@32 <=s f425, f425 <s 2048@32,
(-2048)@32 <=s f426, f426 <s 2048@32, (-2048)@32 <=s f427, f427 <s 2048@32,
(-2048)@32 <=s f428, f428 <s 2048@32, (-2048)@32 <=s f429, f429 <s 2048@32,
(-2048)@32 <=s f430, f430 <s 2048@32, (-2048)@32 <=s f431, f431 <s 2048@32,
(-2048)@32 <=s f432, f432 <s 2048@32, (-2048)@32 <=s f433, f433 <s 2048@32,
(-2048)@32 <=s f434, f434 <s 2048@32, (-2048)@32 <=s f435, f435 <s 2048@32,
(-2048)@32 <=s f436, f436 <s 2048@32, (-2048)@32 <=s f437, f437 <s 2048@32,
(-2048)@32 <=s f438, f438 <s 2048@32, (-2048)@32 <=s f439, f439 <s 2048@32,
(-2048)@32 <=s f440, f440 <s 2048@32, (-2048)@32 <=s f441, f441 <s 2048@32,
(-2048)@32 <=s f442, f442 <s 2048@32, (-2048)@32 <=s f443, f443 <s 2048@32,
(-2048)@32 <=s f444, f444 <s 2048@32, (-2048)@32 <=s f445, f445 <s 2048@32,
(-2048)@32 <=s f446, f446 <s 2048@32, (-2048)@32 <=s f447, f447 <s 2048@32,
(-2048)@32 <=s f448, f448 <s 2048@32, (-2048)@32 <=s f449, f449 <s 2048@32,
(-2048)@32 <=s f450, f450 <s 2048@32, (-2048)@32 <=s f451, f451 <s 2048@32,
(-2048)@32 <=s f452, f452 <s 2048@32, (-2048)@32 <=s f453, f453 <s 2048@32,
(-2048)@32 <=s f454, f454 <s 2048@32, (-2048)@32 <=s f455, f455 <s 2048@32,
(-2048)@32 <=s f456, f456 <s 2048@32, (-2048)@32 <=s f457, f457 <s 2048@32,
(-2048)@32 <=s f458, f458 <s 2048@32, (-2048)@32 <=s f459, f459 <s 2048@32,
(-2048)@32 <=s f460, f460 <s 2048@32, (-2048)@32 <=s f461, f461 <s 2048@32,
(-2048)@32 <=s f462, f462 <s 2048@32, (-2048)@32 <=s f463, f463 <s 2048@32,
(-2048)@32 <=s f464, f464 <s 2048@32, (-2048)@32 <=s f465, f465 <s 2048@32,
(-2048)@32 <=s f466, f466 <s 2048@32, (-2048)@32 <=s f467, f467 <s 2048@32,
(-2048)@32 <=s f468, f468 <s 2048@32, (-2048)@32 <=s f469, f469 <s 2048@32,
(-2048)@32 <=s f470, f470 <s 2048@32, (-2048)@32 <=s f471, f471 <s 2048@32,
(-2048)@32 <=s f472, f472 <s 2048@32, (-2048)@32 <=s f473, f473 <s 2048@32,
(-2048)@32 <=s f474, f474 <s 2048@32, (-2048)@32 <=s f475, f475 <s 2048@32,
(-2048)@32 <=s f476, f476 <s 2048@32, (-2048)@32 <=s f477, f477 <s 2048@32,
(-2048)@32 <=s f478, f478 <s 2048@32, (-2048)@32 <=s f479, f479 <s 2048@32,
(-2048)@32 <=s f480, f480 <s 2048@32, (-2048)@32 <=s f481, f481 <s 2048@32,
(-2048)@32 <=s f482, f482 <s 2048@32, (-2048)@32 <=s f483, f483 <s 2048@32,
(-2048)@32 <=s f484, f484 <s 2048@32, (-2048)@32 <=s f485, f485 <s 2048@32,
(-2048)@32 <=s f486, f486 <s 2048@32, (-2048)@32 <=s f487, f487 <s 2048@32,
(-2048)@32 <=s f488, f488 <s 2048@32, (-2048)@32 <=s f489, f489 <s 2048@32,
(-2048)@32 <=s f490, f490 <s 2048@32, (-2048)@32 <=s f491, f491 <s 2048@32,
(-2048)@32 <=s f492, f492 <s 2048@32, (-2048)@32 <=s f493, f493 <s 2048@32,
(-2048)@32 <=s f494, f494 <s 2048@32, (-2048)@32 <=s f495, f495 <s 2048@32,
(-2048)@32 <=s f496, f496 <s 2048@32, (-2048)@32 <=s f497, f497 <s 2048@32,
(-2048)@32 <=s f498, f498 <s 2048@32, (-2048)@32 <=s f499, f499 <s 2048@32,
(-2048)@32 <=s f500, f500 <s 2048@32, (-2048)@32 <=s f501, f501 <s 2048@32,
(-2048)@32 <=s f502, f502 <s 2048@32, (-2048)@32 <=s f503, f503 <s 2048@32,
(-2048)@32 <=s f504, f504 <s 2048@32, (-2048)@32 <=s f505, f505 <s 2048@32,
(-2048)@32 <=s f506, f506 <s 2048@32, (-2048)@32 <=s f507, f507 <s 2048@32,
(-2048)@32 <=s f508, f508 <s 2048@32
]
}

(**************** initialization ****************)

mov r2 1993076223@uint32; mov r3 1043969@sint32;
mov L0x20016fd0 f000; mov L0x20016fd2 f001; mov L0x20016fd4 f002;
mov L0x20016fd6 f003; mov L0x20016fd8 f004; mov L0x20016fda f005;
mov L0x20016fdc f006; mov L0x20016fde f007; mov L0x20016fe0 f008;
mov L0x20016fe2 f009; mov L0x20016fe4 f010; mov L0x20016fe6 f011;
mov L0x20016fe8 f012; mov L0x20016fea f013; mov L0x20016fec f014;
mov L0x20016fee f015; mov L0x20016ff0 f016; mov L0x20016ff2 f017;
mov L0x20016ff4 f018; mov L0x20016ff6 f019; mov L0x20016ff8 f020;
mov L0x20016ffa f021; mov L0x20016ffc f022; mov L0x20016ffe f023;
mov L0x20017000 f024; mov L0x20017002 f025; mov L0x20017004 f026;
mov L0x20017006 f027; mov L0x20017008 f028; mov L0x2001700a f029;
mov L0x2001700c f030; mov L0x2001700e f031; mov L0x20017010 f032;
mov L0x20017012 f033; mov L0x20017014 f034; mov L0x20017016 f035;
mov L0x20017018 f036; mov L0x2001701a f037; mov L0x2001701c f038;
mov L0x2001701e f039; mov L0x20017020 f040; mov L0x20017022 f041;
mov L0x20017024 f042; mov L0x20017026 f043; mov L0x20017028 f044;
mov L0x2001702a f045; mov L0x2001702c f046; mov L0x2001702e f047;
mov L0x20017030 f048; mov L0x20017032 f049; mov L0x20017034 f050;
mov L0x20017036 f051; mov L0x20017038 f052; mov L0x2001703a f053;
mov L0x2001703c f054; mov L0x2001703e f055; mov L0x20017040 f056;
mov L0x20017042 f057; mov L0x20017044 f058; mov L0x20017046 f059;
mov L0x20017048 f060; mov L0x2001704a f061; mov L0x2001704c f062;
mov L0x2001704e f063; mov L0x20017050 f064; mov L0x20017052 f065;
mov L0x20017054 f066; mov L0x20017056 f067; mov L0x20017058 f068;
mov L0x2001705a f069; mov L0x2001705c f070; mov L0x2001705e f071;
mov L0x20017060 f072; mov L0x20017062 f073; mov L0x20017064 f074;
mov L0x20017066 f075; mov L0x20017068 f076; mov L0x2001706a f077;
mov L0x2001706c f078; mov L0x2001706e f079; mov L0x20017070 f080;
mov L0x20017072 f081; mov L0x20017074 f082; mov L0x20017076 f083;
mov L0x20017078 f084; mov L0x2001707a f085; mov L0x2001707c f086;
mov L0x2001707e f087; mov L0x20017080 f088; mov L0x20017082 f089;
mov L0x20017084 f090; mov L0x20017086 f091; mov L0x20017088 f092;
mov L0x2001708a f093; mov L0x2001708c f094; mov L0x2001708e f095;
mov L0x20017090 f096; mov L0x20017092 f097; mov L0x20017094 f098;
mov L0x20017096 f099; mov L0x20017098 f100; mov L0x2001709a f101;
mov L0x2001709c f102; mov L0x2001709e f103; mov L0x200170a0 f104;
mov L0x200170a2 f105; mov L0x200170a4 f106; mov L0x200170a6 f107;
mov L0x200170a8 f108; mov L0x200170aa f109; mov L0x200170ac f110;
mov L0x200170ae f111; mov L0x200170b0 f112; mov L0x200170b2 f113;
mov L0x200170b4 f114; mov L0x200170b6 f115; mov L0x200170b8 f116;
mov L0x200170ba f117; mov L0x200170bc f118; mov L0x200170be f119;
mov L0x200170c0 f120; mov L0x200170c2 f121; mov L0x200170c4 f122;
mov L0x200170c6 f123; mov L0x200170c8 f124; mov L0x200170ca f125;
mov L0x200170cc f126; mov L0x200170ce f127; mov L0x200170d0 f128;
mov L0x200170d2 f129; mov L0x200170d4 f130; mov L0x200170d6 f131;
mov L0x200170d8 f132; mov L0x200170da f133; mov L0x200170dc f134;
mov L0x200170de f135; mov L0x200170e0 f136; mov L0x200170e2 f137;
mov L0x200170e4 f138; mov L0x200170e6 f139; mov L0x200170e8 f140;
mov L0x200170ea f141; mov L0x200170ec f142; mov L0x200170ee f143;
mov L0x200170f0 f144; mov L0x200170f2 f145; mov L0x200170f4 f146;
mov L0x200170f6 f147; mov L0x200170f8 f148; mov L0x200170fa f149;
mov L0x200170fc f150; mov L0x200170fe f151; mov L0x20017100 f152;
mov L0x20017102 f153; mov L0x20017104 f154; mov L0x20017106 f155;
mov L0x20017108 f156; mov L0x2001710a f157; mov L0x2001710c f158;
mov L0x2001710e f159; mov L0x20017110 f160; mov L0x20017112 f161;
mov L0x20017114 f162; mov L0x20017116 f163; mov L0x20017118 f164;
mov L0x2001711a f165; mov L0x2001711c f166; mov L0x2001711e f167;
mov L0x20017120 f168; mov L0x20017122 f169; mov L0x20017124 f170;
mov L0x20017126 f171; mov L0x20017128 f172; mov L0x2001712a f173;
mov L0x2001712c f174; mov L0x2001712e f175; mov L0x20017130 f176;
mov L0x20017132 f177; mov L0x20017134 f178; mov L0x20017136 f179;
mov L0x20017138 f180; mov L0x2001713a f181; mov L0x2001713c f182;
mov L0x2001713e f183; mov L0x20017140 f184; mov L0x20017142 f185;
mov L0x20017144 f186; mov L0x20017146 f187; mov L0x20017148 f188;
mov L0x2001714a f189; mov L0x2001714c f190; mov L0x2001714e f191;
mov L0x20017150 f192; mov L0x20017152 f193; mov L0x20017154 f194;
mov L0x20017156 f195; mov L0x20017158 f196; mov L0x2001715a f197;
mov L0x2001715c f198; mov L0x2001715e f199; mov L0x20017160 f200;
mov L0x20017162 f201; mov L0x20017164 f202; mov L0x20017166 f203;
mov L0x20017168 f204; mov L0x2001716a f205; mov L0x2001716c f206;
mov L0x2001716e f207; mov L0x20017170 f208; mov L0x20017172 f209;
mov L0x20017174 f210; mov L0x20017176 f211; mov L0x20017178 f212;
mov L0x2001717a f213; mov L0x2001717c f214; mov L0x2001717e f215;
mov L0x20017180 f216; mov L0x20017182 f217; mov L0x20017184 f218;
mov L0x20017186 f219; mov L0x20017188 f220; mov L0x2001718a f221;
mov L0x2001718c f222; mov L0x2001718e f223; mov L0x20017190 f224;
mov L0x20017192 f225; mov L0x20017194 f226; mov L0x20017196 f227;
mov L0x20017198 f228; mov L0x2001719a f229; mov L0x2001719c f230;
mov L0x2001719e f231; mov L0x200171a0 f232; mov L0x200171a2 f233;
mov L0x200171a4 f234; mov L0x200171a6 f235; mov L0x200171a8 f236;
mov L0x200171aa f237; mov L0x200171ac f238; mov L0x200171ae f239;
mov L0x200171b0 f240; mov L0x200171b2 f241; mov L0x200171b4 f242;
mov L0x200171b6 f243; mov L0x200171b8 f244; mov L0x200171ba f245;
mov L0x200171bc f246; mov L0x200171be f247; mov L0x200171c0 f248;
mov L0x200171c2 f249; mov L0x200171c4 f250; mov L0x200171c6 f251;
mov L0x200171c8 f252; mov L0x200171ca f253; mov L0x200171cc f254;
mov L0x200171ce f255; mov L0x200171d0 f256; mov L0x200171d2 f257;
mov L0x200171d4 f258; mov L0x200171d6 f259; mov L0x200171d8 f260;
mov L0x200171da f261; mov L0x200171dc f262; mov L0x200171de f263;
mov L0x200171e0 f264; mov L0x200171e2 f265; mov L0x200171e4 f266;
mov L0x200171e6 f267; mov L0x200171e8 f268; mov L0x200171ea f269;
mov L0x200171ec f270; mov L0x200171ee f271; mov L0x200171f0 f272;
mov L0x200171f2 f273; mov L0x200171f4 f274; mov L0x200171f6 f275;
mov L0x200171f8 f276; mov L0x200171fa f277; mov L0x200171fc f278;
mov L0x200171fe f279; mov L0x20017200 f280; mov L0x20017202 f281;
mov L0x20017204 f282; mov L0x20017206 f283; mov L0x20017208 f284;
mov L0x2001720a f285; mov L0x2001720c f286; mov L0x2001720e f287;
mov L0x20017210 f288; mov L0x20017212 f289; mov L0x20017214 f290;
mov L0x20017216 f291; mov L0x20017218 f292; mov L0x2001721a f293;
mov L0x2001721c f294; mov L0x2001721e f295; mov L0x20017220 f296;
mov L0x20017222 f297; mov L0x20017224 f298; mov L0x20017226 f299;
mov L0x20017228 f300; mov L0x2001722a f301; mov L0x2001722c f302;
mov L0x2001722e f303; mov L0x20017230 f304; mov L0x20017232 f305;
mov L0x20017234 f306; mov L0x20017236 f307; mov L0x20017238 f308;
mov L0x2001723a f309; mov L0x2001723c f310; mov L0x2001723e f311;
mov L0x20017240 f312; mov L0x20017242 f313; mov L0x20017244 f314;
mov L0x20017246 f315; mov L0x20017248 f316; mov L0x2001724a f317;
mov L0x2001724c f318; mov L0x2001724e f319; mov L0x20017250 f320;
mov L0x20017252 f321; mov L0x20017254 f322; mov L0x20017256 f323;
mov L0x20017258 f324; mov L0x2001725a f325; mov L0x2001725c f326;
mov L0x2001725e f327; mov L0x20017260 f328; mov L0x20017262 f329;
mov L0x20017264 f330; mov L0x20017266 f331; mov L0x20017268 f332;
mov L0x2001726a f333; mov L0x2001726c f334; mov L0x2001726e f335;
mov L0x20017270 f336; mov L0x20017272 f337; mov L0x20017274 f338;
mov L0x20017276 f339; mov L0x20017278 f340; mov L0x2001727a f341;
mov L0x2001727c f342; mov L0x2001727e f343; mov L0x20017280 f344;
mov L0x20017282 f345; mov L0x20017284 f346; mov L0x20017286 f347;
mov L0x20017288 f348; mov L0x2001728a f349; mov L0x2001728c f350;
mov L0x2001728e f351; mov L0x20017290 f352; mov L0x20017292 f353;
mov L0x20017294 f354; mov L0x20017296 f355; mov L0x20017298 f356;
mov L0x2001729a f357; mov L0x2001729c f358; mov L0x2001729e f359;
mov L0x200172a0 f360; mov L0x200172a2 f361; mov L0x200172a4 f362;
mov L0x200172a6 f363; mov L0x200172a8 f364; mov L0x200172aa f365;
mov L0x200172ac f366; mov L0x200172ae f367; mov L0x200172b0 f368;
mov L0x200172b2 f369; mov L0x200172b4 f370; mov L0x200172b6 f371;
mov L0x200172b8 f372; mov L0x200172ba f373; mov L0x200172bc f374;
mov L0x200172be f375; mov L0x200172c0 f376; mov L0x200172c2 f377;
mov L0x200172c4 f378; mov L0x200172c6 f379; mov L0x200172c8 f380;
mov L0x200172ca f381; mov L0x200172cc f382; mov L0x200172ce f383;
mov L0x200172d0 f384; mov L0x200172d2 f385; mov L0x200172d4 f386;
mov L0x200172d6 f387; mov L0x200172d8 f388; mov L0x200172da f389;
mov L0x200172dc f390; mov L0x200172de f391; mov L0x200172e0 f392;
mov L0x200172e2 f393; mov L0x200172e4 f394; mov L0x200172e6 f395;
mov L0x200172e8 f396; mov L0x200172ea f397; mov L0x200172ec f398;
mov L0x200172ee f399; mov L0x200172f0 f400; mov L0x200172f2 f401;
mov L0x200172f4 f402; mov L0x200172f6 f403; mov L0x200172f8 f404;
mov L0x200172fa f405; mov L0x200172fc f406; mov L0x200172fe f407;
mov L0x20017300 f408; mov L0x20017302 f409; mov L0x20017304 f410;
mov L0x20017306 f411; mov L0x20017308 f412; mov L0x2001730a f413;
mov L0x2001730c f414; mov L0x2001730e f415; mov L0x20017310 f416;
mov L0x20017312 f417; mov L0x20017314 f418; mov L0x20017316 f419;
mov L0x20017318 f420; mov L0x2001731a f421; mov L0x2001731c f422;
mov L0x2001731e f423; mov L0x20017320 f424; mov L0x20017322 f425;
mov L0x20017324 f426; mov L0x20017326 f427; mov L0x20017328 f428;
mov L0x2001732a f429; mov L0x2001732c f430; mov L0x2001732e f431;
mov L0x20017330 f432; mov L0x20017332 f433; mov L0x20017334 f434;
mov L0x20017336 f435; mov L0x20017338 f436; mov L0x2001733a f437;
mov L0x2001733c f438; mov L0x2001733e f439; mov L0x20017340 f440;
mov L0x20017342 f441; mov L0x20017344 f442; mov L0x20017346 f443;
mov L0x20017348 f444; mov L0x2001734a f445; mov L0x2001734c f446;
mov L0x2001734e f447; mov L0x20017350 f448; mov L0x20017352 f449;
mov L0x20017354 f450; mov L0x20017356 f451; mov L0x20017358 f452;
mov L0x2001735a f453; mov L0x2001735c f454; mov L0x2001735e f455;
mov L0x20017360 f456; mov L0x20017362 f457; mov L0x20017364 f458;
mov L0x20017366 f459; mov L0x20017368 f460; mov L0x2001736a f461;
mov L0x2001736c f462; mov L0x2001736e f463; mov L0x20017370 f464;
mov L0x20017372 f465; mov L0x20017374 f466; mov L0x20017376 f467;
mov L0x20017378 f468; mov L0x2001737a f469; mov L0x2001737c f470;
mov L0x2001737e f471; mov L0x20017380 f472; mov L0x20017382 f473;
mov L0x20017384 f474; mov L0x20017386 f475; mov L0x20017388 f476;
mov L0x2001738a f477; mov L0x2001738c f478; mov L0x2001738e f479;
mov L0x20017390 f480; mov L0x20017392 f481; mov L0x20017394 f482;
mov L0x20017396 f483; mov L0x20017398 f484; mov L0x2001739a f485;
mov L0x2001739c f486; mov L0x2001739e f487; mov L0x200173a0 f488;
mov L0x200173a2 f489; mov L0x200173a4 f490; mov L0x200173a6 f491;
mov L0x200173a8 f492; mov L0x200173aa f493; mov L0x200173ac f494;
mov L0x200173ae f495; mov L0x200173b0 f496; mov L0x200173b2 f497;
mov L0x200173b4 f498; mov L0x200173b6 f499; mov L0x200173b8 f500;
mov L0x200173ba f501; mov L0x200173bc f502; mov L0x200173be f503;
mov L0x200173c0 f504; mov L0x200173c2 f505; mov L0x200173c4 f506;
mov L0x200173c6 f507; mov L0x200173c8 f508;



(**************** pointers ****************)

nondet lr@uint32; nondet r0@uint32;



(**************** input poly ****************)

ghost x@bit, inp_poly@bit,
inp_poly_0@bit, inp_poly_1@bit, inp_poly_2@bit, inp_poly_3@bit,
inp_poly_4@bit, inp_poly_5@bit, inp_poly_6@bit, inp_poly_7@bit :
and [
inp_poly_0**2 = 
f000*(x**0)+f001*(x**1)+f002*(x**2)+f003*(x**3)+f004*(x**4)+
f005*(x**5)+f006*(x**6)+f007*(x**7)+f008*(x**8)+f009*(x**9)+
f010*(x**10)+f011*(x**11)+f012*(x**12)+f013*(x**13)+f014*(x**14)+
f015*(x**15)+f016*(x**16)+f017*(x**17)+f018*(x**18)+f019*(x**19)+
f020*(x**20)+f021*(x**21)+f022*(x**22)+f023*(x**23)+f024*(x**24)+
f025*(x**25)+f026*(x**26)+f027*(x**27)+f028*(x**28)+f029*(x**29)+
f030*(x**30)+f031*(x**31)+f032*(x**32)+f033*(x**33)+f034*(x**34)+
f035*(x**35)+f036*(x**36)+f037*(x**37)+f038*(x**38)+f039*(x**39)+
f040*(x**40)+f041*(x**41)+f042*(x**42)+f043*(x**43)+f044*(x**44)+
f045*(x**45)+f046*(x**46)+f047*(x**47)+f048*(x**48)+f049*(x**49)+
f050*(x**50)+f051*(x**51)+f052*(x**52)+f053*(x**53)+f054*(x**54)+
f055*(x**55)+f056*(x**56)+f057*(x**57)+f058*(x**58)+f059*(x**59)+
f060*(x**60)+f061*(x**61)+f062*(x**62)+f063*(x**63),
inp_poly_1**2 = 
f064*(x**0)+f065*(x**1)+f066*(x**2)+f067*(x**3)+f068*(x**4)+
f069*(x**5)+f070*(x**6)+f071*(x**7)+f072*(x**8)+f073*(x**9)+
f074*(x**10)+f075*(x**11)+f076*(x**12)+f077*(x**13)+f078*(x**14)+
f079*(x**15)+f080*(x**16)+f081*(x**17)+f082*(x**18)+f083*(x**19)+
f084*(x**20)+f085*(x**21)+f086*(x**22)+f087*(x**23)+f088*(x**24)+
f089*(x**25)+f090*(x**26)+f091*(x**27)+f092*(x**28)+f093*(x**29)+
f094*(x**30)+f095*(x**31)+f096*(x**32)+f097*(x**33)+f098*(x**34)+
f099*(x**35)+f100*(x**36)+f101*(x**37)+f102*(x**38)+f103*(x**39)+
f104*(x**40)+f105*(x**41)+f106*(x**42)+f107*(x**43)+f108*(x**44)+
f109*(x**45)+f110*(x**46)+f111*(x**47)+f112*(x**48)+f113*(x**49)+
f114*(x**50)+f115*(x**51)+f116*(x**52)+f117*(x**53)+f118*(x**54)+
f119*(x**55)+f120*(x**56)+f121*(x**57)+f122*(x**58)+f123*(x**59)+
f124*(x**60)+f125*(x**61)+f126*(x**62)+f127*(x**63),
inp_poly_2**2 = 
f128*(x**0)+f129*(x**1)+f130*(x**2)+f131*(x**3)+f132*(x**4)+
f133*(x**5)+f134*(x**6)+f135*(x**7)+f136*(x**8)+f137*(x**9)+
f138*(x**10)+f139*(x**11)+f140*(x**12)+f141*(x**13)+f142*(x**14)+
f143*(x**15)+f144*(x**16)+f145*(x**17)+f146*(x**18)+f147*(x**19)+
f148*(x**20)+f149*(x**21)+f150*(x**22)+f151*(x**23)+f152*(x**24)+
f153*(x**25)+f154*(x**26)+f155*(x**27)+f156*(x**28)+f157*(x**29)+
f158*(x**30)+f159*(x**31)+f160*(x**32)+f161*(x**33)+f162*(x**34)+
f163*(x**35)+f164*(x**36)+f165*(x**37)+f166*(x**38)+f167*(x**39)+
f168*(x**40)+f169*(x**41)+f170*(x**42)+f171*(x**43)+f172*(x**44)+
f173*(x**45)+f174*(x**46)+f175*(x**47)+f176*(x**48)+f177*(x**49)+
f178*(x**50)+f179*(x**51)+f180*(x**52)+f181*(x**53)+f182*(x**54)+
f183*(x**55)+f184*(x**56)+f185*(x**57)+f186*(x**58)+f187*(x**59)+
f188*(x**60)+f189*(x**61)+f190*(x**62)+f191*(x**63),
inp_poly_3**2 = 
f192*(x**0)+f193*(x**1)+f194*(x**2)+f195*(x**3)+f196*(x**4)+
f197*(x**5)+f198*(x**6)+f199*(x**7)+f200*(x**8)+f201*(x**9)+
f202*(x**10)+f203*(x**11)+f204*(x**12)+f205*(x**13)+f206*(x**14)+
f207*(x**15)+f208*(x**16)+f209*(x**17)+f210*(x**18)+f211*(x**19)+
f212*(x**20)+f213*(x**21)+f214*(x**22)+f215*(x**23)+f216*(x**24)+
f217*(x**25)+f218*(x**26)+f219*(x**27)+f220*(x**28)+f221*(x**29)+
f222*(x**30)+f223*(x**31)+f224*(x**32)+f225*(x**33)+f226*(x**34)+
f227*(x**35)+f228*(x**36)+f229*(x**37)+f230*(x**38)+f231*(x**39)+
f232*(x**40)+f233*(x**41)+f234*(x**42)+f235*(x**43)+f236*(x**44)+
f237*(x**45)+f238*(x**46)+f239*(x**47)+f240*(x**48)+f241*(x**49)+
f242*(x**50)+f243*(x**51)+f244*(x**52)+f245*(x**53)+f246*(x**54)+
f247*(x**55)+f248*(x**56)+f249*(x**57)+f250*(x**58)+f251*(x**59)+
f252*(x**60)+f253*(x**61)+f254*(x**62)+f255*(x**63),
inp_poly_4**2 = 
f256*(x**0)+f257*(x**1)+f258*(x**2)+f259*(x**3)+f260*(x**4)+
f261*(x**5)+f262*(x**6)+f263*(x**7)+f264*(x**8)+f265*(x**9)+
f266*(x**10)+f267*(x**11)+f268*(x**12)+f269*(x**13)+f270*(x**14)+
f271*(x**15)+f272*(x**16)+f273*(x**17)+f274*(x**18)+f275*(x**19)+
f276*(x**20)+f277*(x**21)+f278*(x**22)+f279*(x**23)+f280*(x**24)+
f281*(x**25)+f282*(x**26)+f283*(x**27)+f284*(x**28)+f285*(x**29)+
f286*(x**30)+f287*(x**31)+f288*(x**32)+f289*(x**33)+f290*(x**34)+
f291*(x**35)+f292*(x**36)+f293*(x**37)+f294*(x**38)+f295*(x**39)+
f296*(x**40)+f297*(x**41)+f298*(x**42)+f299*(x**43)+f300*(x**44)+
f301*(x**45)+f302*(x**46)+f303*(x**47)+f304*(x**48)+f305*(x**49)+
f306*(x**50)+f307*(x**51)+f308*(x**52)+f309*(x**53)+f310*(x**54)+
f311*(x**55)+f312*(x**56)+f313*(x**57)+f314*(x**58)+f315*(x**59)+
f316*(x**60)+f317*(x**61)+f318*(x**62)+f319*(x**63),
inp_poly_5**2 = 
f320*(x**0)+f321*(x**1)+f322*(x**2)+f323*(x**3)+f324*(x**4)+
f325*(x**5)+f326*(x**6)+f327*(x**7)+f328*(x**8)+f329*(x**9)+
f330*(x**10)+f331*(x**11)+f332*(x**12)+f333*(x**13)+f334*(x**14)+
f335*(x**15)+f336*(x**16)+f337*(x**17)+f338*(x**18)+f339*(x**19)+
f340*(x**20)+f341*(x**21)+f342*(x**22)+f343*(x**23)+f344*(x**24)+
f345*(x**25)+f346*(x**26)+f347*(x**27)+f348*(x**28)+f349*(x**29)+
f350*(x**30)+f351*(x**31)+f352*(x**32)+f353*(x**33)+f354*(x**34)+
f355*(x**35)+f356*(x**36)+f357*(x**37)+f358*(x**38)+f359*(x**39)+
f360*(x**40)+f361*(x**41)+f362*(x**42)+f363*(x**43)+f364*(x**44)+
f365*(x**45)+f366*(x**46)+f367*(x**47)+f368*(x**48)+f369*(x**49)+
f370*(x**50)+f371*(x**51)+f372*(x**52)+f373*(x**53)+f374*(x**54)+
f375*(x**55)+f376*(x**56)+f377*(x**57)+f378*(x**58)+f379*(x**59)+
f380*(x**60)+f381*(x**61)+f382*(x**62)+f383*(x**63),
inp_poly_6**2 = 
f384*(x**0)+f385*(x**1)+f386*(x**2)+f387*(x**3)+f388*(x**4)+
f389*(x**5)+f390*(x**6)+f391*(x**7)+f392*(x**8)+f393*(x**9)+
f394*(x**10)+f395*(x**11)+f396*(x**12)+f397*(x**13)+f398*(x**14)+
f399*(x**15)+f400*(x**16)+f401*(x**17)+f402*(x**18)+f403*(x**19)+
f404*(x**20)+f405*(x**21)+f406*(x**22)+f407*(x**23)+f408*(x**24)+
f409*(x**25)+f410*(x**26)+f411*(x**27)+f412*(x**28)+f413*(x**29)+
f414*(x**30)+f415*(x**31)+f416*(x**32)+f417*(x**33)+f418*(x**34)+
f419*(x**35)+f420*(x**36)+f421*(x**37)+f422*(x**38)+f423*(x**39)+
f424*(x**40)+f425*(x**41)+f426*(x**42)+f427*(x**43)+f428*(x**44)+
f429*(x**45)+f430*(x**46)+f431*(x**47)+f432*(x**48)+f433*(x**49)+
f434*(x**50)+f435*(x**51)+f436*(x**52)+f437*(x**53)+f438*(x**54)+
f439*(x**55)+f440*(x**56)+f441*(x**57)+f442*(x**58)+f443*(x**59)+
f444*(x**60)+f445*(x**61)+f446*(x**62)+f447*(x**63),
inp_poly_7**2 = 
f448*(x**0)+f449*(x**1)+f450*(x**2)+f451*(x**3)+f452*(x**4)+
f453*(x**5)+f454*(x**6)+f455*(x**7)+f456*(x**8)+f457*(x**9)+
f458*(x**10)+f459*(x**11)+f460*(x**12)+f461*(x**13)+f462*(x**14)+
f463*(x**15)+f464*(x**16)+f465*(x**17)+f466*(x**18)+f467*(x**19)+
f468*(x**20)+f469*(x**21)+f470*(x**22)+f471*(x**23)+f472*(x**24)+
f473*(x**25)+f474*(x**26)+f475*(x**27)+f476*(x**28)+f477*(x**29)+
f478*(x**30)+f479*(x**31)+f480*(x**32)+f481*(x**33)+f482*(x**34)+
f483*(x**35)+f484*(x**36)+f485*(x**37)+f486*(x**38)+f487*(x**39)+
f488*(x**40)+f489*(x**41)+f490*(x**42)+f491*(x**43)+f492*(x**44)+
f493*(x**45)+f494*(x**46)+f495*(x**47)+f496*(x**48)+f497*(x**49)+
f498*(x**50)+f499*(x**51)+f500*(x**52)+f501*(x**53)+f502*(x**54)+
f503*(x**55)+f504*(x**56)+f505*(x**57)+f506*(x**58)+f507*(x**59)+
f508*(x**60),
inp_poly**2 = 
(inp_poly_0**2)*(x**0)+(inp_poly_1**2)*(x**64)+(inp_poly_2**2)*(x**128)+
(inp_poly_3**2)*(x**192)+(inp_poly_4**2)*(x**256)+(inp_poly_5**2)*(x**320)+
(inp_poly_6**2)*(x**384)+(inp_poly_7**2)*(x**448)
] && true;



(**************** constants ****************)

mov L0x8006ddc (  78830)@sint32; mov L0x8006de0 (  78830)@sint32;
mov L0x8006de4 ( 191052)@sint32; mov L0x8006de8 (  78830)@sint32;
mov L0x8006dec ( 191052)@sint32; mov L0x8006df0 (-311503)@sint32;
mov L0x8006df4 ( 207751)@sint32; mov L0x8006df8 (  78830)@sint32;
mov L0x8006dfc ( 191052)@sint32; mov L0x8006e00 (-311503)@sint32;
mov L0x8006e04 ( 207751)@sint32; mov L0x8006e08 ( 468028)@sint32;
mov L0x8006e0c (-149945)@sint32; mov L0x8006e10 ( 114478)@sint32;
mov L0x8006e14 ( -82425)@sint32; mov L0x8006e18 (  78830)@sint32;
mov L0x8006e1c (  78830)@sint32; mov L0x8006e20 ( 191052)@sint32;
mov L0x8006e24 (  78830)@sint32; mov L0x8006e28 ( 191052)@sint32;
mov L0x8006e2c (-311503)@sint32; mov L0x8006e30 ( 207751)@sint32;
mov L0x8006e34 (  78830)@sint32; mov L0x8006e38 ( 191052)@sint32;
mov L0x8006e3c (-311503)@sint32; mov L0x8006e40 ( 207751)@sint32;
mov L0x8006e44 ( 468028)@sint32; mov L0x8006e48 (-149945)@sint32;
mov L0x8006e4c ( 114478)@sint32; mov L0x8006e50 ( -82425)@sint32;
mov L0x8006e54 ( 191052)@sint32; mov L0x8006e58 (-311503)@sint32;
mov L0x8006e5c ( 207751)@sint32; mov L0x8006e60 ( 468028)@sint32;
mov L0x8006e64 (-149945)@sint32; mov L0x8006e68 ( 114478)@sint32;
mov L0x8006e6c ( -82425)@sint32; mov L0x8006e70 ( 254425)@sint32;
mov L0x8006e74 ( -83285)@sint32; mov L0x8006e78 (-205022)@sint32;
mov L0x8006e7c ( 318314)@sint32; mov L0x8006e80 ( 365552)@sint32;
mov L0x8006e84 (-403894)@sint32; mov L0x8006e88 ( 235060)@sint32;
mov L0x8006e8c ( 449706)@sint32; mov L0x8006e90 (-311503)@sint32;
mov L0x8006e94 ( 468028)@sint32; mov L0x8006e98 (-149945)@sint32;
mov L0x8006e9c ( 254425)@sint32; mov L0x8006ea0 ( -83285)@sint32;
mov L0x8006ea4 (-205022)@sint32; mov L0x8006ea8 ( 318314)@sint32;
mov L0x8006eac ( 378933)@sint32; mov L0x8006eb0 ( 313241)@sint32;
mov L0x8006eb4 (-397250)@sint32; mov L0x8006eb8 ( 288321)@sint32;
mov L0x8006ebc (  74768)@sint32; mov L0x8006ec0 (  22897)@sint32;
mov L0x8006ec4 ( 129870)@sint32; mov L0x8006ec8 (-461967)@sint32;
mov L0x8006ecc ( 207751)@sint32; mov L0x8006ed0 ( 114478)@sint32;
mov L0x8006ed4 ( -82425)@sint32; mov L0x8006ed8 ( 365552)@sint32;
mov L0x8006edc (-403894)@sint32; mov L0x8006ee0 ( 235060)@sint32;
mov L0x8006ee4 ( 449706)@sint32; mov L0x8006ee8 ( -35962)@sint32;
mov L0x8006eec ( 370478)@sint32; mov L0x8006ef0 ( 232373)@sint32;
mov L0x8006ef4 ( 159337)@sint32; mov L0x8006ef8 ( 405929)@sint32;
mov L0x8006efc (  59399)@sint32; mov L0x8006f00 ( -40385)@sint32;
mov L0x8006f04 ( 317168)@sint32; mov L0x8006f08 ( 468028)@sint32;
mov L0x8006f0c ( 254425)@sint32; mov L0x8006f10 ( -83285)@sint32;
mov L0x8006f14 ( 378933)@sint32; mov L0x8006f18 ( 313241)@sint32;
mov L0x8006f1c (-397250)@sint32; mov L0x8006f20 ( 288321)@sint32;
mov L0x8006f24 (  13867)@sint32; mov L0x8006f28 (  21742)@sint32;
mov L0x8006f2c ( 486841)@sint32; mov L0x8006f30 ( -73546)@sint32;
mov L0x8006f34 (   7056)@sint32; mov L0x8006f38 (-391031)@sint32;
mov L0x8006f3c (-493755)@sint32; mov L0x8006f40 ( -78001)@sint32;
mov L0x8006f44 (-149945)@sint32; mov L0x8006f48 (-205022)@sint32;
mov L0x8006f4c ( 318314)@sint32; mov L0x8006f50 (  74768)@sint32;
mov L0x8006f54 (  22897)@sint32; mov L0x8006f58 ( 129870)@sint32;
mov L0x8006f5c (-461967)@sint32; mov L0x8006f60 (-495107)@sint32;
mov L0x8006f64 ( 279814)@sint32; mov L0x8006f68 (-363890)@sint32;
mov L0x8006f6c (-182676)@sint32; mov L0x8006f70 (-207660)@sint32;
mov L0x8006f74 (  75978)@sint32; mov L0x8006f78 ( 187423)@sint32;
mov L0x8006f7c ( -78196)@sint32; mov L0x8006f80 ( 114478)@sint32;
mov L0x8006f84 ( 365552)@sint32; mov L0x8006f88 (-403894)@sint32;
mov L0x8006f8c ( -35962)@sint32; mov L0x8006f90 ( 370478)@sint32;
mov L0x8006f94 ( 232373)@sint32; mov L0x8006f98 ( 159337)@sint32;
mov L0x8006f9c ( 507011)@sint32; mov L0x8006fa0 ( 331715)@sint32;
mov L0x8006fa4 ( 297886)@sint32; mov L0x8006fa8 (-346620)@sint32;
mov L0x8006fac (-299045)@sint32; mov L0x8006fb0 ( 275767)@sint32;
mov L0x8006fb4 ( -51317)@sint32; mov L0x8006fb8 ( 402791)@sint32;
mov L0x8006fbc ( -82425)@sint32; mov L0x8006fc0 ( 235060)@sint32;
mov L0x8006fc4 ( 449706)@sint32; mov L0x8006fc8 ( 405929)@sint32;
mov L0x8006fcc (  59399)@sint32; mov L0x8006fd0 ( -40385)@sint32;
mov L0x8006fd4 ( 317168)@sint32; mov L0x8006fd8 (-272172)@sint32;
mov L0x8006fdc (-375619)@sint32; mov L0x8006fe0 ( 376740)@sint32;
mov L0x8006fe4 (-409013)@sint32; mov L0x8006fe8 ( -42578)@sint32;
mov L0x8006fec (-405086)@sint32; mov L0x8006ff0 (  81030)@sint32;
mov L0x8006ff4 (-422078)@sint32; mov L0x8006ff8 ( 254425)@sint32;
mov L0x8006ffc ( 378933)@sint32; mov L0x8007000 ( 313241)@sint32;
mov L0x8007004 (  13867)@sint32; mov L0x8007008 (  21742)@sint32;
mov L0x800700c ( 486841)@sint32; mov L0x8007010 ( -73546)@sint32;
mov L0x8007014 ( 487892)@sint32; mov L0x8007018 (-428144)@sint32;
mov L0x800701c ( -43370)@sint32; mov L0x8007020 (-393153)@sint32;
mov L0x8007024 ( 248256)@sint32; mov L0x8007028 (-228921)@sint32;
mov L0x800702c ( -29446)@sint32; mov L0x8007030 ( -59870)@sint32;
mov L0x8007034 ( -83285)@sint32; mov L0x8007038 (-397250)@sint32;
mov L0x800703c ( 288321)@sint32; mov L0x8007040 (   7056)@sint32;
mov L0x8007044 (-391031)@sint32; mov L0x8007048 (-493755)@sint32;
mov L0x800704c ( -78001)@sint32; mov L0x8007050 ( 285179)@sint32;
mov L0x8007054 ( 257414)@sint32; mov L0x8007058 (-147526)@sint32;
mov L0x800705c ( 390544)@sint32; mov L0x8007060 (-275430)@sint32;
mov L0x8007064 (-160445)@sint32; mov L0x8007068 (-436582)@sint32;
mov L0x800706c ( 316768)@sint32; mov L0x8007070 (-205022)@sint32;
mov L0x8007074 (  74768)@sint32; mov L0x8007078 (  22897)@sint32;
mov L0x800707c (-495107)@sint32; mov L0x8007080 ( 279814)@sint32;
mov L0x8007084 (-363890)@sint32; mov L0x8007088 (-182676)@sint32;
mov L0x800708c (  27120)@sint32; mov L0x8007090 (-345344)@sint32;
mov L0x8007094 (-470298)@sint32; mov L0x8007098 (-498651)@sint32;
mov L0x800709c (-358783)@sint32; mov L0x80070a0 ( 477219)@sint32;
mov L0x80070a4 ( 133279)@sint32; mov L0x80070a8 (-401288)@sint32;
mov L0x80070ac ( 318314)@sint32; mov L0x80070b0 ( 129870)@sint32;
mov L0x80070b4 (-461967)@sint32; mov L0x80070b8 (-207660)@sint32;
mov L0x80070bc (  75978)@sint32; mov L0x80070c0 ( 187423)@sint32;
mov L0x80070c4 ( -78196)@sint32; mov L0x80070c8 ( 288431)@sint32;
mov L0x80070cc (-155391)@sint32; mov L0x80070d0 ( -18223)@sint32;
mov L0x80070d4 (-478095)@sint32; mov L0x80070d8 (-347554)@sint32;
mov L0x80070dc ( 256625)@sint32; mov L0x80070e0 (-153141)@sint32;
mov L0x80070e4 (-298605)@sint32; mov L0x80070e8 ( 365552)@sint32;
mov L0x80070ec ( -35962)@sint32; mov L0x80070f0 ( 370478)@sint32;
mov L0x80070f4 ( 507011)@sint32; mov L0x80070f8 ( 331715)@sint32;
mov L0x80070fc ( 297886)@sint32; mov L0x8007100 (-346620)@sint32;
mov L0x8007104 (-367175)@sint32; mov L0x8007108 (-334857)@sint32;
mov L0x800710c (  42298)@sint32; mov L0x8007110 (-465942)@sint32;
mov L0x8007114 ( 329620)@sint32; mov L0x8007118 ( -89230)@sint32;
mov L0x800711c (-288348)@sint32; mov L0x8007120 ( 279364)@sint32;
mov L0x8007124 (-403894)@sint32; mov L0x8007128 ( 232373)@sint32;
mov L0x800712c ( 159337)@sint32; mov L0x8007130 (-299045)@sint32;
mov L0x8007134 ( 275767)@sint32; mov L0x8007138 ( -51317)@sint32;
mov L0x800713c ( 402791)@sint32; mov L0x8007140 ( 268720)@sint32;
mov L0x8007144 ( 464538)@sint32; mov L0x8007148 ( 356621)@sint32;
mov L0x800714c ( 343605)@sint32; mov L0x8007150 ( 476116)@sint32;
mov L0x8007154 (  44548)@sint32; mov L0x8007158 ( 347463)@sint32;
mov L0x800715c ( 399863)@sint32; mov L0x8007160 ( 235060)@sint32;
mov L0x8007164 ( 405929)@sint32; mov L0x8007168 (  59399)@sint32;
mov L0x800716c (-272172)@sint32; mov L0x8007170 (-375619)@sint32;
mov L0x8007174 ( 376740)@sint32; mov L0x8007178 (-409013)@sint32;
mov L0x800717c (-188449)@sint32; mov L0x8007180 (-309697)@sint32;
mov L0x8007184 (-118699)@sint32; mov L0x8007188 ( 418878)@sint32;
mov L0x800718c (-284025)@sint32; mov L0x8007190 ( 170731)@sint32;
mov L0x8007194 ( 515076)@sint32; mov L0x8007198 ( 290607)@sint32;
mov L0x800719c ( 449706)@sint32; mov L0x80071a0 ( -40385)@sint32;
mov L0x80071a4 ( 317168)@sint32; mov L0x80071a8 ( -42578)@sint32;
mov L0x80071ac (-405086)@sint32; mov L0x80071b0 (  81030)@sint32;
mov L0x80071b4 (-422078)@sint32; mov L0x80071b8 ( 445216)@sint32;
mov L0x80071bc ( 114673)@sint32; mov L0x80071c0 (   1019)@sint32;
mov L0x80071c4 (-364661)@sint32; mov L0x80071c8 (-179242)@sint32;
mov L0x80071cc (-317922)@sint32; mov L0x80071d0 (-202373)@sint32;
mov L0x80071d4 ( 400989)@sint32;

(* #! -> SP = 0x20016fb0 *)
#! 0x20016fb0 = 0x20016fb0;
(* #stmdb	sp!, {r4, r5, r6, r7, r8, r9, r10, r11, r12, lr}#! EA = L0x20016fb0; PC = 0x8000b5c *)
#stmdb	sp!, {%%r4, %%r5, %%r6, %%r7, %%r8, %%r9, %%r10, %%r11, %%r12, %%lr}#! L0x20016fb0 = L0x20016fb0; 0x8000b5c = 0x8000b5c;
(* #vpush	{s16-s31}                                 #! PC = 0x8000b60 *)
#vpush	{%%s16-%%s31}                                 #! 0x8000b60 = 0x8000b60;
(* #ldr.w	lr, [sp, #104]	; 0x68                     #! EA = L0x20016fb0; Value = 0x20016fd0; PC = 0x8000b64 *)
#ldr.w	%%lr, [sp, #104]	; 0x68                     #! L0x20016fb0 = L0x20016fb0; 0x20016fd0 = 0x20016fd0; 0x8000b64 = 0x8000b64;
(* vldmia	r1!, {s4-s18}                             #! EA = L0x8006ddc; PC = 0x8000b68 *)
mov s4 L0x8006ddc;
mov s5 L0x8006de0;
mov s6 L0x8006de4;
mov s7 L0x8006de8;
mov s8 L0x8006dec;
mov s9 L0x8006df0;
mov s10 L0x8006df4;
mov s11 L0x8006df8;
mov s12 L0x8006dfc;
mov s13 L0x8006e00;
mov s14 L0x8006e04;
mov s15 L0x8006e08;
mov s16 L0x8006e0c;
mov s17 L0x8006e10;
mov s18 L0x8006e14;
(* vmov	s0, lr                                     #! PC = 0x8000b6c *)
mov s0 lr;
(* add.w	r12, r0, #244	; 0xf4                      #! PC = 0x8000b70 *)
adds dontcare r12 r0 244@uint32;
(* vmov	s2, r12                                    #! PC = 0x8000b74 *)
mov s2 r12;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017050; Value = 0x01d4fe30; PC = 0x8000b7c *)
mov r4 L0x20017050;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017150; Value = 0xfcf20159; PC = 0x8000b80 *)
mov r5 L0x20017150;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017250; Value = 0x00b7fdde; PC = 0x8000b84 *)
mov r6 L0x20017250;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017350; Value = 0xfe94035c; PC = 0x8000b88 *)
mov r7 L0x20017350;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf064@sint32 : and [cf064 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf192@sint32 : and [cf192 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf320@sint32 : and [cf320 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf448@sint32 : and [cf448 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170d0; Value = 0xfcfe008a; PC = 0x8000c9c *)
mov r5 L0x200170d0;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171d0; Value = 0xfe2a00cc; PC = 0x8000ca0 *)
mov r6 L0x200171d0;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172d0; Value = 0x0204ff1a; PC = 0x8000ca4 *)
mov r7 L0x200172d0;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fd0; Value = 0x00cf002d; PC = 0x8000ca8 *)
mov r4 L0x20016fd0;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf000@sint32 : and [cf000 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf128@sint32 : and [cf128 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf256@sint32 : and [cf256 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf384@sint32 : and [cf384 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197d0; PC = 0x8000d54 *)
mov L0x200197d0 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198d0; PC = 0x8000d58 *)
mov L0x200198d0 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199d0; PC = 0x8000d5c *)
mov L0x200199d0 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019ad0; PC = 0x8000d60 *)
mov L0x20019ad0 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019bd0; PC = 0x8000d64 *)
mov L0x20019bd0 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019cd0; PC = 0x8000d68 *)
mov L0x20019cd0 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019dd0; PC = 0x8000d6c *)
mov L0x20019dd0 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019ed0; PC = 0x8000d70 *)
mov L0x20019ed0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193d0; PC = 0x8000d9c *)
mov L0x200193d0 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194d0; PC = 0x8000da0 *)
mov L0x200194d0 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195d0; PC = 0x8000da4 *)
mov L0x200195d0 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196d0; PC = 0x8000da8 *)
mov L0x200196d0 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190d0; PC = 0x8000dac *)
mov L0x200190d0 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191d0; PC = 0x8000db0 *)
mov L0x200191d0 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192d0; PC = 0x8000db4 *)
mov L0x200192d0 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018fd0; PC = 0x8000db8 *)
mov L0x20018fd0 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017052; Value = 0xfd3401d4; PC = 0x8000b7c *)
mov r4 L0x20017052;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017152; Value = 0xfe6ffcf2; PC = 0x8000b80 *)
mov r5 L0x20017152;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017252; Value = 0x015d00b7; PC = 0x8000b84 *)
mov r6 L0x20017252;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017352; Value = 0x0218fe94; PC = 0x8000b88 *)
mov r7 L0x20017352;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf065@sint32 : and [cf065 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf193@sint32 : and [cf193 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf321@sint32 : and [cf321 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf449@sint32 : and [cf449 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170d2; Value = 0x0021fcfe; PC = 0x8000c9c *)
mov r5 L0x200170d2;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171d2; Value = 0xfeaffe2a; PC = 0x8000ca0 *)
mov r6 L0x200171d2;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172d2; Value = 0xffa30204; PC = 0x8000ca4 *)
mov r7 L0x200172d2;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fd2; Value = 0x024600cf; PC = 0x8000ca8 *)
mov r4 L0x20016fd2;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf001@sint32 : and [cf001 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf129@sint32 : and [cf129 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf257@sint32 : and [cf257 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf385@sint32 : and [cf385 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197d4; PC = 0x8000d54 *)
mov L0x200197d4 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198d4; PC = 0x8000d58 *)
mov L0x200198d4 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199d4; PC = 0x8000d5c *)
mov L0x200199d4 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019ad4; PC = 0x8000d60 *)
mov L0x20019ad4 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019bd4; PC = 0x8000d64 *)
mov L0x20019bd4 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019cd4; PC = 0x8000d68 *)
mov L0x20019cd4 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019dd4; PC = 0x8000d6c *)
mov L0x20019dd4 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019ed4; PC = 0x8000d70 *)
mov L0x20019ed4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193d4; PC = 0x8000d9c *)
mov L0x200193d4 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194d4; PC = 0x8000da0 *)
mov L0x200194d4 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195d4; PC = 0x8000da4 *)
mov L0x200195d4 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196d4; PC = 0x8000da8 *)
mov L0x200196d4 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190d4; PC = 0x8000dac *)
mov L0x200190d4 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191d4; PC = 0x8000db0 *)
mov L0x200191d4 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192d4; PC = 0x8000db4 *)
mov L0x200192d4 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018fd4; PC = 0x8000db8 *)
mov L0x20018fd4 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017054; Value = 0xfc9dfd34; PC = 0x8000b7c *)
mov r4 L0x20017054;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017154; Value = 0x03fafe6f; PC = 0x8000b80 *)
mov r5 L0x20017154;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017254; Value = 0x008b015d; PC = 0x8000b84 *)
mov r6 L0x20017254;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017354; Value = 0x02d70218; PC = 0x8000b88 *)
mov r7 L0x20017354;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf066@sint32 : and [cf066 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf194@sint32 : and [cf194 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf322@sint32 : and [cf322 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf450@sint32 : and [cf450 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170d4; Value = 0x001b0021; PC = 0x8000c9c *)
mov r5 L0x200170d4;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171d4; Value = 0xfeaffeaf; PC = 0x8000ca0 *)
mov r6 L0x200171d4;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172d4; Value = 0x037fffa3; PC = 0x8000ca4 *)
mov r7 L0x200172d4;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fd4; Value = 0xfd290246; PC = 0x8000ca8 *)
mov r4 L0x20016fd4;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf002@sint32 : and [cf002 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf130@sint32 : and [cf130 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf258@sint32 : and [cf258 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf386@sint32 : and [cf386 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197d8; PC = 0x8000d54 *)
mov L0x200197d8 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198d8; PC = 0x8000d58 *)
mov L0x200198d8 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199d8; PC = 0x8000d5c *)
mov L0x200199d8 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019ad8; PC = 0x8000d60 *)
mov L0x20019ad8 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019bd8; PC = 0x8000d64 *)
mov L0x20019bd8 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019cd8; PC = 0x8000d68 *)
mov L0x20019cd8 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019dd8; PC = 0x8000d6c *)
mov L0x20019dd8 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019ed8; PC = 0x8000d70 *)
mov L0x20019ed8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193d8; PC = 0x8000d9c *)
mov L0x200193d8 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194d8; PC = 0x8000da0 *)
mov L0x200194d8 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195d8; PC = 0x8000da4 *)
mov L0x200195d8 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196d8; PC = 0x8000da8 *)
mov L0x200196d8 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190d8; PC = 0x8000dac *)
mov L0x200190d8 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191d8; PC = 0x8000db0 *)
mov L0x200191d8 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192d8; PC = 0x8000db4 *)
mov L0x200192d8 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018fd8; PC = 0x8000db8 *)
mov L0x20018fd8 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017056; Value = 0x003afc9d; PC = 0x8000b7c *)
mov r4 L0x20017056;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017156; Value = 0x02e703fa; PC = 0x8000b80 *)
mov r5 L0x20017156;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017256; Value = 0x0284008b; PC = 0x8000b84 *)
mov r6 L0x20017256;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017356; Value = 0xfc4902d7; PC = 0x8000b88 *)
mov r7 L0x20017356;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf067@sint32 : and [cf067 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf195@sint32 : and [cf195 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf323@sint32 : and [cf323 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf451@sint32 : and [cf451 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170d6; Value = 0xfd19001b; PC = 0x8000c9c *)
mov r5 L0x200170d6;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171d6; Value = 0x0062feaf; PC = 0x8000ca0 *)
mov r6 L0x200171d6;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172d6; Value = 0x0141037f; PC = 0x8000ca4 *)
mov r7 L0x200172d6;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fd6; Value = 0xff04fd29; PC = 0x8000ca8 *)
mov r4 L0x20016fd6;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf003@sint32 : and [cf003 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf131@sint32 : and [cf131 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf259@sint32 : and [cf259 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf387@sint32 : and [cf387 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197dc; PC = 0x8000d54 *)
mov L0x200197dc r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198dc; PC = 0x8000d58 *)
mov L0x200198dc r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199dc; PC = 0x8000d5c *)
mov L0x200199dc r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019adc; PC = 0x8000d60 *)
mov L0x20019adc r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019bdc; PC = 0x8000d64 *)
mov L0x20019bdc r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019cdc; PC = 0x8000d68 *)
mov L0x20019cdc r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019ddc; PC = 0x8000d6c *)
mov L0x20019ddc r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019edc; PC = 0x8000d70 *)
mov L0x20019edc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193dc; PC = 0x8000d9c *)
mov L0x200193dc r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194dc; PC = 0x8000da0 *)
mov L0x200194dc r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195dc; PC = 0x8000da4 *)
mov L0x200195dc r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196dc; PC = 0x8000da8 *)
mov L0x200196dc r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190dc; PC = 0x8000dac *)
mov L0x200190dc r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191dc; PC = 0x8000db0 *)
mov L0x200191dc r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192dc; PC = 0x8000db4 *)
mov L0x200192dc r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018fdc; PC = 0x8000db8 *)
mov L0x20018fdc r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017058; Value = 0xfc0d003a; PC = 0x8000b7c *)
mov r4 L0x20017058;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017158; Value = 0x00ec02e7; PC = 0x8000b80 *)
mov r5 L0x20017158;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017258; Value = 0xfe360284; PC = 0x8000b84 *)
mov r6 L0x20017258;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017358; Value = 0xff1ffc49; PC = 0x8000b88 *)
mov r7 L0x20017358;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf068@sint32 : and [cf068 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf196@sint32 : and [cf196 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf324@sint32 : and [cf324 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf452@sint32 : and [cf452 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170d8; Value = 0xff7efd19; PC = 0x8000c9c *)
mov r5 L0x200170d8;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171d8; Value = 0xfd0f0062; PC = 0x8000ca0 *)
mov r6 L0x200171d8;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172d8; Value = 0x01ff0141; PC = 0x8000ca4 *)
mov r7 L0x200172d8;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fd8; Value = 0x01b4ff04; PC = 0x8000ca8 *)
mov r4 L0x20016fd8;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf004@sint32 : and [cf004 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf132@sint32 : and [cf132 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf260@sint32 : and [cf260 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf388@sint32 : and [cf388 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197e0; PC = 0x8000d54 *)
mov L0x200197e0 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198e0; PC = 0x8000d58 *)
mov L0x200198e0 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199e0; PC = 0x8000d5c *)
mov L0x200199e0 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019ae0; PC = 0x8000d60 *)
mov L0x20019ae0 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019be0; PC = 0x8000d64 *)
mov L0x20019be0 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019ce0; PC = 0x8000d68 *)
mov L0x20019ce0 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019de0; PC = 0x8000d6c *)
mov L0x20019de0 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019ee0; PC = 0x8000d70 *)
mov L0x20019ee0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193e0; PC = 0x8000d9c *)
mov L0x200193e0 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194e0; PC = 0x8000da0 *)
mov L0x200194e0 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195e0; PC = 0x8000da4 *)
mov L0x200195e0 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196e0; PC = 0x8000da8 *)
mov L0x200196e0 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190e0; PC = 0x8000dac *)
mov L0x200190e0 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191e0; PC = 0x8000db0 *)
mov L0x200191e0 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192e0; PC = 0x8000db4 *)
mov L0x200192e0 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018fe0; PC = 0x8000db8 *)
mov L0x20018fe0 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001705a; Value = 0x020ffc0d; PC = 0x8000b7c *)
mov r4 L0x2001705a;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001715a; Value = 0xfca900ec; PC = 0x8000b80 *)
mov r5 L0x2001715a;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001725a; Value = 0xfee0fe36; PC = 0x8000b84 *)
mov r6 L0x2001725a;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001735a; Value = 0x0130ff1f; PC = 0x8000b88 *)
mov r7 L0x2001735a;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf069@sint32 : and [cf069 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf197@sint32 : and [cf197 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf325@sint32 : and [cf325 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf453@sint32 : and [cf453 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170da; Value = 0xfce4ff7e; PC = 0x8000c9c *)
mov r5 L0x200170da;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171da; Value = 0xfe86fd0f; PC = 0x8000ca0 *)
mov r6 L0x200171da;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172da; Value = 0xfc1201ff; PC = 0x8000ca4 *)
mov r7 L0x200172da;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fda; Value = 0x037801b4; PC = 0x8000ca8 *)
mov r4 L0x20016fda;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf005@sint32 : and [cf005 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf133@sint32 : and [cf133 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf261@sint32 : and [cf261 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf389@sint32 : and [cf389 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197e4; PC = 0x8000d54 *)
mov L0x200197e4 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198e4; PC = 0x8000d58 *)
mov L0x200198e4 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199e4; PC = 0x8000d5c *)
mov L0x200199e4 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019ae4; PC = 0x8000d60 *)
mov L0x20019ae4 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019be4; PC = 0x8000d64 *)
mov L0x20019be4 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019ce4; PC = 0x8000d68 *)
mov L0x20019ce4 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019de4; PC = 0x8000d6c *)
mov L0x20019de4 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019ee4; PC = 0x8000d70 *)
mov L0x20019ee4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193e4; PC = 0x8000d9c *)
mov L0x200193e4 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194e4; PC = 0x8000da0 *)
mov L0x200194e4 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195e4; PC = 0x8000da4 *)
mov L0x200195e4 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196e4; PC = 0x8000da8 *)
mov L0x200196e4 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190e4; PC = 0x8000dac *)
mov L0x200190e4 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191e4; PC = 0x8000db0 *)
mov L0x200191e4 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192e4; PC = 0x8000db4 *)
mov L0x200192e4 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018fe4; PC = 0x8000db8 *)
mov L0x20018fe4 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001705c; Value = 0x02bd020f; PC = 0x8000b7c *)
mov r4 L0x2001705c;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001715c; Value = 0x02e6fca9; PC = 0x8000b80 *)
mov r5 L0x2001715c;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001725c; Value = 0xffbbfee0; PC = 0x8000b84 *)
mov r6 L0x2001725c;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001735c; Value = 0xfd810130; PC = 0x8000b88 *)
mov r7 L0x2001735c;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf070@sint32 : and [cf070 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf198@sint32 : and [cf198 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf326@sint32 : and [cf326 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf454@sint32 : and [cf454 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170dc; Value = 0xff75fce4; PC = 0x8000c9c *)
mov r5 L0x200170dc;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171dc; Value = 0xfc75fe86; PC = 0x8000ca0 *)
mov r6 L0x200171dc;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172dc; Value = 0x004ffc12; PC = 0x8000ca4 *)
mov r7 L0x200172dc;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fdc; Value = 0xfdd80378; PC = 0x8000ca8 *)
mov r4 L0x20016fdc;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf006@sint32 : and [cf006 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf134@sint32 : and [cf134 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf262@sint32 : and [cf262 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf390@sint32 : and [cf390 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197e8; PC = 0x8000d54 *)
mov L0x200197e8 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198e8; PC = 0x8000d58 *)
mov L0x200198e8 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199e8; PC = 0x8000d5c *)
mov L0x200199e8 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019ae8; PC = 0x8000d60 *)
mov L0x20019ae8 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019be8; PC = 0x8000d64 *)
mov L0x20019be8 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019ce8; PC = 0x8000d68 *)
mov L0x20019ce8 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019de8; PC = 0x8000d6c *)
mov L0x20019de8 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019ee8; PC = 0x8000d70 *)
mov L0x20019ee8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193e8; PC = 0x8000d9c *)
mov L0x200193e8 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194e8; PC = 0x8000da0 *)
mov L0x200194e8 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195e8; PC = 0x8000da4 *)
mov L0x200195e8 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196e8; PC = 0x8000da8 *)
mov L0x200196e8 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190e8; PC = 0x8000dac *)
mov L0x200190e8 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191e8; PC = 0x8000db0 *)
mov L0x200191e8 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192e8; PC = 0x8000db4 *)
mov L0x200192e8 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018fe8; PC = 0x8000db8 *)
mov L0x20018fe8 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001705e; Value = 0xfe2f02bd; PC = 0x8000b7c *)
mov r4 L0x2001705e;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001715e; Value = 0xfcab02e6; PC = 0x8000b80 *)
mov r5 L0x2001715e;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001725e; Value = 0x01bfffbb; PC = 0x8000b84 *)
mov r6 L0x2001725e;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001735e; Value = 0xfd21fd81; PC = 0x8000b88 *)
mov r7 L0x2001735e;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf071@sint32 : and [cf071 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf199@sint32 : and [cf199 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf327@sint32 : and [cf327 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf455@sint32 : and [cf455 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170de; Value = 0x008eff75; PC = 0x8000c9c *)
mov r5 L0x200170de;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171de; Value = 0xff55fc75; PC = 0x8000ca0 *)
mov r6 L0x200171de;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172de; Value = 0x0242004f; PC = 0x8000ca4 *)
mov r7 L0x200172de;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fde; Value = 0xfc68fdd8; PC = 0x8000ca8 *)
mov r4 L0x20016fde;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf007@sint32 : and [cf007 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf135@sint32 : and [cf135 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf263@sint32 : and [cf263 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf391@sint32 : and [cf391 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197ec; PC = 0x8000d54 *)
mov L0x200197ec r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198ec; PC = 0x8000d58 *)
mov L0x200198ec r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199ec; PC = 0x8000d5c *)
mov L0x200199ec r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019aec; PC = 0x8000d60 *)
mov L0x20019aec r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019bec; PC = 0x8000d64 *)
mov L0x20019bec r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019cec; PC = 0x8000d68 *)
mov L0x20019cec r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019dec; PC = 0x8000d6c *)
mov L0x20019dec r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019eec; PC = 0x8000d70 *)
mov L0x20019eec r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193ec; PC = 0x8000d9c *)
mov L0x200193ec r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194ec; PC = 0x8000da0 *)
mov L0x200194ec r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195ec; PC = 0x8000da4 *)
mov L0x200195ec r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196ec; PC = 0x8000da8 *)
mov L0x200196ec r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190ec; PC = 0x8000dac *)
mov L0x200190ec r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191ec; PC = 0x8000db0 *)
mov L0x200191ec r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192ec; PC = 0x8000db4 *)
mov L0x200192ec r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018fec; PC = 0x8000db8 *)
mov L0x20018fec r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017060; Value = 0x01a1fe2f; PC = 0x8000b7c *)
mov r4 L0x20017060;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017160; Value = 0x03aafcab; PC = 0x8000b80 *)
mov r5 L0x20017160;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017260; Value = 0x009701bf; PC = 0x8000b84 *)
mov r6 L0x20017260;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017360; Value = 0x03cdfd21; PC = 0x8000b88 *)
mov r7 L0x20017360;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf072@sint32 : and [cf072 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf200@sint32 : and [cf200 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf328@sint32 : and [cf328 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf456@sint32 : and [cf456 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170e0; Value = 0x00c6008e; PC = 0x8000c9c *)
mov r5 L0x200170e0;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171e0; Value = 0x0374ff55; PC = 0x8000ca0 *)
mov r6 L0x200171e0;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172e0; Value = 0xfc980242; PC = 0x8000ca4 *)
mov r7 L0x200172e0;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fe0; Value = 0x02a7fc68; PC = 0x8000ca8 *)
mov r4 L0x20016fe0;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf008@sint32 : and [cf008 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf136@sint32 : and [cf136 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf264@sint32 : and [cf264 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf392@sint32 : and [cf392 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197f0; PC = 0x8000d54 *)
mov L0x200197f0 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198f0; PC = 0x8000d58 *)
mov L0x200198f0 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199f0; PC = 0x8000d5c *)
mov L0x200199f0 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019af0; PC = 0x8000d60 *)
mov L0x20019af0 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019bf0; PC = 0x8000d64 *)
mov L0x20019bf0 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019cf0; PC = 0x8000d68 *)
mov L0x20019cf0 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019df0; PC = 0x8000d6c *)
mov L0x20019df0 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019ef0; PC = 0x8000d70 *)
mov L0x20019ef0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193f0; PC = 0x8000d9c *)
mov L0x200193f0 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194f0; PC = 0x8000da0 *)
mov L0x200194f0 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195f0; PC = 0x8000da4 *)
mov L0x200195f0 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196f0; PC = 0x8000da8 *)
mov L0x200196f0 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190f0; PC = 0x8000dac *)
mov L0x200190f0 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191f0; PC = 0x8000db0 *)
mov L0x200191f0 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192f0; PC = 0x8000db4 *)
mov L0x200192f0 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018ff0; PC = 0x8000db8 *)
mov L0x20018ff0 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017062; Value = 0x03f701a1; PC = 0x8000b7c *)
mov r4 L0x20017062;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017162; Value = 0xfc3903aa; PC = 0x8000b80 *)
mov r5 L0x20017162;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017262; Value = 0x02850097; PC = 0x8000b84 *)
mov r6 L0x20017262;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017362; Value = 0xff0d03cd; PC = 0x8000b88 *)
mov r7 L0x20017362;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf073@sint32 : and [cf073 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf201@sint32 : and [cf201 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf329@sint32 : and [cf329 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf457@sint32 : and [cf457 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170e2; Value = 0x018700c6; PC = 0x8000c9c *)
mov r5 L0x200170e2;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171e2; Value = 0xfe1c0374; PC = 0x8000ca0 *)
mov r6 L0x200171e2;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172e2; Value = 0xfccbfc98; PC = 0x8000ca4 *)
mov r7 L0x200172e2;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fe2; Value = 0x03ff02a7; PC = 0x8000ca8 *)
mov r4 L0x20016fe2;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf009@sint32 : and [cf009 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf137@sint32 : and [cf137 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf265@sint32 : and [cf265 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf393@sint32 : and [cf393 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197f4; PC = 0x8000d54 *)
mov L0x200197f4 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198f4; PC = 0x8000d58 *)
mov L0x200198f4 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199f4; PC = 0x8000d5c *)
mov L0x200199f4 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019af4; PC = 0x8000d60 *)
mov L0x20019af4 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019bf4; PC = 0x8000d64 *)
mov L0x20019bf4 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019cf4; PC = 0x8000d68 *)
mov L0x20019cf4 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019df4; PC = 0x8000d6c *)
mov L0x20019df4 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019ef4; PC = 0x8000d70 *)
mov L0x20019ef4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193f4; PC = 0x8000d9c *)
mov L0x200193f4 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194f4; PC = 0x8000da0 *)
mov L0x200194f4 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195f4; PC = 0x8000da4 *)
mov L0x200195f4 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196f4; PC = 0x8000da8 *)
mov L0x200196f4 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190f4; PC = 0x8000dac *)
mov L0x200190f4 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191f4; PC = 0x8000db0 *)
mov L0x200191f4 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192f4; PC = 0x8000db4 *)
mov L0x200192f4 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018ff4; PC = 0x8000db8 *)
mov L0x20018ff4 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017064; Value = 0xfd0f03f7; PC = 0x8000b7c *)
mov r4 L0x20017064;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017164; Value = 0x02bffc39; PC = 0x8000b80 *)
mov r5 L0x20017164;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017264; Value = 0x02a20285; PC = 0x8000b84 *)
mov r6 L0x20017264;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017364; Value = 0xfcd9ff0d; PC = 0x8000b88 *)
mov r7 L0x20017364;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf074@sint32 : and [cf074 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf202@sint32 : and [cf202 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf330@sint32 : and [cf330 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf458@sint32 : and [cf458 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170e4; Value = 0xfc850187; PC = 0x8000c9c *)
mov r5 L0x200170e4;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171e4; Value = 0x0070fe1c; PC = 0x8000ca0 *)
mov r6 L0x200171e4;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172e4; Value = 0x0273fccb; PC = 0x8000ca4 *)
mov r7 L0x200172e4;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fe4; Value = 0x003f03ff; PC = 0x8000ca8 *)
mov r4 L0x20016fe4;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf010@sint32 : and [cf010 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf138@sint32 : and [cf138 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf266@sint32 : and [cf266 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf394@sint32 : and [cf394 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197f8; PC = 0x8000d54 *)
mov L0x200197f8 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198f8; PC = 0x8000d58 *)
mov L0x200198f8 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199f8; PC = 0x8000d5c *)
mov L0x200199f8 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019af8; PC = 0x8000d60 *)
mov L0x20019af8 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019bf8; PC = 0x8000d64 *)
mov L0x20019bf8 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019cf8; PC = 0x8000d68 *)
mov L0x20019cf8 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019df8; PC = 0x8000d6c *)
mov L0x20019df8 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019ef8; PC = 0x8000d70 *)
mov L0x20019ef8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193f8; PC = 0x8000d9c *)
mov L0x200193f8 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194f8; PC = 0x8000da0 *)
mov L0x200194f8 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195f8; PC = 0x8000da4 *)
mov L0x200195f8 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196f8; PC = 0x8000da8 *)
mov L0x200196f8 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190f8; PC = 0x8000dac *)
mov L0x200190f8 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191f8; PC = 0x8000db0 *)
mov L0x200191f8 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192f8; PC = 0x8000db4 *)
mov L0x200192f8 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018ff8; PC = 0x8000db8 *)
mov L0x20018ff8 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017066; Value = 0xfcd9fd0f; PC = 0x8000b7c *)
mov r4 L0x20017066;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017166; Value = 0x03ba02bf; PC = 0x8000b80 *)
mov r5 L0x20017166;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017266; Value = 0x020a02a2; PC = 0x8000b84 *)
mov r6 L0x20017266;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017366; Value = 0x0146fcd9; PC = 0x8000b88 *)
mov r7 L0x20017366;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf075@sint32 : and [cf075 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf203@sint32 : and [cf203 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf331@sint32 : and [cf331 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf459@sint32 : and [cf459 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170e6; Value = 0x0058fc85; PC = 0x8000c9c *)
mov r5 L0x200170e6;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171e6; Value = 0xffa30070; PC = 0x8000ca0 *)
mov r6 L0x200171e6;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172e6; Value = 0x02f60273; PC = 0x8000ca4 *)
mov r7 L0x200172e6;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fe6; Value = 0x012b003f; PC = 0x8000ca8 *)
mov r4 L0x20016fe6;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf011@sint32 : and [cf011 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf139@sint32 : and [cf139 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf267@sint32 : and [cf267 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf395@sint32 : and [cf395 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197fc; PC = 0x8000d54 *)
mov L0x200197fc r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198fc; PC = 0x8000d58 *)
mov L0x200198fc r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199fc; PC = 0x8000d5c *)
mov L0x200199fc r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019afc; PC = 0x8000d60 *)
mov L0x20019afc r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019bfc; PC = 0x8000d64 *)
mov L0x20019bfc r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019cfc; PC = 0x8000d68 *)
mov L0x20019cfc r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019dfc; PC = 0x8000d6c *)
mov L0x20019dfc r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019efc; PC = 0x8000d70 *)
mov L0x20019efc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193fc; PC = 0x8000d9c *)
mov L0x200193fc r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194fc; PC = 0x8000da0 *)
mov L0x200194fc r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195fc; PC = 0x8000da4 *)
mov L0x200195fc r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196fc; PC = 0x8000da8 *)
mov L0x200196fc r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190fc; PC = 0x8000dac *)
mov L0x200190fc r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191fc; PC = 0x8000db0 *)
mov L0x200191fc r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192fc; PC = 0x8000db4 *)
mov L0x200192fc r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018ffc; PC = 0x8000db8 *)
mov L0x20018ffc r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017068; Value = 0xfd68fcd9; PC = 0x8000b7c *)
mov r4 L0x20017068;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017168; Value = 0x035f03ba; PC = 0x8000b80 *)
mov r5 L0x20017168;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017268; Value = 0xfec1020a; PC = 0x8000b84 *)
mov r6 L0x20017268;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017368; Value = 0xfc710146; PC = 0x8000b88 *)
mov r7 L0x20017368;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf076@sint32 : and [cf076 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf204@sint32 : and [cf204 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf332@sint32 : and [cf332 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf460@sint32 : and [cf460 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170e8; Value = 0x00dd0058; PC = 0x8000c9c *)
mov r5 L0x200170e8;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171e8; Value = 0xfe09ffa3; PC = 0x8000ca0 *)
mov r6 L0x200171e8;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172e8; Value = 0xfdf902f6; PC = 0x8000ca4 *)
mov r7 L0x200172e8;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fe8; Value = 0xfef1012b; PC = 0x8000ca8 *)
mov r4 L0x20016fe8;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf012@sint32 : and [cf012 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf140@sint32 : and [cf140 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf268@sint32 : and [cf268 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf396@sint32 : and [cf396 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019800; PC = 0x8000d54 *)
mov L0x20019800 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019900; PC = 0x8000d58 *)
mov L0x20019900 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a00; PC = 0x8000d5c *)
mov L0x20019a00 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b00; PC = 0x8000d60 *)
mov L0x20019b00 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c00; PC = 0x8000d64 *)
mov L0x20019c00 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d00; PC = 0x8000d68 *)
mov L0x20019d00 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e00; PC = 0x8000d6c *)
mov L0x20019e00 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f00; PC = 0x8000d70 *)
mov L0x20019f00 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019400; PC = 0x8000d9c *)
mov L0x20019400 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019500; PC = 0x8000da0 *)
mov L0x20019500 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019600; PC = 0x8000da4 *)
mov L0x20019600 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019700; PC = 0x8000da8 *)
mov L0x20019700 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019100; PC = 0x8000dac *)
mov L0x20019100 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019200; PC = 0x8000db0 *)
mov L0x20019200 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019300; PC = 0x8000db4 *)
mov L0x20019300 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019000; PC = 0x8000db8 *)
mov L0x20019000 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001706a; Value = 0xfef4fd68; PC = 0x8000b7c *)
mov r4 L0x2001706a;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001716a; Value = 0x00a5035f; PC = 0x8000b80 *)
mov r5 L0x2001716a;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001726a; Value = 0xfd5afec1; PC = 0x8000b84 *)
mov r6 L0x2001726a;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001736a; Value = 0x01b1fc71; PC = 0x8000b88 *)
mov r7 L0x2001736a;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf077@sint32 : and [cf077 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf205@sint32 : and [cf205 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf333@sint32 : and [cf333 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf461@sint32 : and [cf461 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170ea; Value = 0xfc2400dd; PC = 0x8000c9c *)
mov r5 L0x200170ea;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171ea; Value = 0x00e3fe09; PC = 0x8000ca0 *)
mov r6 L0x200171ea;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172ea; Value = 0xfd54fdf9; PC = 0x8000ca4 *)
mov r7 L0x200172ea;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fea; Value = 0xfdfcfef1; PC = 0x8000ca8 *)
mov r4 L0x20016fea;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf013@sint32 : and [cf013 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf141@sint32 : and [cf141 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf269@sint32 : and [cf269 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf397@sint32 : and [cf397 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019804; PC = 0x8000d54 *)
mov L0x20019804 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019904; PC = 0x8000d58 *)
mov L0x20019904 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a04; PC = 0x8000d5c *)
mov L0x20019a04 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b04; PC = 0x8000d60 *)
mov L0x20019b04 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c04; PC = 0x8000d64 *)
mov L0x20019c04 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d04; PC = 0x8000d68 *)
mov L0x20019d04 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e04; PC = 0x8000d6c *)
mov L0x20019e04 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f04; PC = 0x8000d70 *)
mov L0x20019f04 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019404; PC = 0x8000d9c *)
mov L0x20019404 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019504; PC = 0x8000da0 *)
mov L0x20019504 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019604; PC = 0x8000da4 *)
mov L0x20019604 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019704; PC = 0x8000da8 *)
mov L0x20019704 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019104; PC = 0x8000dac *)
mov L0x20019104 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019204; PC = 0x8000db0 *)
mov L0x20019204 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019304; PC = 0x8000db4 *)
mov L0x20019304 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019004; PC = 0x8000db8 *)
mov L0x20019004 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001706c; Value = 0xfed9fef4; PC = 0x8000b7c *)
mov r4 L0x2001706c;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001716c; Value = 0xfcb100a5; PC = 0x8000b80 *)
mov r5 L0x2001716c;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001726c; Value = 0xfc48fd5a; PC = 0x8000b84 *)
mov r6 L0x2001726c;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001736c; Value = 0xff9a01b1; PC = 0x8000b88 *)
mov r7 L0x2001736c;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf078@sint32 : and [cf078 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf206@sint32 : and [cf206 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf334@sint32 : and [cf334 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf462@sint32 : and [cf462 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170ec; Value = 0xfce6fc24; PC = 0x8000c9c *)
mov r5 L0x200170ec;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171ec; Value = 0xff5f00e3; PC = 0x8000ca0 *)
mov r6 L0x200171ec;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172ec; Value = 0xfcb3fd54; PC = 0x8000ca4 *)
mov r7 L0x200172ec;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fec; Value = 0x03d9fdfc; PC = 0x8000ca8 *)
mov r4 L0x20016fec;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf014@sint32 : and [cf014 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf142@sint32 : and [cf142 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf270@sint32 : and [cf270 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf398@sint32 : and [cf398 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019808; PC = 0x8000d54 *)
mov L0x20019808 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019908; PC = 0x8000d58 *)
mov L0x20019908 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a08; PC = 0x8000d5c *)
mov L0x20019a08 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b08; PC = 0x8000d60 *)
mov L0x20019b08 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c08; PC = 0x8000d64 *)
mov L0x20019c08 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d08; PC = 0x8000d68 *)
mov L0x20019d08 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e08; PC = 0x8000d6c *)
mov L0x20019e08 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f08; PC = 0x8000d70 *)
mov L0x20019f08 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019408; PC = 0x8000d9c *)
mov L0x20019408 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019508; PC = 0x8000da0 *)
mov L0x20019508 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019608; PC = 0x8000da4 *)
mov L0x20019608 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019708; PC = 0x8000da8 *)
mov L0x20019708 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019108; PC = 0x8000dac *)
mov L0x20019108 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019208; PC = 0x8000db0 *)
mov L0x20019208 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019308; PC = 0x8000db4 *)
mov L0x20019308 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019008; PC = 0x8000db8 *)
mov L0x20019008 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001706e; Value = 0xfd3cfed9; PC = 0x8000b7c *)
mov r4 L0x2001706e;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001716e; Value = 0xfcaefcb1; PC = 0x8000b80 *)
mov r5 L0x2001716e;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001726e; Value = 0x011cfc48; PC = 0x8000b84 *)
mov r6 L0x2001726e;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001736e; Value = 0xfd72ff9a; PC = 0x8000b88 *)
mov r7 L0x2001736e;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf079@sint32 : and [cf079 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf207@sint32 : and [cf207 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf335@sint32 : and [cf335 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf463@sint32 : and [cf463 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170ee; Value = 0x029dfce6; PC = 0x8000c9c *)
mov r5 L0x200170ee;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171ee; Value = 0x00beff5f; PC = 0x8000ca0 *)
mov r6 L0x200171ee;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172ee; Value = 0xfdd4fcb3; PC = 0x8000ca4 *)
mov r7 L0x200172ee;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fee; Value = 0x037a03d9; PC = 0x8000ca8 *)
mov r4 L0x20016fee;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf015@sint32 : and [cf015 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf143@sint32 : and [cf143 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf271@sint32 : and [cf271 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf399@sint32 : and [cf399 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x2001980c; PC = 0x8000d54 *)
mov L0x2001980c r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x2001990c; PC = 0x8000d58 *)
mov L0x2001990c r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a0c; PC = 0x8000d5c *)
mov L0x20019a0c r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b0c; PC = 0x8000d60 *)
mov L0x20019b0c r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c0c; PC = 0x8000d64 *)
mov L0x20019c0c r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d0c; PC = 0x8000d68 *)
mov L0x20019d0c r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e0c; PC = 0x8000d6c *)
mov L0x20019e0c r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f0c; PC = 0x8000d70 *)
mov L0x20019f0c r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x2001940c; PC = 0x8000d9c *)
mov L0x2001940c r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x2001950c; PC = 0x8000da0 *)
mov L0x2001950c r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x2001960c; PC = 0x8000da4 *)
mov L0x2001960c r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x2001970c; PC = 0x8000da8 *)
mov L0x2001970c r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x2001910c; PC = 0x8000dac *)
mov L0x2001910c r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x2001920c; PC = 0x8000db0 *)
mov L0x2001920c r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x2001930c; PC = 0x8000db4 *)
mov L0x2001930c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x2001900c; PC = 0x8000db8 *)
mov L0x2001900c r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017070; Value = 0xffa8fd3c; PC = 0x8000b7c *)
mov r4 L0x20017070;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017170; Value = 0xfd24fcae; PC = 0x8000b80 *)
mov r5 L0x20017170;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017270; Value = 0x03c9011c; PC = 0x8000b84 *)
mov r6 L0x20017270;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017370; Value = 0xfc2afd72; PC = 0x8000b88 *)
mov r7 L0x20017370;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf080@sint32 : and [cf080 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf208@sint32 : and [cf208 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf336@sint32 : and [cf336 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf464@sint32 : and [cf464 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170f0; Value = 0x025d029d; PC = 0x8000c9c *)
mov r5 L0x200170f0;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171f0; Value = 0x01b300be; PC = 0x8000ca0 *)
mov r6 L0x200171f0;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172f0; Value = 0x02ecfdd4; PC = 0x8000ca4 *)
mov r7 L0x200172f0;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016ff0; Value = 0x0396037a; PC = 0x8000ca8 *)
mov r4 L0x20016ff0;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf016@sint32 : and [cf016 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf144@sint32 : and [cf144 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf272@sint32 : and [cf272 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf400@sint32 : and [cf400 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019810; PC = 0x8000d54 *)
mov L0x20019810 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019910; PC = 0x8000d58 *)
mov L0x20019910 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a10; PC = 0x8000d5c *)
mov L0x20019a10 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b10; PC = 0x8000d60 *)
mov L0x20019b10 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c10; PC = 0x8000d64 *)
mov L0x20019c10 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d10; PC = 0x8000d68 *)
mov L0x20019d10 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e10; PC = 0x8000d6c *)
mov L0x20019e10 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f10; PC = 0x8000d70 *)
mov L0x20019f10 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019410; PC = 0x8000d9c *)
mov L0x20019410 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019510; PC = 0x8000da0 *)
mov L0x20019510 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019610; PC = 0x8000da4 *)
mov L0x20019610 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019710; PC = 0x8000da8 *)
mov L0x20019710 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019110; PC = 0x8000dac *)
mov L0x20019110 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019210; PC = 0x8000db0 *)
mov L0x20019210 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019310; PC = 0x8000db4 *)
mov L0x20019310 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019010; PC = 0x8000db8 *)
mov L0x20019010 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017072; Value = 0xfd35ffa8; PC = 0x8000b7c *)
mov r4 L0x20017072;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017172; Value = 0xfe81fd24; PC = 0x8000b80 *)
mov r5 L0x20017172;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017272; Value = 0x00c503c9; PC = 0x8000b84 *)
mov r6 L0x20017272;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017372; Value = 0x0125fc2a; PC = 0x8000b88 *)
mov r7 L0x20017372;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf081@sint32 : and [cf081 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf209@sint32 : and [cf209 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf337@sint32 : and [cf337 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf465@sint32 : and [cf465 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170f2; Value = 0xfe44025d; PC = 0x8000c9c *)
mov r5 L0x200170f2;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171f2; Value = 0xfc1401b3; PC = 0x8000ca0 *)
mov r6 L0x200171f2;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172f2; Value = 0x00b502ec; PC = 0x8000ca4 *)
mov r7 L0x200172f2;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016ff2; Value = 0x02090396; PC = 0x8000ca8 *)
mov r4 L0x20016ff2;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf017@sint32 : and [cf017 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf145@sint32 : and [cf145 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf273@sint32 : and [cf273 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf401@sint32 : and [cf401 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019814; PC = 0x8000d54 *)
mov L0x20019814 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019914; PC = 0x8000d58 *)
mov L0x20019914 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a14; PC = 0x8000d5c *)
mov L0x20019a14 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b14; PC = 0x8000d60 *)
mov L0x20019b14 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c14; PC = 0x8000d64 *)
mov L0x20019c14 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d14; PC = 0x8000d68 *)
mov L0x20019d14 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e14; PC = 0x8000d6c *)
mov L0x20019e14 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f14; PC = 0x8000d70 *)
mov L0x20019f14 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019414; PC = 0x8000d9c *)
mov L0x20019414 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019514; PC = 0x8000da0 *)
mov L0x20019514 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019614; PC = 0x8000da4 *)
mov L0x20019614 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019714; PC = 0x8000da8 *)
mov L0x20019714 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019114; PC = 0x8000dac *)
mov L0x20019114 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019214; PC = 0x8000db0 *)
mov L0x20019214 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019314; PC = 0x8000db4 *)
mov L0x20019314 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019014; PC = 0x8000db8 *)
mov L0x20019014 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017074; Value = 0x02e5fd35; PC = 0x8000b7c *)
mov r4 L0x20017074;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017174; Value = 0xfd61fe81; PC = 0x8000b80 *)
mov r5 L0x20017174;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017274; Value = 0x018900c5; PC = 0x8000b84 *)
mov r6 L0x20017274;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017374; Value = 0xfd110125; PC = 0x8000b88 *)
mov r7 L0x20017374;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf082@sint32 : and [cf082 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf210@sint32 : and [cf210 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf338@sint32 : and [cf338 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf466@sint32 : and [cf466 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170f4; Value = 0xfc40fe44; PC = 0x8000c9c *)
mov r5 L0x200170f4;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171f4; Value = 0x017afc14; PC = 0x8000ca0 *)
mov r6 L0x200171f4;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172f4; Value = 0xfc1e00b5; PC = 0x8000ca4 *)
mov r7 L0x200172f4;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016ff4; Value = 0xfc2c0209; PC = 0x8000ca8 *)
mov r4 L0x20016ff4;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf018@sint32 : and [cf018 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf146@sint32 : and [cf146 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf274@sint32 : and [cf274 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf402@sint32 : and [cf402 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019818; PC = 0x8000d54 *)
mov L0x20019818 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019918; PC = 0x8000d58 *)
mov L0x20019918 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a18; PC = 0x8000d5c *)
mov L0x20019a18 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b18; PC = 0x8000d60 *)
mov L0x20019b18 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c18; PC = 0x8000d64 *)
mov L0x20019c18 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d18; PC = 0x8000d68 *)
mov L0x20019d18 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e18; PC = 0x8000d6c *)
mov L0x20019e18 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f18; PC = 0x8000d70 *)
mov L0x20019f18 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019418; PC = 0x8000d9c *)
mov L0x20019418 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019518; PC = 0x8000da0 *)
mov L0x20019518 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019618; PC = 0x8000da4 *)
mov L0x20019618 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019718; PC = 0x8000da8 *)
mov L0x20019718 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019118; PC = 0x8000dac *)
mov L0x20019118 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019218; PC = 0x8000db0 *)
mov L0x20019218 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019318; PC = 0x8000db4 *)
mov L0x20019318 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019018; PC = 0x8000db8 *)
mov L0x20019018 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017076; Value = 0x018f02e5; PC = 0x8000b7c *)
mov r4 L0x20017076;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017176; Value = 0xff8afd61; PC = 0x8000b80 *)
mov r5 L0x20017176;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017276; Value = 0xff690189; PC = 0x8000b84 *)
mov r6 L0x20017276;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017376; Value = 0x038cfd11; PC = 0x8000b88 *)
mov r7 L0x20017376;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf083@sint32 : and [cf083 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf211@sint32 : and [cf211 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf339@sint32 : and [cf339 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf467@sint32 : and [cf467 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170f6; Value = 0x028bfc40; PC = 0x8000c9c *)
mov r5 L0x200170f6;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171f6; Value = 0xff27017a; PC = 0x8000ca0 *)
mov r6 L0x200171f6;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172f6; Value = 0x0282fc1e; PC = 0x8000ca4 *)
mov r7 L0x200172f6;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016ff6; Value = 0x00a5fc2c; PC = 0x8000ca8 *)
mov r4 L0x20016ff6;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf019@sint32 : and [cf019 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf147@sint32 : and [cf147 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf275@sint32 : and [cf275 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf403@sint32 : and [cf403 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x2001981c; PC = 0x8000d54 *)
mov L0x2001981c r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x2001991c; PC = 0x8000d58 *)
mov L0x2001991c r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a1c; PC = 0x8000d5c *)
mov L0x20019a1c r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b1c; PC = 0x8000d60 *)
mov L0x20019b1c r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c1c; PC = 0x8000d64 *)
mov L0x20019c1c r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d1c; PC = 0x8000d68 *)
mov L0x20019d1c r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e1c; PC = 0x8000d6c *)
mov L0x20019e1c r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f1c; PC = 0x8000d70 *)
mov L0x20019f1c r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x2001941c; PC = 0x8000d9c *)
mov L0x2001941c r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x2001951c; PC = 0x8000da0 *)
mov L0x2001951c r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x2001961c; PC = 0x8000da4 *)
mov L0x2001961c r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x2001971c; PC = 0x8000da8 *)
mov L0x2001971c r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x2001911c; PC = 0x8000dac *)
mov L0x2001911c r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x2001921c; PC = 0x8000db0 *)
mov L0x2001921c r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x2001931c; PC = 0x8000db4 *)
mov L0x2001931c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x2001901c; PC = 0x8000db8 *)
mov L0x2001901c r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017078; Value = 0x022a018f; PC = 0x8000b7c *)
mov r4 L0x20017078;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017178; Value = 0x0078ff8a; PC = 0x8000b80 *)
mov r5 L0x20017178;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017278; Value = 0x00d2ff69; PC = 0x8000b84 *)
mov r6 L0x20017278;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017378; Value = 0x0081038c; PC = 0x8000b88 *)
mov r7 L0x20017378;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf084@sint32 : and [cf084 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf212@sint32 : and [cf212 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf340@sint32 : and [cf340 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf468@sint32 : and [cf468 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170f8; Value = 0xfc5a028b; PC = 0x8000c9c *)
mov r5 L0x200170f8;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171f8; Value = 0x0058ff27; PC = 0x8000ca0 *)
mov r6 L0x200171f8;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172f8; Value = 0xff7f0282; PC = 0x8000ca4 *)
mov r7 L0x200172f8;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016ff8; Value = 0xfd5700a5; PC = 0x8000ca8 *)
mov r4 L0x20016ff8;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf020@sint32 : and [cf020 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf148@sint32 : and [cf148 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf276@sint32 : and [cf276 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf404@sint32 : and [cf404 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019820; PC = 0x8000d54 *)
mov L0x20019820 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019920; PC = 0x8000d58 *)
mov L0x20019920 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a20; PC = 0x8000d5c *)
mov L0x20019a20 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b20; PC = 0x8000d60 *)
mov L0x20019b20 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c20; PC = 0x8000d64 *)
mov L0x20019c20 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d20; PC = 0x8000d68 *)
mov L0x20019d20 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e20; PC = 0x8000d6c *)
mov L0x20019e20 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f20; PC = 0x8000d70 *)
mov L0x20019f20 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019420; PC = 0x8000d9c *)
mov L0x20019420 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019520; PC = 0x8000da0 *)
mov L0x20019520 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019620; PC = 0x8000da4 *)
mov L0x20019620 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019720; PC = 0x8000da8 *)
mov L0x20019720 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019120; PC = 0x8000dac *)
mov L0x20019120 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019220; PC = 0x8000db0 *)
mov L0x20019220 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019320; PC = 0x8000db4 *)
mov L0x20019320 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019020; PC = 0x8000db8 *)
mov L0x20019020 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001707a; Value = 0xfdcd022a; PC = 0x8000b7c *)
mov r4 L0x2001707a;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001717a; Value = 0x03450078; PC = 0x8000b80 *)
mov r5 L0x2001717a;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001727a; Value = 0xffaa00d2; PC = 0x8000b84 *)
mov r6 L0x2001727a;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001737a; Value = 0x021a0081; PC = 0x8000b88 *)
mov r7 L0x2001737a;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf085@sint32 : and [cf085 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf213@sint32 : and [cf213 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf341@sint32 : and [cf341 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf469@sint32 : and [cf469 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170fa; Value = 0xfe23fc5a; PC = 0x8000c9c *)
mov r5 L0x200170fa;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171fa; Value = 0xfd270058; PC = 0x8000ca0 *)
mov r6 L0x200171fa;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172fa; Value = 0xfd56ff7f; PC = 0x8000ca4 *)
mov r7 L0x200172fa;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016ffa; Value = 0x0374fd57; PC = 0x8000ca8 *)
mov r4 L0x20016ffa;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf021@sint32 : and [cf021 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf149@sint32 : and [cf149 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf277@sint32 : and [cf277 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf405@sint32 : and [cf405 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019824; PC = 0x8000d54 *)
mov L0x20019824 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019924; PC = 0x8000d58 *)
mov L0x20019924 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a24; PC = 0x8000d5c *)
mov L0x20019a24 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b24; PC = 0x8000d60 *)
mov L0x20019b24 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c24; PC = 0x8000d64 *)
mov L0x20019c24 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d24; PC = 0x8000d68 *)
mov L0x20019d24 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e24; PC = 0x8000d6c *)
mov L0x20019e24 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f24; PC = 0x8000d70 *)
mov L0x20019f24 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019424; PC = 0x8000d9c *)
mov L0x20019424 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019524; PC = 0x8000da0 *)
mov L0x20019524 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019624; PC = 0x8000da4 *)
mov L0x20019624 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019724; PC = 0x8000da8 *)
mov L0x20019724 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019124; PC = 0x8000dac *)
mov L0x20019124 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019224; PC = 0x8000db0 *)
mov L0x20019224 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019324; PC = 0x8000db4 *)
mov L0x20019324 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019024; PC = 0x8000db8 *)
mov L0x20019024 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001707c; Value = 0x00c2fdcd; PC = 0x8000b7c *)
mov r4 L0x2001707c;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001717c; Value = 0xfde40345; PC = 0x8000b80 *)
mov r5 L0x2001717c;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001727c; Value = 0xfe32ffaa; PC = 0x8000b84 *)
mov r6 L0x2001727c;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001737c; Value = 0xfed4021a; PC = 0x8000b88 *)
mov r7 L0x2001737c;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf086@sint32 : and [cf086 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf214@sint32 : and [cf214 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf342@sint32 : and [cf342 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf470@sint32 : and [cf470 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170fc; Value = 0xfe5efe23; PC = 0x8000c9c *)
mov r5 L0x200170fc;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171fc; Value = 0x006bfd27; PC = 0x8000ca0 *)
mov r6 L0x200171fc;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172fc; Value = 0x0263fd56; PC = 0x8000ca4 *)
mov r7 L0x200172fc;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016ffc; Value = 0xff640374; PC = 0x8000ca8 *)
mov r4 L0x20016ffc;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf022@sint32 : and [cf022 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf150@sint32 : and [cf150 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf278@sint32 : and [cf278 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf406@sint32 : and [cf406 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019828; PC = 0x8000d54 *)
mov L0x20019828 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019928; PC = 0x8000d58 *)
mov L0x20019928 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a28; PC = 0x8000d5c *)
mov L0x20019a28 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b28; PC = 0x8000d60 *)
mov L0x20019b28 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c28; PC = 0x8000d64 *)
mov L0x20019c28 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d28; PC = 0x8000d68 *)
mov L0x20019d28 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e28; PC = 0x8000d6c *)
mov L0x20019e28 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f28; PC = 0x8000d70 *)
mov L0x20019f28 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019428; PC = 0x8000d9c *)
mov L0x20019428 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019528; PC = 0x8000da0 *)
mov L0x20019528 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019628; PC = 0x8000da4 *)
mov L0x20019628 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019728; PC = 0x8000da8 *)
mov L0x20019728 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019128; PC = 0x8000dac *)
mov L0x20019128 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019228; PC = 0x8000db0 *)
mov L0x20019228 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019328; PC = 0x8000db4 *)
mov L0x20019328 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019028; PC = 0x8000db8 *)
mov L0x20019028 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001707e; Value = 0x004000c2; PC = 0x8000b7c *)
mov r4 L0x2001707e;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001717e; Value = 0x023cfde4; PC = 0x8000b80 *)
mov r5 L0x2001717e;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001727e; Value = 0xfdcffe32; PC = 0x8000b84 *)
mov r6 L0x2001727e;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001737e; Value = 0xff2dfed4; PC = 0x8000b88 *)
mov r7 L0x2001737e;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf087@sint32 : and [cf087 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf215@sint32 : and [cf215 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf343@sint32 : and [cf343 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf471@sint32 : and [cf471 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170fe; Value = 0x01a9fe5e; PC = 0x8000c9c *)
mov r5 L0x200170fe;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171fe; Value = 0xff10006b; PC = 0x8000ca0 *)
mov r6 L0x200171fe;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172fe; Value = 0xfd7b0263; PC = 0x8000ca4 *)
mov r7 L0x200172fe;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016ffe; Value = 0x00c4ff64; PC = 0x8000ca8 *)
mov r4 L0x20016ffe;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf023@sint32 : and [cf023 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf151@sint32 : and [cf151 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf279@sint32 : and [cf279 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf407@sint32 : and [cf407 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x2001982c; PC = 0x8000d54 *)
mov L0x2001982c r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x2001992c; PC = 0x8000d58 *)
mov L0x2001992c r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a2c; PC = 0x8000d5c *)
mov L0x20019a2c r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b2c; PC = 0x8000d60 *)
mov L0x20019b2c r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c2c; PC = 0x8000d64 *)
mov L0x20019c2c r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d2c; PC = 0x8000d68 *)
mov L0x20019d2c r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e2c; PC = 0x8000d6c *)
mov L0x20019e2c r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f2c; PC = 0x8000d70 *)
mov L0x20019f2c r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x2001942c; PC = 0x8000d9c *)
mov L0x2001942c r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x2001952c; PC = 0x8000da0 *)
mov L0x2001952c r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x2001962c; PC = 0x8000da4 *)
mov L0x2001962c r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x2001972c; PC = 0x8000da8 *)
mov L0x2001972c r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x2001912c; PC = 0x8000dac *)
mov L0x2001912c r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x2001922c; PC = 0x8000db0 *)
mov L0x2001922c r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x2001932c; PC = 0x8000db4 *)
mov L0x2001932c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x2001902c; PC = 0x8000db8 *)
mov L0x2001902c r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017080; Value = 0x03c10040; PC = 0x8000b7c *)
mov r4 L0x20017080;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017180; Value = 0x0247023c; PC = 0x8000b80 *)
mov r5 L0x20017180;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017280; Value = 0xfd71fdcf; PC = 0x8000b84 *)
mov r6 L0x20017280;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017380; Value = 0xfd34ff2d; PC = 0x8000b88 *)
mov r7 L0x20017380;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf088@sint32 : and [cf088 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf216@sint32 : and [cf216 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf344@sint32 : and [cf344 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf472@sint32 : and [cf472 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017100; Value = 0x01ef01a9; PC = 0x8000c9c *)
mov r5 L0x20017100;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017200; Value = 0x02d8ff10; PC = 0x8000ca0 *)
mov r6 L0x20017200;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017300; Value = 0x0160fd7b; PC = 0x8000ca4 *)
mov r7 L0x20017300;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017000; Value = 0x03af00c4; PC = 0x8000ca8 *)
mov r4 L0x20017000;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf024@sint32 : and [cf024 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf152@sint32 : and [cf152 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf280@sint32 : and [cf280 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf408@sint32 : and [cf408 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019830; PC = 0x8000d54 *)
mov L0x20019830 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019930; PC = 0x8000d58 *)
mov L0x20019930 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a30; PC = 0x8000d5c *)
mov L0x20019a30 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b30; PC = 0x8000d60 *)
mov L0x20019b30 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c30; PC = 0x8000d64 *)
mov L0x20019c30 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d30; PC = 0x8000d68 *)
mov L0x20019d30 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e30; PC = 0x8000d6c *)
mov L0x20019e30 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f30; PC = 0x8000d70 *)
mov L0x20019f30 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019430; PC = 0x8000d9c *)
mov L0x20019430 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019530; PC = 0x8000da0 *)
mov L0x20019530 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019630; PC = 0x8000da4 *)
mov L0x20019630 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019730; PC = 0x8000da8 *)
mov L0x20019730 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019130; PC = 0x8000dac *)
mov L0x20019130 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019230; PC = 0x8000db0 *)
mov L0x20019230 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019330; PC = 0x8000db4 *)
mov L0x20019330 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019030; PC = 0x8000db8 *)
mov L0x20019030 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017082; Value = 0xfecf03c1; PC = 0x8000b7c *)
mov r4 L0x20017082;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017182; Value = 0x00780247; PC = 0x8000b80 *)
mov r5 L0x20017182;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017282; Value = 0xfdc8fd71; PC = 0x8000b84 *)
mov r6 L0x20017282;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017382; Value = 0xfcaafd34; PC = 0x8000b88 *)
mov r7 L0x20017382;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf089@sint32 : and [cf089 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf217@sint32 : and [cf217 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf345@sint32 : and [cf345 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf473@sint32 : and [cf473 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017102; Value = 0xfeec01ef; PC = 0x8000c9c *)
mov r5 L0x20017102;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017202; Value = 0x00b102d8; PC = 0x8000ca0 *)
mov r6 L0x20017202;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017302; Value = 0xfda70160; PC = 0x8000ca4 *)
mov r7 L0x20017302;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017002; Value = 0xfd1503af; PC = 0x8000ca8 *)
mov r4 L0x20017002;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf025@sint32 : and [cf025 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf153@sint32 : and [cf153 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf281@sint32 : and [cf281 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf409@sint32 : and [cf409 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019834; PC = 0x8000d54 *)
mov L0x20019834 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019934; PC = 0x8000d58 *)
mov L0x20019934 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a34; PC = 0x8000d5c *)
mov L0x20019a34 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b34; PC = 0x8000d60 *)
mov L0x20019b34 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c34; PC = 0x8000d64 *)
mov L0x20019c34 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d34; PC = 0x8000d68 *)
mov L0x20019d34 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e34; PC = 0x8000d6c *)
mov L0x20019e34 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f34; PC = 0x8000d70 *)
mov L0x20019f34 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019434; PC = 0x8000d9c *)
mov L0x20019434 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019534; PC = 0x8000da0 *)
mov L0x20019534 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019634; PC = 0x8000da4 *)
mov L0x20019634 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019734; PC = 0x8000da8 *)
mov L0x20019734 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019134; PC = 0x8000dac *)
mov L0x20019134 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019234; PC = 0x8000db0 *)
mov L0x20019234 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019334; PC = 0x8000db4 *)
mov L0x20019334 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019034; PC = 0x8000db8 *)
mov L0x20019034 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017084; Value = 0x0052fecf; PC = 0x8000b7c *)
mov r4 L0x20017084;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017184; Value = 0x02390078; PC = 0x8000b80 *)
mov r5 L0x20017184;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017284; Value = 0xfdd7fdc8; PC = 0x8000b84 *)
mov r6 L0x20017284;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017384; Value = 0x0368fcaa; PC = 0x8000b88 *)
mov r7 L0x20017384;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf090@sint32 : and [cf090 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf218@sint32 : and [cf218 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf346@sint32 : and [cf346 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf474@sint32 : and [cf474 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017104; Value = 0x023ffeec; PC = 0x8000c9c *)
mov r5 L0x20017104;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017204; Value = 0x011000b1; PC = 0x8000ca0 *)
mov r6 L0x20017204;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017304; Value = 0xfdaafda7; PC = 0x8000ca4 *)
mov r7 L0x20017304;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017004; Value = 0x0228fd15; PC = 0x8000ca8 *)
mov r4 L0x20017004;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf026@sint32 : and [cf026 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf154@sint32 : and [cf154 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf282@sint32 : and [cf282 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf410@sint32 : and [cf410 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019838; PC = 0x8000d54 *)
mov L0x20019838 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019938; PC = 0x8000d58 *)
mov L0x20019938 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a38; PC = 0x8000d5c *)
mov L0x20019a38 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b38; PC = 0x8000d60 *)
mov L0x20019b38 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c38; PC = 0x8000d64 *)
mov L0x20019c38 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d38; PC = 0x8000d68 *)
mov L0x20019d38 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e38; PC = 0x8000d6c *)
mov L0x20019e38 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f38; PC = 0x8000d70 *)
mov L0x20019f38 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019438; PC = 0x8000d9c *)
mov L0x20019438 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019538; PC = 0x8000da0 *)
mov L0x20019538 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019638; PC = 0x8000da4 *)
mov L0x20019638 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019738; PC = 0x8000da8 *)
mov L0x20019738 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019138; PC = 0x8000dac *)
mov L0x20019138 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019238; PC = 0x8000db0 *)
mov L0x20019238 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019338; PC = 0x8000db4 *)
mov L0x20019338 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019038; PC = 0x8000db8 *)
mov L0x20019038 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017086; Value = 0xfd710052; PC = 0x8000b7c *)
mov r4 L0x20017086;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017186; Value = 0x00470239; PC = 0x8000b80 *)
mov r5 L0x20017186;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017286; Value = 0x0214fdd7; PC = 0x8000b84 *)
mov r6 L0x20017286;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017386; Value = 0xfdac0368; PC = 0x8000b88 *)
mov r7 L0x20017386;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf091@sint32 : and [cf091 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf219@sint32 : and [cf219 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf347@sint32 : and [cf347 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf475@sint32 : and [cf475 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017106; Value = 0x0391023f; PC = 0x8000c9c *)
mov r5 L0x20017106;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017206; Value = 0xfd9a0110; PC = 0x8000ca0 *)
mov r6 L0x20017206;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017306; Value = 0xfd7ffdaa; PC = 0x8000ca4 *)
mov r7 L0x20017306;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017006; Value = 0x03a40228; PC = 0x8000ca8 *)
mov r4 L0x20017006;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf027@sint32 : and [cf027 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf155@sint32 : and [cf155 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf283@sint32 : and [cf283 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf411@sint32 : and [cf411 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x2001983c; PC = 0x8000d54 *)
mov L0x2001983c r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x2001993c; PC = 0x8000d58 *)
mov L0x2001993c r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a3c; PC = 0x8000d5c *)
mov L0x20019a3c r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b3c; PC = 0x8000d60 *)
mov L0x20019b3c r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c3c; PC = 0x8000d64 *)
mov L0x20019c3c r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d3c; PC = 0x8000d68 *)
mov L0x20019d3c r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e3c; PC = 0x8000d6c *)
mov L0x20019e3c r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f3c; PC = 0x8000d70 *)
mov L0x20019f3c r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x2001943c; PC = 0x8000d9c *)
mov L0x2001943c r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x2001953c; PC = 0x8000da0 *)
mov L0x2001953c r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x2001963c; PC = 0x8000da4 *)
mov L0x2001963c r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x2001973c; PC = 0x8000da8 *)
mov L0x2001973c r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x2001913c; PC = 0x8000dac *)
mov L0x2001913c r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x2001923c; PC = 0x8000db0 *)
mov L0x2001923c r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x2001933c; PC = 0x8000db4 *)
mov L0x2001933c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x2001903c; PC = 0x8000db8 *)
mov L0x2001903c r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017088; Value = 0x026afd71; PC = 0x8000b7c *)
mov r4 L0x20017088;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017188; Value = 0x00c70047; PC = 0x8000b80 *)
mov r5 L0x20017188;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017288; Value = 0x02510214; PC = 0x8000b84 *)
mov r6 L0x20017288;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017388; Value = 0x0276fdac; PC = 0x8000b88 *)
mov r7 L0x20017388;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf092@sint32 : and [cf092 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf220@sint32 : and [cf220 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf348@sint32 : and [cf348 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf476@sint32 : and [cf476 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017108; Value = 0xfd9c0391; PC = 0x8000c9c *)
mov r5 L0x20017108;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017208; Value = 0x00aefd9a; PC = 0x8000ca0 *)
mov r6 L0x20017208;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017308; Value = 0x00acfd7f; PC = 0x8000ca4 *)
mov r7 L0x20017308;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017008; Value = 0xfce903a4; PC = 0x8000ca8 *)
mov r4 L0x20017008;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf028@sint32 : and [cf028 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf156@sint32 : and [cf156 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf284@sint32 : and [cf284 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf412@sint32 : and [cf412 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019840; PC = 0x8000d54 *)
mov L0x20019840 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019940; PC = 0x8000d58 *)
mov L0x20019940 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a40; PC = 0x8000d5c *)
mov L0x20019a40 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b40; PC = 0x8000d60 *)
mov L0x20019b40 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c40; PC = 0x8000d64 *)
mov L0x20019c40 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d40; PC = 0x8000d68 *)
mov L0x20019d40 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e40; PC = 0x8000d6c *)
mov L0x20019e40 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f40; PC = 0x8000d70 *)
mov L0x20019f40 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019440; PC = 0x8000d9c *)
mov L0x20019440 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019540; PC = 0x8000da0 *)
mov L0x20019540 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019640; PC = 0x8000da4 *)
mov L0x20019640 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019740; PC = 0x8000da8 *)
mov L0x20019740 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019140; PC = 0x8000dac *)
mov L0x20019140 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019240; PC = 0x8000db0 *)
mov L0x20019240 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019340; PC = 0x8000db4 *)
mov L0x20019340 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019040; PC = 0x8000db8 *)
mov L0x20019040 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001708a; Value = 0xfe72026a; PC = 0x8000b7c *)
mov r4 L0x2001708a;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001718a; Value = 0x019d00c7; PC = 0x8000b80 *)
mov r5 L0x2001718a;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001728a; Value = 0x00a20251; PC = 0x8000b84 *)
mov r6 L0x2001728a;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001738a; Value = 0xfe720276; PC = 0x8000b88 *)
mov r7 L0x2001738a;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf093@sint32 : and [cf093 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf221@sint32 : and [cf221 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf349@sint32 : and [cf349 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf477@sint32 : and [cf477 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001710a; Value = 0xff7afd9c; PC = 0x8000c9c *)
mov r5 L0x2001710a;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001720a; Value = 0xfd0500ae; PC = 0x8000ca0 *)
mov r6 L0x2001720a;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001730a; Value = 0xfce100ac; PC = 0x8000ca4 *)
mov r7 L0x2001730a;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001700a; Value = 0x0257fce9; PC = 0x8000ca8 *)
mov r4 L0x2001700a;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf029@sint32 : and [cf029 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf157@sint32 : and [cf157 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf285@sint32 : and [cf285 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf413@sint32 : and [cf413 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019844; PC = 0x8000d54 *)
mov L0x20019844 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019944; PC = 0x8000d58 *)
mov L0x20019944 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a44; PC = 0x8000d5c *)
mov L0x20019a44 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b44; PC = 0x8000d60 *)
mov L0x20019b44 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c44; PC = 0x8000d64 *)
mov L0x20019c44 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d44; PC = 0x8000d68 *)
mov L0x20019d44 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e44; PC = 0x8000d6c *)
mov L0x20019e44 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f44; PC = 0x8000d70 *)
mov L0x20019f44 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019444; PC = 0x8000d9c *)
mov L0x20019444 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019544; PC = 0x8000da0 *)
mov L0x20019544 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019644; PC = 0x8000da4 *)
mov L0x20019644 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019744; PC = 0x8000da8 *)
mov L0x20019744 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019144; PC = 0x8000dac *)
mov L0x20019144 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019244; PC = 0x8000db0 *)
mov L0x20019244 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019344; PC = 0x8000db4 *)
mov L0x20019344 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019044; PC = 0x8000db8 *)
mov L0x20019044 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001708c; Value = 0xff29fe72; PC = 0x8000b7c *)
mov r4 L0x2001708c;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001718c; Value = 0xfdd9019d; PC = 0x8000b80 *)
mov r5 L0x2001718c;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001728c; Value = 0xfe0600a2; PC = 0x8000b84 *)
mov r6 L0x2001728c;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001738c; Value = 0xfc81fe72; PC = 0x8000b88 *)
mov r7 L0x2001738c;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf094@sint32 : and [cf094 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf222@sint32 : and [cf222 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf350@sint32 : and [cf350 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf478@sint32 : and [cf478 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001710c; Value = 0xfe41ff7a; PC = 0x8000c9c *)
mov r5 L0x2001710c;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001720c; Value = 0xfe08fd05; PC = 0x8000ca0 *)
mov r6 L0x2001720c;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001730c; Value = 0xfff0fce1; PC = 0x8000ca4 *)
mov r7 L0x2001730c;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001700c; Value = 0x03db0257; PC = 0x8000ca8 *)
mov r4 L0x2001700c;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf030@sint32 : and [cf030 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf158@sint32 : and [cf158 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf286@sint32 : and [cf286 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf414@sint32 : and [cf414 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019848; PC = 0x8000d54 *)
mov L0x20019848 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019948; PC = 0x8000d58 *)
mov L0x20019948 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a48; PC = 0x8000d5c *)
mov L0x20019a48 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b48; PC = 0x8000d60 *)
mov L0x20019b48 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c48; PC = 0x8000d64 *)
mov L0x20019c48 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d48; PC = 0x8000d68 *)
mov L0x20019d48 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e48; PC = 0x8000d6c *)
mov L0x20019e48 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f48; PC = 0x8000d70 *)
mov L0x20019f48 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019448; PC = 0x8000d9c *)
mov L0x20019448 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019548; PC = 0x8000da0 *)
mov L0x20019548 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019648; PC = 0x8000da4 *)
mov L0x20019648 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019748; PC = 0x8000da8 *)
mov L0x20019748 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019148; PC = 0x8000dac *)
mov L0x20019148 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019248; PC = 0x8000db0 *)
mov L0x20019248 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019348; PC = 0x8000db4 *)
mov L0x20019348 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019048; PC = 0x8000db8 *)
mov L0x20019048 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001708e; Value = 0xfc23ff29; PC = 0x8000b7c *)
mov r4 L0x2001708e;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001718e; Value = 0x004afdd9; PC = 0x8000b80 *)
mov r5 L0x2001718e;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001728e; Value = 0x028dfe06; PC = 0x8000b84 *)
mov r6 L0x2001728e;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001738e; Value = 0x0187fc81; PC = 0x8000b88 *)
mov r7 L0x2001738e;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf095@sint32 : and [cf095 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf223@sint32 : and [cf223 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf351@sint32 : and [cf351 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf479@sint32 : and [cf479 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001710e; Value = 0x0317fe41; PC = 0x8000c9c *)
mov r5 L0x2001710e;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001720e; Value = 0xff38fe08; PC = 0x8000ca0 *)
mov r6 L0x2001720e;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001730e; Value = 0xfce4fff0; PC = 0x8000ca4 *)
mov r7 L0x2001730e;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001700e; Value = 0xfe5e03db; PC = 0x8000ca8 *)
mov r4 L0x2001700e;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf031@sint32 : and [cf031 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf159@sint32 : and [cf159 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf287@sint32 : and [cf287 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf415@sint32 : and [cf415 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x2001984c; PC = 0x8000d54 *)
mov L0x2001984c r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x2001994c; PC = 0x8000d58 *)
mov L0x2001994c r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a4c; PC = 0x8000d5c *)
mov L0x20019a4c r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b4c; PC = 0x8000d60 *)
mov L0x20019b4c r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c4c; PC = 0x8000d64 *)
mov L0x20019c4c r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d4c; PC = 0x8000d68 *)
mov L0x20019d4c r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e4c; PC = 0x8000d6c *)
mov L0x20019e4c r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f4c; PC = 0x8000d70 *)
mov L0x20019f4c r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x2001944c; PC = 0x8000d9c *)
mov L0x2001944c r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x2001954c; PC = 0x8000da0 *)
mov L0x2001954c r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x2001964c; PC = 0x8000da4 *)
mov L0x2001964c r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x2001974c; PC = 0x8000da8 *)
mov L0x2001974c r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x2001914c; PC = 0x8000dac *)
mov L0x2001914c r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x2001924c; PC = 0x8000db0 *)
mov L0x2001924c r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x2001934c; PC = 0x8000db4 *)
mov L0x2001934c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x2001904c; PC = 0x8000db8 *)
mov L0x2001904c r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017090; Value = 0x01b0fc23; PC = 0x8000b7c *)
mov r4 L0x20017090;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017190; Value = 0x03af004a; PC = 0x8000b80 *)
mov r5 L0x20017190;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017290; Value = 0xff1d028d; PC = 0x8000b84 *)
mov r6 L0x20017290;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017390; Value = 0xfc8b0187; PC = 0x8000b88 *)
mov r7 L0x20017390;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf096@sint32 : and [cf096 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf224@sint32 : and [cf224 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf352@sint32 : and [cf352 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf480@sint32 : and [cf480 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017110; Value = 0xff180317; PC = 0x8000c9c *)
mov r5 L0x20017110;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017210; Value = 0x0184ff38; PC = 0x8000ca0 *)
mov r6 L0x20017210;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017310; Value = 0x0138fce4; PC = 0x8000ca4 *)
mov r7 L0x20017310;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017010; Value = 0xfc20fe5e; PC = 0x8000ca8 *)
mov r4 L0x20017010;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf032@sint32 : and [cf032 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf160@sint32 : and [cf160 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf288@sint32 : and [cf288 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf416@sint32 : and [cf416 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019850; PC = 0x8000d54 *)
mov L0x20019850 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019950; PC = 0x8000d58 *)
mov L0x20019950 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a50; PC = 0x8000d5c *)
mov L0x20019a50 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b50; PC = 0x8000d60 *)
mov L0x20019b50 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c50; PC = 0x8000d64 *)
mov L0x20019c50 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d50; PC = 0x8000d68 *)
mov L0x20019d50 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e50; PC = 0x8000d6c *)
mov L0x20019e50 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f50; PC = 0x8000d70 *)
mov L0x20019f50 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019450; PC = 0x8000d9c *)
mov L0x20019450 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019550; PC = 0x8000da0 *)
mov L0x20019550 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019650; PC = 0x8000da4 *)
mov L0x20019650 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019750; PC = 0x8000da8 *)
mov L0x20019750 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019150; PC = 0x8000dac *)
mov L0x20019150 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019250; PC = 0x8000db0 *)
mov L0x20019250 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019350; PC = 0x8000db4 *)
mov L0x20019350 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019050; PC = 0x8000db8 *)
mov L0x20019050 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017092; Value = 0xfe5b01b0; PC = 0x8000b7c *)
mov r4 L0x20017092;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017192; Value = 0x016103af; PC = 0x8000b80 *)
mov r5 L0x20017192;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017292; Value = 0xffcdff1d; PC = 0x8000b84 *)
mov r6 L0x20017292;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017392; Value = 0xff2bfc8b; PC = 0x8000b88 *)
mov r7 L0x20017392;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf097@sint32 : and [cf097 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf225@sint32 : and [cf225 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf353@sint32 : and [cf353 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf481@sint32 : and [cf481 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017112; Value = 0xfd72ff18; PC = 0x8000c9c *)
mov r5 L0x20017112;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017212; Value = 0xffd60184; PC = 0x8000ca0 *)
mov r6 L0x20017212;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017312; Value = 0x004d0138; PC = 0x8000ca4 *)
mov r7 L0x20017312;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017012; Value = 0x00fbfc20; PC = 0x8000ca8 *)
mov r4 L0x20017012;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf033@sint32 : and [cf033 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf161@sint32 : and [cf161 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf289@sint32 : and [cf289 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf417@sint32 : and [cf417 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019854; PC = 0x8000d54 *)
mov L0x20019854 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019954; PC = 0x8000d58 *)
mov L0x20019954 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a54; PC = 0x8000d5c *)
mov L0x20019a54 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b54; PC = 0x8000d60 *)
mov L0x20019b54 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c54; PC = 0x8000d64 *)
mov L0x20019c54 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d54; PC = 0x8000d68 *)
mov L0x20019d54 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e54; PC = 0x8000d6c *)
mov L0x20019e54 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f54; PC = 0x8000d70 *)
mov L0x20019f54 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019454; PC = 0x8000d9c *)
mov L0x20019454 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019554; PC = 0x8000da0 *)
mov L0x20019554 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019654; PC = 0x8000da4 *)
mov L0x20019654 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019754; PC = 0x8000da8 *)
mov L0x20019754 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019154; PC = 0x8000dac *)
mov L0x20019154 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019254; PC = 0x8000db0 *)
mov L0x20019254 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019354; PC = 0x8000db4 *)
mov L0x20019354 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019054; PC = 0x8000db8 *)
mov L0x20019054 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017094; Value = 0x011bfe5b; PC = 0x8000b7c *)
mov r4 L0x20017094;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017194; Value = 0xfc170161; PC = 0x8000b80 *)
mov r5 L0x20017194;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017294; Value = 0x03edffcd; PC = 0x8000b84 *)
mov r6 L0x20017294;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017394; Value = 0xff5aff2b; PC = 0x8000b88 *)
mov r7 L0x20017394;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf098@sint32 : and [cf098 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf226@sint32 : and [cf226 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf354@sint32 : and [cf354 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf482@sint32 : and [cf482 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017114; Value = 0x02ebfd72; PC = 0x8000c9c *)
mov r5 L0x20017114;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017214; Value = 0xfd0affd6; PC = 0x8000ca0 *)
mov r6 L0x20017214;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017314; Value = 0xfc66004d; PC = 0x8000ca4 *)
mov r7 L0x20017314;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017014; Value = 0xfc3800fb; PC = 0x8000ca8 *)
mov r4 L0x20017014;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf034@sint32 : and [cf034 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf162@sint32 : and [cf162 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf290@sint32 : and [cf290 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf418@sint32 : and [cf418 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019858; PC = 0x8000d54 *)
mov L0x20019858 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019958; PC = 0x8000d58 *)
mov L0x20019958 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a58; PC = 0x8000d5c *)
mov L0x20019a58 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b58; PC = 0x8000d60 *)
mov L0x20019b58 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c58; PC = 0x8000d64 *)
mov L0x20019c58 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d58; PC = 0x8000d68 *)
mov L0x20019d58 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e58; PC = 0x8000d6c *)
mov L0x20019e58 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f58; PC = 0x8000d70 *)
mov L0x20019f58 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019458; PC = 0x8000d9c *)
mov L0x20019458 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019558; PC = 0x8000da0 *)
mov L0x20019558 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019658; PC = 0x8000da4 *)
mov L0x20019658 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019758; PC = 0x8000da8 *)
mov L0x20019758 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019158; PC = 0x8000dac *)
mov L0x20019158 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019258; PC = 0x8000db0 *)
mov L0x20019258 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019358; PC = 0x8000db4 *)
mov L0x20019358 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019058; PC = 0x8000db8 *)
mov L0x20019058 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017096; Value = 0xfda3011b; PC = 0x8000b7c *)
mov r4 L0x20017096;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017196; Value = 0xff3bfc17; PC = 0x8000b80 *)
mov r5 L0x20017196;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017296; Value = 0x01c903ed; PC = 0x8000b84 *)
mov r6 L0x20017296;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017396; Value = 0x032fff5a; PC = 0x8000b88 *)
mov r7 L0x20017396;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf099@sint32 : and [cf099 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf227@sint32 : and [cf227 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf355@sint32 : and [cf355 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf483@sint32 : and [cf483 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017116; Value = 0x020602eb; PC = 0x8000c9c *)
mov r5 L0x20017116;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017216; Value = 0xfd87fd0a; PC = 0x8000ca0 *)
mov r6 L0x20017216;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017316; Value = 0x023afc66; PC = 0x8000ca4 *)
mov r7 L0x20017316;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017016; Value = 0x01a8fc38; PC = 0x8000ca8 *)
mov r4 L0x20017016;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf035@sint32 : and [cf035 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf163@sint32 : and [cf163 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf291@sint32 : and [cf291 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf419@sint32 : and [cf419 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x2001985c; PC = 0x8000d54 *)
mov L0x2001985c r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x2001995c; PC = 0x8000d58 *)
mov L0x2001995c r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a5c; PC = 0x8000d5c *)
mov L0x20019a5c r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b5c; PC = 0x8000d60 *)
mov L0x20019b5c r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c5c; PC = 0x8000d64 *)
mov L0x20019c5c r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d5c; PC = 0x8000d68 *)
mov L0x20019d5c r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e5c; PC = 0x8000d6c *)
mov L0x20019e5c r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f5c; PC = 0x8000d70 *)
mov L0x20019f5c r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x2001945c; PC = 0x8000d9c *)
mov L0x2001945c r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x2001955c; PC = 0x8000da0 *)
mov L0x2001955c r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x2001965c; PC = 0x8000da4 *)
mov L0x2001965c r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x2001975c; PC = 0x8000da8 *)
mov L0x2001975c r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x2001915c; PC = 0x8000dac *)
mov L0x2001915c r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x2001925c; PC = 0x8000db0 *)
mov L0x2001925c r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x2001935c; PC = 0x8000db4 *)
mov L0x2001935c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x2001905c; PC = 0x8000db8 *)
mov L0x2001905c r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017098; Value = 0x032afda3; PC = 0x8000b7c *)
mov r4 L0x20017098;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017198; Value = 0xffbdff3b; PC = 0x8000b80 *)
mov r5 L0x20017198;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017298; Value = 0x03fa01c9; PC = 0x8000b84 *)
mov r6 L0x20017298;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017398; Value = 0xfe26032f; PC = 0x8000b88 *)
mov r7 L0x20017398;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf100@sint32 : and [cf100 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf228@sint32 : and [cf228 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf356@sint32 : and [cf356 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf484@sint32 : and [cf484 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017118; Value = 0xfff20206; PC = 0x8000c9c *)
mov r5 L0x20017118;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017218; Value = 0xfe55fd87; PC = 0x8000ca0 *)
mov r6 L0x20017218;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017318; Value = 0x031e023a; PC = 0x8000ca4 *)
mov r7 L0x20017318;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017018; Value = 0xfc4e01a8; PC = 0x8000ca8 *)
mov r4 L0x20017018;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf036@sint32 : and [cf036 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf164@sint32 : and [cf164 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf292@sint32 : and [cf292 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf420@sint32 : and [cf420 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019860; PC = 0x8000d54 *)
mov L0x20019860 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019960; PC = 0x8000d58 *)
mov L0x20019960 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a60; PC = 0x8000d5c *)
mov L0x20019a60 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b60; PC = 0x8000d60 *)
mov L0x20019b60 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c60; PC = 0x8000d64 *)
mov L0x20019c60 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d60; PC = 0x8000d68 *)
mov L0x20019d60 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e60; PC = 0x8000d6c *)
mov L0x20019e60 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f60; PC = 0x8000d70 *)
mov L0x20019f60 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019460; PC = 0x8000d9c *)
mov L0x20019460 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019560; PC = 0x8000da0 *)
mov L0x20019560 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019660; PC = 0x8000da4 *)
mov L0x20019660 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019760; PC = 0x8000da8 *)
mov L0x20019760 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019160; PC = 0x8000dac *)
mov L0x20019160 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019260; PC = 0x8000db0 *)
mov L0x20019260 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019360; PC = 0x8000db4 *)
mov L0x20019360 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019060; PC = 0x8000db8 *)
mov L0x20019060 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001709a; Value = 0x0282032a; PC = 0x8000b7c *)
mov r4 L0x2001709a;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001719a; Value = 0xfe6affbd; PC = 0x8000b80 *)
mov r5 L0x2001719a;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001729a; Value = 0x03c903fa; PC = 0x8000b84 *)
mov r6 L0x2001729a;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001739a; Value = 0xfdbefe26; PC = 0x8000b88 *)
mov r7 L0x2001739a;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf101@sint32 : and [cf101 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf229@sint32 : and [cf229 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf357@sint32 : and [cf357 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf485@sint32 : and [cf485 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001711a; Value = 0x0391fff2; PC = 0x8000c9c *)
mov r5 L0x2001711a;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001721a; Value = 0xfc7afe55; PC = 0x8000ca0 *)
mov r6 L0x2001721a;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001731a; Value = 0x01b7031e; PC = 0x8000ca4 *)
mov r7 L0x2001731a;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001701a; Value = 0x03a6fc4e; PC = 0x8000ca8 *)
mov r4 L0x2001701a;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf037@sint32 : and [cf037 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf165@sint32 : and [cf165 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf293@sint32 : and [cf293 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf421@sint32 : and [cf421 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019864; PC = 0x8000d54 *)
mov L0x20019864 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019964; PC = 0x8000d58 *)
mov L0x20019964 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a64; PC = 0x8000d5c *)
mov L0x20019a64 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b64; PC = 0x8000d60 *)
mov L0x20019b64 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c64; PC = 0x8000d64 *)
mov L0x20019c64 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d64; PC = 0x8000d68 *)
mov L0x20019d64 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e64; PC = 0x8000d6c *)
mov L0x20019e64 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f64; PC = 0x8000d70 *)
mov L0x20019f64 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019464; PC = 0x8000d9c *)
mov L0x20019464 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019564; PC = 0x8000da0 *)
mov L0x20019564 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019664; PC = 0x8000da4 *)
mov L0x20019664 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019764; PC = 0x8000da8 *)
mov L0x20019764 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019164; PC = 0x8000dac *)
mov L0x20019164 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019264; PC = 0x8000db0 *)
mov L0x20019264 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019364; PC = 0x8000db4 *)
mov L0x20019364 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019064; PC = 0x8000db8 *)
mov L0x20019064 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001709c; Value = 0x00880282; PC = 0x8000b7c *)
mov r4 L0x2001709c;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001719c; Value = 0xfcf5fe6a; PC = 0x8000b80 *)
mov r5 L0x2001719c;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001729c; Value = 0x021803c9; PC = 0x8000b84 *)
mov r6 L0x2001729c;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001739c; Value = 0xffdefdbe; PC = 0x8000b88 *)
mov r7 L0x2001739c;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf102@sint32 : and [cf102 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf230@sint32 : and [cf230 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf358@sint32 : and [cf358 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf486@sint32 : and [cf486 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001711c; Value = 0x00ee0391; PC = 0x8000c9c *)
mov r5 L0x2001711c;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001721c; Value = 0xffa3fc7a; PC = 0x8000ca0 *)
mov r6 L0x2001721c;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001731c; Value = 0xff5001b7; PC = 0x8000ca4 *)
mov r7 L0x2001731c;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001701c; Value = 0x031403a6; PC = 0x8000ca8 *)
mov r4 L0x2001701c;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf038@sint32 : and [cf038 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf166@sint32 : and [cf166 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf294@sint32 : and [cf294 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf422@sint32 : and [cf422 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019868; PC = 0x8000d54 *)
mov L0x20019868 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019968; PC = 0x8000d58 *)
mov L0x20019968 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a68; PC = 0x8000d5c *)
mov L0x20019a68 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b68; PC = 0x8000d60 *)
mov L0x20019b68 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c68; PC = 0x8000d64 *)
mov L0x20019c68 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d68; PC = 0x8000d68 *)
mov L0x20019d68 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e68; PC = 0x8000d6c *)
mov L0x20019e68 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f68; PC = 0x8000d70 *)
mov L0x20019f68 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019468; PC = 0x8000d9c *)
mov L0x20019468 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019568; PC = 0x8000da0 *)
mov L0x20019568 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019668; PC = 0x8000da4 *)
mov L0x20019668 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019768; PC = 0x8000da8 *)
mov L0x20019768 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019168; PC = 0x8000dac *)
mov L0x20019168 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019268; PC = 0x8000db0 *)
mov L0x20019268 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019368; PC = 0x8000db4 *)
mov L0x20019368 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019068; PC = 0x8000db8 *)
mov L0x20019068 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001709e; Value = 0xff440088; PC = 0x8000b7c *)
mov r4 L0x2001709e;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001719e; Value = 0x03d0fcf5; PC = 0x8000b80 *)
mov r5 L0x2001719e;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001729e; Value = 0xff570218; PC = 0x8000b84 *)
mov r6 L0x2001729e;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001739e; Value = 0xfeecffde; PC = 0x8000b88 *)
mov r7 L0x2001739e;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf103@sint32 : and [cf103 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf231@sint32 : and [cf231 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf359@sint32 : and [cf359 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf487@sint32 : and [cf487 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001711e; Value = 0xfeb800ee; PC = 0x8000c9c *)
mov r5 L0x2001711e;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001721e; Value = 0x0275ffa3; PC = 0x8000ca0 *)
mov r6 L0x2001721e;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001731e; Value = 0x00aaff50; PC = 0x8000ca4 *)
mov r7 L0x2001731e;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001701e; Value = 0x01930314; PC = 0x8000ca8 *)
mov r4 L0x2001701e;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf039@sint32 : and [cf039 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf167@sint32 : and [cf167 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf295@sint32 : and [cf295 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf423@sint32 : and [cf423 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x2001986c; PC = 0x8000d54 *)
mov L0x2001986c r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x2001996c; PC = 0x8000d58 *)
mov L0x2001996c r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a6c; PC = 0x8000d5c *)
mov L0x20019a6c r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b6c; PC = 0x8000d60 *)
mov L0x20019b6c r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c6c; PC = 0x8000d64 *)
mov L0x20019c6c r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d6c; PC = 0x8000d68 *)
mov L0x20019d6c r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e6c; PC = 0x8000d6c *)
mov L0x20019e6c r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f6c; PC = 0x8000d70 *)
mov L0x20019f6c r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x2001946c; PC = 0x8000d9c *)
mov L0x2001946c r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x2001956c; PC = 0x8000da0 *)
mov L0x2001956c r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x2001966c; PC = 0x8000da4 *)
mov L0x2001966c r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x2001976c; PC = 0x8000da8 *)
mov L0x2001976c r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x2001916c; PC = 0x8000dac *)
mov L0x2001916c r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x2001926c; PC = 0x8000db0 *)
mov L0x2001926c r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x2001936c; PC = 0x8000db4 *)
mov L0x2001936c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x2001906c; PC = 0x8000db8 *)
mov L0x2001906c r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170a0; Value = 0x0019ff44; PC = 0x8000b7c *)
mov r4 L0x200170a0;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171a0; Value = 0x014403d0; PC = 0x8000b80 *)
mov r5 L0x200171a0;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172a0; Value = 0xff05ff57; PC = 0x8000b84 *)
mov r6 L0x200172a0;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173a0; Value = 0x02bffeec; PC = 0x8000b88 *)
mov r7 L0x200173a0;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf104@sint32 : and [cf104 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf232@sint32 : and [cf232 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf360@sint32 : and [cf360 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf488@sint32 : and [cf488 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017120; Value = 0xfe6cfeb8; PC = 0x8000c9c *)
mov r5 L0x20017120;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017220; Value = 0xfe350275; PC = 0x8000ca0 *)
mov r6 L0x20017220;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017320; Value = 0x036700aa; PC = 0x8000ca4 *)
mov r7 L0x20017320;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017020; Value = 0x03250193; PC = 0x8000ca8 *)
mov r4 L0x20017020;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf040@sint32 : and [cf040 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf168@sint32 : and [cf168 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf296@sint32 : and [cf296 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf424@sint32 : and [cf424 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019870; PC = 0x8000d54 *)
mov L0x20019870 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019970; PC = 0x8000d58 *)
mov L0x20019970 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a70; PC = 0x8000d5c *)
mov L0x20019a70 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b70; PC = 0x8000d60 *)
mov L0x20019b70 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c70; PC = 0x8000d64 *)
mov L0x20019c70 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d70; PC = 0x8000d68 *)
mov L0x20019d70 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e70; PC = 0x8000d6c *)
mov L0x20019e70 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f70; PC = 0x8000d70 *)
mov L0x20019f70 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019470; PC = 0x8000d9c *)
mov L0x20019470 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019570; PC = 0x8000da0 *)
mov L0x20019570 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019670; PC = 0x8000da4 *)
mov L0x20019670 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019770; PC = 0x8000da8 *)
mov L0x20019770 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019170; PC = 0x8000dac *)
mov L0x20019170 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019270; PC = 0x8000db0 *)
mov L0x20019270 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019370; PC = 0x8000db4 *)
mov L0x20019370 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019070; PC = 0x8000db8 *)
mov L0x20019070 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170a2; Value = 0xff0e0019; PC = 0x8000b7c *)
mov r4 L0x200170a2;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171a2; Value = 0xff5b0144; PC = 0x8000b80 *)
mov r5 L0x200171a2;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172a2; Value = 0xff67ff05; PC = 0x8000b84 *)
mov r6 L0x200172a2;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173a2; Value = 0xfe3802bf; PC = 0x8000b88 *)
mov r7 L0x200173a2;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf105@sint32 : and [cf105 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf233@sint32 : and [cf233 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf361@sint32 : and [cf361 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf489@sint32 : and [cf489 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017122; Value = 0x02befe6c; PC = 0x8000c9c *)
mov r5 L0x20017122;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017222; Value = 0xfebffe35; PC = 0x8000ca0 *)
mov r6 L0x20017222;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017322; Value = 0x03380367; PC = 0x8000ca4 *)
mov r7 L0x20017322;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017022; Value = 0x00560325; PC = 0x8000ca8 *)
mov r4 L0x20017022;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf041@sint32 : and [cf041 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf169@sint32 : and [cf169 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf297@sint32 : and [cf297 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf425@sint32 : and [cf425 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019874; PC = 0x8000d54 *)
mov L0x20019874 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019974; PC = 0x8000d58 *)
mov L0x20019974 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a74; PC = 0x8000d5c *)
mov L0x20019a74 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b74; PC = 0x8000d60 *)
mov L0x20019b74 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c74; PC = 0x8000d64 *)
mov L0x20019c74 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d74; PC = 0x8000d68 *)
mov L0x20019d74 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e74; PC = 0x8000d6c *)
mov L0x20019e74 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f74; PC = 0x8000d70 *)
mov L0x20019f74 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019474; PC = 0x8000d9c *)
mov L0x20019474 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019574; PC = 0x8000da0 *)
mov L0x20019574 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019674; PC = 0x8000da4 *)
mov L0x20019674 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019774; PC = 0x8000da8 *)
mov L0x20019774 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019174; PC = 0x8000dac *)
mov L0x20019174 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019274; PC = 0x8000db0 *)
mov L0x20019274 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019374; PC = 0x8000db4 *)
mov L0x20019374 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019074; PC = 0x8000db8 *)
mov L0x20019074 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170a4; Value = 0x0230ff0e; PC = 0x8000b7c *)
mov r4 L0x200170a4;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171a4; Value = 0xfd91ff5b; PC = 0x8000b80 *)
mov r5 L0x200171a4;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172a4; Value = 0x00fdff67; PC = 0x8000b84 *)
mov r6 L0x200172a4;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173a4; Value = 0xfcadfe38; PC = 0x8000b88 *)
mov r7 L0x200173a4;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf106@sint32 : and [cf106 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf234@sint32 : and [cf234 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf362@sint32 : and [cf362 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf490@sint32 : and [cf490 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017124; Value = 0xfdd902be; PC = 0x8000c9c *)
mov r5 L0x20017124;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017224; Value = 0xfe3efebf; PC = 0x8000ca0 *)
mov r6 L0x20017224;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017324; Value = 0x02e00338; PC = 0x8000ca4 *)
mov r7 L0x20017324;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017024; Value = 0x02240056; PC = 0x8000ca8 *)
mov r4 L0x20017024;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf042@sint32 : and [cf042 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf170@sint32 : and [cf170 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf298@sint32 : and [cf298 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf426@sint32 : and [cf426 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019878; PC = 0x8000d54 *)
mov L0x20019878 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019978; PC = 0x8000d58 *)
mov L0x20019978 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a78; PC = 0x8000d5c *)
mov L0x20019a78 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b78; PC = 0x8000d60 *)
mov L0x20019b78 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c78; PC = 0x8000d64 *)
mov L0x20019c78 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d78; PC = 0x8000d68 *)
mov L0x20019d78 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e78; PC = 0x8000d6c *)
mov L0x20019e78 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f78; PC = 0x8000d70 *)
mov L0x20019f78 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019478; PC = 0x8000d9c *)
mov L0x20019478 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019578; PC = 0x8000da0 *)
mov L0x20019578 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019678; PC = 0x8000da4 *)
mov L0x20019678 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019778; PC = 0x8000da8 *)
mov L0x20019778 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019178; PC = 0x8000dac *)
mov L0x20019178 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019278; PC = 0x8000db0 *)
mov L0x20019278 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019378; PC = 0x8000db4 *)
mov L0x20019378 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019078; PC = 0x8000db8 *)
mov L0x20019078 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170a6; Value = 0xfdf20230; PC = 0x8000b7c *)
mov r4 L0x200170a6;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171a6; Value = 0xfdcdfd91; PC = 0x8000b80 *)
mov r5 L0x200171a6;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172a6; Value = 0xfd8900fd; PC = 0x8000b84 *)
mov r6 L0x200172a6;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173a6; Value = 0x03b0fcad; PC = 0x8000b88 *)
mov r7 L0x200173a6;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf107@sint32 : and [cf107 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf235@sint32 : and [cf235 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf363@sint32 : and [cf363 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf491@sint32 : and [cf491 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017126; Value = 0xff34fdd9; PC = 0x8000c9c *)
mov r5 L0x20017126;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017226; Value = 0xfc71fe3e; PC = 0x8000ca0 *)
mov r6 L0x20017226;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017326; Value = 0xffee02e0; PC = 0x8000ca4 *)
mov r7 L0x20017326;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017026; Value = 0x02440224; PC = 0x8000ca8 *)
mov r4 L0x20017026;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf043@sint32 : and [cf043 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf171@sint32 : and [cf171 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf299@sint32 : and [cf299 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf427@sint32 : and [cf427 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x2001987c; PC = 0x8000d54 *)
mov L0x2001987c r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x2001997c; PC = 0x8000d58 *)
mov L0x2001997c r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a7c; PC = 0x8000d5c *)
mov L0x20019a7c r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b7c; PC = 0x8000d60 *)
mov L0x20019b7c r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c7c; PC = 0x8000d64 *)
mov L0x20019c7c r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d7c; PC = 0x8000d68 *)
mov L0x20019d7c r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e7c; PC = 0x8000d6c *)
mov L0x20019e7c r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f7c; PC = 0x8000d70 *)
mov L0x20019f7c r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x2001947c; PC = 0x8000d9c *)
mov L0x2001947c r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x2001957c; PC = 0x8000da0 *)
mov L0x2001957c r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x2001967c; PC = 0x8000da4 *)
mov L0x2001967c r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x2001977c; PC = 0x8000da8 *)
mov L0x2001977c r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x2001917c; PC = 0x8000dac *)
mov L0x2001917c r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x2001927c; PC = 0x8000db0 *)
mov L0x2001927c r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x2001937c; PC = 0x8000db4 *)
mov L0x2001937c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x2001907c; PC = 0x8000db8 *)
mov L0x2001907c r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170a8; Value = 0x016bfdf2; PC = 0x8000b7c *)
mov r4 L0x200170a8;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171a8; Value = 0x035ffdcd; PC = 0x8000b80 *)
mov r5 L0x200171a8;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172a8; Value = 0x0394fd89; PC = 0x8000b84 *)
mov r6 L0x200172a8;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173a8; Value = 0xfedf03b0; PC = 0x8000b88 *)
mov r7 L0x200173a8;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf108@sint32 : and [cf108 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf236@sint32 : and [cf236 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf364@sint32 : and [cf364 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf492@sint32 : and [cf492 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017128; Value = 0x01f7ff34; PC = 0x8000c9c *)
mov r5 L0x20017128;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017228; Value = 0x026cfc71; PC = 0x8000ca0 *)
mov r6 L0x20017228;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017328; Value = 0x00b6ffee; PC = 0x8000ca4 *)
mov r7 L0x20017328;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017028; Value = 0xfedf0244; PC = 0x8000ca8 *)
mov r4 L0x20017028;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf044@sint32 : and [cf044 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf172@sint32 : and [cf172 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf300@sint32 : and [cf300 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf428@sint32 : and [cf428 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019880; PC = 0x8000d54 *)
mov L0x20019880 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019980; PC = 0x8000d58 *)
mov L0x20019980 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a80; PC = 0x8000d5c *)
mov L0x20019a80 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b80; PC = 0x8000d60 *)
mov L0x20019b80 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c80; PC = 0x8000d64 *)
mov L0x20019c80 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d80; PC = 0x8000d68 *)
mov L0x20019d80 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e80; PC = 0x8000d6c *)
mov L0x20019e80 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f80; PC = 0x8000d70 *)
mov L0x20019f80 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019480; PC = 0x8000d9c *)
mov L0x20019480 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019580; PC = 0x8000da0 *)
mov L0x20019580 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019680; PC = 0x8000da4 *)
mov L0x20019680 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019780; PC = 0x8000da8 *)
mov L0x20019780 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019180; PC = 0x8000dac *)
mov L0x20019180 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019280; PC = 0x8000db0 *)
mov L0x20019280 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019380; PC = 0x8000db4 *)
mov L0x20019380 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019080; PC = 0x8000db8 *)
mov L0x20019080 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170aa; Value = 0xfea3016b; PC = 0x8000b7c *)
mov r4 L0x200170aa;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171aa; Value = 0x0175035f; PC = 0x8000b80 *)
mov r5 L0x200171aa;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172aa; Value = 0x00ea0394; PC = 0x8000b84 *)
mov r6 L0x200172aa;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173aa; Value = 0xffa3fedf; PC = 0x8000b88 *)
mov r7 L0x200173aa;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf109@sint32 : and [cf109 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf237@sint32 : and [cf237 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf365@sint32 : and [cf365 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf493@sint32 : and [cf493 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001712a; Value = 0xfe3401f7; PC = 0x8000c9c *)
mov r5 L0x2001712a;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001722a; Value = 0xfc84026c; PC = 0x8000ca0 *)
mov r6 L0x2001722a;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001732a; Value = 0x006600b6; PC = 0x8000ca4 *)
mov r7 L0x2001732a;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001702a; Value = 0x0159fedf; PC = 0x8000ca8 *)
mov r4 L0x2001702a;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf045@sint32 : and [cf045 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf173@sint32 : and [cf173 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf301@sint32 : and [cf301 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf429@sint32 : and [cf429 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019884; PC = 0x8000d54 *)
mov L0x20019884 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019984; PC = 0x8000d58 *)
mov L0x20019984 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a84; PC = 0x8000d5c *)
mov L0x20019a84 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b84; PC = 0x8000d60 *)
mov L0x20019b84 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c84; PC = 0x8000d64 *)
mov L0x20019c84 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d84; PC = 0x8000d68 *)
mov L0x20019d84 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e84; PC = 0x8000d6c *)
mov L0x20019e84 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f84; PC = 0x8000d70 *)
mov L0x20019f84 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019484; PC = 0x8000d9c *)
mov L0x20019484 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019584; PC = 0x8000da0 *)
mov L0x20019584 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019684; PC = 0x8000da4 *)
mov L0x20019684 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019784; PC = 0x8000da8 *)
mov L0x20019784 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019184; PC = 0x8000dac *)
mov L0x20019184 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019284; PC = 0x8000db0 *)
mov L0x20019284 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019384; PC = 0x8000db4 *)
mov L0x20019384 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019084; PC = 0x8000db8 *)
mov L0x20019084 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170ac; Value = 0xfcaefea3; PC = 0x8000b7c *)
mov r4 L0x200170ac;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171ac; Value = 0xff5f0175; PC = 0x8000b80 *)
mov r5 L0x200171ac;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172ac; Value = 0x005300ea; PC = 0x8000b84 *)
mov r6 L0x200172ac;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173ac; Value = 0xfd0dffa3; PC = 0x8000b88 *)
mov r7 L0x200173ac;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf110@sint32 : and [cf110 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf238@sint32 : and [cf238 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf366@sint32 : and [cf366 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf494@sint32 : and [cf494 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001712c; Value = 0x0323fe34; PC = 0x8000c9c *)
mov r5 L0x2001712c;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001722c; Value = 0xfc27fc84; PC = 0x8000ca0 *)
mov r6 L0x2001722c;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001732c; Value = 0x028c0066; PC = 0x8000ca4 *)
mov r7 L0x2001732c;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001702c; Value = 0x018d0159; PC = 0x8000ca8 *)
mov r4 L0x2001702c;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf046@sint32 : and [cf046 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf174@sint32 : and [cf174 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf302@sint32 : and [cf302 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf430@sint32 : and [cf430 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019888; PC = 0x8000d54 *)
mov L0x20019888 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019988; PC = 0x8000d58 *)
mov L0x20019988 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a88; PC = 0x8000d5c *)
mov L0x20019a88 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b88; PC = 0x8000d60 *)
mov L0x20019b88 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c88; PC = 0x8000d64 *)
mov L0x20019c88 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d88; PC = 0x8000d68 *)
mov L0x20019d88 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e88; PC = 0x8000d6c *)
mov L0x20019e88 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f88; PC = 0x8000d70 *)
mov L0x20019f88 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019488; PC = 0x8000d9c *)
mov L0x20019488 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019588; PC = 0x8000da0 *)
mov L0x20019588 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019688; PC = 0x8000da4 *)
mov L0x20019688 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019788; PC = 0x8000da8 *)
mov L0x20019788 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019188; PC = 0x8000dac *)
mov L0x20019188 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019288; PC = 0x8000db0 *)
mov L0x20019288 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019388; PC = 0x8000db4 *)
mov L0x20019388 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019088; PC = 0x8000db8 *)
mov L0x20019088 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170ae; Value = 0xfc0ffcae; PC = 0x8000b7c *)
mov r4 L0x200170ae;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171ae; Value = 0x0341ff5f; PC = 0x8000b80 *)
mov r5 L0x200171ae;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172ae; Value = 0x03bd0053; PC = 0x8000b84 *)
mov r6 L0x200172ae;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173ae; Value = 0x0029fd0d; PC = 0x8000b88 *)
mov r7 L0x200173ae;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf111@sint32 : and [cf111 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf239@sint32 : and [cf239 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf367@sint32 : and [cf367 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf495@sint32 : and [cf495 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001712e; Value = 0xff060323; PC = 0x8000c9c *)
mov r5 L0x2001712e;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001722e; Value = 0xfe77fc27; PC = 0x8000ca0 *)
mov r6 L0x2001722e;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001732e; Value = 0xfc47028c; PC = 0x8000ca4 *)
mov r7 L0x2001732e;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001702e; Value = 0x0143018d; PC = 0x8000ca8 *)
mov r4 L0x2001702e;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf047@sint32 : and [cf047 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf175@sint32 : and [cf175 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf303@sint32 : and [cf303 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf431@sint32 : and [cf431 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x2001988c; PC = 0x8000d54 *)
mov L0x2001988c r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x2001998c; PC = 0x8000d58 *)
mov L0x2001998c r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a8c; PC = 0x8000d5c *)
mov L0x20019a8c r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b8c; PC = 0x8000d60 *)
mov L0x20019b8c r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c8c; PC = 0x8000d64 *)
mov L0x20019c8c r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d8c; PC = 0x8000d68 *)
mov L0x20019d8c r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e8c; PC = 0x8000d6c *)
mov L0x20019e8c r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f8c; PC = 0x8000d70 *)
mov L0x20019f8c r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x2001948c; PC = 0x8000d9c *)
mov L0x2001948c r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x2001958c; PC = 0x8000da0 *)
mov L0x2001958c r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x2001968c; PC = 0x8000da4 *)
mov L0x2001968c r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x2001978c; PC = 0x8000da8 *)
mov L0x2001978c r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x2001918c; PC = 0x8000dac *)
mov L0x2001918c r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x2001928c; PC = 0x8000db0 *)
mov L0x2001928c r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x2001938c; PC = 0x8000db4 *)
mov L0x2001938c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x2001908c; PC = 0x8000db8 *)
mov L0x2001908c r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170b0; Value = 0xfe84fc0f; PC = 0x8000b7c *)
mov r4 L0x200170b0;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171b0; Value = 0xffb80341; PC = 0x8000b80 *)
mov r5 L0x200171b0;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172b0; Value = 0x02ff03bd; PC = 0x8000b84 *)
mov r6 L0x200172b0;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173b0; Value = 0x01c80029; PC = 0x8000b88 *)
mov r7 L0x200173b0;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf112@sint32 : and [cf112 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf240@sint32 : and [cf240 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf368@sint32 : and [cf368 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf496@sint32 : and [cf496 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017130; Value = 0x017aff06; PC = 0x8000c9c *)
mov r5 L0x20017130;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017230; Value = 0xfdc1fe77; PC = 0x8000ca0 *)
mov r6 L0x20017230;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017330; Value = 0x0383fc47; PC = 0x8000ca4 *)
mov r7 L0x20017330;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017030; Value = 0x037b0143; PC = 0x8000ca8 *)
mov r4 L0x20017030;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf048@sint32 : and [cf048 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf176@sint32 : and [cf176 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf304@sint32 : and [cf304 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf432@sint32 : and [cf432 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019890; PC = 0x8000d54 *)
mov L0x20019890 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019990; PC = 0x8000d58 *)
mov L0x20019990 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a90; PC = 0x8000d5c *)
mov L0x20019a90 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b90; PC = 0x8000d60 *)
mov L0x20019b90 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c90; PC = 0x8000d64 *)
mov L0x20019c90 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d90; PC = 0x8000d68 *)
mov L0x20019d90 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e90; PC = 0x8000d6c *)
mov L0x20019e90 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f90; PC = 0x8000d70 *)
mov L0x20019f90 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019490; PC = 0x8000d9c *)
mov L0x20019490 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019590; PC = 0x8000da0 *)
mov L0x20019590 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019690; PC = 0x8000da4 *)
mov L0x20019690 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019790; PC = 0x8000da8 *)
mov L0x20019790 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019190; PC = 0x8000dac *)
mov L0x20019190 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019290; PC = 0x8000db0 *)
mov L0x20019290 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019390; PC = 0x8000db4 *)
mov L0x20019390 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019090; PC = 0x8000db8 *)
mov L0x20019090 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170b2; Value = 0x01cdfe84; PC = 0x8000b7c *)
mov r4 L0x200170b2;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171b2; Value = 0x01d5ffb8; PC = 0x8000b80 *)
mov r5 L0x200171b2;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172b2; Value = 0x024402ff; PC = 0x8000b84 *)
mov r6 L0x200172b2;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173b2; Value = 0x006f01c8; PC = 0x8000b88 *)
mov r7 L0x200173b2;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf113@sint32 : and [cf113 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf241@sint32 : and [cf241 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf369@sint32 : and [cf369 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf497@sint32 : and [cf497 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017132; Value = 0x0133017a; PC = 0x8000c9c *)
mov r5 L0x20017132;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017232; Value = 0x03d1fdc1; PC = 0x8000ca0 *)
mov r6 L0x20017232;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017332; Value = 0x02070383; PC = 0x8000ca4 *)
mov r7 L0x20017332;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017032; Value = 0xffbe037b; PC = 0x8000ca8 *)
mov r4 L0x20017032;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf049@sint32 : and [cf049 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf177@sint32 : and [cf177 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf305@sint32 : and [cf305 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf433@sint32 : and [cf433 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019894; PC = 0x8000d54 *)
mov L0x20019894 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019994; PC = 0x8000d58 *)
mov L0x20019994 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a94; PC = 0x8000d5c *)
mov L0x20019a94 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b94; PC = 0x8000d60 *)
mov L0x20019b94 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c94; PC = 0x8000d64 *)
mov L0x20019c94 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d94; PC = 0x8000d68 *)
mov L0x20019d94 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e94; PC = 0x8000d6c *)
mov L0x20019e94 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f94; PC = 0x8000d70 *)
mov L0x20019f94 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019494; PC = 0x8000d9c *)
mov L0x20019494 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019594; PC = 0x8000da0 *)
mov L0x20019594 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019694; PC = 0x8000da4 *)
mov L0x20019694 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019794; PC = 0x8000da8 *)
mov L0x20019794 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019194; PC = 0x8000dac *)
mov L0x20019194 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019294; PC = 0x8000db0 *)
mov L0x20019294 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019394; PC = 0x8000db4 *)
mov L0x20019394 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019094; PC = 0x8000db8 *)
mov L0x20019094 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170b4; Value = 0x034901cd; PC = 0x8000b7c *)
mov r4 L0x200170b4;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171b4; Value = 0x003801d5; PC = 0x8000b80 *)
mov r5 L0x200171b4;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172b4; Value = 0xfddc0244; PC = 0x8000b84 *)
mov r6 L0x200172b4;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173b4; Value = 0xffdf006f; PC = 0x8000b88 *)
mov r7 L0x200173b4;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf114@sint32 : and [cf114 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf242@sint32 : and [cf242 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf370@sint32 : and [cf370 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf498@sint32 : and [cf498 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017134; Value = 0x02700133; PC = 0x8000c9c *)
mov r5 L0x20017134;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017234; Value = 0x01c203d1; PC = 0x8000ca0 *)
mov r6 L0x20017234;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017334; Value = 0xfcc30207; PC = 0x8000ca4 *)
mov r7 L0x20017334;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017034; Value = 0xff90ffbe; PC = 0x8000ca8 *)
mov r4 L0x20017034;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf050@sint32 : and [cf050 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf178@sint32 : and [cf178 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf306@sint32 : and [cf306 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf434@sint32 : and [cf434 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019898; PC = 0x8000d54 *)
mov L0x20019898 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019998; PC = 0x8000d58 *)
mov L0x20019998 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a98; PC = 0x8000d5c *)
mov L0x20019a98 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b98; PC = 0x8000d60 *)
mov L0x20019b98 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c98; PC = 0x8000d64 *)
mov L0x20019c98 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d98; PC = 0x8000d68 *)
mov L0x20019d98 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e98; PC = 0x8000d6c *)
mov L0x20019e98 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f98; PC = 0x8000d70 *)
mov L0x20019f98 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019498; PC = 0x8000d9c *)
mov L0x20019498 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019598; PC = 0x8000da0 *)
mov L0x20019598 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019698; PC = 0x8000da4 *)
mov L0x20019698 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019798; PC = 0x8000da8 *)
mov L0x20019798 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019198; PC = 0x8000dac *)
mov L0x20019198 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019298; PC = 0x8000db0 *)
mov L0x20019298 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019398; PC = 0x8000db4 *)
mov L0x20019398 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019098; PC = 0x8000db8 *)
mov L0x20019098 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170b6; Value = 0x02d60349; PC = 0x8000b7c *)
mov r4 L0x200170b6;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171b6; Value = 0xfc070038; PC = 0x8000b80 *)
mov r5 L0x200171b6;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172b6; Value = 0xfca5fddc; PC = 0x8000b84 *)
mov r6 L0x200172b6;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173b6; Value = 0xfc78ffdf; PC = 0x8000b88 *)
mov r7 L0x200173b6;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf115@sint32 : and [cf115 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf243@sint32 : and [cf243 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf371@sint32 : and [cf371 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf499@sint32 : and [cf499 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017136; Value = 0xfe970270; PC = 0x8000c9c *)
mov r5 L0x20017136;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017236; Value = 0x01e001c2; PC = 0x8000ca0 *)
mov r6 L0x20017236;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017336; Value = 0x0364fcc3; PC = 0x8000ca4 *)
mov r7 L0x20017336;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017036; Value = 0x0316ff90; PC = 0x8000ca8 *)
mov r4 L0x20017036;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf051@sint32 : and [cf051 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf179@sint32 : and [cf179 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf307@sint32 : and [cf307 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf435@sint32 : and [cf435 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x2001989c; PC = 0x8000d54 *)
mov L0x2001989c r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x2001999c; PC = 0x8000d58 *)
mov L0x2001999c r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a9c; PC = 0x8000d5c *)
mov L0x20019a9c r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b9c; PC = 0x8000d60 *)
mov L0x20019b9c r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c9c; PC = 0x8000d64 *)
mov L0x20019c9c r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d9c; PC = 0x8000d68 *)
mov L0x20019d9c r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e9c; PC = 0x8000d6c *)
mov L0x20019e9c r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f9c; PC = 0x8000d70 *)
mov L0x20019f9c r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x2001949c; PC = 0x8000d9c *)
mov L0x2001949c r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x2001959c; PC = 0x8000da0 *)
mov L0x2001959c r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x2001969c; PC = 0x8000da4 *)
mov L0x2001969c r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x2001979c; PC = 0x8000da8 *)
mov L0x2001979c r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x2001919c; PC = 0x8000dac *)
mov L0x2001919c r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x2001929c; PC = 0x8000db0 *)
mov L0x2001929c r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x2001939c; PC = 0x8000db4 *)
mov L0x2001939c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x2001909c; PC = 0x8000db8 *)
mov L0x2001909c r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170b8; Value = 0xfe8302d6; PC = 0x8000b7c *)
mov r4 L0x200170b8;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171b8; Value = 0x01fefc07; PC = 0x8000b80 *)
mov r5 L0x200171b8;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172b8; Value = 0x0120fca5; PC = 0x8000b84 *)
mov r6 L0x200172b8;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173b8; Value = 0x0206fc78; PC = 0x8000b88 *)
mov r7 L0x200173b8;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf116@sint32 : and [cf116 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf244@sint32 : and [cf244 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf372@sint32 : and [cf372 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf500@sint32 : and [cf500 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017138; Value = 0xfcd7fe97; PC = 0x8000c9c *)
mov r5 L0x20017138;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017238; Value = 0xff9601e0; PC = 0x8000ca0 *)
mov r6 L0x20017238;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017338; Value = 0x01c30364; PC = 0x8000ca4 *)
mov r7 L0x20017338;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017038; Value = 0x01890316; PC = 0x8000ca8 *)
mov r4 L0x20017038;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf052@sint32 : and [cf052 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf180@sint32 : and [cf180 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf308@sint32 : and [cf308 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf436@sint32 : and [cf436 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198a0; PC = 0x8000d54 *)
mov L0x200198a0 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199a0; PC = 0x8000d58 *)
mov L0x200199a0 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019aa0; PC = 0x8000d5c *)
mov L0x20019aa0 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019ba0; PC = 0x8000d60 *)
mov L0x20019ba0 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019ca0; PC = 0x8000d64 *)
mov L0x20019ca0 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019da0; PC = 0x8000d68 *)
mov L0x20019da0 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019ea0; PC = 0x8000d6c *)
mov L0x20019ea0 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fa0; PC = 0x8000d70 *)
mov L0x20019fa0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194a0; PC = 0x8000d9c *)
mov L0x200194a0 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195a0; PC = 0x8000da0 *)
mov L0x200195a0 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196a0; PC = 0x8000da4 *)
mov L0x200196a0 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197a0; PC = 0x8000da8 *)
mov L0x200197a0 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191a0; PC = 0x8000dac *)
mov L0x200191a0 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192a0; PC = 0x8000db0 *)
mov L0x200192a0 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193a0; PC = 0x8000db4 *)
mov L0x200193a0 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190a0; PC = 0x8000db8 *)
mov L0x200190a0 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170ba; Value = 0x0142fe83; PC = 0x8000b7c *)
mov r4 L0x200170ba;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171ba; Value = 0xffd801fe; PC = 0x8000b80 *)
mov r5 L0x200171ba;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172ba; Value = 0x03d50120; PC = 0x8000b84 *)
mov r6 L0x200172ba;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173ba; Value = 0x01b80206; PC = 0x8000b88 *)
mov r7 L0x200173ba;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf117@sint32 : and [cf117 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf245@sint32 : and [cf245 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf373@sint32 : and [cf373 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf501@sint32 : and [cf501 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001713a; Value = 0x0052fcd7; PC = 0x8000c9c *)
mov r5 L0x2001713a;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001723a; Value = 0xfc65ff96; PC = 0x8000ca0 *)
mov r6 L0x2001723a;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001733a; Value = 0x000c01c3; PC = 0x8000ca4 *)
mov r7 L0x2001733a;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001703a; Value = 0xfc9d0189; PC = 0x8000ca8 *)
mov r4 L0x2001703a;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf053@sint32 : and [cf053 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf181@sint32 : and [cf181 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf309@sint32 : and [cf309 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf437@sint32 : and [cf437 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198a4; PC = 0x8000d54 *)
mov L0x200198a4 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199a4; PC = 0x8000d58 *)
mov L0x200199a4 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019aa4; PC = 0x8000d5c *)
mov L0x20019aa4 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019ba4; PC = 0x8000d60 *)
mov L0x20019ba4 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019ca4; PC = 0x8000d64 *)
mov L0x20019ca4 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019da4; PC = 0x8000d68 *)
mov L0x20019da4 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019ea4; PC = 0x8000d6c *)
mov L0x20019ea4 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fa4; PC = 0x8000d70 *)
mov L0x20019fa4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194a4; PC = 0x8000d9c *)
mov L0x200194a4 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195a4; PC = 0x8000da0 *)
mov L0x200195a4 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196a4; PC = 0x8000da4 *)
mov L0x200196a4 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197a4; PC = 0x8000da8 *)
mov L0x200197a4 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191a4; PC = 0x8000dac *)
mov L0x200191a4 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192a4; PC = 0x8000db0 *)
mov L0x200192a4 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193a4; PC = 0x8000db4 *)
mov L0x200193a4 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190a4; PC = 0x8000db8 *)
mov L0x200190a4 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170bc; Value = 0x00170142; PC = 0x8000b7c *)
mov r4 L0x200170bc;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171bc; Value = 0x015affd8; PC = 0x8000b80 *)
mov r5 L0x200171bc;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172bc; Value = 0x01a303d5; PC = 0x8000b84 *)
mov r6 L0x200172bc;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173bc; Value = 0xfd8401b8; PC = 0x8000b88 *)
mov r7 L0x200173bc;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf118@sint32 : and [cf118 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf246@sint32 : and [cf246 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf374@sint32 : and [cf374 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf502@sint32 : and [cf502 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001713c; Value = 0x02120052; PC = 0x8000c9c *)
mov r5 L0x2001713c;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001723c; Value = 0x0220fc65; PC = 0x8000ca0 *)
mov r6 L0x2001723c;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001733c; Value = 0x01ea000c; PC = 0x8000ca4 *)
mov r7 L0x2001733c;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001703c; Value = 0xfc7efc9d; PC = 0x8000ca8 *)
mov r4 L0x2001703c;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf054@sint32 : and [cf054 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf182@sint32 : and [cf182 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf310@sint32 : and [cf310 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf438@sint32 : and [cf438 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198a8; PC = 0x8000d54 *)
mov L0x200198a8 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199a8; PC = 0x8000d58 *)
mov L0x200199a8 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019aa8; PC = 0x8000d5c *)
mov L0x20019aa8 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019ba8; PC = 0x8000d60 *)
mov L0x20019ba8 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019ca8; PC = 0x8000d64 *)
mov L0x20019ca8 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019da8; PC = 0x8000d68 *)
mov L0x20019da8 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019ea8; PC = 0x8000d6c *)
mov L0x20019ea8 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fa8; PC = 0x8000d70 *)
mov L0x20019fa8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194a8; PC = 0x8000d9c *)
mov L0x200194a8 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195a8; PC = 0x8000da0 *)
mov L0x200195a8 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196a8; PC = 0x8000da4 *)
mov L0x200196a8 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197a8; PC = 0x8000da8 *)
mov L0x200197a8 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191a8; PC = 0x8000dac *)
mov L0x200191a8 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192a8; PC = 0x8000db0 *)
mov L0x200192a8 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193a8; PC = 0x8000db4 *)
mov L0x200193a8 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190a8; PC = 0x8000db8 *)
mov L0x200190a8 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170be; Value = 0x03e70017; PC = 0x8000b7c *)
mov r4 L0x200170be;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171be; Value = 0x00b1015a; PC = 0x8000b80 *)
mov r5 L0x200171be;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172be; Value = 0xfe4c01a3; PC = 0x8000b84 *)
mov r6 L0x200172be;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173be; Value = 0xff0afd84; PC = 0x8000b88 *)
mov r7 L0x200173be;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf119@sint32 : and [cf119 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf247@sint32 : and [cf247 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf375@sint32 : and [cf375 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf503@sint32 : and [cf503 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001713e; Value = 0xfef50212; PC = 0x8000c9c *)
mov r5 L0x2001713e;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001723e; Value = 0xfd640220; PC = 0x8000ca0 *)
mov r6 L0x2001723e;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001733e; Value = 0x026001ea; PC = 0x8000ca4 *)
mov r7 L0x2001733e;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001703e; Value = 0xff77fc7e; PC = 0x8000ca8 *)
mov r4 L0x2001703e;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf055@sint32 : and [cf055 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf183@sint32 : and [cf183 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf311@sint32 : and [cf311 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf439@sint32 : and [cf439 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198ac; PC = 0x8000d54 *)
mov L0x200198ac r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199ac; PC = 0x8000d58 *)
mov L0x200199ac r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019aac; PC = 0x8000d5c *)
mov L0x20019aac r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019bac; PC = 0x8000d60 *)
mov L0x20019bac r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019cac; PC = 0x8000d64 *)
mov L0x20019cac r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019dac; PC = 0x8000d68 *)
mov L0x20019dac r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019eac; PC = 0x8000d6c *)
mov L0x20019eac r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fac; PC = 0x8000d70 *)
mov L0x20019fac r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194ac; PC = 0x8000d9c *)
mov L0x200194ac r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195ac; PC = 0x8000da0 *)
mov L0x200195ac r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196ac; PC = 0x8000da4 *)
mov L0x200196ac r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197ac; PC = 0x8000da8 *)
mov L0x200197ac r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191ac; PC = 0x8000dac *)
mov L0x200191ac r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192ac; PC = 0x8000db0 *)
mov L0x200192ac r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193ac; PC = 0x8000db4 *)
mov L0x200193ac r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190ac; PC = 0x8000db8 *)
mov L0x200190ac r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170c0; Value = 0x03a303e7; PC = 0x8000b7c *)
mov r4 L0x200170c0;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171c0; Value = 0x012f00b1; PC = 0x8000b80 *)
mov r5 L0x200171c0;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172c0; Value = 0x018efe4c; PC = 0x8000b84 *)
mov r6 L0x200172c0;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173c0; Value = 0xfc72ff0a; PC = 0x8000b88 *)
mov r7 L0x200173c0;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf120@sint32 : and [cf120 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf248@sint32 : and [cf248 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf376@sint32 : and [cf376 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf504@sint32 : and [cf504 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017140; Value = 0x007efef5; PC = 0x8000c9c *)
mov r5 L0x20017140;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017240; Value = 0xfc6dfd64; PC = 0x8000ca0 *)
mov r6 L0x20017240;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017340; Value = 0x02480260; PC = 0x8000ca4 *)
mov r7 L0x20017340;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017040; Value = 0x02c6ff77; PC = 0x8000ca8 *)
mov r4 L0x20017040;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf056@sint32 : and [cf056 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf184@sint32 : and [cf184 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf312@sint32 : and [cf312 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf440@sint32 : and [cf440 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198b0; PC = 0x8000d54 *)
mov L0x200198b0 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199b0; PC = 0x8000d58 *)
mov L0x200199b0 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019ab0; PC = 0x8000d5c *)
mov L0x20019ab0 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019bb0; PC = 0x8000d60 *)
mov L0x20019bb0 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019cb0; PC = 0x8000d64 *)
mov L0x20019cb0 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019db0; PC = 0x8000d68 *)
mov L0x20019db0 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019eb0; PC = 0x8000d6c *)
mov L0x20019eb0 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fb0; PC = 0x8000d70 *)
mov L0x20019fb0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194b0; PC = 0x8000d9c *)
mov L0x200194b0 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195b0; PC = 0x8000da0 *)
mov L0x200195b0 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196b0; PC = 0x8000da4 *)
mov L0x200196b0 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197b0; PC = 0x8000da8 *)
mov L0x200197b0 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191b0; PC = 0x8000dac *)
mov L0x200191b0 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192b0; PC = 0x8000db0 *)
mov L0x200192b0 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193b0; PC = 0x8000db4 *)
mov L0x200193b0 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190b0; PC = 0x8000db8 *)
mov L0x200190b0 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170c2; Value = 0xfc3903a3; PC = 0x8000b7c *)
mov r4 L0x200170c2;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171c2; Value = 0xfc46012f; PC = 0x8000b80 *)
mov r5 L0x200171c2;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172c2; Value = 0xff94018e; PC = 0x8000b84 *)
mov r6 L0x200172c2;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173c2; Value = 0x01a2fc72; PC = 0x8000b88 *)
mov r7 L0x200173c2;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf121@sint32 : and [cf121 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf249@sint32 : and [cf249 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf377@sint32 : and [cf377 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf505@sint32 : and [cf505 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017142; Value = 0xfed2007e; PC = 0x8000c9c *)
mov r5 L0x20017142;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017242; Value = 0xff74fc6d; PC = 0x8000ca0 *)
mov r6 L0x20017242;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017342; Value = 0xfeee0248; PC = 0x8000ca4 *)
mov r7 L0x20017342;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017042; Value = 0xff2f02c6; PC = 0x8000ca8 *)
mov r4 L0x20017042;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf057@sint32 : and [cf057 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf185@sint32 : and [cf185 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf313@sint32 : and [cf313 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf441@sint32 : and [cf441 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198b4; PC = 0x8000d54 *)
mov L0x200198b4 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199b4; PC = 0x8000d58 *)
mov L0x200199b4 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019ab4; PC = 0x8000d5c *)
mov L0x20019ab4 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019bb4; PC = 0x8000d60 *)
mov L0x20019bb4 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019cb4; PC = 0x8000d64 *)
mov L0x20019cb4 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019db4; PC = 0x8000d68 *)
mov L0x20019db4 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019eb4; PC = 0x8000d6c *)
mov L0x20019eb4 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fb4; PC = 0x8000d70 *)
mov L0x20019fb4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194b4; PC = 0x8000d9c *)
mov L0x200194b4 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195b4; PC = 0x8000da0 *)
mov L0x200195b4 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196b4; PC = 0x8000da4 *)
mov L0x200196b4 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197b4; PC = 0x8000da8 *)
mov L0x200197b4 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191b4; PC = 0x8000dac *)
mov L0x200191b4 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192b4; PC = 0x8000db0 *)
mov L0x200192b4 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193b4; PC = 0x8000db4 *)
mov L0x200193b4 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190b4; PC = 0x8000db8 *)
mov L0x200190b4 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170c4; Value = 0x0093fc39; PC = 0x8000b7c *)
mov r4 L0x200170c4;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171c4; Value = 0x01bcfc46; PC = 0x8000b80 *)
mov r5 L0x200171c4;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172c4; Value = 0xfd93ff94; PC = 0x8000b84 *)
mov r6 L0x200172c4;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173c4; Value = 0xfc2a01a2; PC = 0x8000b88 *)
mov r7 L0x200173c4;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf122@sint32 : and [cf122 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf250@sint32 : and [cf250 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf378@sint32 : and [cf378 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf506@sint32 : and [cf506 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017144; Value = 0x0394fed2; PC = 0x8000c9c *)
mov r5 L0x20017144;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017244; Value = 0xff07ff74; PC = 0x8000ca0 *)
mov r6 L0x20017244;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017344; Value = 0xfd66feee; PC = 0x8000ca4 *)
mov r7 L0x20017344;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017044; Value = 0x0326ff2f; PC = 0x8000ca8 *)
mov r4 L0x20017044;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf058@sint32 : and [cf058 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf186@sint32 : and [cf186 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf314@sint32 : and [cf314 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf442@sint32 : and [cf442 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198b8; PC = 0x8000d54 *)
mov L0x200198b8 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199b8; PC = 0x8000d58 *)
mov L0x200199b8 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019ab8; PC = 0x8000d5c *)
mov L0x20019ab8 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019bb8; PC = 0x8000d60 *)
mov L0x20019bb8 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019cb8; PC = 0x8000d64 *)
mov L0x20019cb8 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019db8; PC = 0x8000d68 *)
mov L0x20019db8 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019eb8; PC = 0x8000d6c *)
mov L0x20019eb8 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fb8; PC = 0x8000d70 *)
mov L0x20019fb8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194b8; PC = 0x8000d9c *)
mov L0x200194b8 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195b8; PC = 0x8000da0 *)
mov L0x200195b8 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196b8; PC = 0x8000da4 *)
mov L0x200196b8 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197b8; PC = 0x8000da8 *)
mov L0x200197b8 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191b8; PC = 0x8000dac *)
mov L0x200191b8 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192b8; PC = 0x8000db0 *)
mov L0x200192b8 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193b8; PC = 0x8000db4 *)
mov L0x200193b8 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190b8; PC = 0x8000db8 *)
mov L0x200190b8 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170c6; Value = 0xfc730093; PC = 0x8000b7c *)
mov r4 L0x200170c6;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171c6; Value = 0x008e01bc; PC = 0x8000b80 *)
mov r5 L0x200171c6;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172c6; Value = 0x01a1fd93; PC = 0x8000b84 *)
mov r6 L0x200172c6;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173c6; Value = 0xfc4cfc2a; PC = 0x8000b88 *)
mov r7 L0x200173c6;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf123@sint32 : and [cf123 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf251@sint32 : and [cf251 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf379@sint32 : and [cf379 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf507@sint32 : and [cf507 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017146; Value = 0xfe950394; PC = 0x8000c9c *)
mov r5 L0x20017146;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017246; Value = 0x000bff07; PC = 0x8000ca0 *)
mov r6 L0x20017246;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017346; Value = 0xfd83fd66; PC = 0x8000ca4 *)
mov r7 L0x20017346;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017046; Value = 0xfe980326; PC = 0x8000ca8 *)
mov r4 L0x20017046;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf059@sint32 : and [cf059 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf187@sint32 : and [cf187 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf315@sint32 : and [cf315 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf443@sint32 : and [cf443 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198bc; PC = 0x8000d54 *)
mov L0x200198bc r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199bc; PC = 0x8000d58 *)
mov L0x200199bc r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019abc; PC = 0x8000d5c *)
mov L0x20019abc r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019bbc; PC = 0x8000d60 *)
mov L0x20019bbc r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019cbc; PC = 0x8000d64 *)
mov L0x20019cbc r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019dbc; PC = 0x8000d68 *)
mov L0x20019dbc r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019ebc; PC = 0x8000d6c *)
mov L0x20019ebc r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fbc; PC = 0x8000d70 *)
mov L0x20019fbc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194bc; PC = 0x8000d9c *)
mov L0x200194bc r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195bc; PC = 0x8000da0 *)
mov L0x200195bc r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196bc; PC = 0x8000da4 *)
mov L0x200196bc r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197bc; PC = 0x8000da8 *)
mov L0x200197bc r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191bc; PC = 0x8000dac *)
mov L0x200191bc r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192bc; PC = 0x8000db0 *)
mov L0x200192bc r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193bc; PC = 0x8000db4 *)
mov L0x200193bc r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190bc; PC = 0x8000db8 *)
mov L0x200190bc r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170c8; Value = 0xffc6fc73; PC = 0x8000b7c *)
mov r4 L0x200170c8;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171c8; Value = 0xfc35008e; PC = 0x8000b80 *)
mov r5 L0x200171c8;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172c8; Value = 0x012401a1; PC = 0x8000b84 *)
mov r6 L0x200172c8;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173c8; Value = 0x9341fc4c; PC = 0x8000b88 *)
mov r7 L0x200173c8;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf124@sint32 : and [cf124 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf252@sint32 : and [cf252 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf380@sint32 : and [cf380 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf508@sint32 : and [cf508 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017148; Value = 0xff66fe95; PC = 0x8000c9c *)
mov r5 L0x20017148;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017248; Value = 0xfe5d000b; PC = 0x8000ca0 *)
mov r6 L0x20017248;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017348; Value = 0x00abfd83; PC = 0x8000ca4 *)
mov r7 L0x20017348;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017048; Value = 0x0288fe98; PC = 0x8000ca8 *)
mov r4 L0x20017048;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf060@sint32 : and [cf060 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf188@sint32 : and [cf188 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf316@sint32 : and [cf316 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf444@sint32 : and [cf444 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198c0; PC = 0x8000d54 *)
mov L0x200198c0 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199c0; PC = 0x8000d58 *)
mov L0x200199c0 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019ac0; PC = 0x8000d5c *)
mov L0x20019ac0 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019bc0; PC = 0x8000d60 *)
mov L0x20019bc0 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019cc0; PC = 0x8000d64 *)
mov L0x20019cc0 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019dc0; PC = 0x8000d68 *)
mov L0x20019dc0 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019ec0; PC = 0x8000d6c *)
mov L0x20019ec0 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fc0; PC = 0x8000d70 *)
mov L0x20019fc0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194c0; PC = 0x8000d9c *)
mov L0x200194c0 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195c0; PC = 0x8000da0 *)
mov L0x200195c0 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196c0; PC = 0x8000da4 *)
mov L0x200196c0 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197c0; PC = 0x8000da8 *)
mov L0x200197c0 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191c0; PC = 0x8000dac *)
mov L0x200191c0 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192c0; PC = 0x8000db0 *)
mov L0x200192c0 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193c0; PC = 0x8000db4 *)
mov L0x200193c0 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190c0; PC = 0x8000db8 *)
mov L0x200190c0 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* add.w	r12, r0, #12                              #! PC = 0x8000dc8 *)
adds dontcare r12 r0 12@uint32;
(* vmov	s2, r12                                    #! PC = 0x8000dcc *)
mov s2 r12;
(* vmov	lr, s0                                     #! PC = 0x8000dd0 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170ca; Value = 0x02e6ffc6; PC = 0x8000dd4 *)
mov r4 L0x200170ca;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171ca; Value = 0xfd2afc35; PC = 0x8000dd8 *)
mov r5 L0x200171ca;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172ca; Value = 0xfc740124; PC = 0x8000ddc *)
mov r6 L0x200172ca;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000de0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf125@sint32 : and [cf125 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000de4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf253@sint32 : and [cf253 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000de8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf381@sint32 : and [cf381 = r6] && true;
(* movw	r7, #0                                     #! PC = 0x8000dec *)
mov r7 0@sint32;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000df0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000df4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000df8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000dfc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000e00 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000e04 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000e08 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000e0c *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000e10 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000e14 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000e18 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e1c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000e20 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000e24 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000e28 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000e2c *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000e2e *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000e30 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000e34 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000e38 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e3c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000e40 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000e44 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000e46 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000e48 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000e4a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000e4c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000e50 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000e54 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000e58 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000e5c *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000e60 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e64 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000e68 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000e6c *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000e70 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e74 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000e78 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000e7c *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000e80 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e84 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000e88 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000e8c *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000e90 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e94 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000e98 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000e9c *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000ea0 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000ea4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000ea8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000eac *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000eb0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000eb4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000eb8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000ebc *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000ec0 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000ec4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000ec8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000ecc *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000ed0 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000ed4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000ed8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000edc *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000ee0 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000ee4 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000ee8 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000eec *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001714a; Value = 0x0206ff66; PC = 0x8000ef0 *)
mov r5 L0x2001714a;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001724a; Value = 0xff87fe5d; PC = 0x8000ef4 *)
mov r6 L0x2001724a;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001734a; Value = 0x022700ab; PC = 0x8000ef8 *)
mov r7 L0x2001734a;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001704a; Value = 0x01f50288; PC = 0x8000efc *)
mov r4 L0x2001704a;
(* vmov	s0, lr                                     #! PC = 0x8000f00 *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000f04 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf061@sint32 : and [cf061 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000f08 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf189@sint32 : and [cf189 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000f0c *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf317@sint32 : and [cf317 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000f10 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf445@sint32 : and [cf445 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000f14 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000f18 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000f1c *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000f20 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000f24 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000f28 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000f2c *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000f30 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000f34 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000f38 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000f3c *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000f40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000f44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000f48 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000f4c *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000f50 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000f52 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000f54 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000f58 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000f5c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000f60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000f64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000f68 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000f6a *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000f6c *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000f6e *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000f70 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000f74 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000f78 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000f7c *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000f80 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000f84 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000f88 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000f8c *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000f90 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000f92 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000f94 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000f96 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000f98 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000f9c *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000fa0 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000fa4 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198c4; PC = 0x8000fa8 *)
mov L0x200198c4 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199c4; PC = 0x8000fac *)
mov L0x200199c4 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019ac4; PC = 0x8000fb0 *)
mov L0x20019ac4 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019bc4; PC = 0x8000fb4 *)
mov L0x20019bc4 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019cc4; PC = 0x8000fb8 *)
mov L0x20019cc4 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019dc4; PC = 0x8000fbc *)
mov L0x20019dc4 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019ec4; PC = 0x8000fc0 *)
mov L0x20019ec4 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fc4; PC = 0x8000fc4 *)
mov L0x20019fc4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000fc8 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000fcc *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000fd0 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000fd4 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000fd8 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000fda *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000fdc *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000fde *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000fe0 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000fe4 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000fe8 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000fec *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194c4; PC = 0x8000ff0 *)
mov L0x200194c4 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195c4; PC = 0x8000ff4 *)
mov L0x200195c4 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196c4; PC = 0x8000ff8 *)
mov L0x200196c4 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197c4; PC = 0x8000ffc *)
mov L0x200197c4 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191c4; PC = 0x8001000 *)
mov L0x200191c4 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192c4; PC = 0x8001004 *)
mov L0x200192c4 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193c4; PC = 0x8001008 *)
mov L0x200193c4 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190c4; PC = 0x800100c *)
mov L0x200190c4 r8;
(* vmov	r12, s2                                    #! PC = 0x8001010 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8001014 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000dd0 <_0_1_2_3_last>                #! PC = 0x8001018 *)
#bne.w	0x8000dd0 <_0_1_2_3_last>                #! 0x8001018 = 0x8001018;
(* vmov	lr, s0                                     #! PC = 0x8000dd0 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170cc; Value = 0x02f102e6; PC = 0x8000dd4 *)
mov r4 L0x200170cc;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171cc; Value = 0x0338fd2a; PC = 0x8000dd8 *)
mov r5 L0x200171cc;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172cc; Value = 0x00cdfc74; PC = 0x8000ddc *)
mov r6 L0x200172cc;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000de0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf126@sint32 : and [cf126 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000de4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf254@sint32 : and [cf254 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000de8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf382@sint32 : and [cf382 = r6] && true;
(* movw	r7, #0                                     #! PC = 0x8000dec *)
mov r7 0@sint32;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000df0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000df4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000df8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000dfc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000e00 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000e04 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000e08 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000e0c *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000e10 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000e14 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000e18 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e1c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000e20 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000e24 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000e28 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000e2c *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000e2e *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000e30 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000e34 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000e38 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e3c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000e40 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000e44 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000e46 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000e48 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000e4a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000e4c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000e50 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000e54 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000e58 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000e5c *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000e60 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e64 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000e68 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000e6c *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000e70 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e74 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000e78 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000e7c *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000e80 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e84 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000e88 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000e8c *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000e90 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e94 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000e98 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000e9c *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000ea0 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000ea4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000ea8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000eac *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000eb0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000eb4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000eb8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000ebc *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000ec0 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000ec4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000ec8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000ecc *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000ed0 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000ed4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000ed8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000edc *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000ee0 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000ee4 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000ee8 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000eec *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001714c; Value = 0xfd230206; PC = 0x8000ef0 *)
mov r5 L0x2001714c;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001724c; Value = 0xfd60ff87; PC = 0x8000ef4 *)
mov r6 L0x2001724c;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001734c; Value = 0x03730227; PC = 0x8000ef8 *)
mov r7 L0x2001734c;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001704c; Value = 0xfdb401f5; PC = 0x8000efc *)
mov r4 L0x2001704c;
(* vmov	s0, lr                                     #! PC = 0x8000f00 *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000f04 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf062@sint32 : and [cf062 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000f08 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf190@sint32 : and [cf190 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000f0c *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf318@sint32 : and [cf318 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000f10 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf446@sint32 : and [cf446 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000f14 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000f18 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000f1c *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000f20 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000f24 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000f28 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000f2c *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000f30 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000f34 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000f38 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000f3c *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000f40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000f44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000f48 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000f4c *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000f50 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000f52 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000f54 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000f58 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000f5c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000f60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000f64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000f68 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000f6a *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000f6c *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000f6e *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000f70 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000f74 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000f78 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000f7c *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000f80 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000f84 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000f88 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000f8c *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000f90 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000f92 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000f94 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000f96 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000f98 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000f9c *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000fa0 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000fa4 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198c8; PC = 0x8000fa8 *)
mov L0x200198c8 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199c8; PC = 0x8000fac *)
mov L0x200199c8 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019ac8; PC = 0x8000fb0 *)
mov L0x20019ac8 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019bc8; PC = 0x8000fb4 *)
mov L0x20019bc8 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019cc8; PC = 0x8000fb8 *)
mov L0x20019cc8 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019dc8; PC = 0x8000fbc *)
mov L0x20019dc8 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019ec8; PC = 0x8000fc0 *)
mov L0x20019ec8 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fc8; PC = 0x8000fc4 *)
mov L0x20019fc8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000fc8 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000fcc *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000fd0 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000fd4 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000fd8 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000fda *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000fdc *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000fde *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000fe0 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000fe4 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000fe8 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000fec *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194c8; PC = 0x8000ff0 *)
mov L0x200194c8 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195c8; PC = 0x8000ff4 *)
mov L0x200195c8 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196c8; PC = 0x8000ff8 *)
mov L0x200196c8 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197c8; PC = 0x8000ffc *)
mov L0x200197c8 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191c8; PC = 0x8001000 *)
mov L0x200191c8 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192c8; PC = 0x8001004 *)
mov L0x200192c8 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193c8; PC = 0x8001008 *)
mov L0x200193c8 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190c8; PC = 0x800100c *)
mov L0x200190c8 r8;
(* vmov	r12, s2                                    #! PC = 0x8001010 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8001014 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000dd0 <_0_1_2_3_last>                #! PC = 0x8001018 *)
#bne.w	0x8000dd0 <_0_1_2_3_last>                #! 0x8001018 = 0x8001018;
(* vmov	lr, s0                                     #! PC = 0x8000dd0 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170ce; Value = 0x008a02f1; PC = 0x8000dd4 *)
mov r4 L0x200170ce;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171ce; Value = 0x00cc0338; PC = 0x8000dd8 *)
mov r5 L0x200171ce;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172ce; Value = 0xff1a00cd; PC = 0x8000ddc *)
mov r6 L0x200172ce;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000de0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf127@sint32 : and [cf127 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000de4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf255@sint32 : and [cf255 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000de8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf383@sint32 : and [cf383 = r6] && true;
(* movw	r7, #0                                     #! PC = 0x8000dec *)
mov r7 0@sint32;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000df0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000df4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000df8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000dfc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000e00 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000e04 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000e08 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000e0c *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000e10 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000e14 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000e18 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e1c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000e20 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000e24 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000e28 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000e2c *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000e2e *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000e30 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000e34 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000e38 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e3c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000e40 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000e44 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000e46 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000e48 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000e4a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000e4c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000e50 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000e54 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000e58 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000e5c *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000e60 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e64 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000e68 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000e6c *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000e70 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e74 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000e78 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000e7c *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000e80 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e84 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000e88 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000e8c *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000e90 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e94 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000e98 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000e9c *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000ea0 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000ea4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000ea8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000eac *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000eb0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000eb4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000eb8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000ebc *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000ec0 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000ec4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000ec8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000ecc *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000ed0 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000ed4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000ed8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000edc *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000ee0 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000ee4 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000ee8 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000eec *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001714e; Value = 0x0159fd23; PC = 0x8000ef0 *)
mov r5 L0x2001714e;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001724e; Value = 0xfddefd60; PC = 0x8000ef4 *)
mov r6 L0x2001724e;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001734e; Value = 0x035c0373; PC = 0x8000ef8 *)
mov r7 L0x2001734e;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001704e; Value = 0xfe30fdb4; PC = 0x8000efc *)
mov r4 L0x2001704e;
(* vmov	s0, lr                                     #! PC = 0x8000f00 *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000f04 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf063@sint32 : and [cf063 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000f08 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf191@sint32 : and [cf191 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000f0c *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf319@sint32 : and [cf319 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000f10 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf447@sint32 : and [cf447 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000f14 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000f18 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000f1c *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000f20 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000f24 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000f28 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000f2c *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000f30 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000f34 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000f38 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000f3c *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000f40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000f44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000f48 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000f4c *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000f50 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000f52 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000f54 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000f58 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000f5c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000f60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000f64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000f68 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000f6a *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000f6c *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000f6e *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000f70 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000f74 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000f78 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000f7c *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000f80 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000f84 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000f88 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000f8c *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000f90 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000f92 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000f94 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000f96 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000f98 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000f9c *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000fa0 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000fa4 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198cc; PC = 0x8000fa8 *)
mov L0x200198cc r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199cc; PC = 0x8000fac *)
mov L0x200199cc r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019acc; PC = 0x8000fb0 *)
mov L0x20019acc r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019bcc; PC = 0x8000fb4 *)
mov L0x20019bcc r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019ccc; PC = 0x8000fb8 *)
mov L0x20019ccc r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019dcc; PC = 0x8000fbc *)
mov L0x20019dcc r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019ecc; PC = 0x8000fc0 *)
mov L0x20019ecc r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fcc; PC = 0x8000fc4 *)
mov L0x20019fcc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000fc8 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000fcc *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000fd0 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000fd4 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000fd8 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000fda *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000fdc *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000fde *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000fe0 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000fe4 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000fe8 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000fec *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194cc; PC = 0x8000ff0 *)
mov L0x200194cc r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195cc; PC = 0x8000ff4 *)
mov L0x200195cc r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196cc; PC = 0x8000ff8 *)
mov L0x200196cc r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197cc; PC = 0x8000ffc *)
mov L0x200197cc r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191cc; PC = 0x8001000 *)
mov L0x200191cc r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192cc; PC = 0x8001004 *)
mov L0x200192cc r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193cc; PC = 0x8001008 *)
mov L0x200193cc r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190cc; PC = 0x800100c *)
mov L0x200190cc r8;
(* vmov	r12, s2                                    #! PC = 0x8001010 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8001014 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000dd0 <_0_1_2_3_last>                #! PC = 0x8001018 *)
#bne.w	0x8000dd0 <_0_1_2_3_last>                #! 0x8001018 = 0x8001018;
(* #vpop	{s16-s31}                                  #! PC = 0x800101c *)
#vpop	{%%s16-%%s31}                                  #! 0x800101c = 0x800101c;
(* #ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, r10, r11, r12, pc}#! EA = L0x20016f88; Value = 0x20016fd0; PC = 0x8001020 *)
#ldmia.w	sp!, {%%r4, %%r5, %%r6, %%r7, %%r8, %%r9, %%r10, %%r11, %%r12, pc}#! L0x20016f88 = L0x20016f88; 0x20016fd0 = 0x20016fd0; 0x8001020 = 0x8001020;
(* #add.w	r8, r8, #32                               #! PC = 0x800031c *)
#add.w	%%r8, %%r8, #32                               #! 0x800031c = 0x800031c;

(**************** center input poly ****************)

ghost cinp_poly@bit, 
cinp_poly_0@bit, cinp_poly_1@bit, cinp_poly_2@bit, cinp_poly_3@bit,
cinp_poly_4@bit, cinp_poly_5@bit, cinp_poly_6@bit, cinp_poly_7@bit :
and [
cinp_poly_0**2 = 
cf000*(x**0)+cf001*(x**1)+cf002*(x**2)+cf003*(x**3)+cf004*(x**4)+
cf005*(x**5)+cf006*(x**6)+cf007*(x**7)+cf008*(x**8)+cf009*(x**9)+
cf010*(x**10)+cf011*(x**11)+cf012*(x**12)+cf013*(x**13)+cf014*(x**14)+
cf015*(x**15)+cf016*(x**16)+cf017*(x**17)+cf018*(x**18)+cf019*(x**19)+
cf020*(x**20)+cf021*(x**21)+cf022*(x**22)+cf023*(x**23)+cf024*(x**24)+
cf025*(x**25)+cf026*(x**26)+cf027*(x**27)+cf028*(x**28)+cf029*(x**29)+
cf030*(x**30)+cf031*(x**31)+cf032*(x**32)+cf033*(x**33)+cf034*(x**34)+
cf035*(x**35)+cf036*(x**36)+cf037*(x**37)+cf038*(x**38)+cf039*(x**39)+
cf040*(x**40)+cf041*(x**41)+cf042*(x**42)+cf043*(x**43)+cf044*(x**44)+
cf045*(x**45)+cf046*(x**46)+cf047*(x**47)+cf048*(x**48)+cf049*(x**49)+
cf050*(x**50)+cf051*(x**51)+cf052*(x**52)+cf053*(x**53)+cf054*(x**54)+
cf055*(x**55)+cf056*(x**56)+cf057*(x**57)+cf058*(x**58)+cf059*(x**59)+
cf060*(x**60)+cf061*(x**61)+cf062*(x**62)+cf063*(x**63),
cinp_poly_1**2 = 
cf064*(x**0)+cf065*(x**1)+cf066*(x**2)+cf067*(x**3)+cf068*(x**4)+
cf069*(x**5)+cf070*(x**6)+cf071*(x**7)+cf072*(x**8)+cf073*(x**9)+
cf074*(x**10)+cf075*(x**11)+cf076*(x**12)+cf077*(x**13)+cf078*(x**14)+
cf079*(x**15)+cf080*(x**16)+cf081*(x**17)+cf082*(x**18)+cf083*(x**19)+
cf084*(x**20)+cf085*(x**21)+cf086*(x**22)+cf087*(x**23)+cf088*(x**24)+
cf089*(x**25)+cf090*(x**26)+cf091*(x**27)+cf092*(x**28)+cf093*(x**29)+
cf094*(x**30)+cf095*(x**31)+cf096*(x**32)+cf097*(x**33)+cf098*(x**34)+
cf099*(x**35)+cf100*(x**36)+cf101*(x**37)+cf102*(x**38)+cf103*(x**39)+
cf104*(x**40)+cf105*(x**41)+cf106*(x**42)+cf107*(x**43)+cf108*(x**44)+
cf109*(x**45)+cf110*(x**46)+cf111*(x**47)+cf112*(x**48)+cf113*(x**49)+
cf114*(x**50)+cf115*(x**51)+cf116*(x**52)+cf117*(x**53)+cf118*(x**54)+
cf119*(x**55)+cf120*(x**56)+cf121*(x**57)+cf122*(x**58)+cf123*(x**59)+
cf124*(x**60)+cf125*(x**61)+cf126*(x**62)+cf127*(x**63),
cinp_poly_2**2 = 
cf128*(x**0)+cf129*(x**1)+cf130*(x**2)+cf131*(x**3)+cf132*(x**4)+
cf133*(x**5)+cf134*(x**6)+cf135*(x**7)+cf136*(x**8)+cf137*(x**9)+
cf138*(x**10)+cf139*(x**11)+cf140*(x**12)+cf141*(x**13)+cf142*(x**14)+
cf143*(x**15)+cf144*(x**16)+cf145*(x**17)+cf146*(x**18)+cf147*(x**19)+
cf148*(x**20)+cf149*(x**21)+cf150*(x**22)+cf151*(x**23)+cf152*(x**24)+
cf153*(x**25)+cf154*(x**26)+cf155*(x**27)+cf156*(x**28)+cf157*(x**29)+
cf158*(x**30)+cf159*(x**31)+cf160*(x**32)+cf161*(x**33)+cf162*(x**34)+
cf163*(x**35)+cf164*(x**36)+cf165*(x**37)+cf166*(x**38)+cf167*(x**39)+
cf168*(x**40)+cf169*(x**41)+cf170*(x**42)+cf171*(x**43)+cf172*(x**44)+
cf173*(x**45)+cf174*(x**46)+cf175*(x**47)+cf176*(x**48)+cf177*(x**49)+
cf178*(x**50)+cf179*(x**51)+cf180*(x**52)+cf181*(x**53)+cf182*(x**54)+
cf183*(x**55)+cf184*(x**56)+cf185*(x**57)+cf186*(x**58)+cf187*(x**59)+
cf188*(x**60)+cf189*(x**61)+cf190*(x**62)+cf191*(x**63),
cinp_poly_3**2 = 
cf192*(x**0)+cf193*(x**1)+cf194*(x**2)+cf195*(x**3)+cf196*(x**4)+
cf197*(x**5)+cf198*(x**6)+cf199*(x**7)+cf200*(x**8)+cf201*(x**9)+
cf202*(x**10)+cf203*(x**11)+cf204*(x**12)+cf205*(x**13)+cf206*(x**14)+
cf207*(x**15)+cf208*(x**16)+cf209*(x**17)+cf210*(x**18)+cf211*(x**19)+
cf212*(x**20)+cf213*(x**21)+cf214*(x**22)+cf215*(x**23)+cf216*(x**24)+
cf217*(x**25)+cf218*(x**26)+cf219*(x**27)+cf220*(x**28)+cf221*(x**29)+
cf222*(x**30)+cf223*(x**31)+cf224*(x**32)+cf225*(x**33)+cf226*(x**34)+
cf227*(x**35)+cf228*(x**36)+cf229*(x**37)+cf230*(x**38)+cf231*(x**39)+
cf232*(x**40)+cf233*(x**41)+cf234*(x**42)+cf235*(x**43)+cf236*(x**44)+
cf237*(x**45)+cf238*(x**46)+cf239*(x**47)+cf240*(x**48)+cf241*(x**49)+
cf242*(x**50)+cf243*(x**51)+cf244*(x**52)+cf245*(x**53)+cf246*(x**54)+
cf247*(x**55)+cf248*(x**56)+cf249*(x**57)+cf250*(x**58)+cf251*(x**59)+
cf252*(x**60)+cf253*(x**61)+cf254*(x**62)+cf255*(x**63),
cinp_poly_4**2 = 
cf256*(x**0)+cf257*(x**1)+cf258*(x**2)+cf259*(x**3)+cf260*(x**4)+
cf261*(x**5)+cf262*(x**6)+cf263*(x**7)+cf264*(x**8)+cf265*(x**9)+
cf266*(x**10)+cf267*(x**11)+cf268*(x**12)+cf269*(x**13)+cf270*(x**14)+
cf271*(x**15)+cf272*(x**16)+cf273*(x**17)+cf274*(x**18)+cf275*(x**19)+
cf276*(x**20)+cf277*(x**21)+cf278*(x**22)+cf279*(x**23)+cf280*(x**24)+
cf281*(x**25)+cf282*(x**26)+cf283*(x**27)+cf284*(x**28)+cf285*(x**29)+
cf286*(x**30)+cf287*(x**31)+cf288*(x**32)+cf289*(x**33)+cf290*(x**34)+
cf291*(x**35)+cf292*(x**36)+cf293*(x**37)+cf294*(x**38)+cf295*(x**39)+
cf296*(x**40)+cf297*(x**41)+cf298*(x**42)+cf299*(x**43)+cf300*(x**44)+
cf301*(x**45)+cf302*(x**46)+cf303*(x**47)+cf304*(x**48)+cf305*(x**49)+
cf306*(x**50)+cf307*(x**51)+cf308*(x**52)+cf309*(x**53)+cf310*(x**54)+
cf311*(x**55)+cf312*(x**56)+cf313*(x**57)+cf314*(x**58)+cf315*(x**59)+
cf316*(x**60)+cf317*(x**61)+cf318*(x**62)+cf319*(x**63),
cinp_poly_5**2 = 
cf320*(x**0)+cf321*(x**1)+cf322*(x**2)+cf323*(x**3)+cf324*(x**4)+
cf325*(x**5)+cf326*(x**6)+cf327*(x**7)+cf328*(x**8)+cf329*(x**9)+
cf330*(x**10)+cf331*(x**11)+cf332*(x**12)+cf333*(x**13)+cf334*(x**14)+
cf335*(x**15)+cf336*(x**16)+cf337*(x**17)+cf338*(x**18)+cf339*(x**19)+
cf340*(x**20)+cf341*(x**21)+cf342*(x**22)+cf343*(x**23)+cf344*(x**24)+
cf345*(x**25)+cf346*(x**26)+cf347*(x**27)+cf348*(x**28)+cf349*(x**29)+
cf350*(x**30)+cf351*(x**31)+cf352*(x**32)+cf353*(x**33)+cf354*(x**34)+
cf355*(x**35)+cf356*(x**36)+cf357*(x**37)+cf358*(x**38)+cf359*(x**39)+
cf360*(x**40)+cf361*(x**41)+cf362*(x**42)+cf363*(x**43)+cf364*(x**44)+
cf365*(x**45)+cf366*(x**46)+cf367*(x**47)+cf368*(x**48)+cf369*(x**49)+
cf370*(x**50)+cf371*(x**51)+cf372*(x**52)+cf373*(x**53)+cf374*(x**54)+
cf375*(x**55)+cf376*(x**56)+cf377*(x**57)+cf378*(x**58)+cf379*(x**59)+
cf380*(x**60)+cf381*(x**61)+cf382*(x**62)+cf383*(x**63),
cinp_poly_6**2 = 
cf384*(x**0)+cf385*(x**1)+cf386*(x**2)+cf387*(x**3)+cf388*(x**4)+
cf389*(x**5)+cf390*(x**6)+cf391*(x**7)+cf392*(x**8)+cf393*(x**9)+
cf394*(x**10)+cf395*(x**11)+cf396*(x**12)+cf397*(x**13)+cf398*(x**14)+
cf399*(x**15)+cf400*(x**16)+cf401*(x**17)+cf402*(x**18)+cf403*(x**19)+
cf404*(x**20)+cf405*(x**21)+cf406*(x**22)+cf407*(x**23)+cf408*(x**24)+
cf409*(x**25)+cf410*(x**26)+cf411*(x**27)+cf412*(x**28)+cf413*(x**29)+
cf414*(x**30)+cf415*(x**31)+cf416*(x**32)+cf417*(x**33)+cf418*(x**34)+
cf419*(x**35)+cf420*(x**36)+cf421*(x**37)+cf422*(x**38)+cf423*(x**39)+
cf424*(x**40)+cf425*(x**41)+cf426*(x**42)+cf427*(x**43)+cf428*(x**44)+
cf429*(x**45)+cf430*(x**46)+cf431*(x**47)+cf432*(x**48)+cf433*(x**49)+
cf434*(x**50)+cf435*(x**51)+cf436*(x**52)+cf437*(x**53)+cf438*(x**54)+
cf439*(x**55)+cf440*(x**56)+cf441*(x**57)+cf442*(x**58)+cf443*(x**59)+
cf444*(x**60)+cf445*(x**61)+cf446*(x**62)+cf447*(x**63),
cinp_poly_7**2 = 
cf448*(x**0)+cf449*(x**1)+cf450*(x**2)+cf451*(x**3)+cf452*(x**4)+
cf453*(x**5)+cf454*(x**6)+cf455*(x**7)+cf456*(x**8)+cf457*(x**9)+
cf458*(x**10)+cf459*(x**11)+cf460*(x**12)+cf461*(x**13)+cf462*(x**14)+
cf463*(x**15)+cf464*(x**16)+cf465*(x**17)+cf466*(x**18)+cf467*(x**19)+
cf468*(x**20)+cf469*(x**21)+cf470*(x**22)+cf471*(x**23)+cf472*(x**24)+
cf473*(x**25)+cf474*(x**26)+cf475*(x**27)+cf476*(x**28)+cf477*(x**29)+
cf478*(x**30)+cf479*(x**31)+cf480*(x**32)+cf481*(x**33)+cf482*(x**34)+
cf483*(x**35)+cf484*(x**36)+cf485*(x**37)+cf486*(x**38)+cf487*(x**39)+
cf488*(x**40)+cf489*(x**41)+cf490*(x**42)+cf491*(x**43)+cf492*(x**44)+
cf493*(x**45)+cf494*(x**46)+cf495*(x**47)+cf496*(x**48)+cf497*(x**49)+
cf498*(x**50)+cf499*(x**51)+cf500*(x**52)+cf501*(x**53)+cf502*(x**54)+
cf503*(x**55)+cf504*(x**56)+cf505*(x**57)+cf506*(x**58)+cf507*(x**59)+
cf508*(x**60),
cinp_poly**2 = 
(cinp_poly_0**2)*(x**0)+(cinp_poly_1**2)*(x**64)+(cinp_poly_2**2)*(x**128)+
(cinp_poly_3**2)*(x**192)+(cinp_poly_4**2)*(x**256)+(cinp_poly_5**2)*(x**320)+
(cinp_poly_6**2)*(x**384)+(cinp_poly_7**2)*(x**448)
] && true;

{
and [
eqmod (inp_poly**2) (cinp_poly**2) (2**11),
eqmod (cinp_poly**2)
    (L0x20018fd0*(x** 0)+L0x20018fd4*(x** 1)+L0x20018fd8*(x** 2)+
     L0x20018fdc*(x** 3)+L0x20018fe0*(x** 4)+L0x20018fe4*(x** 5)+
     L0x20018fe8*(x** 6)+L0x20018fec*(x** 7)+L0x20018ff0*(x** 8)+
     L0x20018ff4*(x** 9)+L0x20018ff8*(x**10)+L0x20018ffc*(x**11)+
     L0x20019000*(x**12)+L0x20019004*(x**13)+L0x20019008*(x**14)+
     L0x2001900c*(x**15)+L0x20019010*(x**16)+L0x20019014*(x**17)+
     L0x20019018*(x**18)+L0x2001901c*(x**19)+L0x20019020*(x**20)+
     L0x20019024*(x**21)+L0x20019028*(x**22)+L0x2001902c*(x**23)+
     L0x20019030*(x**24)+L0x20019034*(x**25)+L0x20019038*(x**26)+
     L0x2001903c*(x**27)+L0x20019040*(x**28)+L0x20019044*(x**29)+
     L0x20019048*(x**30)+L0x2001904c*(x**31)+L0x20019050*(x**32)+
     L0x20019054*(x**33)+L0x20019058*(x**34)+L0x2001905c*(x**35)+
     L0x20019060*(x**36)+L0x20019064*(x**37)+L0x20019068*(x**38)+
     L0x2001906c*(x**39)+L0x20019070*(x**40)+L0x20019074*(x**41)+
     L0x20019078*(x**42)+L0x2001907c*(x**43)+L0x20019080*(x**44)+
     L0x20019084*(x**45)+L0x20019088*(x**46)+L0x2001908c*(x**47)+
     L0x20019090*(x**48)+L0x20019094*(x**49)+L0x20019098*(x**50)+
     L0x2001909c*(x**51)+L0x200190a0*(x**52)+L0x200190a4*(x**53)+
     L0x200190a8*(x**54)+L0x200190ac*(x**55)+L0x200190b0*(x**56)+
     L0x200190b4*(x**57)+L0x200190b8*(x**58)+L0x200190bc*(x**59)+
     L0x200190c0*(x**60)+L0x200190c4*(x**61)+L0x200190c8*(x**62)+
     L0x200190cc*(x**63))
    [1043969, x**64 - 1],
eqmod (cinp_poly**2)
    (L0x200190d0*(x** 0)+L0x200190d4*(x** 1)+L0x200190d8*(x** 2)+
     L0x200190dc*(x** 3)+L0x200190e0*(x** 4)+L0x200190e4*(x** 5)+
     L0x200190e8*(x** 6)+L0x200190ec*(x** 7)+L0x200190f0*(x** 8)+
     L0x200190f4*(x** 9)+L0x200190f8*(x**10)+L0x200190fc*(x**11)+
     L0x20019100*(x**12)+L0x20019104*(x**13)+L0x20019108*(x**14)+
     L0x2001910c*(x**15)+L0x20019110*(x**16)+L0x20019114*(x**17)+
     L0x20019118*(x**18)+L0x2001911c*(x**19)+L0x20019120*(x**20)+
     L0x20019124*(x**21)+L0x20019128*(x**22)+L0x2001912c*(x**23)+
     L0x20019130*(x**24)+L0x20019134*(x**25)+L0x20019138*(x**26)+
     L0x2001913c*(x**27)+L0x20019140*(x**28)+L0x20019144*(x**29)+
     L0x20019148*(x**30)+L0x2001914c*(x**31)+L0x20019150*(x**32)+
     L0x20019154*(x**33)+L0x20019158*(x**34)+L0x2001915c*(x**35)+
     L0x20019160*(x**36)+L0x20019164*(x**37)+L0x20019168*(x**38)+
     L0x2001916c*(x**39)+L0x20019170*(x**40)+L0x20019174*(x**41)+
     L0x20019178*(x**42)+L0x2001917c*(x**43)+L0x20019180*(x**44)+
     L0x20019184*(x**45)+L0x20019188*(x**46)+L0x2001918c*(x**47)+
     L0x20019190*(x**48)+L0x20019194*(x**49)+L0x20019198*(x**50)+
     L0x2001919c*(x**51)+L0x200191a0*(x**52)+L0x200191a4*(x**53)+
     L0x200191a8*(x**54)+L0x200191ac*(x**55)+L0x200191b0*(x**56)+
     L0x200191b4*(x**57)+L0x200191b8*(x**58)+L0x200191bc*(x**59)+
     L0x200191c0*(x**60)+L0x200191c4*(x**61)+L0x200191c8*(x**62)+
     L0x200191cc*(x**63))
    [1043969, x**64 - 1043968],
eqmod (cinp_poly**2)
    (L0x200191d0*(x** 0)+L0x200191d4*(x** 1)+L0x200191d8*(x** 2)+
     L0x200191dc*(x** 3)+L0x200191e0*(x** 4)+L0x200191e4*(x** 5)+
     L0x200191e8*(x** 6)+L0x200191ec*(x** 7)+L0x200191f0*(x** 8)+
     L0x200191f4*(x** 9)+L0x200191f8*(x**10)+L0x200191fc*(x**11)+
     L0x20019200*(x**12)+L0x20019204*(x**13)+L0x20019208*(x**14)+
     L0x2001920c*(x**15)+L0x20019210*(x**16)+L0x20019214*(x**17)+
     L0x20019218*(x**18)+L0x2001921c*(x**19)+L0x20019220*(x**20)+
     L0x20019224*(x**21)+L0x20019228*(x**22)+L0x2001922c*(x**23)+
     L0x20019230*(x**24)+L0x20019234*(x**25)+L0x20019238*(x**26)+
     L0x2001923c*(x**27)+L0x20019240*(x**28)+L0x20019244*(x**29)+
     L0x20019248*(x**30)+L0x2001924c*(x**31)+L0x20019250*(x**32)+
     L0x20019254*(x**33)+L0x20019258*(x**34)+L0x2001925c*(x**35)+
     L0x20019260*(x**36)+L0x20019264*(x**37)+L0x20019268*(x**38)+
     L0x2001926c*(x**39)+L0x20019270*(x**40)+L0x20019274*(x**41)+
     L0x20019278*(x**42)+L0x2001927c*(x**43)+L0x20019280*(x**44)+
     L0x20019284*(x**45)+L0x20019288*(x**46)+L0x2001928c*(x**47)+
     L0x20019290*(x**48)+L0x20019294*(x**49)+L0x20019298*(x**50)+
     L0x2001929c*(x**51)+L0x200192a0*(x**52)+L0x200192a4*(x**53)+
     L0x200192a8*(x**54)+L0x200192ac*(x**55)+L0x200192b0*(x**56)+
     L0x200192b4*(x**57)+L0x200192b8*(x**58)+L0x200192bc*(x**59)+
     L0x200192c0*(x**60)+L0x200192c4*(x**61)+L0x200192c8*(x**62)+
     L0x200192cc*(x**63))
    [1043969, x**64 - 554923],
eqmod (cinp_poly**2)
    (L0x200192d0*(x** 0)+L0x200192d4*(x** 1)+L0x200192d8*(x** 2)+
     L0x200192dc*(x** 3)+L0x200192e0*(x** 4)+L0x200192e4*(x** 5)+
     L0x200192e8*(x** 6)+L0x200192ec*(x** 7)+L0x200192f0*(x** 8)+
     L0x200192f4*(x** 9)+L0x200192f8*(x**10)+L0x200192fc*(x**11)+
     L0x20019300*(x**12)+L0x20019304*(x**13)+L0x20019308*(x**14)+
     L0x2001930c*(x**15)+L0x20019310*(x**16)+L0x20019314*(x**17)+
     L0x20019318*(x**18)+L0x2001931c*(x**19)+L0x20019320*(x**20)+
     L0x20019324*(x**21)+L0x20019328*(x**22)+L0x2001932c*(x**23)+
     L0x20019330*(x**24)+L0x20019334*(x**25)+L0x20019338*(x**26)+
     L0x2001933c*(x**27)+L0x20019340*(x**28)+L0x20019344*(x**29)+
     L0x20019348*(x**30)+L0x2001934c*(x**31)+L0x20019350*(x**32)+
     L0x20019354*(x**33)+L0x20019358*(x**34)+L0x2001935c*(x**35)+
     L0x20019360*(x**36)+L0x20019364*(x**37)+L0x20019368*(x**38)+
     L0x2001936c*(x**39)+L0x20019370*(x**40)+L0x20019374*(x**41)+
     L0x20019378*(x**42)+L0x2001937c*(x**43)+L0x20019380*(x**44)+
     L0x20019384*(x**45)+L0x20019388*(x**46)+L0x2001938c*(x**47)+
     L0x20019390*(x**48)+L0x20019394*(x**49)+L0x20019398*(x**50)+
     L0x2001939c*(x**51)+L0x200193a0*(x**52)+L0x200193a4*(x**53)+
     L0x200193a8*(x**54)+L0x200193ac*(x**55)+L0x200193b0*(x**56)+
     L0x200193b4*(x**57)+L0x200193b8*(x**58)+L0x200193bc*(x**59)+
     L0x200193c0*(x**60)+L0x200193c4*(x**61)+L0x200193c8*(x**62)+
     L0x200193cc*(x**63))
    [1043969, x**64 - 489046],
eqmod (cinp_poly**2)
    (L0x200193d0*(x** 0)+L0x200193d4*(x** 1)+L0x200193d8*(x** 2)+
     L0x200193dc*(x** 3)+L0x200193e0*(x** 4)+L0x200193e4*(x** 5)+
     L0x200193e8*(x** 6)+L0x200193ec*(x** 7)+L0x200193f0*(x** 8)+
     L0x200193f4*(x** 9)+L0x200193f8*(x**10)+L0x200193fc*(x**11)+
     L0x20019400*(x**12)+L0x20019404*(x**13)+L0x20019408*(x**14)+
     L0x2001940c*(x**15)+L0x20019410*(x**16)+L0x20019414*(x**17)+
     L0x20019418*(x**18)+L0x2001941c*(x**19)+L0x20019420*(x**20)+
     L0x20019424*(x**21)+L0x20019428*(x**22)+L0x2001942c*(x**23)+
     L0x20019430*(x**24)+L0x20019434*(x**25)+L0x20019438*(x**26)+
     L0x2001943c*(x**27)+L0x20019440*(x**28)+L0x20019444*(x**29)+
     L0x20019448*(x**30)+L0x2001944c*(x**31)+L0x20019450*(x**32)+
     L0x20019454*(x**33)+L0x20019458*(x**34)+L0x2001945c*(x**35)+
     L0x20019460*(x**36)+L0x20019464*(x**37)+L0x20019468*(x**38)+
     L0x2001946c*(x**39)+L0x20019470*(x**40)+L0x20019474*(x**41)+
     L0x20019478*(x**42)+L0x2001947c*(x**43)+L0x20019480*(x**44)+
     L0x20019484*(x**45)+L0x20019488*(x**46)+L0x2001948c*(x**47)+
     L0x20019490*(x**48)+L0x20019494*(x**49)+L0x20019498*(x**50)+
     L0x2001949c*(x**51)+L0x200194a0*(x**52)+L0x200194a4*(x**53)+
     L0x200194a8*(x**54)+L0x200194ac*(x**55)+L0x200194b0*(x**56)+
     L0x200194b4*(x**57)+L0x200194b8*(x**58)+L0x200194bc*(x**59)+
     L0x200194c0*(x**60)+L0x200194c4*(x**61)+L0x200194c8*(x**62)+
     L0x200194cc*(x**63))
    [1043969, x**64 - 287998],
eqmod (cinp_poly**2)
    (L0x200194d0*(x** 0)+L0x200194d4*(x** 1)+L0x200194d8*(x** 2)+
     L0x200194dc*(x** 3)+L0x200194e0*(x** 4)+L0x200194e4*(x** 5)+
     L0x200194e8*(x** 6)+L0x200194ec*(x** 7)+L0x200194f0*(x** 8)+
     L0x200194f4*(x** 9)+L0x200194f8*(x**10)+L0x200194fc*(x**11)+
     L0x20019500*(x**12)+L0x20019504*(x**13)+L0x20019508*(x**14)+
     L0x2001950c*(x**15)+L0x20019510*(x**16)+L0x20019514*(x**17)+
     L0x20019518*(x**18)+L0x2001951c*(x**19)+L0x20019520*(x**20)+
     L0x20019524*(x**21)+L0x20019528*(x**22)+L0x2001952c*(x**23)+
     L0x20019530*(x**24)+L0x20019534*(x**25)+L0x20019538*(x**26)+
     L0x2001953c*(x**27)+L0x20019540*(x**28)+L0x20019544*(x**29)+
     L0x20019548*(x**30)+L0x2001954c*(x**31)+L0x20019550*(x**32)+
     L0x20019554*(x**33)+L0x20019558*(x**34)+L0x2001955c*(x**35)+
     L0x20019560*(x**36)+L0x20019564*(x**37)+L0x20019568*(x**38)+
     L0x2001956c*(x**39)+L0x20019570*(x**40)+L0x20019574*(x**41)+
     L0x20019578*(x**42)+L0x2001957c*(x**43)+L0x20019580*(x**44)+
     L0x20019584*(x**45)+L0x20019588*(x**46)+L0x2001958c*(x**47)+
     L0x20019590*(x**48)+L0x20019594*(x**49)+L0x20019598*(x**50)+
     L0x2001959c*(x**51)+L0x200195a0*(x**52)+L0x200195a4*(x**53)+
     L0x200195a8*(x**54)+L0x200195ac*(x**55)+L0x200195b0*(x**56)+
     L0x200195b4*(x**57)+L0x200195b8*(x**58)+L0x200195bc*(x**59)+
     L0x200195c0*(x**60)+L0x200195c4*(x**61)+L0x200195c8*(x**62)+
     L0x200195cc*(x**63))
    [1043969, x**64 - 755971],
eqmod (cinp_poly**2)
    (L0x200195d0*(x** 0)+L0x200195d4*(x** 1)+L0x200195d8*(x** 2)+
     L0x200195dc*(x** 3)+L0x200195e0*(x** 4)+L0x200195e4*(x** 5)+
     L0x200195e8*(x** 6)+L0x200195ec*(x** 7)+L0x200195f0*(x** 8)+
     L0x200195f4*(x** 9)+L0x200195f8*(x**10)+L0x200195fc*(x**11)+
     L0x20019600*(x**12)+L0x20019604*(x**13)+L0x20019608*(x**14)+
     L0x2001960c*(x**15)+L0x20019610*(x**16)+L0x20019614*(x**17)+
     L0x20019618*(x**18)+L0x2001961c*(x**19)+L0x20019620*(x**20)+
     L0x20019624*(x**21)+L0x20019628*(x**22)+L0x2001962c*(x**23)+
     L0x20019630*(x**24)+L0x20019634*(x**25)+L0x20019638*(x**26)+
     L0x2001963c*(x**27)+L0x20019640*(x**28)+L0x20019644*(x**29)+
     L0x20019648*(x**30)+L0x2001964c*(x**31)+L0x20019650*(x**32)+
     L0x20019654*(x**33)+L0x20019658*(x**34)+L0x2001965c*(x**35)+
     L0x20019660*(x**36)+L0x20019664*(x**37)+L0x20019668*(x**38)+
     L0x2001966c*(x**39)+L0x20019670*(x**40)+L0x20019674*(x**41)+
     L0x20019678*(x**42)+L0x2001967c*(x**43)+L0x20019680*(x**44)+
     L0x20019684*(x**45)+L0x20019688*(x**46)+L0x2001968c*(x**47)+
     L0x20019690*(x**48)+L0x20019694*(x**49)+L0x20019698*(x**50)+
     L0x2001969c*(x**51)+L0x200196a0*(x**52)+L0x200196a4*(x**53)+
     L0x200196a8*(x**54)+L0x200196ac*(x**55)+L0x200196b0*(x**56)+
     L0x200196b4*(x**57)+L0x200196b8*(x**58)+L0x200196bc*(x**59)+
     L0x200196c0*(x**60)+L0x200196c4*(x**61)+L0x200196c8*(x**62)+
     L0x200196cc*(x**63))
    [1043969, x**64 - 719789],
eqmod (cinp_poly**2)
    (L0x200196d0*(x** 0)+L0x200196d4*(x** 1)+L0x200196d8*(x** 2)+
     L0x200196dc*(x** 3)+L0x200196e0*(x** 4)+L0x200196e4*(x** 5)+
     L0x200196e8*(x** 6)+L0x200196ec*(x** 7)+L0x200196f0*(x** 8)+
     L0x200196f4*(x** 9)+L0x200196f8*(x**10)+L0x200196fc*(x**11)+
     L0x20019700*(x**12)+L0x20019704*(x**13)+L0x20019708*(x**14)+
     L0x2001970c*(x**15)+L0x20019710*(x**16)+L0x20019714*(x**17)+
     L0x20019718*(x**18)+L0x2001971c*(x**19)+L0x20019720*(x**20)+
     L0x20019724*(x**21)+L0x20019728*(x**22)+L0x2001972c*(x**23)+
     L0x20019730*(x**24)+L0x20019734*(x**25)+L0x20019738*(x**26)+
     L0x2001973c*(x**27)+L0x20019740*(x**28)+L0x20019744*(x**29)+
     L0x20019748*(x**30)+L0x2001974c*(x**31)+L0x20019750*(x**32)+
     L0x20019754*(x**33)+L0x20019758*(x**34)+L0x2001975c*(x**35)+
     L0x20019760*(x**36)+L0x20019764*(x**37)+L0x20019768*(x**38)+
     L0x2001976c*(x**39)+L0x20019770*(x**40)+L0x20019774*(x**41)+
     L0x20019778*(x**42)+L0x2001977c*(x**43)+L0x20019780*(x**44)+
     L0x20019784*(x**45)+L0x20019788*(x**46)+L0x2001978c*(x**47)+
     L0x20019790*(x**48)+L0x20019794*(x**49)+L0x20019798*(x**50)+
     L0x2001979c*(x**51)+L0x200197a0*(x**52)+L0x200197a4*(x**53)+
     L0x200197a8*(x**54)+L0x200197ac*(x**55)+L0x200197b0*(x**56)+
     L0x200197b4*(x**57)+L0x200197b8*(x**58)+L0x200197bc*(x**59)+
     L0x200197c0*(x**60)+L0x200197c4*(x**61)+L0x200197c8*(x**62)+
     L0x200197cc*(x**63))
    [1043969, x**64 - 324180],
eqmod (cinp_poly**2)
    (L0x200197d0*(x** 0)+L0x200197d4*(x** 1)+L0x200197d8*(x** 2)+
     L0x200197dc*(x** 3)+L0x200197e0*(x** 4)+L0x200197e4*(x** 5)+
     L0x200197e8*(x** 6)+L0x200197ec*(x** 7)+L0x200197f0*(x** 8)+
     L0x200197f4*(x** 9)+L0x200197f8*(x**10)+L0x200197fc*(x**11)+
     L0x20019800*(x**12)+L0x20019804*(x**13)+L0x20019808*(x**14)+
     L0x2001980c*(x**15)+L0x20019810*(x**16)+L0x20019814*(x**17)+
     L0x20019818*(x**18)+L0x2001981c*(x**19)+L0x20019820*(x**20)+
     L0x20019824*(x**21)+L0x20019828*(x**22)+L0x2001982c*(x**23)+
     L0x20019830*(x**24)+L0x20019834*(x**25)+L0x20019838*(x**26)+
     L0x2001983c*(x**27)+L0x20019840*(x**28)+L0x20019844*(x**29)+
     L0x20019848*(x**30)+L0x2001984c*(x**31)+L0x20019850*(x**32)+
     L0x20019854*(x**33)+L0x20019858*(x**34)+L0x2001985c*(x**35)+
     L0x20019860*(x**36)+L0x20019864*(x**37)+L0x20019868*(x**38)+
     L0x2001986c*(x**39)+L0x20019870*(x**40)+L0x20019874*(x**41)+
     L0x20019878*(x**42)+L0x2001987c*(x**43)+L0x20019880*(x**44)+
     L0x20019884*(x**45)+L0x20019888*(x**46)+L0x2001988c*(x**47)+
     L0x20019890*(x**48)+L0x20019894*(x**49)+L0x20019898*(x**50)+
     L0x2001989c*(x**51)+L0x200198a0*(x**52)+L0x200198a4*(x**53)+
     L0x200198a8*(x**54)+L0x200198ac*(x**55)+L0x200198b0*(x**56)+
     L0x200198b4*(x**57)+L0x200198b8*(x**58)+L0x200198bc*(x**59)+
     L0x200198c0*(x**60)+L0x200198c4*(x**61)+L0x200198c8*(x**62)+
     L0x200198cc*(x**63))
    [1043969, x**64 - 29512],
eqmod (cinp_poly**2)
    (L0x200198d0*(x** 0)+L0x200198d4*(x** 1)+L0x200198d8*(x** 2)+
     L0x200198dc*(x** 3)+L0x200198e0*(x** 4)+L0x200198e4*(x** 5)+
     L0x200198e8*(x** 6)+L0x200198ec*(x** 7)+L0x200198f0*(x** 8)+
     L0x200198f4*(x** 9)+L0x200198f8*(x**10)+L0x200198fc*(x**11)+
     L0x20019900*(x**12)+L0x20019904*(x**13)+L0x20019908*(x**14)+
     L0x2001990c*(x**15)+L0x20019910*(x**16)+L0x20019914*(x**17)+
     L0x20019918*(x**18)+L0x2001991c*(x**19)+L0x20019920*(x**20)+
     L0x20019924*(x**21)+L0x20019928*(x**22)+L0x2001992c*(x**23)+
     L0x20019930*(x**24)+L0x20019934*(x**25)+L0x20019938*(x**26)+
     L0x2001993c*(x**27)+L0x20019940*(x**28)+L0x20019944*(x**29)+
     L0x20019948*(x**30)+L0x2001994c*(x**31)+L0x20019950*(x**32)+
     L0x20019954*(x**33)+L0x20019958*(x**34)+L0x2001995c*(x**35)+
     L0x20019960*(x**36)+L0x20019964*(x**37)+L0x20019968*(x**38)+
     L0x2001996c*(x**39)+L0x20019970*(x**40)+L0x20019974*(x**41)+
     L0x20019978*(x**42)+L0x2001997c*(x**43)+L0x20019980*(x**44)+
     L0x20019984*(x**45)+L0x20019988*(x**46)+L0x2001998c*(x**47)+
     L0x20019990*(x**48)+L0x20019994*(x**49)+L0x20019998*(x**50)+
     L0x2001999c*(x**51)+L0x200199a0*(x**52)+L0x200199a4*(x**53)+
     L0x200199a8*(x**54)+L0x200199ac*(x**55)+L0x200199b0*(x**56)+
     L0x200199b4*(x**57)+L0x200199b8*(x**58)+L0x200199bc*(x**59)+
     L0x200199c0*(x**60)+L0x200199c4*(x**61)+L0x200199c8*(x**62)+
     L0x200199cc*(x**63))
    [1043969, x**64 - 1014457],
eqmod (cinp_poly**2)
    (L0x200199d0*(x** 0)+L0x200199d4*(x** 1)+L0x200199d8*(x** 2)+
     L0x200199dc*(x** 3)+L0x200199e0*(x** 4)+L0x200199e4*(x** 5)+
     L0x200199e8*(x** 6)+L0x200199ec*(x** 7)+L0x200199f0*(x** 8)+
     L0x200199f4*(x** 9)+L0x200199f8*(x**10)+L0x200199fc*(x**11)+
     L0x20019a00*(x**12)+L0x20019a04*(x**13)+L0x20019a08*(x**14)+
     L0x20019a0c*(x**15)+L0x20019a10*(x**16)+L0x20019a14*(x**17)+
     L0x20019a18*(x**18)+L0x20019a1c*(x**19)+L0x20019a20*(x**20)+
     L0x20019a24*(x**21)+L0x20019a28*(x**22)+L0x20019a2c*(x**23)+
     L0x20019a30*(x**24)+L0x20019a34*(x**25)+L0x20019a38*(x**26)+
     L0x20019a3c*(x**27)+L0x20019a40*(x**28)+L0x20019a44*(x**29)+
     L0x20019a48*(x**30)+L0x20019a4c*(x**31)+L0x20019a50*(x**32)+
     L0x20019a54*(x**33)+L0x20019a58*(x**34)+L0x20019a5c*(x**35)+
     L0x20019a60*(x**36)+L0x20019a64*(x**37)+L0x20019a68*(x**38)+
     L0x20019a6c*(x**39)+L0x20019a70*(x**40)+L0x20019a74*(x**41)+
     L0x20019a78*(x**42)+L0x20019a7c*(x**43)+L0x20019a80*(x**44)+
     L0x20019a84*(x**45)+L0x20019a88*(x**46)+L0x20019a8c*(x**47)+
     L0x20019a90*(x**48)+L0x20019a94*(x**49)+L0x20019a98*(x**50)+
     L0x20019a9c*(x**51)+L0x20019aa0*(x**52)+L0x20019aa4*(x**53)+
     L0x20019aa8*(x**54)+L0x20019aac*(x**55)+L0x20019ab0*(x**56)+
     L0x20019ab4*(x**57)+L0x20019ab8*(x**58)+L0x20019abc*(x**59)+
     L0x20019ac0*(x**60)+L0x20019ac4*(x**61)+L0x20019ac8*(x**62)+
     L0x20019acc*(x**63))
    [1043969, x**64 - 145873],
eqmod (cinp_poly**2)
    (L0x20019ad0*(x** 0)+L0x20019ad4*(x** 1)+L0x20019ad8*(x** 2)+
     L0x20019adc*(x** 3)+L0x20019ae0*(x** 4)+L0x20019ae4*(x** 5)+
     L0x20019ae8*(x** 6)+L0x20019aec*(x** 7)+L0x20019af0*(x** 8)+
     L0x20019af4*(x** 9)+L0x20019af8*(x**10)+L0x20019afc*(x**11)+
     L0x20019b00*(x**12)+L0x20019b04*(x**13)+L0x20019b08*(x**14)+
     L0x20019b0c*(x**15)+L0x20019b10*(x**16)+L0x20019b14*(x**17)+
     L0x20019b18*(x**18)+L0x20019b1c*(x**19)+L0x20019b20*(x**20)+
     L0x20019b24*(x**21)+L0x20019b28*(x**22)+L0x20019b2c*(x**23)+
     L0x20019b30*(x**24)+L0x20019b34*(x**25)+L0x20019b38*(x**26)+
     L0x20019b3c*(x**27)+L0x20019b40*(x**28)+L0x20019b44*(x**29)+
     L0x20019b48*(x**30)+L0x20019b4c*(x**31)+L0x20019b50*(x**32)+
     L0x20019b54*(x**33)+L0x20019b58*(x**34)+L0x20019b5c*(x**35)+
     L0x20019b60*(x**36)+L0x20019b64*(x**37)+L0x20019b68*(x**38)+
     L0x20019b6c*(x**39)+L0x20019b70*(x**40)+L0x20019b74*(x**41)+
     L0x20019b78*(x**42)+L0x20019b7c*(x**43)+L0x20019b80*(x**44)+
     L0x20019b84*(x**45)+L0x20019b88*(x**46)+L0x20019b8c*(x**47)+
     L0x20019b90*(x**48)+L0x20019b94*(x**49)+L0x20019b98*(x**50)+
     L0x20019b9c*(x**51)+L0x20019ba0*(x**52)+L0x20019ba4*(x**53)+
     L0x20019ba8*(x**54)+L0x20019bac*(x**55)+L0x20019bb0*(x**56)+
     L0x20019bb4*(x**57)+L0x20019bb8*(x**58)+L0x20019bbc*(x**59)+
     L0x20019bc0*(x**60)+L0x20019bc4*(x**61)+L0x20019bc8*(x**62)+
     L0x20019bcc*(x**63))
    [1043969, x**64 - 898096],
eqmod (cinp_poly**2)
    (L0x20019bd0*(x** 0)+L0x20019bd4*(x** 1)+L0x20019bd8*(x** 2)+
     L0x20019bdc*(x** 3)+L0x20019be0*(x** 4)+L0x20019be4*(x** 5)+
     L0x20019be8*(x** 6)+L0x20019bec*(x** 7)+L0x20019bf0*(x** 8)+
     L0x20019bf4*(x** 9)+L0x20019bf8*(x**10)+L0x20019bfc*(x**11)+
     L0x20019c00*(x**12)+L0x20019c04*(x**13)+L0x20019c08*(x**14)+
     L0x20019c0c*(x**15)+L0x20019c10*(x**16)+L0x20019c14*(x**17)+
     L0x20019c18*(x**18)+L0x20019c1c*(x**19)+L0x20019c20*(x**20)+
     L0x20019c24*(x**21)+L0x20019c28*(x**22)+L0x20019c2c*(x**23)+
     L0x20019c30*(x**24)+L0x20019c34*(x**25)+L0x20019c38*(x**26)+
     L0x20019c3c*(x**27)+L0x20019c40*(x**28)+L0x20019c44*(x**29)+
     L0x20019c48*(x**30)+L0x20019c4c*(x**31)+L0x20019c50*(x**32)+
     L0x20019c54*(x**33)+L0x20019c58*(x**34)+L0x20019c5c*(x**35)+
     L0x20019c60*(x**36)+L0x20019c64*(x**37)+L0x20019c68*(x**38)+
     L0x20019c6c*(x**39)+L0x20019c70*(x**40)+L0x20019c74*(x**41)+
     L0x20019c78*(x**42)+L0x20019c7c*(x**43)+L0x20019c80*(x**44)+
     L0x20019c84*(x**45)+L0x20019c88*(x**46)+L0x20019c8c*(x**47)+
     L0x20019c90*(x**48)+L0x20019c94*(x**49)+L0x20019c98*(x**50)+
     L0x20019c9c*(x**51)+L0x20019ca0*(x**52)+L0x20019ca4*(x**53)+
     L0x20019ca8*(x**54)+L0x20019cac*(x**55)+L0x20019cb0*(x**56)+
     L0x20019cb4*(x**57)+L0x20019cb8*(x**58)+L0x20019cbc*(x**59)+
     L0x20019cc0*(x**60)+L0x20019cc4*(x**61)+L0x20019cc8*(x**62)+
     L0x20019ccc*(x**63))
    [1043969, x**64 - 445347],
eqmod (cinp_poly**2)
    (L0x20019cd0*(x** 0)+L0x20019cd4*(x** 1)+L0x20019cd8*(x** 2)+
     L0x20019cdc*(x** 3)+L0x20019ce0*(x** 4)+L0x20019ce4*(x** 5)+
     L0x20019ce8*(x** 6)+L0x20019cec*(x** 7)+L0x20019cf0*(x** 8)+
     L0x20019cf4*(x** 9)+L0x20019cf8*(x**10)+L0x20019cfc*(x**11)+
     L0x20019d00*(x**12)+L0x20019d04*(x**13)+L0x20019d08*(x**14)+
     L0x20019d0c*(x**15)+L0x20019d10*(x**16)+L0x20019d14*(x**17)+
     L0x20019d18*(x**18)+L0x20019d1c*(x**19)+L0x20019d20*(x**20)+
     L0x20019d24*(x**21)+L0x20019d28*(x**22)+L0x20019d2c*(x**23)+
     L0x20019d30*(x**24)+L0x20019d34*(x**25)+L0x20019d38*(x**26)+
     L0x20019d3c*(x**27)+L0x20019d40*(x**28)+L0x20019d44*(x**29)+
     L0x20019d48*(x**30)+L0x20019d4c*(x**31)+L0x20019d50*(x**32)+
     L0x20019d54*(x**33)+L0x20019d58*(x**34)+L0x20019d5c*(x**35)+
     L0x20019d60*(x**36)+L0x20019d64*(x**37)+L0x20019d68*(x**38)+
     L0x20019d6c*(x**39)+L0x20019d70*(x**40)+L0x20019d74*(x**41)+
     L0x20019d78*(x**42)+L0x20019d7c*(x**43)+L0x20019d80*(x**44)+
     L0x20019d84*(x**45)+L0x20019d88*(x**46)+L0x20019d8c*(x**47)+
     L0x20019d90*(x**48)+L0x20019d94*(x**49)+L0x20019d98*(x**50)+
     L0x20019d9c*(x**51)+L0x20019da0*(x**52)+L0x20019da4*(x**53)+
     L0x20019da8*(x**54)+L0x20019dac*(x**55)+L0x20019db0*(x**56)+
     L0x20019db4*(x**57)+L0x20019db8*(x**58)+L0x20019dbc*(x**59)+
     L0x20019dc0*(x**60)+L0x20019dc4*(x**61)+L0x20019dc8*(x**62)+
     L0x20019dcc*(x**63))
    [1043969, x**64 - 598622],
eqmod (cinp_poly**2)
    (L0x20019dd0*(x** 0)+L0x20019dd4*(x** 1)+L0x20019dd8*(x** 2)+
     L0x20019ddc*(x** 3)+L0x20019de0*(x** 4)+L0x20019de4*(x** 5)+
     L0x20019de8*(x** 6)+L0x20019dec*(x** 7)+L0x20019df0*(x** 8)+
     L0x20019df4*(x** 9)+L0x20019df8*(x**10)+L0x20019dfc*(x**11)+
     L0x20019e00*(x**12)+L0x20019e04*(x**13)+L0x20019e08*(x**14)+
     L0x20019e0c*(x**15)+L0x20019e10*(x**16)+L0x20019e14*(x**17)+
     L0x20019e18*(x**18)+L0x20019e1c*(x**19)+L0x20019e20*(x**20)+
     L0x20019e24*(x**21)+L0x20019e28*(x**22)+L0x20019e2c*(x**23)+
     L0x20019e30*(x**24)+L0x20019e34*(x**25)+L0x20019e38*(x**26)+
     L0x20019e3c*(x**27)+L0x20019e40*(x**28)+L0x20019e44*(x**29)+
     L0x20019e48*(x**30)+L0x20019e4c*(x**31)+L0x20019e50*(x**32)+
     L0x20019e54*(x**33)+L0x20019e58*(x**34)+L0x20019e5c*(x**35)+
     L0x20019e60*(x**36)+L0x20019e64*(x**37)+L0x20019e68*(x**38)+
     L0x20019e6c*(x**39)+L0x20019e70*(x**40)+L0x20019e74*(x**41)+
     L0x20019e78*(x**42)+L0x20019e7c*(x**43)+L0x20019e80*(x**44)+
     L0x20019e84*(x**45)+L0x20019e88*(x**46)+L0x20019e8c*(x**47)+
     L0x20019e90*(x**48)+L0x20019e94*(x**49)+L0x20019e98*(x**50)+
     L0x20019e9c*(x**51)+L0x20019ea0*(x**52)+L0x20019ea4*(x**53)+
     L0x20019ea8*(x**54)+L0x20019eac*(x**55)+L0x20019eb0*(x**56)+
     L0x20019eb4*(x**57)+L0x20019eb8*(x**58)+L0x20019ebc*(x**59)+
     L0x20019ec0*(x**60)+L0x20019ec4*(x**61)+L0x20019ec8*(x**62)+
     L0x20019ecc*(x**63))
    [1043969, x**64 - 775725],
eqmod (cinp_poly**2)
    (L0x20019ed0*(x** 0)+L0x20019ed4*(x** 1)+L0x20019ed8*(x** 2)+
     L0x20019edc*(x** 3)+L0x20019ee0*(x** 4)+L0x20019ee4*(x** 5)+
     L0x20019ee8*(x** 6)+L0x20019eec*(x** 7)+L0x20019ef0*(x** 8)+
     L0x20019ef4*(x** 9)+L0x20019ef8*(x**10)+L0x20019efc*(x**11)+
     L0x20019f00*(x**12)+L0x20019f04*(x**13)+L0x20019f08*(x**14)+
     L0x20019f0c*(x**15)+L0x20019f10*(x**16)+L0x20019f14*(x**17)+
     L0x20019f18*(x**18)+L0x20019f1c*(x**19)+L0x20019f20*(x**20)+
     L0x20019f24*(x**21)+L0x20019f28*(x**22)+L0x20019f2c*(x**23)+
     L0x20019f30*(x**24)+L0x20019f34*(x**25)+L0x20019f38*(x**26)+
     L0x20019f3c*(x**27)+L0x20019f40*(x**28)+L0x20019f44*(x**29)+
     L0x20019f48*(x**30)+L0x20019f4c*(x**31)+L0x20019f50*(x**32)+
     L0x20019f54*(x**33)+L0x20019f58*(x**34)+L0x20019f5c*(x**35)+
     L0x20019f60*(x**36)+L0x20019f64*(x**37)+L0x20019f68*(x**38)+
     L0x20019f6c*(x**39)+L0x20019f70*(x**40)+L0x20019f74*(x**41)+
     L0x20019f78*(x**42)+L0x20019f7c*(x**43)+L0x20019f80*(x**44)+
     L0x20019f84*(x**45)+L0x20019f88*(x**46)+L0x20019f8c*(x**47)+
     L0x20019f90*(x**48)+L0x20019f94*(x**49)+L0x20019f98*(x**50)+
     L0x20019f9c*(x**51)+L0x20019fa0*(x**52)+L0x20019fa4*(x**53)+
     L0x20019fa8*(x**54)+L0x20019fac*(x**55)+L0x20019fb0*(x**56)+
     L0x20019fb4*(x**57)+L0x20019fb8*(x**58)+L0x20019fbc*(x**59)+
     L0x20019fc0*(x**60)+L0x20019fc4*(x**61)+L0x20019fc8*(x**62)+
     L0x20019fcc*(x**63))
    [1043969, x**64 - 268244]
] && and [
(-5)@32*1043969@32 <s L0x20018fd0, L0x20018fd0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018fd4, L0x20018fd4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018fd8, L0x20018fd8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018fdc, L0x20018fdc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018fe0, L0x20018fe0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018fe4, L0x20018fe4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018fe8, L0x20018fe8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018fec, L0x20018fec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018ff0, L0x20018ff0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018ff4, L0x20018ff4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018ff8, L0x20018ff8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018ffc, L0x20018ffc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019000, L0x20019000 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019004, L0x20019004 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019008, L0x20019008 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001900c, L0x2001900c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019010, L0x20019010 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019014, L0x20019014 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019018, L0x20019018 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001901c, L0x2001901c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019020, L0x20019020 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019024, L0x20019024 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019028, L0x20019028 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001902c, L0x2001902c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019030, L0x20019030 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019034, L0x20019034 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019038, L0x20019038 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001903c, L0x2001903c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019040, L0x20019040 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019044, L0x20019044 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019048, L0x20019048 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001904c, L0x2001904c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019050, L0x20019050 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019054, L0x20019054 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019058, L0x20019058 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001905c, L0x2001905c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019060, L0x20019060 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019064, L0x20019064 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019068, L0x20019068 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001906c, L0x2001906c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019070, L0x20019070 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019074, L0x20019074 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019078, L0x20019078 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001907c, L0x2001907c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019080, L0x20019080 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019084, L0x20019084 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019088, L0x20019088 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001908c, L0x2001908c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019090, L0x20019090 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019094, L0x20019094 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019098, L0x20019098 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001909c, L0x2001909c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190a0, L0x200190a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190a4, L0x200190a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190a8, L0x200190a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190ac, L0x200190ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190b0, L0x200190b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190b4, L0x200190b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190b8, L0x200190b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190bc, L0x200190bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190c0, L0x200190c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190c4, L0x200190c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190c8, L0x200190c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190cc, L0x200190cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190d0, L0x200190d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190d4, L0x200190d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190d8, L0x200190d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190dc, L0x200190dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190e0, L0x200190e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190e4, L0x200190e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190e8, L0x200190e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190ec, L0x200190ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190f0, L0x200190f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190f4, L0x200190f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190f8, L0x200190f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190fc, L0x200190fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019100, L0x20019100 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019104, L0x20019104 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019108, L0x20019108 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001910c, L0x2001910c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019110, L0x20019110 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019114, L0x20019114 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019118, L0x20019118 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001911c, L0x2001911c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019120, L0x20019120 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019124, L0x20019124 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019128, L0x20019128 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001912c, L0x2001912c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019130, L0x20019130 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019134, L0x20019134 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019138, L0x20019138 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001913c, L0x2001913c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019140, L0x20019140 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019144, L0x20019144 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019148, L0x20019148 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001914c, L0x2001914c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019150, L0x20019150 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019154, L0x20019154 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019158, L0x20019158 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001915c, L0x2001915c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019160, L0x20019160 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019164, L0x20019164 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019168, L0x20019168 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001916c, L0x2001916c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019170, L0x20019170 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019174, L0x20019174 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019178, L0x20019178 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001917c, L0x2001917c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019180, L0x20019180 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019184, L0x20019184 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019188, L0x20019188 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001918c, L0x2001918c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019190, L0x20019190 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019194, L0x20019194 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019198, L0x20019198 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001919c, L0x2001919c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191a0, L0x200191a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191a4, L0x200191a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191a8, L0x200191a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191ac, L0x200191ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191b0, L0x200191b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191b4, L0x200191b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191b8, L0x200191b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191bc, L0x200191bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191c0, L0x200191c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191c4, L0x200191c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191c8, L0x200191c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191cc, L0x200191cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191d0, L0x200191d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191d4, L0x200191d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191d8, L0x200191d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191dc, L0x200191dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191e0, L0x200191e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191e4, L0x200191e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191e8, L0x200191e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191ec, L0x200191ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191f0, L0x200191f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191f4, L0x200191f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191f8, L0x200191f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191fc, L0x200191fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019200, L0x20019200 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019204, L0x20019204 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019208, L0x20019208 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001920c, L0x2001920c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019210, L0x20019210 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019214, L0x20019214 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019218, L0x20019218 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001921c, L0x2001921c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019220, L0x20019220 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019224, L0x20019224 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019228, L0x20019228 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001922c, L0x2001922c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019230, L0x20019230 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019234, L0x20019234 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019238, L0x20019238 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001923c, L0x2001923c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019240, L0x20019240 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019244, L0x20019244 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019248, L0x20019248 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001924c, L0x2001924c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019250, L0x20019250 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019254, L0x20019254 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019258, L0x20019258 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001925c, L0x2001925c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019260, L0x20019260 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019264, L0x20019264 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019268, L0x20019268 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001926c, L0x2001926c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019270, L0x20019270 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019274, L0x20019274 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019278, L0x20019278 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001927c, L0x2001927c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019280, L0x20019280 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019284, L0x20019284 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019288, L0x20019288 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001928c, L0x2001928c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019290, L0x20019290 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019294, L0x20019294 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019298, L0x20019298 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001929c, L0x2001929c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192a0, L0x200192a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192a4, L0x200192a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192a8, L0x200192a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192ac, L0x200192ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192b0, L0x200192b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192b4, L0x200192b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192b8, L0x200192b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192bc, L0x200192bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192c0, L0x200192c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192c4, L0x200192c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192c8, L0x200192c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192cc, L0x200192cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192d0, L0x200192d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192d4, L0x200192d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192d8, L0x200192d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192dc, L0x200192dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192e0, L0x200192e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192e4, L0x200192e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192e8, L0x200192e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192ec, L0x200192ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192f0, L0x200192f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192f4, L0x200192f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192f8, L0x200192f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192fc, L0x200192fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019300, L0x20019300 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019304, L0x20019304 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019308, L0x20019308 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001930c, L0x2001930c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019310, L0x20019310 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019314, L0x20019314 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019318, L0x20019318 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001931c, L0x2001931c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019320, L0x20019320 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019324, L0x20019324 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019328, L0x20019328 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001932c, L0x2001932c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019330, L0x20019330 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019334, L0x20019334 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019338, L0x20019338 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001933c, L0x2001933c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019340, L0x20019340 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019344, L0x20019344 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019348, L0x20019348 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001934c, L0x2001934c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019350, L0x20019350 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019354, L0x20019354 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019358, L0x20019358 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001935c, L0x2001935c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019360, L0x20019360 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019364, L0x20019364 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019368, L0x20019368 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001936c, L0x2001936c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019370, L0x20019370 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019374, L0x20019374 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019378, L0x20019378 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001937c, L0x2001937c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019380, L0x20019380 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019384, L0x20019384 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019388, L0x20019388 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001938c, L0x2001938c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019390, L0x20019390 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019394, L0x20019394 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019398, L0x20019398 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001939c, L0x2001939c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193a0, L0x200193a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193a4, L0x200193a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193a8, L0x200193a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193ac, L0x200193ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193b0, L0x200193b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193b4, L0x200193b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193b8, L0x200193b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193bc, L0x200193bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193c0, L0x200193c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193c4, L0x200193c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193c8, L0x200193c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193cc, L0x200193cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193d0, L0x200193d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193d4, L0x200193d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193d8, L0x200193d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193dc, L0x200193dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193e0, L0x200193e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193e4, L0x200193e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193e8, L0x200193e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193ec, L0x200193ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193f0, L0x200193f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193f4, L0x200193f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193f8, L0x200193f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193fc, L0x200193fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019400, L0x20019400 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019404, L0x20019404 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019408, L0x20019408 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001940c, L0x2001940c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019410, L0x20019410 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019414, L0x20019414 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019418, L0x20019418 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001941c, L0x2001941c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019420, L0x20019420 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019424, L0x20019424 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019428, L0x20019428 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001942c, L0x2001942c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019430, L0x20019430 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019434, L0x20019434 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019438, L0x20019438 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001943c, L0x2001943c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019440, L0x20019440 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019444, L0x20019444 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019448, L0x20019448 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001944c, L0x2001944c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019450, L0x20019450 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019454, L0x20019454 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019458, L0x20019458 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001945c, L0x2001945c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019460, L0x20019460 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019464, L0x20019464 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019468, L0x20019468 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001946c, L0x2001946c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019470, L0x20019470 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019474, L0x20019474 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019478, L0x20019478 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001947c, L0x2001947c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019480, L0x20019480 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019484, L0x20019484 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019488, L0x20019488 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001948c, L0x2001948c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019490, L0x20019490 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019494, L0x20019494 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019498, L0x20019498 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001949c, L0x2001949c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194a0, L0x200194a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194a4, L0x200194a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194a8, L0x200194a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194ac, L0x200194ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194b0, L0x200194b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194b4, L0x200194b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194b8, L0x200194b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194bc, L0x200194bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194c0, L0x200194c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194c4, L0x200194c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194c8, L0x200194c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194cc, L0x200194cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194d0, L0x200194d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194d4, L0x200194d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194d8, L0x200194d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194dc, L0x200194dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194e0, L0x200194e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194e4, L0x200194e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194e8, L0x200194e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194ec, L0x200194ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194f0, L0x200194f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194f4, L0x200194f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194f8, L0x200194f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194fc, L0x200194fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019500, L0x20019500 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019504, L0x20019504 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019508, L0x20019508 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001950c, L0x2001950c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019510, L0x20019510 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019514, L0x20019514 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019518, L0x20019518 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001951c, L0x2001951c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019520, L0x20019520 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019524, L0x20019524 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019528, L0x20019528 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001952c, L0x2001952c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019530, L0x20019530 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019534, L0x20019534 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019538, L0x20019538 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001953c, L0x2001953c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019540, L0x20019540 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019544, L0x20019544 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019548, L0x20019548 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001954c, L0x2001954c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019550, L0x20019550 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019554, L0x20019554 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019558, L0x20019558 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001955c, L0x2001955c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019560, L0x20019560 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019564, L0x20019564 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019568, L0x20019568 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001956c, L0x2001956c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019570, L0x20019570 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019574, L0x20019574 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019578, L0x20019578 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001957c, L0x2001957c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019580, L0x20019580 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019584, L0x20019584 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019588, L0x20019588 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001958c, L0x2001958c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019590, L0x20019590 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019594, L0x20019594 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019598, L0x20019598 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001959c, L0x2001959c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195a0, L0x200195a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195a4, L0x200195a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195a8, L0x200195a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195ac, L0x200195ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195b0, L0x200195b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195b4, L0x200195b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195b8, L0x200195b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195bc, L0x200195bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195c0, L0x200195c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195c4, L0x200195c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195c8, L0x200195c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195cc, L0x200195cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195d0, L0x200195d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195d4, L0x200195d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195d8, L0x200195d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195dc, L0x200195dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195e0, L0x200195e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195e4, L0x200195e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195e8, L0x200195e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195ec, L0x200195ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195f0, L0x200195f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195f4, L0x200195f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195f8, L0x200195f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195fc, L0x200195fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019600, L0x20019600 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019604, L0x20019604 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019608, L0x20019608 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001960c, L0x2001960c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019610, L0x20019610 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019614, L0x20019614 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019618, L0x20019618 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001961c, L0x2001961c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019620, L0x20019620 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019624, L0x20019624 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019628, L0x20019628 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001962c, L0x2001962c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019630, L0x20019630 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019634, L0x20019634 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019638, L0x20019638 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001963c, L0x2001963c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019640, L0x20019640 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019644, L0x20019644 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019648, L0x20019648 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001964c, L0x2001964c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019650, L0x20019650 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019654, L0x20019654 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019658, L0x20019658 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001965c, L0x2001965c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019660, L0x20019660 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019664, L0x20019664 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019668, L0x20019668 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001966c, L0x2001966c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019670, L0x20019670 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019674, L0x20019674 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019678, L0x20019678 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001967c, L0x2001967c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019680, L0x20019680 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019684, L0x20019684 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019688, L0x20019688 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001968c, L0x2001968c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019690, L0x20019690 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019694, L0x20019694 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019698, L0x20019698 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001969c, L0x2001969c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196a0, L0x200196a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196a4, L0x200196a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196a8, L0x200196a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196ac, L0x200196ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196b0, L0x200196b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196b4, L0x200196b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196b8, L0x200196b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196bc, L0x200196bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196c0, L0x200196c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196c4, L0x200196c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196c8, L0x200196c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196cc, L0x200196cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196d0, L0x200196d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196d4, L0x200196d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196d8, L0x200196d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196dc, L0x200196dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196e0, L0x200196e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196e4, L0x200196e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196e8, L0x200196e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196ec, L0x200196ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196f0, L0x200196f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196f4, L0x200196f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196f8, L0x200196f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196fc, L0x200196fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019700, L0x20019700 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019704, L0x20019704 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019708, L0x20019708 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001970c, L0x2001970c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019710, L0x20019710 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019714, L0x20019714 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019718, L0x20019718 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001971c, L0x2001971c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019720, L0x20019720 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019724, L0x20019724 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019728, L0x20019728 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001972c, L0x2001972c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019730, L0x20019730 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019734, L0x20019734 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019738, L0x20019738 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001973c, L0x2001973c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019740, L0x20019740 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019744, L0x20019744 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019748, L0x20019748 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001974c, L0x2001974c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019750, L0x20019750 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019754, L0x20019754 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019758, L0x20019758 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001975c, L0x2001975c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019760, L0x20019760 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019764, L0x20019764 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019768, L0x20019768 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001976c, L0x2001976c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019770, L0x20019770 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019774, L0x20019774 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019778, L0x20019778 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001977c, L0x2001977c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019780, L0x20019780 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019784, L0x20019784 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019788, L0x20019788 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001978c, L0x2001978c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019790, L0x20019790 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019794, L0x20019794 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019798, L0x20019798 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001979c, L0x2001979c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197a0, L0x200197a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197a4, L0x200197a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197a8, L0x200197a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197ac, L0x200197ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197b0, L0x200197b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197b4, L0x200197b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197b8, L0x200197b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197bc, L0x200197bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197c0, L0x200197c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197c4, L0x200197c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197c8, L0x200197c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197cc, L0x200197cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197d0, L0x200197d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197d4, L0x200197d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197d8, L0x200197d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197dc, L0x200197dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197e0, L0x200197e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197e4, L0x200197e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197e8, L0x200197e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197ec, L0x200197ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197f0, L0x200197f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197f4, L0x200197f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197f8, L0x200197f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197fc, L0x200197fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019800, L0x20019800 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019804, L0x20019804 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019808, L0x20019808 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001980c, L0x2001980c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019810, L0x20019810 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019814, L0x20019814 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019818, L0x20019818 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001981c, L0x2001981c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019820, L0x20019820 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019824, L0x20019824 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019828, L0x20019828 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001982c, L0x2001982c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019830, L0x20019830 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019834, L0x20019834 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019838, L0x20019838 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001983c, L0x2001983c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019840, L0x20019840 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019844, L0x20019844 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019848, L0x20019848 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001984c, L0x2001984c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019850, L0x20019850 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019854, L0x20019854 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019858, L0x20019858 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001985c, L0x2001985c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019860, L0x20019860 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019864, L0x20019864 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019868, L0x20019868 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001986c, L0x2001986c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019870, L0x20019870 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019874, L0x20019874 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019878, L0x20019878 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001987c, L0x2001987c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019880, L0x20019880 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019884, L0x20019884 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019888, L0x20019888 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001988c, L0x2001988c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019890, L0x20019890 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019894, L0x20019894 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019898, L0x20019898 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001989c, L0x2001989c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198a0, L0x200198a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198a4, L0x200198a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198a8, L0x200198a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198ac, L0x200198ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198b0, L0x200198b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198b4, L0x200198b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198b8, L0x200198b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198bc, L0x200198bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198c0, L0x200198c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198c4, L0x200198c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198c8, L0x200198c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198cc, L0x200198cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198d0, L0x200198d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198d4, L0x200198d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198d8, L0x200198d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198dc, L0x200198dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198e0, L0x200198e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198e4, L0x200198e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198e8, L0x200198e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198ec, L0x200198ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198f0, L0x200198f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198f4, L0x200198f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198f8, L0x200198f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198fc, L0x200198fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019900, L0x20019900 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019904, L0x20019904 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019908, L0x20019908 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001990c, L0x2001990c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019910, L0x20019910 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019914, L0x20019914 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019918, L0x20019918 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001991c, L0x2001991c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019920, L0x20019920 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019924, L0x20019924 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019928, L0x20019928 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001992c, L0x2001992c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019930, L0x20019930 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019934, L0x20019934 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019938, L0x20019938 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001993c, L0x2001993c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019940, L0x20019940 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019944, L0x20019944 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019948, L0x20019948 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001994c, L0x2001994c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019950, L0x20019950 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019954, L0x20019954 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019958, L0x20019958 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001995c, L0x2001995c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019960, L0x20019960 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019964, L0x20019964 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019968, L0x20019968 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001996c, L0x2001996c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019970, L0x20019970 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019974, L0x20019974 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019978, L0x20019978 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001997c, L0x2001997c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019980, L0x20019980 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019984, L0x20019984 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019988, L0x20019988 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001998c, L0x2001998c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019990, L0x20019990 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019994, L0x20019994 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019998, L0x20019998 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001999c, L0x2001999c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199a0, L0x200199a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199a4, L0x200199a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199a8, L0x200199a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199ac, L0x200199ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199b0, L0x200199b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199b4, L0x200199b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199b8, L0x200199b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199bc, L0x200199bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199c0, L0x200199c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199c4, L0x200199c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199c8, L0x200199c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199cc, L0x200199cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199d0, L0x200199d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199d4, L0x200199d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199d8, L0x200199d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199dc, L0x200199dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199e0, L0x200199e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199e4, L0x200199e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199e8, L0x200199e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199ec, L0x200199ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199f0, L0x200199f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199f4, L0x200199f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199f8, L0x200199f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199fc, L0x200199fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a00, L0x20019a00 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a04, L0x20019a04 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a08, L0x20019a08 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a0c, L0x20019a0c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a10, L0x20019a10 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a14, L0x20019a14 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a18, L0x20019a18 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a1c, L0x20019a1c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a20, L0x20019a20 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a24, L0x20019a24 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a28, L0x20019a28 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a2c, L0x20019a2c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a30, L0x20019a30 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a34, L0x20019a34 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a38, L0x20019a38 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a3c, L0x20019a3c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a40, L0x20019a40 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a44, L0x20019a44 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a48, L0x20019a48 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a4c, L0x20019a4c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a50, L0x20019a50 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a54, L0x20019a54 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a58, L0x20019a58 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a5c, L0x20019a5c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a60, L0x20019a60 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a64, L0x20019a64 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a68, L0x20019a68 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a6c, L0x20019a6c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a70, L0x20019a70 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a74, L0x20019a74 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a78, L0x20019a78 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a7c, L0x20019a7c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a80, L0x20019a80 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a84, L0x20019a84 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a88, L0x20019a88 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a8c, L0x20019a8c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a90, L0x20019a90 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a94, L0x20019a94 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a98, L0x20019a98 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a9c, L0x20019a9c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019aa0, L0x20019aa0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019aa4, L0x20019aa4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019aa8, L0x20019aa8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019aac, L0x20019aac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ab0, L0x20019ab0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ab4, L0x20019ab4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ab8, L0x20019ab8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019abc, L0x20019abc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ac0, L0x20019ac0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ac4, L0x20019ac4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ac8, L0x20019ac8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019acc, L0x20019acc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ad0, L0x20019ad0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ad4, L0x20019ad4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ad8, L0x20019ad8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019adc, L0x20019adc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ae0, L0x20019ae0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ae4, L0x20019ae4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ae8, L0x20019ae8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019aec, L0x20019aec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019af0, L0x20019af0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019af4, L0x20019af4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019af8, L0x20019af8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019afc, L0x20019afc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b00, L0x20019b00 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b04, L0x20019b04 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b08, L0x20019b08 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b0c, L0x20019b0c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b10, L0x20019b10 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b14, L0x20019b14 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b18, L0x20019b18 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b1c, L0x20019b1c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b20, L0x20019b20 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b24, L0x20019b24 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b28, L0x20019b28 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b2c, L0x20019b2c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b30, L0x20019b30 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b34, L0x20019b34 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b38, L0x20019b38 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b3c, L0x20019b3c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b40, L0x20019b40 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b44, L0x20019b44 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b48, L0x20019b48 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b4c, L0x20019b4c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b50, L0x20019b50 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b54, L0x20019b54 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b58, L0x20019b58 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b5c, L0x20019b5c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b60, L0x20019b60 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b64, L0x20019b64 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b68, L0x20019b68 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b6c, L0x20019b6c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b70, L0x20019b70 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b74, L0x20019b74 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b78, L0x20019b78 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b7c, L0x20019b7c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b80, L0x20019b80 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b84, L0x20019b84 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b88, L0x20019b88 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b8c, L0x20019b8c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b90, L0x20019b90 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b94, L0x20019b94 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b98, L0x20019b98 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b9c, L0x20019b9c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ba0, L0x20019ba0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ba4, L0x20019ba4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ba8, L0x20019ba8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bac, L0x20019bac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bb0, L0x20019bb0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bb4, L0x20019bb4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bb8, L0x20019bb8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bbc, L0x20019bbc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bc0, L0x20019bc0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bc4, L0x20019bc4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bc8, L0x20019bc8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bcc, L0x20019bcc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bd0, L0x20019bd0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bd4, L0x20019bd4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bd8, L0x20019bd8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bdc, L0x20019bdc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019be0, L0x20019be0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019be4, L0x20019be4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019be8, L0x20019be8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bec, L0x20019bec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bf0, L0x20019bf0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bf4, L0x20019bf4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bf8, L0x20019bf8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bfc, L0x20019bfc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c00, L0x20019c00 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c04, L0x20019c04 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c08, L0x20019c08 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c0c, L0x20019c0c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c10, L0x20019c10 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c14, L0x20019c14 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c18, L0x20019c18 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c1c, L0x20019c1c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c20, L0x20019c20 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c24, L0x20019c24 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c28, L0x20019c28 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c2c, L0x20019c2c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c30, L0x20019c30 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c34, L0x20019c34 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c38, L0x20019c38 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c3c, L0x20019c3c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c40, L0x20019c40 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c44, L0x20019c44 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c48, L0x20019c48 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c4c, L0x20019c4c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c50, L0x20019c50 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c54, L0x20019c54 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c58, L0x20019c58 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c5c, L0x20019c5c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c60, L0x20019c60 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c64, L0x20019c64 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c68, L0x20019c68 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c6c, L0x20019c6c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c70, L0x20019c70 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c74, L0x20019c74 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c78, L0x20019c78 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c7c, L0x20019c7c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c80, L0x20019c80 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c84, L0x20019c84 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c88, L0x20019c88 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c8c, L0x20019c8c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c90, L0x20019c90 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c94, L0x20019c94 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c98, L0x20019c98 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c9c, L0x20019c9c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ca0, L0x20019ca0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ca4, L0x20019ca4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ca8, L0x20019ca8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cac, L0x20019cac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cb0, L0x20019cb0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cb4, L0x20019cb4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cb8, L0x20019cb8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cbc, L0x20019cbc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cc0, L0x20019cc0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cc4, L0x20019cc4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cc8, L0x20019cc8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ccc, L0x20019ccc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cd0, L0x20019cd0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cd4, L0x20019cd4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cd8, L0x20019cd8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cdc, L0x20019cdc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ce0, L0x20019ce0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ce4, L0x20019ce4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ce8, L0x20019ce8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cec, L0x20019cec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cf0, L0x20019cf0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cf4, L0x20019cf4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cf8, L0x20019cf8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cfc, L0x20019cfc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d00, L0x20019d00 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d04, L0x20019d04 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d08, L0x20019d08 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d0c, L0x20019d0c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d10, L0x20019d10 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d14, L0x20019d14 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d18, L0x20019d18 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d1c, L0x20019d1c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d20, L0x20019d20 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d24, L0x20019d24 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d28, L0x20019d28 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d2c, L0x20019d2c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d30, L0x20019d30 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d34, L0x20019d34 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d38, L0x20019d38 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d3c, L0x20019d3c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d40, L0x20019d40 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d44, L0x20019d44 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d48, L0x20019d48 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d4c, L0x20019d4c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d50, L0x20019d50 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d54, L0x20019d54 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d58, L0x20019d58 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d5c, L0x20019d5c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d60, L0x20019d60 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d64, L0x20019d64 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d68, L0x20019d68 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d6c, L0x20019d6c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d70, L0x20019d70 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d74, L0x20019d74 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d78, L0x20019d78 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d7c, L0x20019d7c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d80, L0x20019d80 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d84, L0x20019d84 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d88, L0x20019d88 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d8c, L0x20019d8c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d90, L0x20019d90 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d94, L0x20019d94 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d98, L0x20019d98 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d9c, L0x20019d9c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019da0, L0x20019da0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019da4, L0x20019da4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019da8, L0x20019da8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dac, L0x20019dac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019db0, L0x20019db0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019db4, L0x20019db4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019db8, L0x20019db8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dbc, L0x20019dbc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dc0, L0x20019dc0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dc4, L0x20019dc4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dc8, L0x20019dc8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dcc, L0x20019dcc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dd0, L0x20019dd0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dd4, L0x20019dd4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dd8, L0x20019dd8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ddc, L0x20019ddc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019de0, L0x20019de0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019de4, L0x20019de4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019de8, L0x20019de8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dec, L0x20019dec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019df0, L0x20019df0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019df4, L0x20019df4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019df8, L0x20019df8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dfc, L0x20019dfc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e00, L0x20019e00 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e04, L0x20019e04 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e08, L0x20019e08 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e0c, L0x20019e0c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e10, L0x20019e10 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e14, L0x20019e14 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e18, L0x20019e18 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e1c, L0x20019e1c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e20, L0x20019e20 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e24, L0x20019e24 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e28, L0x20019e28 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e2c, L0x20019e2c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e30, L0x20019e30 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e34, L0x20019e34 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e38, L0x20019e38 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e3c, L0x20019e3c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e40, L0x20019e40 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e44, L0x20019e44 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e48, L0x20019e48 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e4c, L0x20019e4c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e50, L0x20019e50 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e54, L0x20019e54 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e58, L0x20019e58 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e5c, L0x20019e5c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e60, L0x20019e60 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e64, L0x20019e64 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e68, L0x20019e68 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e6c, L0x20019e6c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e70, L0x20019e70 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e74, L0x20019e74 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e78, L0x20019e78 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e7c, L0x20019e7c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e80, L0x20019e80 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e84, L0x20019e84 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e88, L0x20019e88 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e8c, L0x20019e8c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e90, L0x20019e90 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e94, L0x20019e94 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e98, L0x20019e98 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e9c, L0x20019e9c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ea0, L0x20019ea0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ea4, L0x20019ea4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ea8, L0x20019ea8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019eac, L0x20019eac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019eb0, L0x20019eb0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019eb4, L0x20019eb4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019eb8, L0x20019eb8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ebc, L0x20019ebc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ec0, L0x20019ec0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ec4, L0x20019ec4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ec8, L0x20019ec8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ecc, L0x20019ecc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ed0, L0x20019ed0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ed4, L0x20019ed4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ed8, L0x20019ed8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019edc, L0x20019edc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ee0, L0x20019ee0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ee4, L0x20019ee4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ee8, L0x20019ee8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019eec, L0x20019eec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ef0, L0x20019ef0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ef4, L0x20019ef4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ef8, L0x20019ef8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019efc, L0x20019efc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f00, L0x20019f00 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f04, L0x20019f04 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f08, L0x20019f08 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f0c, L0x20019f0c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f10, L0x20019f10 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f14, L0x20019f14 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f18, L0x20019f18 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f1c, L0x20019f1c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f20, L0x20019f20 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f24, L0x20019f24 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f28, L0x20019f28 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f2c, L0x20019f2c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f30, L0x20019f30 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f34, L0x20019f34 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f38, L0x20019f38 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f3c, L0x20019f3c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f40, L0x20019f40 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f44, L0x20019f44 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f48, L0x20019f48 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f4c, L0x20019f4c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f50, L0x20019f50 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f54, L0x20019f54 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f58, L0x20019f58 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f5c, L0x20019f5c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f60, L0x20019f60 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f64, L0x20019f64 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f68, L0x20019f68 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f6c, L0x20019f6c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f70, L0x20019f70 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f74, L0x20019f74 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f78, L0x20019f78 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f7c, L0x20019f7c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f80, L0x20019f80 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f84, L0x20019f84 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f88, L0x20019f88 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f8c, L0x20019f8c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f90, L0x20019f90 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f94, L0x20019f94 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f98, L0x20019f98 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f9c, L0x20019f9c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fa0, L0x20019fa0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fa4, L0x20019fa4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fa8, L0x20019fa8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fac, L0x20019fac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fb0, L0x20019fb0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fb4, L0x20019fb4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fb8, L0x20019fb8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fbc, L0x20019fbc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fc0, L0x20019fc0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fc4, L0x20019fc4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fc8, L0x20019fc8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fcc, L0x20019fcc <s 5@32*1043969@32
]
}

