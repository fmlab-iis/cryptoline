#! (%rsi) = %%EA
#! $1c(%rsi) = %%EA
#! (%rdx) = %%EA
#! $1c(%rdx) = %%EA
#! (%rdi) = %%EA
#! $1c(%rdi) = %%EA
#! (%rcx) = %%EA
#! $1c(%rcx) = %%EA
#! (%rsp) = %%EA
#! $1c(%rsp) = %%rsp_$1c
#! (%rbx) = %%EA
#! $1c(%rbx) = %%EA

#! %rax = %%rax
#! %rbx = %%rbx
#! %rcx = %%rcx
#! %rdx = %%rdx
#! %rbp = %%rbp
#! %rsi = %%rsi
#! %rdi = %%rdi
#! %rsp = %%rsp

#! %r$1c = %%r$1c


#! mov $1v, $2v -> mov $2v $1v
#! movq $1v, $2v -> mov $2v $1v
#! mov $1c, $2v -> mov $2v $1c@uint64
#! movabs $1c, $2v -> mov $2v $1c@uint64
#! add $1v, $2v -> adds carry $2v $1v $2v
#! add $1c, $2v -> adds carry $2v $1c@uint64 $2v
#! adc $1v, $2v -> adcs carry $2v $1v $2v carry
#! adc \$0x0, $2v -> adcs carry $2v 0@uint64 $2v carry;\nassert true && carry = 0@1;\nassume carry = 0 && true
#! adc $1c, $2v -> adcs carry $2v $1c@uint64 $2v carry
#! sub $1v, $2v -> subb carry $2v $2v $1v
#! sub $1c, $2v -> subb carry $2v $2v $1c@uint64
#! sbb $1v, $2v -> sbbs carry $2v $2v $1v carry
#! sbb $1c, $2v -> sbbs carry $2v $2v $1c@uint64 carry
#! cmovb $1v, $2v -> cmov $2v carry $1v $2v
#! and $1v, $2v -> and $2v@uint64 $1v $2v
#! and $1c, $2v -> and $2v@uint64 $1c@uint64 $2v
#! xor $1v, $1v -> mov $1v 0@uint64
#! xor $1v, $2v -> xor $2v@uint64 $1v $2v
#! mul $1v -> umull rdx rax $1v rax
#! mulq $1v -> umull rdx rax $1v rax
#! mulx $1v, $2v, $3v -> umull rdx $3v $1v $2v
#! adox $1v, $2v -> adcs OF $2v $1v $2v OF
#! adcx $1v, $2v -> adcs carry $2v $1v $2v carry
#! cmovae $1v, $2v -> cmov $2v carry $2v $1v
#! btr \$63, $2v -> subb carry dontcare 0x7fffffffffffffff@uint64 $2v;\nand $2v@uint64 $2v 0x7fffffffffffffff@uint64 
#! lea  \($1v,$2v\),$3v -> adds dontcare $3v $1v $2v 
#! lea  \-1\($1v\),$2v -> subb dontcare $2v $1v 1@sint64
#! test \$1, $1v -> and T$1v@sint64 0x1@sint64 $1v;\nsubb ZF dontcare T$1v 1@sint64 
#! cmovne $1v, $2v -> cmov $2v ZF $2v $1v 
#! cmove $1v, $2v -> cmov $2v ZF $1v $2v
#! not $1v -> not $1v@sint64 $1v
#! sar $2c, $1v -> ssplit $1v dontcare $1v $2c 
#! cmovge $1v, $2v -> cmov $2v SFeOF $1v $2v
#! cmovl $1v, $2v -> cmov $2v SFeOF $2v $1v
#! cmp \$0, $2v -> subb SFeOF dontcare $2v (-(2)**(63))@sint64
#! shrd $1c, $2v, $3v -> join tmp $2v $3v;\nusplit tmp dontcare tmp $1c;\ncast $3v@uint64 tmp
#! imul $1v, $2v -> smull dontcare $2v $1v $2v
#! imulq $1v, $2v -> smull dontcare $2v $1v $2v
#! shl $1c, $2v -> and $2v@uint64 $2v (2**(64-$1c)-1)@sint64;\nshl $2v $2v $1c;\ncast $2v@sint64 $2v 

# qhasm: f0 <<= 60
# asm 1: shl  $60,<f0=int64#12
# asm 2: shl  $60,<f0=%r14
shl  $60,%r14

# qhasm: g0 <<= 60
# asm 1: shl  $60,<g0=int64#13
# asm 2: shl  $60,<g0=%r15
shl  $60,%r15

# qhasm: f += f0
# asm 1: add  <f0=int64#12,<f=int64#1
# asm 2: add  <f0=%r14,<f=%rdi
add  %r14,%rdi

# qhasm: g += g0
# asm 1: add  <g0=int64#13,<g=int64#6
# asm 2: add  <g0=%r15,<g=%r9
add  %r15,%r9

# qhasm: f *= u
# asm 1: imul  <u=int64#8,<f=int64#1
# asm 2: imul  <u=%r10,<f=%rdi
imul  %r10,%rdi

# qhasm: g *= v
# asm 1: imul  <v=int64#9,<g=int64#6
# asm 2: imul  <v=%r11,<g=%r9
imul  %r11,%r9

# qhasm: t0 = f + g
# asm 1: lea  (<f=int64#1,<g=int64#6),>t0=int64#1
# asm 2: lea  (<f=%rdi,<g=%r9),>t0=%rdi
lea  (%rdi,%r9),%rdi

# qhasm: (int64) t0 >>= 60
# asm 1: sar  $60,<t0=int64#1
# asm 2: sar  $60,<t0=%rdi
sar  $60,%rdi

# qhasm: u *= t0
# asm 1: imul  <t0=int64#1,<u=int64#8
# asm 2: imul  <t0=%rdi,<u=%r10
imul  %rdi,%r10

# qhasm: v *= t0
# asm 1: imul  <t0=int64#1,<v=int64#9
# asm 2: imul  <t0=%rdi,<v=%r11
imul  %rdi,%r11
