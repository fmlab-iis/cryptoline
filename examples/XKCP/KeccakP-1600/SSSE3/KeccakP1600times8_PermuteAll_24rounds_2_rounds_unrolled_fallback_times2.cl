const KeccakF1600RoundConstants_00 = 0x0000000000000001
const KeccakF1600RoundConstants_01 = 0x0000000000008082
const KeccakF1600RoundConstants_02 = 0x800000000000808a
const KeccakF1600RoundConstants_03 = 0x8000000080008000
const KeccakF1600RoundConstants_04 = 0x000000000000808b
const KeccakF1600RoundConstants_05 = 0x0000000080000001
const KeccakF1600RoundConstants_06 = 0x8000000080008081
const KeccakF1600RoundConstants_07 = 0x8000000000008009
const KeccakF1600RoundConstants_08 = 0x000000000000008a
const KeccakF1600RoundConstants_09 = 0x0000000000000088
const KeccakF1600RoundConstants_10 = 0x0000000080008009
const KeccakF1600RoundConstants_11 = 0x000000008000000a
const KeccakF1600RoundConstants_12 = 0x000000008000808b
const KeccakF1600RoundConstants_13 = 0x800000000000008b
const KeccakF1600RoundConstants_14 = 0x8000000000008089
const KeccakF1600RoundConstants_15 = 0x8000000000008003
const KeccakF1600RoundConstants_16 = 0x8000000000008002
const KeccakF1600RoundConstants_17 = 0x8000000000000080
const KeccakF1600RoundConstants_18 = 0x000000000000800a
const KeccakF1600RoundConstants_19 = 0x800000008000000a
const KeccakF1600RoundConstants_20 = 0x8000000080008081
const KeccakF1600RoundConstants_21 = 0x8000000000008080
const KeccakF1600RoundConstants_22 = 0x0000000080000001
const KeccakF1600RoundConstants_23 = 0x8000000080008008

const rho8_0 = 0x0605040302010007
const rho8_1 = 0x0E0D0C0B0A09080F
const rho56_0 = 0x0007060504030201
const rho56_1 = 0x080F0E0D0C0B0A09

proc stb64(uint64 x; uint8 x_0, uint8 x_1, uint8 x_2, uint8 x_3, uint8 x_4, uint8 x_5, uint8 x_6, uint8 x_7) =
{ true && true }
spl tmp x_0 x 8;
spl tmp x_1 tmp 8;
spl tmp x_2 tmp 8;
spl tmp x_3 tmp 8;
spl tmp x_4 tmp 8;
spl tmp x_5 tmp 8;
spl x_7 x_6 tmp 8;
{ true && true }

proc sel128(uint8 idx, uint128 src; uint8 dst) =
{ true && true }
spl idx_7 dontcare idx 7;
cast idx_7@bit idx_7;
spl dontcare idx_0t4 idx 4;
cast idx_0t4@uint128 idx_0t4;
shr tmp src idx_0t4;
shr tmp tmp idx_0t4;
shr tmp tmp idx_0t4;
shr tmp tmp idx_0t4;
shr tmp tmp idx_0t4;
shr tmp tmp idx_0t4;
shr tmp tmp idx_0t4;
shr tmp tmp idx_0t4;
spl dontcare dst tmp 8;
cmov dst idx_7 0@uint8 dst;
{ true && true }

proc jb64(uint8 src_0, uint8 src_1, uint8 src_2, uint8 src_3, uint8 src_4, uint8 src_5, uint8 src_6, uint8 src_7; uint8 dst) =
{ true && true }
join tmp src_1 src_0;
join tmp src_2 tmp;
join tmp src_3 tmp;
join tmp src_4 tmp;
join tmp src_5 tmp;
join tmp src_6 tmp;
join dst src_7 tmp;
{ true && true }

proc vpshufb128(uint64 idx_0, uint64 idx_1, uint64 src_0, uint64 src_1; uint64 dst_0, uint64 dst_1) =
{ true && true }
inline stb64(idx_0, idx_00, idx_01, idx_02, idx_03, idx_04, idx_05, idx_06, idx_07);
inline stb64(idx_1, idx_08, idx_09, idx_10, idx_11, idx_12, idx_13, idx_14, idx_15);
join src src_1 src_0;
inline sel128(idx_00, src, dst_00);
inline sel128(idx_01, src, dst_01);
inline sel128(idx_02, src, dst_02);
inline sel128(idx_03, src, dst_03);
inline sel128(idx_04, src, dst_04);
inline sel128(idx_05, src, dst_05);
inline sel128(idx_06, src, dst_06);
inline sel128(idx_07, src, dst_07);
inline sel128(idx_08, src, dst_08);
inline sel128(idx_09, src, dst_09);
inline sel128(idx_10, src, dst_10);
inline sel128(idx_11, src, dst_11);
inline sel128(idx_12, src, dst_12);
inline sel128(idx_13, src, dst_13);
inline sel128(idx_14, src, dst_14);
inline sel128(idx_15, src, dst_15);
inline jb64(dst_00, dst_01, dst_02, dst_03, dst_04, dst_05, dst_06, dst_07, dst_0);
inline jb64(dst_08, dst_09, dst_10, dst_11, dst_12, dst_13, dst_14, dst_15, dst_1);
{ true && true }


proc main (uint64 A00, uint64 A01, uint64 A02, uint64 A03, uint64 A04,
           uint64 A05, uint64 A06, uint64 A07, uint64 A08, uint64 A09,
           uint64 A10, uint64 A11, uint64 A12, uint64 A13, uint64 A14,
           uint64 A15, uint64 A16, uint64 A17, uint64 A18, uint64 A19,
           uint64 A20, uint64 A21, uint64 A22, uint64 A23, uint64 A24,
           uint64 B00, uint64 B01, uint64 B02, uint64 B03, uint64 B04,
           uint64 B05, uint64 B06, uint64 B07, uint64 B08, uint64 B09,
           uint64 B10, uint64 B11, uint64 B12, uint64 B13, uint64 B14,
           uint64 B15, uint64 B16, uint64 B17, uint64 B18, uint64 B19,
           uint64 B20, uint64 B21, uint64 B22, uint64 B23, uint64 B24,
           uint64 C00, uint64 C01, uint64 C02, uint64 C03, uint64 C04,
           uint64 C05, uint64 C06, uint64 C07, uint64 C08, uint64 C09,
           uint64 C10, uint64 C11, uint64 C12, uint64 C13, uint64 C14,
           uint64 C15, uint64 C16, uint64 C17, uint64 C18, uint64 C19,
           uint64 C20, uint64 C21, uint64 C22, uint64 C23, uint64 C24,
           uint64 D00, uint64 D01, uint64 D02, uint64 D03, uint64 D04,
           uint64 D05, uint64 D06, uint64 D07, uint64 D08, uint64 D09,
           uint64 D10, uint64 D11, uint64 D12, uint64 D13, uint64 D14,
           uint64 D15, uint64 D16, uint64 D17, uint64 D18, uint64 D19,
           uint64 D20, uint64 D21, uint64 D22, uint64 D23, uint64 D24,
           uint64 E00, uint64 E01, uint64 E02, uint64 E03, uint64 E04,
           uint64 E05, uint64 E06, uint64 E07, uint64 E08, uint64 E09,
           uint64 E10, uint64 E11, uint64 E12, uint64 E13, uint64 E14,
           uint64 E15, uint64 E16, uint64 E17, uint64 E18, uint64 E19,
           uint64 E20, uint64 E21, uint64 E22, uint64 E23, uint64 E24,
           uint64 F00, uint64 F01, uint64 F02, uint64 F03, uint64 F04,
           uint64 F05, uint64 F06, uint64 F07, uint64 F08, uint64 F09,
           uint64 F10, uint64 F11, uint64 F12, uint64 F13, uint64 F14,
           uint64 F15, uint64 F16, uint64 F17, uint64 F18, uint64 F19,
           uint64 F20, uint64 F21, uint64 F22, uint64 F23, uint64 F24,
           uint64 G00, uint64 G01, uint64 G02, uint64 G03, uint64 G04,
           uint64 G05, uint64 G06, uint64 G07, uint64 G08, uint64 G09,
           uint64 G10, uint64 G11, uint64 G12, uint64 G13, uint64 G14,
           uint64 G15, uint64 G16, uint64 G17, uint64 G18, uint64 G19,
           uint64 G20, uint64 G21, uint64 G22, uint64 G23, uint64 G24,
           uint64 H00, uint64 H01, uint64 H02, uint64 H03, uint64 H04,
           uint64 H05, uint64 H06, uint64 H07, uint64 H08, uint64 H09,
           uint64 H10, uint64 H11, uint64 H12, uint64 H13, uint64 H14,
           uint64 H15, uint64 H16, uint64 H17, uint64 H18, uint64 H19,
           uint64 H20, uint64 H21, uint64 H22, uint64 H23, uint64 H24) =
{
  true
  &&
  true
}

(* ===== Initialization ===== *)

mov L0x55555559a440 $rho8_0@uint64;
mov L0x55555559a448 $rho8_1@uint64;
mov L0x55555559a450 $rho56_0@uint64;
mov L0x55555559a458 $rho56_1@uint64;

mov L0x55555559a390 $KeccakF1600RoundConstants_02@uint64;
mov L0x55555559a398 $KeccakF1600RoundConstants_03@uint64;
mov L0x55555559a3a0 $KeccakF1600RoundConstants_04@uint64;
mov L0x55555559a3a8 $KeccakF1600RoundConstants_05@uint64;
mov L0x55555559a3b0 $KeccakF1600RoundConstants_06@uint64;
mov L0x55555559a3b8 $KeccakF1600RoundConstants_07@uint64;
mov L0x55555559a3c0 $KeccakF1600RoundConstants_08@uint64;
mov L0x55555559a3c8 $KeccakF1600RoundConstants_09@uint64;
mov L0x55555559a3d0 $KeccakF1600RoundConstants_10@uint64;
mov L0x55555559a3d8 $KeccakF1600RoundConstants_11@uint64;
mov L0x55555559a3e0 $KeccakF1600RoundConstants_12@uint64;
mov L0x55555559a3e8 $KeccakF1600RoundConstants_13@uint64;
mov L0x55555559a3f0 $KeccakF1600RoundConstants_14@uint64;
mov L0x55555559a3f8 $KeccakF1600RoundConstants_15@uint64;
mov L0x55555559a400 $KeccakF1600RoundConstants_16@uint64;
mov L0x55555559a408 $KeccakF1600RoundConstants_17@uint64;
mov L0x55555559a410 $KeccakF1600RoundConstants_18@uint64;
mov L0x55555559a418 $KeccakF1600RoundConstants_19@uint64;
mov L0x55555559a420 $KeccakF1600RoundConstants_20@uint64;
mov L0x55555559a428 $KeccakF1600RoundConstants_21@uint64;
mov L0x55555559a430 $KeccakF1600RoundConstants_22@uint64;
mov L0x55555559a438 $KeccakF1600RoundConstants_23@uint64;

mov L0x7fffffffd710 A00;
mov L0x7fffffffd718 B00;
mov L0x7fffffffd720 A01;
mov L0x7fffffffd728 B01;
mov L0x7fffffffd730 A02;
mov L0x7fffffffd738 B02;
mov L0x7fffffffd740 A03;
mov L0x7fffffffd748 B03;
mov L0x7fffffffd750 A04;
mov L0x7fffffffd758 B04;
mov L0x7fffffffd760 A05;
mov L0x7fffffffd768 B05;
mov L0x7fffffffd770 A06;
mov L0x7fffffffd778 B06;
mov L0x7fffffffd780 A07;
mov L0x7fffffffd788 B07;
mov L0x7fffffffd790 A08;
mov L0x7fffffffd798 B08;
mov L0x7fffffffd7a0 A09;
mov L0x7fffffffd7a8 B09;
mov L0x7fffffffd7b0 A10;
mov L0x7fffffffd7b8 B10;
mov L0x7fffffffd7c0 A11;
mov L0x7fffffffd7c8 B11;
mov L0x7fffffffd7d0 A12;
mov L0x7fffffffd7d8 B12;
mov L0x7fffffffd7e0 A13;
mov L0x7fffffffd7e8 B13;
mov L0x7fffffffd7f0 A14;
mov L0x7fffffffd7f8 B14;
mov L0x7fffffffd800 A15;
mov L0x7fffffffd808 B15;
mov L0x7fffffffd810 A16;
mov L0x7fffffffd818 B16;
mov L0x7fffffffd820 A17;
mov L0x7fffffffd828 B17;
mov L0x7fffffffd830 A18;
mov L0x7fffffffd838 B18;
mov L0x7fffffffd840 A19;
mov L0x7fffffffd848 B19;
mov L0x7fffffffd850 A20;
mov L0x7fffffffd858 B20;
mov L0x7fffffffd860 A21;
mov L0x7fffffffd868 B21;
mov L0x7fffffffd870 A22;
mov L0x7fffffffd878 B22;
mov L0x7fffffffd880 A23;
mov L0x7fffffffd888 B23;
mov L0x7fffffffd890 A24;
mov L0x7fffffffd898 B24;

mov L0x7fffffffd8a0 C00;
mov L0x7fffffffd8a8 D00;
mov L0x7fffffffd8b0 C01;
mov L0x7fffffffd8b8 D01;
mov L0x7fffffffd8c0 C02;
mov L0x7fffffffd8c8 D02;
mov L0x7fffffffd8d0 C03;
mov L0x7fffffffd8d8 D03;
mov L0x7fffffffd8e0 C04;
mov L0x7fffffffd8e8 D04;
mov L0x7fffffffd8f0 C05;
mov L0x7fffffffd8f8 D05;
mov L0x7fffffffd900 C06;
mov L0x7fffffffd908 D06;
mov L0x7fffffffd910 C07;
mov L0x7fffffffd918 D07;
mov L0x7fffffffd920 C08;
mov L0x7fffffffd928 D08;
mov L0x7fffffffd930 C09;
mov L0x7fffffffd938 D09;
mov L0x7fffffffd940 C10;
mov L0x7fffffffd948 D10;
mov L0x7fffffffd950 C11;
mov L0x7fffffffd958 D11;
mov L0x7fffffffd960 C12;
mov L0x7fffffffd968 D12;
mov L0x7fffffffd970 C13;
mov L0x7fffffffd978 D13;
mov L0x7fffffffd980 C14;
mov L0x7fffffffd988 D14;
mov L0x7fffffffd990 C15;
mov L0x7fffffffd998 D15;
mov L0x7fffffffd9a0 C16;
mov L0x7fffffffd9a8 D16;
mov L0x7fffffffd9b0 C17;
mov L0x7fffffffd9b8 D17;
mov L0x7fffffffd9c0 C18;
mov L0x7fffffffd9c8 D18;
mov L0x7fffffffd9d0 C19;
mov L0x7fffffffd9d8 D19;
mov L0x7fffffffd9e0 C20;
mov L0x7fffffffd9e8 D20;
mov L0x7fffffffd9f0 C21;
mov L0x7fffffffd9f8 D21;
mov L0x7fffffffda00 C22;
mov L0x7fffffffda08 D22;
mov L0x7fffffffda10 C23;
mov L0x7fffffffda18 D23;
mov L0x7fffffffda20 C24;
mov L0x7fffffffda28 D24;

mov L0x7fffffffda30 E00;
mov L0x7fffffffda38 F00;
mov L0x7fffffffda40 E01;
mov L0x7fffffffda48 F01;
mov L0x7fffffffda50 E02;
mov L0x7fffffffda58 F02;
mov L0x7fffffffda60 E03;
mov L0x7fffffffda68 F03;
mov L0x7fffffffda70 E04;
mov L0x7fffffffda78 F04;
mov L0x7fffffffda80 E05;
mov L0x7fffffffda88 F05;
mov L0x7fffffffda90 E06;
mov L0x7fffffffda98 F06;
mov L0x7fffffffdaa0 E07;
mov L0x7fffffffdaa8 F07;
mov L0x7fffffffdab0 E08;
mov L0x7fffffffdab8 F08;
mov L0x7fffffffdac0 E09;
mov L0x7fffffffdac8 F09;
mov L0x7fffffffdad0 E10;
mov L0x7fffffffdad8 F10;
mov L0x7fffffffdae0 E11;
mov L0x7fffffffdae8 F11;
mov L0x7fffffffdaf0 E12;
mov L0x7fffffffdaf8 F12;
mov L0x7fffffffdb00 E13;
mov L0x7fffffffdb08 F13;
mov L0x7fffffffdb10 E14;
mov L0x7fffffffdb18 F14;
mov L0x7fffffffdb20 E15;
mov L0x7fffffffdb28 F15;
mov L0x7fffffffdb30 E16;
mov L0x7fffffffdb38 F16;
mov L0x7fffffffdb40 E17;
mov L0x7fffffffdb48 F17;
mov L0x7fffffffdb50 E18;
mov L0x7fffffffdb58 F18;
mov L0x7fffffffdb60 E19;
mov L0x7fffffffdb68 F19;
mov L0x7fffffffdb70 E20;
mov L0x7fffffffdb78 F20;
mov L0x7fffffffdb80 E21;
mov L0x7fffffffdb88 F21;
mov L0x7fffffffdb90 E22;
mov L0x7fffffffdb98 F22;
mov L0x7fffffffdba0 E23;
mov L0x7fffffffdba8 F23;
mov L0x7fffffffdbb0 E24;
mov L0x7fffffffdbb8 F24;

mov L0x7fffffffdbc0 G00;
mov L0x7fffffffdbc8 H00;
mov L0x7fffffffdbd0 G01;
mov L0x7fffffffdbd8 H01;
mov L0x7fffffffdbe0 G02;
mov L0x7fffffffdbe8 H02;
mov L0x7fffffffdbf0 G03;
mov L0x7fffffffdbf8 H03;
mov L0x7fffffffdc00 G04;
mov L0x7fffffffdc08 H04;
mov L0x7fffffffdc10 G05;
mov L0x7fffffffdc18 H05;
mov L0x7fffffffdc20 G06;
mov L0x7fffffffdc28 H06;
mov L0x7fffffffdc30 G07;
mov L0x7fffffffdc38 H07;
mov L0x7fffffffdc40 G08;
mov L0x7fffffffdc48 H08;
mov L0x7fffffffdc50 G09;
mov L0x7fffffffdc58 H09;
mov L0x7fffffffdc60 G10;
mov L0x7fffffffdc68 H10;
mov L0x7fffffffdc70 G11;
mov L0x7fffffffdc78 H11;
mov L0x7fffffffdc80 G12;
mov L0x7fffffffdc88 H12;
mov L0x7fffffffdc90 G13;
mov L0x7fffffffdc98 H13;
mov L0x7fffffffdca0 G14;
mov L0x7fffffffdca8 H14;
mov L0x7fffffffdcb0 G15;
mov L0x7fffffffdcb8 H15;
mov L0x7fffffffdcc0 G16;
mov L0x7fffffffdcc8 H16;
mov L0x7fffffffdcd0 G17;
mov L0x7fffffffdcd8 H17;
mov L0x7fffffffdce0 G18;
mov L0x7fffffffdce8 H18;
mov L0x7fffffffdcf0 G19;
mov L0x7fffffffdcf8 H19;
mov L0x7fffffffdd00 G20;
mov L0x7fffffffdd08 H20;
mov L0x7fffffffdd10 G21;
mov L0x7fffffffdd18 H21;
mov L0x7fffffffdd20 G22;
mov L0x7fffffffdd28 H22;
mov L0x7fffffffdd30 G23;
mov L0x7fffffffdd38 H23;
mov L0x7fffffffdd40 G24;
mov L0x7fffffffdd48 H24;

nondet rax@uint64;
nondet rdi@uint64;
nondet rsp@uint64;

(* ===== Program ===== *)

(* #! -> SP = 0x7fffffffd708 *)
#! 0x7fffffffd708 = 0x7fffffffd708;
(* mov    %rdi,%rbx                                #! PC = 0x555555586955 *)
mov rbx rdi;
(* #call   0x555555583ea0 <KeccakP1600times2_PermuteAll_24rounds>#! PC = 0x555555586958 *)
#call   0x555555583ea0 <KeccakP1600times2_PermuteAll_24rounds>#! 0x555555586958 = 0x555555586958;
(* #! -> SP = 0x7fffffffd6f8 *)
#! 0x7fffffffd6f8 = 0x7fffffffd6f8;
(* sub    $0x10,%rsp                               #! PC = 0x555555583ea4 *)
subb carry rsp rsp 0x10@uint64;
(* vmovdqa 0xf0(%rdi),%xmm3                        #! EA = L0x7fffffffd800; Value = 0x0000000000000000; PC = 0x555555583ea8 *)
mov xmm3_0 L0x7fffffffd800;
mov xmm3_1 L0x7fffffffd808;
(* vmovdqa64 0x50(%rdi),%xmm29                     #! EA = L0x7fffffffd760; Value = 0x0000003000000000; PC = 0x555555583eb0 *)
mov xmm29_0 L0x7fffffffd760;
mov xmm29_1 L0x7fffffffd768;
(* vmovdqa64 0xa0(%rdi),%xmm21                     #! EA = L0x7fffffffd7b0; Value = 0x00007ffff7eb2580; PC = 0x555555583eb7 *)
mov xmm21_0 L0x7fffffffd7b0;
mov xmm21_1 L0x7fffffffd7b8;
(* vmovdqa64 0x140(%rdi),%xmm16                    #! EA = L0x7fffffffd850; Value = 0x0000000000000000; PC = 0x555555583ebe *)
mov xmm16_0 L0x7fffffffd850;
mov xmm16_1 L0x7fffffffd858;
(* vmovdqa %xmm3,%xmm1                             #! PC = 0x555555583ec5 *)
mov xmm1_0 xmm3_0;
mov xmm1_1 xmm3_1;
(* vmovdqa 0x60(%rdi),%xmm9                        #! EA = L0x7fffffffd770; Value = 0x00007fffffffdcc0; PC = 0x555555583ec9 *)
mov xmm9_0 L0x7fffffffd770;
mov xmm9_1 L0x7fffffffd778;
(* vmovdqa64 0xb0(%rdi),%xmm28                     #! EA = L0x7fffffffd7c0; Value = 0x0000000000000001; PC = 0x555555583ece *)
mov xmm28_0 L0x7fffffffd7c0;
mov xmm28_1 L0x7fffffffd7c8;
(* vmovdqa64 0x100(%rdi),%xmm19                    #! EA = L0x7fffffffd810; Value = 0x0000000000000000; PC = 0x555555583ed5 *)
mov xmm19_0 L0x7fffffffd810;
mov xmm19_1 L0x7fffffffd818;
(* vmovdqa64 0x150(%rdi),%xmm26                    #! EA = L0x7fffffffd860; Value = 0x00007fffffffdd90; PC = 0x555555583edc *)
mov xmm26_0 L0x7fffffffd860;
mov xmm26_1 L0x7fffffffd868;
(* vpxorq %xmm1,%xmm16,%xmm0                       #! PC = 0x555555583ee3 *)
xor xmm0_0@uint64 xmm16_0 xmm1_0;
xor xmm0_1@uint64 xmm16_1 xmm1_1;
(* vmovdqa64 0x70(%rdi),%xmm22                     #! EA = L0x7fffffffd780; Value = 0x0000000000000d68; PC = 0x555555583ee9 *)
mov xmm22_0 L0x7fffffffd780;
mov xmm22_1 L0x7fffffffd788;
(* vmovdqa 0xc0(%rdi),%xmm8                        #! EA = L0x7fffffffd7d0; Value = 0xffffffffffffffff; PC = 0x555555583ef0 *)
mov xmm8_0 L0x7fffffffd7d0;
mov xmm8_1 L0x7fffffffd7d8;
(* vmovdqa64 0x110(%rdi),%xmm18                    #! EA = L0x7fffffffd820; Value = 0x0000000000000000; PC = 0x555555583ef8 *)
mov xmm18_0 L0x7fffffffd820;
mov xmm18_1 L0x7fffffffd828;
(* vmovdqa 0x160(%rdi),%xmm14                      #! EA = L0x7fffffffd870; Value = 0x00007fffffffdda0; PC = 0x555555583eff *)
mov xmm14_0 L0x7fffffffd870;
mov xmm14_1 L0x7fffffffd878;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x555555583f07 *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vmovdqa 0x80(%rdi),%xmm6                        #! EA = L0x7fffffffd790; Value = 0x00007fff00000000; PC = 0x555555583f0d *)
mov xmm6_0 L0x7fffffffd790;
mov xmm6_1 L0x7fffffffd798;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x555555583f15 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm19,%xmm26,%xmm1                      #! PC = 0x555555583f19 *)
xor xmm1_0@uint64 xmm26_0 xmm19_0;
xor xmm1_1@uint64 xmm26_1 xmm19_1;
(* vmovdqa 0xd0(%rdi),%xmm7                        #! EA = L0x7fffffffd7e0; Value = 0x0000000000000000; PC = 0x555555583f1f *)
mov xmm7_0 L0x7fffffffd7e0;
mov xmm7_1 L0x7fffffffd7e8;
(* vmovdqa 0x120(%rdi),%xmm12                      #! EA = L0x7fffffffd830; Value = 0x00007ffff7cfa0d0; PC = 0x555555583f27 *)
mov xmm12_0 L0x7fffffffd830;
mov xmm12_1 L0x7fffffffd838;
(* vmovdqa %xmm3,-0x68(%rsp)                       #! EA = L0x7fffffffd680; PC = 0x555555583f2f *)
mov L0x7fffffffd680 xmm3_0;
mov L0x7fffffffd688 xmm3_1;
(* vpxorq %xmm9,%xmm28,%xmm0                       #! PC = 0x555555583f35 *)
xor xmm0_0@uint64 xmm28_0 xmm9_0;
xor xmm0_1@uint64 xmm28_1 xmm9_1;
(* vmovdqa 0x170(%rdi),%xmm3                       #! EA = L0x7fffffffd880; Value = 0x00007ffff7fbc6e8; PC = 0x555555583f3b *)
mov xmm3_0 L0x7fffffffd880;
mov xmm3_1 L0x7fffffffd888;
(* vmovdqa64 0xe0(%rdi),%xmm25                     #! EA = L0x7fffffffd7f0; Value = 0x0001000000000000; PC = 0x555555583f43 *)
mov xmm25_0 L0x7fffffffd7f0;
mov xmm25_1 L0x7fffffffd7f8;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x555555583f4a *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxorq %xmm18,%xmm14,%xmm2                      #! PC = 0x555555583f4e *)
xor xmm2_0@uint64 xmm14_0 xmm18_0;
xor xmm2_1@uint64 xmm14_1 xmm18_1;
(* vpxorq %xmm22,%xmm8,%xmm1                       #! PC = 0x555555583f54 *)
xor xmm1_0@uint64 xmm8_0 xmm22_0;
xor xmm1_1@uint64 xmm8_1 xmm22_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x555555583f5a *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpxorq %xmm6,%xmm7,%xmm17                       #! PC = 0x555555583f5e *)
xor xmm17_0@uint64 xmm7_0 xmm6_0;
xor xmm17_1@uint64 xmm7_1 xmm6_1;
(* vpxor  %xmm12,%xmm3,%xmm2                       #! PC = 0x555555583f64 *)
xor xmm2_0@uint64 xmm3_0 xmm12_0;
xor xmm2_1@uint64 xmm3_1 xmm12_1;
(* vmovdqa 0x90(%rdi),%xmm5                        #! EA = L0x7fffffffd7a0; Value = 0x00007fff00000000; PC = 0x555555583f69 *)
mov xmm5_0 L0x7fffffffd7a0;
mov xmm5_1 L0x7fffffffd7a8;
(* vmovdqa 0x130(%rdi),%xmm4                       #! EA = L0x7fffffffd840; Value = 0x00007ffff7cfa0d0; PC = 0x555555583f71 *)
mov xmm4_0 L0x7fffffffd840;
mov xmm4_1 L0x7fffffffd848;
(* vmovdqa 0x180(%rdi),%xmm15                      #! EA = L0x7fffffffd890; Value = 0x0000000000000226; PC = 0x555555583f79 *)
mov xmm15_0 L0x7fffffffd890;
mov xmm15_1 L0x7fffffffd898;
(* vmovdqa64 0x40(%rdi),%xmm31                     #! EA = L0x7fffffffd750; Value = 0x0001003000000000; PC = 0x555555583f81 *)
mov xmm31_0 L0x7fffffffd750;
mov xmm31_1 L0x7fffffffd758;
(* vpxorq %xmm2,%xmm17,%xmm17                      #! PC = 0x555555583f88 *)
xor xmm17_0@uint64 xmm17_0 xmm2_0;
xor xmm17_1@uint64 xmm17_1 xmm2_1;
(* vmovdqa64 0x30(%rdi),%xmm24                     #! EA = L0x7fffffffd740; Value = 0x000055555559bcea; PC = 0x555555583f8e *)
mov xmm24_0 L0x7fffffffd740;
mov xmm24_1 L0x7fffffffd748;
(* vmovdqa64 %xmm25,%xmm2                          #! PC = 0x555555583f95 *)
mov xmm2_0 xmm25_0;
mov xmm2_1 xmm25_1;
(* vmovdqa 0x10(%rdi),%xmm11                       #! EA = L0x7fffffffd720; Value = 0x00007ffff7eb2780; PC = 0x555555583f9b *)
mov xmm11_0 L0x7fffffffd720;
mov xmm11_1 L0x7fffffffd728;
(* vmovdqa 0x20(%rdi),%xmm10                       #! EA = L0x7fffffffd730; Value = 0x000055555559bcec; PC = 0x555555583fa0 *)
mov xmm10_0 L0x7fffffffd730;
mov xmm10_1 L0x7fffffffd738;
(* vmovdqa64 (%rdi),%xmm27                         #! EA = L0x7fffffffd710; Value = 0x00007ffff7eb2580; PC = 0x555555583fa5 *)
mov xmm27_0 L0x7fffffffd710;
mov xmm27_1 L0x7fffffffd718;
(* vmovdqa %xmm6,-0x78(%rsp)                       #! EA = L0x7fffffffd670; PC = 0x555555583fab *)
mov L0x7fffffffd670 xmm6_0;
mov L0x7fffffffd678 xmm6_1;
(* vpxor  %xmm2,%xmm5,%xmm2                        #! PC = 0x555555583fb1 *)
xor xmm2_0@uint64 xmm5_0 xmm2_0;
xor xmm2_1@uint64 xmm5_1 xmm2_1;
(* vpxor  %xmm4,%xmm15,%xmm6                       #! PC = 0x555555583fb5 *)
xor xmm6_0@uint64 xmm15_0 xmm4_0;
xor xmm6_1@uint64 xmm15_1 xmm4_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x555555583fb9 *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vpxorq %xmm24,%xmm17,%xmm17                     #! PC = 0x555555583fbd *)
xor xmm17_0@uint64 xmm17_0 xmm24_0;
xor xmm17_1@uint64 xmm17_1 xmm24_1;
(* vmovdqa64 %xmm31,%xmm6                          #! PC = 0x555555583fc3 *)
mov xmm6_0 xmm31_0;
mov xmm6_1 xmm31_1;
(* vmovdqa %xmm11,-0x48(%rsp)                      #! EA = L0x7fffffffd6a0; PC = 0x555555583fd0 *)
mov L0x7fffffffd6a0 xmm11_0;
mov L0x7fffffffd6a8 xmm11_1;
(* vmovdqa %xmm10,-0x8(%rsp)                       #! EA = L0x7fffffffd6e0; PC = 0x555555583fd6 *)
mov L0x7fffffffd6e0 xmm10_0;
mov L0x7fffffffd6e8 xmm10_1;
(* vmovdqa64 %xmm31,-0x58(%rsp)                    #! EA = L0x7fffffffd690; PC = 0x555555583fdc *)
mov L0x7fffffffd690 xmm31_0;
mov L0x7fffffffd698 xmm31_1;
(* vmovdqa64 %xmm25,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x555555583fe7 *)
mov L0x7fffffffd6d0 xmm25_0;
mov L0x7fffffffd6d8 xmm25_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555583ff2 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxor  %xmm11,%xmm0,%xmm0                       #! PC = 0x555555583ff8 *)
xor xmm0_0@uint64 xmm0_0 xmm11_0;
xor xmm0_1@uint64 xmm0_1 xmm11_1;
(* vpxor  %xmm10,%xmm1,%xmm1                       #! PC = 0x555555583ffd *)
xor xmm1_0@uint64 xmm1_0 xmm10_0;
xor xmm1_1@uint64 xmm1_1 xmm10_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x555555584002 *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* mov    $0x8082,%ecx                             #! PC = 0x55555558400d *)
mov rcx 0x8082@uint64;
(* mov    $0x1,%edx                                #! PC = 0x555555584012 *)
mov rdx 0x1@uint64;
(* vmovdqa64 %xmm17,%xmm20                         #! PC = 0x555555584017 *)
mov xmm20_0 xmm17_0;
mov xmm20_1 xmm17_1;
(* #jmp    0x55555558402b <KeccakP1600times2_PermuteAll_24rounds+395>#! PC = 0x55555558401d *)
#jmp    0x55555558402b <KeccakP1600times2_PermuteAll_24rounds+395>#! 0x55555558401d = 0x55555558401d;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x00007ffff7eb2780; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x0001003000000000; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x0001000000000000; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0x00007fff00000000; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0x000055555559bcec; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x0000000000000000; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0x79f643aedcaaa516; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0xc505ea9466391500; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0xc505ea9466391500; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0x79f643aedcaaa516; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x95646d80ba813be1; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0xadcc685f8db06bc9; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0xd0f2064682629756; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0x18c65bda956e4188; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a390; Value = 0x800000000000808a; PC = 0x555555584020 *)
mov rdx L0x55555559a390;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a398; Value = 0x8000000080008000; PC = 0x555555584023 *)
mov rcx L0x55555559a398;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0xadcc685f8db06bc9; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x18c65bda956e4188; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x0b568081814807c4; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0xd0f2064682629756; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0x5574b774e0e0914c; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x95646d80ba813be1; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0x673a3cce424449f1; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0xcac9732e40b9a68c; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0xcac9732e40b9a68c; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0x673a3cce424449f1; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x6f8b2f463dd24a8a; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0x7f303ce944619918; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0x749d2382ba0d2bfa; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0xc6e7ebb443dea85f; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3a0; Value = 0x000000000000808b; PC = 0x555555584020 *)
mov rdx L0x55555559a3a0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3a8; Value = 0x0000000080000001; PC = 0x555555584023 *)
mov rcx L0x55555559a3a8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x7f303ce944619918; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0xc6e7ebb443dea85f; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x9b0931b00b632510; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0x749d2382ba0d2bfa; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0x00c52745f56d3aec; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x6f8b2f463dd24a8a; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0xf202ecfb0aa2349f; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x908fc0d0d0c3b593; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x908fc0d0d0c3b593; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0xf202ecfb0aa2349f; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x7cf3ccc250eae6b4; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0x5217e0cbc7ad30b7; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0x6dce77c0293d8f49; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0xd917589e55bbfb85; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3b0; Value = 0x8000000080008081; PC = 0x555555584020 *)
mov rdx L0x55555559a3b0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3b8; Value = 0x8000000000008009; PC = 0x555555584023 *)
mov rcx L0x55555559a3b8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x5217e0cbc7ad30b7; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0xd917589e55bbfb85; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x32cd5d390efd4bcd; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0x6dce77c0293d8f49; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0xef6f6e445397980e; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x7cf3ccc250eae6b4; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0xf7cd6c96c589d4b2; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x05fae037e4f17f32; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x05fae037e4f17f32; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0xf7cd6c96c589d4b2; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x8419c8c6a416b875; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0x8b54e4bef8d5f1c2; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0x84f596772bea888f; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0x20f48d51604dc786; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3c0; Value = 0x000000000000008a; PC = 0x555555584020 *)
mov rdx L0x55555559a3c0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3c8; Value = 0x0000000000000088; PC = 0x555555584023 *)
mov rcx L0x55555559a3c8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x8b54e4bef8d5f1c2; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x20f48d51604dc786; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0xd374bf7a6a395d5e; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0x84f596772bea888f; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0xc82427907fdb1f3e; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x8419c8c6a416b875; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0x0498c9d9ae8eefa2; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0xc90c798e2e34a9cf; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0xc90c798e2e34a9cf; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0x0498c9d9ae8eefa2; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x8476b5151e341f65; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0x3ea1764aab31b665; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0xfec2c0b01d045826; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0x31063bfd33a1fb88; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3d0; Value = 0x0000000080008009; PC = 0x555555584020 *)
mov rdx L0x55555559a3d0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3d8; Value = 0x000000008000000a; PC = 0x555555584023 *)
mov rcx L0x55555559a3d8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x3ea1764aab31b665; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x31063bfd33a1fb88; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0xfb24217582e343ef; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0xfec2c0b01d045826; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0xdea72e52c45472e3; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x8476b5151e341f65; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0xc2dfdfe111a50464; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x8e4eddd54aad4e90; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x8e4eddd54aad4e90; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0xc2dfdfe111a50464; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0xcda077beb6433838; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0xc41559a2de164209; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0x40b01e47581b0856; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0xb100f1453b2a5d22; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3e0; Value = 0x000000008000808b; PC = 0x555555584020 *)
mov rdx L0x55555559a3e0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3e8; Value = 0x800000000000008b; PC = 0x555555584023 *)
mov rcx L0x55555559a3e8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0xc41559a2de164209; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0xb100f1453b2a5d22; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x6db24e36757b9947; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0x40b01e47581b0856; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0xfabaf09e13dce153; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0xcda077beb6433838; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0x2ac89d6ea05ea1ba; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x07cd15ce7c83d579; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x07cd15ce7c83d579; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0x2ac89d6ea05ea1ba; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x97fef6d3a5919ea6; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0x56c0c5839214c83e; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0xc9e0c419e6f8047d; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0x2c9e5b1b2a02ddc9; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3f0; Value = 0x8000000000008089; PC = 0x555555584020 *)
mov rdx L0x55555559a3f0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3f8; Value = 0x8000000000008003; PC = 0x555555584023 *)
mov rcx L0x55555559a3f8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x56c0c5839214c83e; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x2c9e5b1b2a02ddc9; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0xb7e7079374576a8b; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0xc9e0c419e6f8047d; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0xb6e634d1bcbc4496; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x97fef6d3a5919ea6; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0xe4cfc3e4342b79a6; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0xe2d32975ec9defcc; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0xe2d32975ec9defcc; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0xe4cfc3e4342b79a6; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x74e7314686df4cf5; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0x287d682c0af52132; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0x024750691654f5a4; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0xea8209f13654d56c; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a400; Value = 0x8000000000008002; PC = 0x555555584020 *)
mov rdx L0x55555559a400;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a408; Value = 0x8000000000000080; PC = 0x555555584023 *)
mov rcx L0x55555559a408;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x287d682c0af52132; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0xea8209f13654d56c; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x3d51db961b938361; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0x024750691654f5a4; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0xfa06047a627ed7e0; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x74e7314686df4cf5; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0xe99fa04cee23909b; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x040b5e5481118dcf; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x040b5e5481118dcf; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0xe99fa04cee23909b; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x054f5d4a366eecad; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0xa58bf0a84ad2204c; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0xc24d18f4496d7f4d; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0xa18dc4b0b583e201; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a410; Value = 0x000000000000800a; PC = 0x555555584020 *)
mov rdx L0x55555559a410;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a418; Value = 0x800000008000000a; PC = 0x555555584023 *)
mov rcx L0x55555559a418;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0xa58bf0a84ad2204c; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0xa18dc4b0b583e201; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x963705889b02c909; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0xc24d18f4496d7f4d; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0x2267739f3205a021; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x054f5d4a366eecad; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0xd93e351b8216d926; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x1607bb46f144a5eb; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x1607bb46f144a5eb; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0xd93e351b8216d926; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0xbb0b6de4f8b4b162; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0x4722090dff7ab008; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0xc117e196b3bbc5ae; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0x71f6defd944137fc; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a420; Value = 0x8000000080008081; PC = 0x555555584020 *)
mov rdx L0x55555559a420;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a428; Value = 0x8000000000008080; PC = 0x555555584023 *)
mov rcx L0x55555559a428;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x4722090dff7ab008; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x71f6defd944137fc; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0xc90b3dad4680cc38; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0xc117e196b3bbc5ae; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0xfc42b16f246472d7; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0xbb0b6de4f8b4b162; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0x6c270018644a4836; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x3e107fd5eeaf70c9; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x3e107fd5eeaf70c9; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0x6c270018644a4836; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x6e98f4af6f5876aa; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0xd6bbc6cd7e5d37db; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0x9e9194f70fcfbd2f; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0x220a5607b427d5d9; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a430; Value = 0x0000000080000001; PC = 0x555555584020 *)
mov rdx L0x55555559a430;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a438; Value = 0x8000000080008008; PC = 0x555555584023 *)
mov rcx L0x55555559a438;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0xd6bbc6cd7e5d37db; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x220a5607b427d5d9; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x60ffcc9815a54a68; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0x9e9194f70fcfbd2f; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0xc1c0876d2b2ca866; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x6e98f4af6f5876aa; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0x9267782803ddd68a; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0xd20289179ba3276c; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0xd20289179ba3276c; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0x9267782803ddd68a; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x14bb4dda4641c382; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0xb072273cc289eafa; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0xcfda7552e8a8ee99; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0xc30f1aad934fd53d; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* vmovdqa -0x48(%rsp),%xmm1                       #! EA = L0x7fffffffd6a0; Value = 0xb072273cc289eafa; PC = 0x55555558468a *)
mov xmm1_0 L0x7fffffffd6a0;
mov xmm1_1 L0x7fffffffd6a8;
(* vmovdqa %xmm7,0xd0(%rdi)                        #! EA = L0x7fffffffd7e0; PC = 0x555555584690 *)
mov L0x7fffffffd7e0 xmm7_0;
mov L0x7fffffffd7e8 xmm7_1;
(* vmovdqa %xmm1,0x10(%rdi)                        #! EA = L0x7fffffffd720; PC = 0x555555584698 *)
mov L0x7fffffffd720 xmm1_0;
mov L0x7fffffffd728 xmm1_1;
(* vmovdqa -0x8(%rsp),%xmm1                        #! EA = L0x7fffffffd6e0; Value = 0x6b9103c216a9322d; PC = 0x55555558469d *)
mov xmm1_0 L0x7fffffffd6e0;
mov xmm1_1 L0x7fffffffd6e8;
(* vmovdqa -0x18(%rsp),%xmm7                       #! EA = L0x7fffffffd6d0; Value = 0x00fb2c8c3c0cae13; PC = 0x5555555846a3 *)
mov xmm7_0 L0x7fffffffd6d0;
mov xmm7_1 L0x7fffffffd6d8;
(* vmovdqa %xmm1,0x20(%rdi)                        #! EA = L0x7fffffffd730; PC = 0x5555555846a9 *)
mov L0x7fffffffd730 xmm1_0;
mov L0x7fffffffd738 xmm1_1;
(* vmovdqa -0x58(%rsp),%xmm1                       #! EA = L0x7fffffffd690; Value = 0xc30f1aad934fd53d; PC = 0x5555555846ae *)
mov xmm1_0 L0x7fffffffd690;
mov xmm1_1 L0x7fffffffd698;
(* vmovdqa %xmm7,0xe0(%rdi)                        #! EA = L0x7fffffffd7f0; PC = 0x5555555846b4 *)
mov L0x7fffffffd7f0 xmm7_0;
mov L0x7fffffffd7f8 xmm7_1;
(* vmovdqa %xmm1,0x40(%rdi)                        #! EA = L0x7fffffffd750; PC = 0x5555555846bc *)
mov L0x7fffffffd750 xmm1_0;
mov L0x7fffffffd758 xmm1_1;
(* vmovdqa -0x68(%rsp),%xmm7                       #! EA = L0x7fffffffd680; Value = 0x14bb4dda4641c382; PC = 0x5555555846c1 *)
mov xmm7_0 L0x7fffffffd680;
mov xmm7_1 L0x7fffffffd688;
(* vmovdqa -0x78(%rsp),%xmm1                       #! EA = L0x7fffffffd670; Value = 0xcfda7552e8a8ee99; PC = 0x5555555846c7 *)
mov xmm1_0 L0x7fffffffd670;
mov xmm1_1 L0x7fffffffd678;
(* vmovdqa64 %xmm27,(%rdi)                         #! EA = L0x7fffffffd710; PC = 0x5555555846cd *)
mov L0x7fffffffd710 xmm27_0;
mov L0x7fffffffd718 xmm27_1;
(* vmovdqa64 %xmm24,0x30(%rdi)                     #! EA = L0x7fffffffd740; PC = 0x5555555846d3 *)
mov L0x7fffffffd740 xmm24_0;
mov L0x7fffffffd748 xmm24_1;
(* vmovdqa64 %xmm29,0x50(%rdi)                     #! EA = L0x7fffffffd760; PC = 0x5555555846da *)
mov L0x7fffffffd760 xmm29_0;
mov L0x7fffffffd768 xmm29_1;
(* vmovdqa %xmm9,0x60(%rdi)                        #! EA = L0x7fffffffd770; PC = 0x5555555846e1 *)
mov L0x7fffffffd770 xmm9_0;
mov L0x7fffffffd778 xmm9_1;
(* vmovdqa64 %xmm22,0x70(%rdi)                     #! EA = L0x7fffffffd780; PC = 0x5555555846e6 *)
mov L0x7fffffffd780 xmm22_0;
mov L0x7fffffffd788 xmm22_1;
(* vmovdqa %xmm1,0x80(%rdi)                        #! EA = L0x7fffffffd790; PC = 0x5555555846ed *)
mov L0x7fffffffd790 xmm1_0;
mov L0x7fffffffd798 xmm1_1;
(* vmovdqa %xmm5,0x90(%rdi)                        #! EA = L0x7fffffffd7a0; PC = 0x5555555846f5 *)
mov L0x7fffffffd7a0 xmm5_0;
mov L0x7fffffffd7a8 xmm5_1;
(* vmovdqa64 %xmm21,0xa0(%rdi)                     #! EA = L0x7fffffffd7b0; PC = 0x5555555846fd *)
mov L0x7fffffffd7b0 xmm21_0;
mov L0x7fffffffd7b8 xmm21_1;
(* vmovdqa64 %xmm28,0xb0(%rdi)                     #! EA = L0x7fffffffd7c0; PC = 0x555555584704 *)
mov L0x7fffffffd7c0 xmm28_0;
mov L0x7fffffffd7c8 xmm28_1;
(* vmovdqa %xmm8,0xc0(%rdi)                        #! EA = L0x7fffffffd7d0; PC = 0x55555558470b *)
mov L0x7fffffffd7d0 xmm8_0;
mov L0x7fffffffd7d8 xmm8_1;
(* vmovdqa %xmm7,0xf0(%rdi)                        #! EA = L0x7fffffffd800; PC = 0x555555584713 *)
mov L0x7fffffffd800 xmm7_0;
mov L0x7fffffffd808 xmm7_1;
(* vmovdqa64 %xmm19,0x100(%rdi)                    #! EA = L0x7fffffffd810; PC = 0x55555558471b *)
mov L0x7fffffffd810 xmm19_0;
mov L0x7fffffffd818 xmm19_1;
(* vmovdqa64 %xmm18,0x110(%rdi)                    #! EA = L0x7fffffffd820; PC = 0x555555584722 *)
mov L0x7fffffffd820 xmm18_0;
mov L0x7fffffffd828 xmm18_1;
(* vmovdqa %xmm12,0x120(%rdi)                      #! EA = L0x7fffffffd830; PC = 0x555555584729 *)
mov L0x7fffffffd830 xmm12_0;
mov L0x7fffffffd838 xmm12_1;
(* vmovdqa %xmm4,0x130(%rdi)                       #! EA = L0x7fffffffd840; PC = 0x555555584731 *)
mov L0x7fffffffd840 xmm4_0;
mov L0x7fffffffd848 xmm4_1;
(* vmovdqa64 %xmm16,0x140(%rdi)                    #! EA = L0x7fffffffd850; PC = 0x555555584739 *)
mov L0x7fffffffd850 xmm16_0;
mov L0x7fffffffd858 xmm16_1;
(* vmovdqa64 %xmm26,0x150(%rdi)                    #! EA = L0x7fffffffd860; PC = 0x555555584740 *)
mov L0x7fffffffd860 xmm26_0;
mov L0x7fffffffd868 xmm26_1;
(* vmovdqa %xmm14,0x160(%rdi)                      #! EA = L0x7fffffffd870; PC = 0x555555584747 *)
mov L0x7fffffffd870 xmm14_0;
mov L0x7fffffffd878 xmm14_1;
(* vmovdqa %xmm3,0x170(%rdi)                       #! EA = L0x7fffffffd880; PC = 0x55555558474f *)
mov L0x7fffffffd880 xmm3_0;
mov L0x7fffffffd888 xmm3_1;
(* vmovdqa %xmm15,0x180(%rdi)                      #! EA = L0x7fffffffd890; PC = 0x555555584757 *)
mov L0x7fffffffd890 xmm15_0;
mov L0x7fffffffd898 xmm15_1;
(* add    $0x10,%rsp                               #! PC = 0x55555558475f *)
adds carry rsp rsp 0x10@uint64;
(* #! <- SP = 0x7fffffffd6f8 *)
#! 0x7fffffffd6f8 = 0x7fffffffd6f8;
(* #ret                                            #! PC = 0x555555584763 *)
#ret                                            #! 0x555555584763 = 0x555555584763;
(* #call   0x555555583ea0 <KeccakP1600times2_PermuteAll_24rounds>#! PC = 0x555555586964 *)
#call   0x555555583ea0 <KeccakP1600times2_PermuteAll_24rounds>#! 0x555555586964 = 0x555555586964;
(* #! -> SP = 0x7fffffffd6f8 *)
#! 0x7fffffffd6f8 = 0x7fffffffd6f8;
(* sub    $0x10,%rsp                               #! PC = 0x555555583ea4 *)
subb carry rsp rsp 0x10@uint64;
(* vmovdqa 0xf0(%rdi),%xmm3                        #! EA = L0x7fffffffd990; Value = 0xff27f562809513f7; PC = 0x555555583ea8 *)
mov xmm3_0 L0x7fffffffd990;
mov xmm3_1 L0x7fffffffd998;
(* vmovdqa64 0x50(%rdi),%xmm29                     #! EA = L0x7fffffffd8f0; Value = 0x00007fff00000000; PC = 0x555555583eb0 *)
mov xmm29_0 L0x7fffffffd8f0;
mov xmm29_1 L0x7fffffffd8f8;
(* vmovdqa64 0xa0(%rdi),%xmm21                     #! EA = L0x7fffffffd940; Value = 0xcb86d54bda87a298; PC = 0x555555583eb7 *)
mov xmm21_0 L0x7fffffffd940;
mov xmm21_1 L0x7fffffffd948;
(* vmovdqa64 0x140(%rdi),%xmm16                    #! EA = L0x7fffffffd9e0; Value = 0xde54f91478421d9f; PC = 0x555555583ebe *)
mov xmm16_0 L0x7fffffffd9e0;
mov xmm16_1 L0x7fffffffd9e8;
(* vmovdqa %xmm3,%xmm1                             #! PC = 0x555555583ec5 *)
mov xmm1_0 xmm3_0;
mov xmm1_1 xmm3_1;
(* vmovdqa 0x60(%rdi),%xmm9                        #! EA = L0x7fffffffd900; Value = 0x0000000010000000; PC = 0x555555583ec9 *)
mov xmm9_0 L0x7fffffffd900;
mov xmm9_1 L0x7fffffffd908;
(* vmovdqa64 0xb0(%rdi),%xmm28                     #! EA = L0x7fffffffd950; Value = 0x01cce273534e2339; PC = 0x555555583ece *)
mov xmm28_0 L0x7fffffffd950;
mov xmm28_1 L0x7fffffffd958;
(* vmovdqa64 0x100(%rdi),%xmm19                    #! EA = L0x7fffffffd9a0; Value = 0xf85425a8b8e79a34; PC = 0x555555583ed5 *)
mov xmm19_0 L0x7fffffffd9a0;
mov xmm19_1 L0x7fffffffd9a8;
(* vmovdqa64 0x150(%rdi),%xmm26                    #! EA = L0x7fffffffd9f0; Value = 0x012c1108a43d184e; PC = 0x555555583edc *)
mov xmm26_0 L0x7fffffffd9f0;
mov xmm26_1 L0x7fffffffd9f8;
(* vpxorq %xmm1,%xmm16,%xmm0                       #! PC = 0x555555583ee3 *)
xor xmm0_0@uint64 xmm16_0 xmm1_0;
xor xmm0_1@uint64 xmm16_1 xmm1_1;
(* vmovdqa64 0x70(%rdi),%xmm22                     #! EA = L0x7fffffffd910; Value = 0x0000000000000000; PC = 0x555555583ee9 *)
mov xmm22_0 L0x7fffffffd910;
mov xmm22_1 L0x7fffffffd918;
(* vmovdqa 0xc0(%rdi),%xmm8                        #! EA = L0x7fffffffd960; Value = 0x5aeab8a7bee73645; PC = 0x555555583ef0 *)
mov xmm8_0 L0x7fffffffd960;
mov xmm8_1 L0x7fffffffd968;
(* vmovdqa64 0x110(%rdi),%xmm18                    #! EA = L0x7fffffffd9b0; Value = 0x3019fc6403fc5cbf; PC = 0x555555583ef8 *)
mov xmm18_0 L0x7fffffffd9b0;
mov xmm18_1 L0x7fffffffd9b8;
(* vmovdqa 0x160(%rdi),%xmm14                      #! EA = L0x7fffffffda00; Value = 0x0000000000000000; PC = 0x555555583eff *)
mov xmm14_0 L0x7fffffffda00;
mov xmm14_1 L0x7fffffffda08;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x555555583f07 *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vmovdqa 0x80(%rdi),%xmm6                        #! EA = L0x7fffffffd920; Value = 0x0000000000000000; PC = 0x555555583f0d *)
mov xmm6_0 L0x7fffffffd920;
mov xmm6_1 L0x7fffffffd928;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x555555583f15 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm19,%xmm26,%xmm1                      #! PC = 0x555555583f19 *)
xor xmm1_0@uint64 xmm26_0 xmm19_0;
xor xmm1_1@uint64 xmm26_1 xmm19_1;
(* vmovdqa 0xd0(%rdi),%xmm7                        #! EA = L0x7fffffffd970; Value = 0x529a560d8dbaf494; PC = 0x555555583f1f *)
mov xmm7_0 L0x7fffffffd970;
mov xmm7_1 L0x7fffffffd978;
(* vmovdqa 0x120(%rdi),%xmm12                      #! EA = L0x7fffffffd9c0; Value = 0xbd9b8ffcbdb431ae; PC = 0x555555583f27 *)
mov xmm12_0 L0x7fffffffd9c0;
mov xmm12_1 L0x7fffffffd9c8;
(* vmovdqa %xmm3,-0x68(%rsp)                       #! EA = L0x7fffffffd680; PC = 0x555555583f2f *)
mov L0x7fffffffd680 xmm3_0;
mov L0x7fffffffd688 xmm3_1;
(* vpxorq %xmm9,%xmm28,%xmm0                       #! PC = 0x555555583f35 *)
xor xmm0_0@uint64 xmm28_0 xmm9_0;
xor xmm0_1@uint64 xmm28_1 xmm9_1;
(* vmovdqa 0x170(%rdi),%xmm3                       #! EA = L0x7fffffffda10; Value = 0xe171bd428c4ba64d; PC = 0x555555583f3b *)
mov xmm3_0 L0x7fffffffda10;
mov xmm3_1 L0x7fffffffda18;
(* vmovdqa64 0xe0(%rdi),%xmm25                     #! EA = L0x7fffffffd980; Value = 0xda75604b8015024b; PC = 0x555555583f43 *)
mov xmm25_0 L0x7fffffffd980;
mov xmm25_1 L0x7fffffffd988;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x555555583f4a *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxorq %xmm18,%xmm14,%xmm2                      #! PC = 0x555555583f4e *)
xor xmm2_0@uint64 xmm14_0 xmm18_0;
xor xmm2_1@uint64 xmm14_1 xmm18_1;
(* vpxorq %xmm22,%xmm8,%xmm1                       #! PC = 0x555555583f54 *)
xor xmm1_0@uint64 xmm8_0 xmm22_0;
xor xmm1_1@uint64 xmm8_1 xmm22_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x555555583f5a *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpxorq %xmm6,%xmm7,%xmm17                       #! PC = 0x555555583f5e *)
xor xmm17_0@uint64 xmm7_0 xmm6_0;
xor xmm17_1@uint64 xmm7_1 xmm6_1;
(* vpxor  %xmm12,%xmm3,%xmm2                       #! PC = 0x555555583f64 *)
xor xmm2_0@uint64 xmm3_0 xmm12_0;
xor xmm2_1@uint64 xmm3_1 xmm12_1;
(* vmovdqa 0x90(%rdi),%xmm5                        #! EA = L0x7fffffffd930; Value = 0x0000000000000000; PC = 0x555555583f69 *)
mov xmm5_0 L0x7fffffffd930;
mov xmm5_1 L0x7fffffffd938;
(* vmovdqa 0x130(%rdi),%xmm4                       #! EA = L0x7fffffffd9d0; Value = 0x00007fffffffda10; PC = 0x555555583f71 *)
mov xmm4_0 L0x7fffffffd9d0;
mov xmm4_1 L0x7fffffffd9d8;
(* vmovdqa 0x180(%rdi),%xmm15                      #! EA = L0x7fffffffda20; Value = 0xd37bec52cc7bc4dc; PC = 0x555555583f79 *)
mov xmm15_0 L0x7fffffffda20;
mov xmm15_1 L0x7fffffffda28;
(* vmovdqa64 0x40(%rdi),%xmm31                     #! EA = L0x7fffffffd8e0; Value = 0x00007ffff7fbbf48; PC = 0x555555583f81 *)
mov xmm31_0 L0x7fffffffd8e0;
mov xmm31_1 L0x7fffffffd8e8;
(* vpxorq %xmm2,%xmm17,%xmm17                      #! PC = 0x555555583f88 *)
xor xmm17_0@uint64 xmm17_0 xmm2_0;
xor xmm17_1@uint64 xmm17_1 xmm2_1;
(* vmovdqa64 0x30(%rdi),%xmm24                     #! EA = L0x7fffffffd8d0; Value = 0x00007fffffffd944; PC = 0x555555583f8e *)
mov xmm24_0 L0x7fffffffd8d0;
mov xmm24_1 L0x7fffffffd8d8;
(* vmovdqa64 %xmm25,%xmm2                          #! PC = 0x555555583f95 *)
mov xmm2_0 xmm25_0;
mov xmm2_1 xmm25_1;
(* vmovdqa 0x10(%rdi),%xmm11                       #! EA = L0x7fffffffd8b0; Value = 0x00007fffffffd944; PC = 0x555555583f9b *)
mov xmm11_0 L0x7fffffffd8b0;
mov xmm11_1 L0x7fffffffd8b8;
(* vmovdqa 0x20(%rdi),%xmm10                       #! EA = L0x7fffffffd8c0; Value = 0x00007ffff7cae650; PC = 0x555555583fa0 *)
mov xmm10_0 L0x7fffffffd8c0;
mov xmm10_1 L0x7fffffffd8c8;
(* vmovdqa64 (%rdi),%xmm27                         #! EA = L0x7fffffffd8a0; Value = 0x00007ffff7fbb9d0; PC = 0x555555583fa5 *)
mov xmm27_0 L0x7fffffffd8a0;
mov xmm27_1 L0x7fffffffd8a8;
(* vmovdqa %xmm6,-0x78(%rsp)                       #! EA = L0x7fffffffd670; PC = 0x555555583fab *)
mov L0x7fffffffd670 xmm6_0;
mov L0x7fffffffd678 xmm6_1;
(* vpxor  %xmm2,%xmm5,%xmm2                        #! PC = 0x555555583fb1 *)
xor xmm2_0@uint64 xmm5_0 xmm2_0;
xor xmm2_1@uint64 xmm5_1 xmm2_1;
(* vpxor  %xmm4,%xmm15,%xmm6                       #! PC = 0x555555583fb5 *)
xor xmm6_0@uint64 xmm15_0 xmm4_0;
xor xmm6_1@uint64 xmm15_1 xmm4_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x555555583fb9 *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vpxorq %xmm24,%xmm17,%xmm17                     #! PC = 0x555555583fbd *)
xor xmm17_0@uint64 xmm17_0 xmm24_0;
xor xmm17_1@uint64 xmm17_1 xmm24_1;
(* vmovdqa64 %xmm31,%xmm6                          #! PC = 0x555555583fc3 *)
mov xmm6_0 xmm31_0;
mov xmm6_1 xmm31_1;
(* vmovdqa %xmm11,-0x48(%rsp)                      #! EA = L0x7fffffffd6a0; PC = 0x555555583fd0 *)
mov L0x7fffffffd6a0 xmm11_0;
mov L0x7fffffffd6a8 xmm11_1;
(* vmovdqa %xmm10,-0x8(%rsp)                       #! EA = L0x7fffffffd6e0; PC = 0x555555583fd6 *)
mov L0x7fffffffd6e0 xmm10_0;
mov L0x7fffffffd6e8 xmm10_1;
(* vmovdqa64 %xmm31,-0x58(%rsp)                    #! EA = L0x7fffffffd690; PC = 0x555555583fdc *)
mov L0x7fffffffd690 xmm31_0;
mov L0x7fffffffd698 xmm31_1;
(* vmovdqa64 %xmm25,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x555555583fe7 *)
mov L0x7fffffffd6d0 xmm25_0;
mov L0x7fffffffd6d8 xmm25_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555583ff2 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxor  %xmm11,%xmm0,%xmm0                       #! PC = 0x555555583ff8 *)
xor xmm0_0@uint64 xmm0_0 xmm11_0;
xor xmm0_1@uint64 xmm0_1 xmm11_1;
(* vpxor  %xmm10,%xmm1,%xmm1                       #! PC = 0x555555583ffd *)
xor xmm1_0@uint64 xmm1_0 xmm10_0;
xor xmm1_1@uint64 xmm1_1 xmm10_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x555555584002 *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* mov    $0x8082,%ecx                             #! PC = 0x55555558400d *)
mov rcx 0x8082@uint64;
(* mov    $0x1,%edx                                #! PC = 0x555555584012 *)
mov rdx 0x1@uint64;
(* vmovdqa64 %xmm17,%xmm20                         #! PC = 0x555555584017 *)
mov xmm20_0 xmm17_0;
mov xmm20_1 xmm17_1;
(* #jmp    0x55555558402b <KeccakP1600times2_PermuteAll_24rounds+395>#! PC = 0x55555558401d *)
#jmp    0x55555558402b <KeccakP1600times2_PermuteAll_24rounds+395>#! 0x55555558401d = 0x55555558401d;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x00007fffffffd944; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x00007ffff7fbbf48; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0xda75604b8015024b; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0x0000000000000000; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0x00007ffff7cae650; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0xff27f562809513f7; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0xc8c12cd5751b9cc7; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x11d90b65dd9b9738; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x11d90b65dd9b9738; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0xc8c12cd5751b9cc7; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x0522d7cf6ff40750; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0x6cd280e9f523b77d; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0xe70148d5141e9c6b; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0x1f863f954bcb0522; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a390; Value = 0x800000000000808a; PC = 0x555555584020 *)
mov rdx L0x55555559a390;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a398; Value = 0x8000000080008000; PC = 0x555555584023 *)
mov rcx L0x55555559a398;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x6cd280e9f523b77d; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x1f863f954bcb0522; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x7b80b967e0d1537a; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0xe70148d5141e9c6b; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0x0792f88502d7f983; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x0522d7cf6ff40750; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0x86b046f69f816a1a; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x4d9d5e782b92bd75; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x4d9d5e782b92bd75; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0x86b046f69f816a1a; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x030a82f1f1aeccc7; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0x84cb61963cbaa038; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0xdd5d1c44650a6d10; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0xacc46483ef72038e; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3a0; Value = 0x000000000000808b; PC = 0x555555584020 *)
mov rdx L0x55555559a3a0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3a8; Value = 0x0000000080000001; PC = 0x555555584023 *)
mov rcx L0x55555559a3a8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x84cb61963cbaa038; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0xacc46483ef72038e; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x2f9b8c875e102c48; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0xdd5d1c44650a6d10; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0xb90b45962b5dca9f; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x030a82f1f1aeccc7; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0xc52953c394dbd992; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0xd82bf6663943da71; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0xd82bf6663943da71; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0xc52953c394dbd992; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x1abfca7fe230d3a9; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0x50c2eaac2ac6ba61; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0x0c70490198d6a2ba; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0x3b49decd2e448f71; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3b0; Value = 0x8000000080008081; PC = 0x555555584020 *)
mov rdx L0x55555559a3b0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3b8; Value = 0x8000000000008009; PC = 0x555555584023 *)
mov rcx L0x55555559a3b8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x50c2eaac2ac6ba61; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x3b49decd2e448f71; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x6dfed4396d7057be; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0x0c70490198d6a2ba; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0xd54c99bd61d087d2; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x1abfca7fe230d3a9; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0x1dcdcd9db757d658; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x0ef006dc9579fa6d; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x0ef006dc9579fa6d; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0x1dcdcd9db757d658; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x39fad70340f497e4; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0x197a8b65a634e7ba; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0xeeab3d7bd523e087; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0x1912fccab8597d1b; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3c0; Value = 0x000000000000008a; PC = 0x555555584020 *)
mov rdx L0x55555559a3c0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3c8; Value = 0x0000000000000088; PC = 0x555555584023 *)
mov rcx L0x55555559a3c8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x197a8b65a634e7ba; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x1912fccab8597d1b; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x4b2f602af16db126; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0xeeab3d7bd523e087; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0x805ec33b888910a4; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x39fad70340f497e4; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0xae2dae57901cfee1; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0xf521c364f047737d; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0xf521c364f047737d; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0xae2dae57901cfee1; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x67695ac1b1e0e6fb; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0xfa027652477f7168; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0xa381d5d8f2d283ad; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0xdf33959724841f85; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3d0; Value = 0x0000000080008009; PC = 0x555555584020 *)
mov rdx L0x55555559a3d0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3d8; Value = 0x000000008000000a; PC = 0x555555584023 *)
mov rcx L0x55555559a3d8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0xfa027652477f7168; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0xdf33959724841f85; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0xf7a7faa850e5846f; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0xa381d5d8f2d283ad; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0x338ed8b5cce4bb9c; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x67695ac1b1e0e6fb; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0xeb909b1534a7da46; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x8e238c36a2b3840d; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x8e238c36a2b3840d; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0xeb909b1534a7da46; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0xf1a18e10dbd430c7; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0xf9025896f9798fef; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0x9aeae1501b1a1c50; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0x85b5836b871f786f; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3e0; Value = 0x000000008000808b; PC = 0x555555584020 *)
mov rdx L0x55555559a3e0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3e8; Value = 0x800000000000008b; PC = 0x555555584023 *)
mov rcx L0x55555559a3e8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0xf9025896f9798fef; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x85b5836b871f786f; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x5aa4b2d865017ff8; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0x9aeae1501b1a1c50; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0x9a71994ebe83b89c; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0xf1a18e10dbd430c7; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0xccd5c4b02744dffd; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x20aa0c594e125709; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x20aa0c594e125709; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0xccd5c4b02744dffd; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0xf667ca02c9400622; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0xc9ecf5bc694fea4b; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0xeb45dbf6f801c700; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0x14fb74ab8ae4b3c5; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3f0; Value = 0x8000000000008089; PC = 0x555555584020 *)
mov rdx L0x55555559a3f0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3f8; Value = 0x8000000000008003; PC = 0x555555584023 *)
mov rcx L0x55555559a3f8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0xc9ecf5bc694fea4b; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x14fb74ab8ae4b3c5; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x25d1e40286cedfd2; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0xeb45dbf6f801c700; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0x067e786ea4f282e6; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0xf667ca02c9400622; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0xa748f8304860b2a5; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x238d59c14aa2fc81; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x238d59c14aa2fc81; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0xa748f8304860b2a5; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0xe92265860d852c32; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0x64b747a89deb9f5d; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0xb36419983b62a33e; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0x9b4a8d462ae6819a; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a400; Value = 0x8000000000008002; PC = 0x555555584020 *)
mov rdx L0x55555559a400;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a408; Value = 0x8000000000000080; PC = 0x555555584023 *)
mov rcx L0x55555559a408;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x64b747a89deb9f5d; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x9b4a8d462ae6819a; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x41a30b1e073f4fe1; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0xb36419983b62a33e; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0xaa0cd595d6f4d75b; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0xe92265860d852c32; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0x32061c56cfc18d85; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x40780409e3fe2314; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x40780409e3fe2314; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0x32061c56cfc18d85; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0xa48f713df4ceec64; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0xefc3ecf87283b90b; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0xb4fc5a69d5a2361a; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0xb1e4b23b5bb95155; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a410; Value = 0x000000000000800a; PC = 0x555555584020 *)
mov rdx L0x55555559a410;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a418; Value = 0x800000008000000a; PC = 0x555555584023 *)
mov rcx L0x55555559a418;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0xefc3ecf87283b90b; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0xb1e4b23b5bb95155; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x5eb7428e0fd08ff5; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0xb4fc5a69d5a2361a; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0x12a56d24ad1c4557; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0xa48f713df4ceec64; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0xa983231c977ce0a3; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x66e8489f17d4363e; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x66e8489f17d4363e; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0xa983231c977ce0a3; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0xadba93539b75517d; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0xdc39c0ff88db2517; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0xe086feb4e595fc5a; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0x6b8f0c281ec14c24; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a420; Value = 0x8000000080008081; PC = 0x555555584020 *)
mov rdx L0x55555559a420;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a428; Value = 0x8000000000008080; PC = 0x555555584023 *)
mov rcx L0x55555559a428;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0xdc39c0ff88db2517; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x6b8f0c281ec14c24; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x21366d8023a658e3; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0xe086feb4e595fc5a; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0x7978e503a43aaf24; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0xadba93539b75517d; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0x75f7d13e4ae53e52; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x5fdae0970f0a2758; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x5fdae0970f0a2758; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0x75f7d13e4ae53e52; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x0b9474217bc30189; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0x985200f5347485e1; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0xb93750bd0f5f6092; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0x1afddb217f02e5ea; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a430; Value = 0x0000000080000001; PC = 0x555555584020 *)
mov rdx L0x55555559a430;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a438; Value = 0x8000000080008008; PC = 0x555555584023 *)
mov rcx L0x55555559a438;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x985200f5347485e1; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x1afddb217f02e5ea; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0xec0fd3c98f676501; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0xb93750bd0f5f6092; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0xc6992bc3548f7de7; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x0b9474217bc30189; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0x38648534229c54a6; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x94cdd863fc90b32c; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x94cdd863fc90b32c; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0x38648534229c54a6; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0xd77a7934f518e18e; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0x2d485fc3a7b27afc; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0xc03a89218d6642d0; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0x9efb219e7a1c3380; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* vmovdqa -0x48(%rsp),%xmm1                       #! EA = L0x7fffffffd6a0; Value = 0x2d485fc3a7b27afc; PC = 0x55555558468a *)
mov xmm1_0 L0x7fffffffd6a0;
mov xmm1_1 L0x7fffffffd6a8;
(* vmovdqa %xmm7,0xd0(%rdi)                        #! EA = L0x7fffffffd970; PC = 0x555555584690 *)
mov L0x7fffffffd970 xmm7_0;
mov L0x7fffffffd978 xmm7_1;
(* vmovdqa %xmm1,0x10(%rdi)                        #! EA = L0x7fffffffd8b0; PC = 0x555555584698 *)
mov L0x7fffffffd8b0 xmm1_0;
mov L0x7fffffffd8b8 xmm1_1;
(* vmovdqa -0x8(%rsp),%xmm1                        #! EA = L0x7fffffffd6e0; Value = 0xee4d9a141e0815f5; PC = 0x55555558469d *)
mov xmm1_0 L0x7fffffffd6e0;
mov xmm1_1 L0x7fffffffd6e8;
(* vmovdqa -0x18(%rsp),%xmm7                       #! EA = L0x7fffffffd6d0; Value = 0x6ddff35b74185ec9; PC = 0x5555555846a3 *)
mov xmm7_0 L0x7fffffffd6d0;
mov xmm7_1 L0x7fffffffd6d8;
(* vmovdqa %xmm1,0x20(%rdi)                        #! EA = L0x7fffffffd8c0; PC = 0x5555555846a9 *)
mov L0x7fffffffd8c0 xmm1_0;
mov L0x7fffffffd8c8 xmm1_1;
(* vmovdqa -0x58(%rsp),%xmm1                       #! EA = L0x7fffffffd690; Value = 0x9efb219e7a1c3380; PC = 0x5555555846ae *)
mov xmm1_0 L0x7fffffffd690;
mov xmm1_1 L0x7fffffffd698;
(* vmovdqa %xmm7,0xe0(%rdi)                        #! EA = L0x7fffffffd980; PC = 0x5555555846b4 *)
mov L0x7fffffffd980 xmm7_0;
mov L0x7fffffffd988 xmm7_1;
(* vmovdqa %xmm1,0x40(%rdi)                        #! EA = L0x7fffffffd8e0; PC = 0x5555555846bc *)
mov L0x7fffffffd8e0 xmm1_0;
mov L0x7fffffffd8e8 xmm1_1;
(* vmovdqa -0x68(%rsp),%xmm7                       #! EA = L0x7fffffffd680; Value = 0xd77a7934f518e18e; PC = 0x5555555846c1 *)
mov xmm7_0 L0x7fffffffd680;
mov xmm7_1 L0x7fffffffd688;
(* vmovdqa -0x78(%rsp),%xmm1                       #! EA = L0x7fffffffd670; Value = 0xc03a89218d6642d0; PC = 0x5555555846c7 *)
mov xmm1_0 L0x7fffffffd670;
mov xmm1_1 L0x7fffffffd678;
(* vmovdqa64 %xmm27,(%rdi)                         #! EA = L0x7fffffffd8a0; PC = 0x5555555846cd *)
mov L0x7fffffffd8a0 xmm27_0;
mov L0x7fffffffd8a8 xmm27_1;
(* vmovdqa64 %xmm24,0x30(%rdi)                     #! EA = L0x7fffffffd8d0; PC = 0x5555555846d3 *)
mov L0x7fffffffd8d0 xmm24_0;
mov L0x7fffffffd8d8 xmm24_1;
(* vmovdqa64 %xmm29,0x50(%rdi)                     #! EA = L0x7fffffffd8f0; PC = 0x5555555846da *)
mov L0x7fffffffd8f0 xmm29_0;
mov L0x7fffffffd8f8 xmm29_1;
(* vmovdqa %xmm9,0x60(%rdi)                        #! EA = L0x7fffffffd900; PC = 0x5555555846e1 *)
mov L0x7fffffffd900 xmm9_0;
mov L0x7fffffffd908 xmm9_1;
(* vmovdqa64 %xmm22,0x70(%rdi)                     #! EA = L0x7fffffffd910; PC = 0x5555555846e6 *)
mov L0x7fffffffd910 xmm22_0;
mov L0x7fffffffd918 xmm22_1;
(* vmovdqa %xmm1,0x80(%rdi)                        #! EA = L0x7fffffffd920; PC = 0x5555555846ed *)
mov L0x7fffffffd920 xmm1_0;
mov L0x7fffffffd928 xmm1_1;
(* vmovdqa %xmm5,0x90(%rdi)                        #! EA = L0x7fffffffd930; PC = 0x5555555846f5 *)
mov L0x7fffffffd930 xmm5_0;
mov L0x7fffffffd938 xmm5_1;
(* vmovdqa64 %xmm21,0xa0(%rdi)                     #! EA = L0x7fffffffd940; PC = 0x5555555846fd *)
mov L0x7fffffffd940 xmm21_0;
mov L0x7fffffffd948 xmm21_1;
(* vmovdqa64 %xmm28,0xb0(%rdi)                     #! EA = L0x7fffffffd950; PC = 0x555555584704 *)
mov L0x7fffffffd950 xmm28_0;
mov L0x7fffffffd958 xmm28_1;
(* vmovdqa %xmm8,0xc0(%rdi)                        #! EA = L0x7fffffffd960; PC = 0x55555558470b *)
mov L0x7fffffffd960 xmm8_0;
mov L0x7fffffffd968 xmm8_1;
(* vmovdqa %xmm7,0xf0(%rdi)                        #! EA = L0x7fffffffd990; PC = 0x555555584713 *)
mov L0x7fffffffd990 xmm7_0;
mov L0x7fffffffd998 xmm7_1;
(* vmovdqa64 %xmm19,0x100(%rdi)                    #! EA = L0x7fffffffd9a0; PC = 0x55555558471b *)
mov L0x7fffffffd9a0 xmm19_0;
mov L0x7fffffffd9a8 xmm19_1;
(* vmovdqa64 %xmm18,0x110(%rdi)                    #! EA = L0x7fffffffd9b0; PC = 0x555555584722 *)
mov L0x7fffffffd9b0 xmm18_0;
mov L0x7fffffffd9b8 xmm18_1;
(* vmovdqa %xmm12,0x120(%rdi)                      #! EA = L0x7fffffffd9c0; PC = 0x555555584729 *)
mov L0x7fffffffd9c0 xmm12_0;
mov L0x7fffffffd9c8 xmm12_1;
(* vmovdqa %xmm4,0x130(%rdi)                       #! EA = L0x7fffffffd9d0; PC = 0x555555584731 *)
mov L0x7fffffffd9d0 xmm4_0;
mov L0x7fffffffd9d8 xmm4_1;
(* vmovdqa64 %xmm16,0x140(%rdi)                    #! EA = L0x7fffffffd9e0; PC = 0x555555584739 *)
mov L0x7fffffffd9e0 xmm16_0;
mov L0x7fffffffd9e8 xmm16_1;
(* vmovdqa64 %xmm26,0x150(%rdi)                    #! EA = L0x7fffffffd9f0; PC = 0x555555584740 *)
mov L0x7fffffffd9f0 xmm26_0;
mov L0x7fffffffd9f8 xmm26_1;
(* vmovdqa %xmm14,0x160(%rdi)                      #! EA = L0x7fffffffda00; PC = 0x555555584747 *)
mov L0x7fffffffda00 xmm14_0;
mov L0x7fffffffda08 xmm14_1;
(* vmovdqa %xmm3,0x170(%rdi)                       #! EA = L0x7fffffffda10; PC = 0x55555558474f *)
mov L0x7fffffffda10 xmm3_0;
mov L0x7fffffffda18 xmm3_1;
(* vmovdqa %xmm15,0x180(%rdi)                      #! EA = L0x7fffffffda20; PC = 0x555555584757 *)
mov L0x7fffffffda20 xmm15_0;
mov L0x7fffffffda28 xmm15_1;
(* add    $0x10,%rsp                               #! PC = 0x55555558475f *)
adds carry rsp rsp 0x10@uint64;
(* #! <- SP = 0x7fffffffd6f8 *)
#! 0x7fffffffd6f8 = 0x7fffffffd6f8;
(* #ret                                            #! PC = 0x555555584763 *)
#ret                                            #! 0x555555584763 = 0x555555584763;
(* #call   0x555555583ea0 <KeccakP1600times2_PermuteAll_24rounds>#! PC = 0x555555586970 *)
#call   0x555555583ea0 <KeccakP1600times2_PermuteAll_24rounds>#! 0x555555586970 = 0x555555586970;
(* #! -> SP = 0x7fffffffd6f8 *)
#! 0x7fffffffd6f8 = 0x7fffffffd6f8;
(* sub    $0x10,%rsp                               #! PC = 0x555555583ea4 *)
subb carry rsp rsp 0x10@uint64;
(* vmovdqa 0xf0(%rdi),%xmm3                        #! EA = L0x7fffffffdb20; Value = 0x643cf6f87f4cba7d; PC = 0x555555583ea8 *)
mov xmm3_0 L0x7fffffffdb20;
mov xmm3_1 L0x7fffffffdb28;
(* vmovdqa64 0x50(%rdi),%xmm29                     #! EA = L0x7fffffffda80; Value = 0x5ceb6e7db7075c56; PC = 0x555555583eb0 *)
mov xmm29_0 L0x7fffffffda80;
mov xmm29_1 L0x7fffffffda88;
(* vmovdqa64 0xa0(%rdi),%xmm21                     #! EA = L0x7fffffffdad0; Value = 0xa56d6df6c7415a53; PC = 0x555555583eb7 *)
mov xmm21_0 L0x7fffffffdad0;
mov xmm21_1 L0x7fffffffdad8;
(* vmovdqa64 0x140(%rdi),%xmm16                    #! EA = L0x7fffffffdb70; Value = 0x82c7d0b473d7745b; PC = 0x555555583ebe *)
mov xmm16_0 L0x7fffffffdb70;
mov xmm16_1 L0x7fffffffdb78;
(* vmovdqa %xmm3,%xmm1                             #! PC = 0x555555583ec5 *)
mov xmm1_0 xmm3_0;
mov xmm1_1 xmm3_1;
(* vmovdqa 0x60(%rdi),%xmm9                        #! EA = L0x7fffffffda90; Value = 0xcb86d54bda87a298; PC = 0x555555583ec9 *)
mov xmm9_0 L0x7fffffffda90;
mov xmm9_1 L0x7fffffffda98;
(* vmovdqa64 0xb0(%rdi),%xmm28                     #! EA = L0x7fffffffdae0; Value = 0xae7805e94da18354; PC = 0x555555583ece *)
mov xmm28_0 L0x7fffffffdae0;
mov xmm28_1 L0x7fffffffdae8;
(* vmovdqa64 0x100(%rdi),%xmm19                    #! EA = L0x7fffffffdb30; Value = 0x1b035f41fe689aec; PC = 0x555555583ed5 *)
mov xmm19_0 L0x7fffffffdb30;
mov xmm19_1 L0x7fffffffdb38;
(* vmovdqa64 0x150(%rdi),%xmm26                    #! EA = L0x7fffffffdb80; Value = 0xa0761db27d7fe667; PC = 0x555555583edc *)
mov xmm26_0 L0x7fffffffdb80;
mov xmm26_1 L0x7fffffffdb88;
(* vpxorq %xmm1,%xmm16,%xmm0                       #! PC = 0x555555583ee3 *)
xor xmm0_0@uint64 xmm16_0 xmm1_0;
xor xmm0_1@uint64 xmm16_1 xmm1_1;
(* vmovdqa64 0x70(%rdi),%xmm22                     #! EA = L0x7fffffffdaa0; Value = 0x9d53321056e2f855; PC = 0x555555583ee9 *)
mov xmm22_0 L0x7fffffffdaa0;
mov xmm22_1 L0x7fffffffdaa8;
(* vmovdqa 0xc0(%rdi),%xmm8                        #! EA = L0x7fffffffdaf0; Value = 0x6d8edf952e279683; PC = 0x555555583ef0 *)
mov xmm8_0 L0x7fffffffdaf0;
mov xmm8_1 L0x7fffffffdaf8;
(* vmovdqa64 0x110(%rdi),%xmm18                    #! EA = L0x7fffffffdb40; Value = 0xb940a4e578145d53; PC = 0x555555583ef8 *)
mov xmm18_0 L0x7fffffffdb40;
mov xmm18_1 L0x7fffffffdb48;
(* vmovdqa 0x160(%rdi),%xmm14                      #! EA = L0x7fffffffdb90; Value = 0x15af466865cdf4c3; PC = 0x555555583eff *)
mov xmm14_0 L0x7fffffffdb90;
mov xmm14_1 L0x7fffffffdb98;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x555555583f07 *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vmovdqa 0x80(%rdi),%xmm6                        #! EA = L0x7fffffffdab0; Value = 0xc077b2fd01aceea4; PC = 0x555555583f0d *)
mov xmm6_0 L0x7fffffffdab0;
mov xmm6_1 L0x7fffffffdab8;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x555555583f15 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm19,%xmm26,%xmm1                      #! PC = 0x555555583f19 *)
xor xmm1_0@uint64 xmm26_0 xmm19_0;
xor xmm1_1@uint64 xmm26_1 xmm19_1;
(* vmovdqa 0xd0(%rdi),%xmm7                        #! EA = L0x7fffffffdb00; Value = 0xa7ab29f18324ed01; PC = 0x555555583f1f *)
mov xmm7_0 L0x7fffffffdb00;
mov xmm7_1 L0x7fffffffdb08;
(* vmovdqa 0x120(%rdi),%xmm12                      #! EA = L0x7fffffffdb50; Value = 0x57801ce9f69346e0; PC = 0x555555583f27 *)
mov xmm12_0 L0x7fffffffdb50;
mov xmm12_1 L0x7fffffffdb58;
(* vmovdqa %xmm3,-0x68(%rsp)                       #! EA = L0x7fffffffd680; PC = 0x555555583f2f *)
mov L0x7fffffffd680 xmm3_0;
mov L0x7fffffffd688 xmm3_1;
(* vpxorq %xmm9,%xmm28,%xmm0                       #! PC = 0x555555583f35 *)
xor xmm0_0@uint64 xmm28_0 xmm9_0;
xor xmm0_1@uint64 xmm28_1 xmm9_1;
(* vmovdqa 0x170(%rdi),%xmm3                       #! EA = L0x7fffffffdba0; Value = 0x6110958352df4852; PC = 0x555555583f3b *)
mov xmm3_0 L0x7fffffffdba0;
mov xmm3_1 L0x7fffffffdba8;
(* vmovdqa64 0xe0(%rdi),%xmm25                     #! EA = L0x7fffffffdb10; Value = 0x8e5cbdd4a8bfa206; PC = 0x555555583f43 *)
mov xmm25_0 L0x7fffffffdb10;
mov xmm25_1 L0x7fffffffdb18;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x555555583f4a *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxorq %xmm18,%xmm14,%xmm2                      #! PC = 0x555555583f4e *)
xor xmm2_0@uint64 xmm14_0 xmm18_0;
xor xmm2_1@uint64 xmm14_1 xmm18_1;
(* vpxorq %xmm22,%xmm8,%xmm1                       #! PC = 0x555555583f54 *)
xor xmm1_0@uint64 xmm8_0 xmm22_0;
xor xmm1_1@uint64 xmm8_1 xmm22_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x555555583f5a *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpxorq %xmm6,%xmm7,%xmm17                       #! PC = 0x555555583f5e *)
xor xmm17_0@uint64 xmm7_0 xmm6_0;
xor xmm17_1@uint64 xmm7_1 xmm6_1;
(* vpxor  %xmm12,%xmm3,%xmm2                       #! PC = 0x555555583f64 *)
xor xmm2_0@uint64 xmm3_0 xmm12_0;
xor xmm2_1@uint64 xmm3_1 xmm12_1;
(* vmovdqa 0x90(%rdi),%xmm5                        #! EA = L0x7fffffffdac0; Value = 0x3cbdb5cf7e7a44c9; PC = 0x555555583f69 *)
mov xmm5_0 L0x7fffffffdac0;
mov xmm5_1 L0x7fffffffdac8;
(* vmovdqa 0x130(%rdi),%xmm4                       #! EA = L0x7fffffffdb60; Value = 0xb4769f7116727a96; PC = 0x555555583f71 *)
mov xmm4_0 L0x7fffffffdb60;
mov xmm4_1 L0x7fffffffdb68;
(* vmovdqa 0x180(%rdi),%xmm15                      #! EA = L0x7fffffffdbb0; Value = 0xda75604b8015024b; PC = 0x555555583f79 *)
mov xmm15_0 L0x7fffffffdbb0;
mov xmm15_1 L0x7fffffffdbb8;
(* vmovdqa64 0x40(%rdi),%xmm31                     #! EA = L0x7fffffffda70; Value = 0xab9315d59a08452a; PC = 0x555555583f81 *)
mov xmm31_0 L0x7fffffffda70;
mov xmm31_1 L0x7fffffffda78;
(* vpxorq %xmm2,%xmm17,%xmm17                      #! PC = 0x555555583f88 *)
xor xmm17_0@uint64 xmm17_0 xmm2_0;
xor xmm17_1@uint64 xmm17_1 xmm2_1;
(* vmovdqa64 0x30(%rdi),%xmm24                     #! EA = L0x7fffffffda60; Value = 0xc5503d566f179cb0; PC = 0x555555583f8e *)
mov xmm24_0 L0x7fffffffda60;
mov xmm24_1 L0x7fffffffda68;
(* vmovdqa64 %xmm25,%xmm2                          #! PC = 0x555555583f95 *)
mov xmm2_0 xmm25_0;
mov xmm2_1 xmm25_1;
(* vmovdqa 0x10(%rdi),%xmm11                       #! EA = L0x7fffffffda40; Value = 0x3fcf6b3bea12218e; PC = 0x555555583f9b *)
mov xmm11_0 L0x7fffffffda40;
mov xmm11_1 L0x7fffffffda48;
(* vmovdqa 0x20(%rdi),%xmm10                       #! EA = L0x7fffffffda50; Value = 0xb0abdf17210cf5de; PC = 0x555555583fa0 *)
mov xmm10_0 L0x7fffffffda50;
mov xmm10_1 L0x7fffffffda58;
(* vmovdqa64 (%rdi),%xmm27                         #! EA = L0x7fffffffda30; Value = 0x5c03a76191b3ea0e; PC = 0x555555583fa5 *)
mov xmm27_0 L0x7fffffffda30;
mov xmm27_1 L0x7fffffffda38;
(* vmovdqa %xmm6,-0x78(%rsp)                       #! EA = L0x7fffffffd670; PC = 0x555555583fab *)
mov L0x7fffffffd670 xmm6_0;
mov L0x7fffffffd678 xmm6_1;
(* vpxor  %xmm2,%xmm5,%xmm2                        #! PC = 0x555555583fb1 *)
xor xmm2_0@uint64 xmm5_0 xmm2_0;
xor xmm2_1@uint64 xmm5_1 xmm2_1;
(* vpxor  %xmm4,%xmm15,%xmm6                       #! PC = 0x555555583fb5 *)
xor xmm6_0@uint64 xmm15_0 xmm4_0;
xor xmm6_1@uint64 xmm15_1 xmm4_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x555555583fb9 *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vpxorq %xmm24,%xmm17,%xmm17                     #! PC = 0x555555583fbd *)
xor xmm17_0@uint64 xmm17_0 xmm24_0;
xor xmm17_1@uint64 xmm17_1 xmm24_1;
(* vmovdqa64 %xmm31,%xmm6                          #! PC = 0x555555583fc3 *)
mov xmm6_0 xmm31_0;
mov xmm6_1 xmm31_1;
(* vmovdqa %xmm11,-0x48(%rsp)                      #! EA = L0x7fffffffd6a0; PC = 0x555555583fd0 *)
mov L0x7fffffffd6a0 xmm11_0;
mov L0x7fffffffd6a8 xmm11_1;
(* vmovdqa %xmm10,-0x8(%rsp)                       #! EA = L0x7fffffffd6e0; PC = 0x555555583fd6 *)
mov L0x7fffffffd6e0 xmm10_0;
mov L0x7fffffffd6e8 xmm10_1;
(* vmovdqa64 %xmm31,-0x58(%rsp)                    #! EA = L0x7fffffffd690; PC = 0x555555583fdc *)
mov L0x7fffffffd690 xmm31_0;
mov L0x7fffffffd698 xmm31_1;
(* vmovdqa64 %xmm25,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x555555583fe7 *)
mov L0x7fffffffd6d0 xmm25_0;
mov L0x7fffffffd6d8 xmm25_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555583ff2 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxor  %xmm11,%xmm0,%xmm0                       #! PC = 0x555555583ff8 *)
xor xmm0_0@uint64 xmm0_0 xmm11_0;
xor xmm0_1@uint64 xmm0_1 xmm11_1;
(* vpxor  %xmm10,%xmm1,%xmm1                       #! PC = 0x555555583ffd *)
xor xmm1_0@uint64 xmm1_0 xmm10_0;
xor xmm1_1@uint64 xmm1_1 xmm10_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x555555584002 *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* mov    $0x8082,%ecx                             #! PC = 0x55555558400d *)
mov rcx 0x8082@uint64;
(* mov    $0x1,%edx                                #! PC = 0x555555584012 *)
mov rdx 0x1@uint64;
(* vmovdqa64 %xmm17,%xmm20                         #! PC = 0x555555584017 *)
mov xmm20_0 xmm17_0;
mov xmm20_1 xmm17_1;
(* #jmp    0x55555558402b <KeccakP1600times2_PermuteAll_24rounds+395>#! PC = 0x55555558401d *)
#jmp    0x55555558402b <KeccakP1600times2_PermuteAll_24rounds+395>#! 0x55555558401d = 0x55555558401d;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x3fcf6b3bea12218e; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0xab9315d59a08452a; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x8e5cbdd4a8bfa206; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0xc077b2fd01aceea4; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0xb0abdf17210cf5de; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x643cf6f87f4cba7d; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0x0e4529212721fa39; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x048dc4dabdcd817d; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x048dc4dabdcd817d; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0x0e4529212721fa39; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x972201842856b8fd; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0x7e3977d8dbd2ba68; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0x2b373406203d8fbb; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0xefd48a7ff98a452c; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a390; Value = 0x800000000000808a; PC = 0x555555584020 *)
mov rdx L0x55555559a390;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a398; Value = 0x8000000080008000; PC = 0x555555584023 *)
mov rcx L0x55555559a398;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x7e3977d8dbd2ba68; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0xefd48a7ff98a452c; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x69fcea7c910e7b8d; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0x2b373406203d8fbb; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0x22cf5b20c53ff250; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x972201842856b8fd; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0x64a4e0a722e16f98; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x30f998e3d4d1eb98; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x30f998e3d4d1eb98; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0x64a4e0a722e16f98; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x2b1247ad72ef1c16; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0xb823d93bf0abb754; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0x9c5224f3dce1c5d9; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0xff0c66e24bc93aac; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3a0; Value = 0x000000000000808b; PC = 0x555555584020 *)
mov rdx L0x55555559a3a0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3a8; Value = 0x0000000080000001; PC = 0x555555584023 *)
mov rcx L0x55555559a3a8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0xb823d93bf0abb754; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0xff0c66e24bc93aac; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x95e2e106e2a4e916; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0x9c5224f3dce1c5d9; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0xc0a1572f41f82f16; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x2b1247ad72ef1c16; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0xf258bbde92a30197; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x727f3d27b30e1bc7; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x727f3d27b30e1bc7; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0xf258bbde92a30197; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x6bc4a0de6cdfee8e; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0x3e8d7ce8728a48f9; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0x521e7837642cb625; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0xb7989e70614c0bdb; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3b0; Value = 0x8000000080008081; PC = 0x555555584020 *)
mov rdx L0x55555559a3b0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3b8; Value = 0x8000000000008009; PC = 0x555555584023 *)
mov rcx L0x55555559a3b8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x3e8d7ce8728a48f9; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0xb7989e70614c0bdb; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x1697c84f885e63b2; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0x521e7837642cb625; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0x2674c1998e7e6140; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x6bc4a0de6cdfee8e; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0x2c2397be6adcbcc8; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x6c5f110063104fba; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x6c5f110063104fba; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0x2c2397be6adcbcc8; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x432273ebba42bcd9; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0xd9db0c051e31691c; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0x703d35865026f3e9; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0xc7ff90f8ac42c141; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3c0; Value = 0x000000000000008a; PC = 0x555555584020 *)
mov rdx L0x55555559a3c0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3c8; Value = 0x0000000000000088; PC = 0x555555584023 *)
mov rcx L0x55555559a3c8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0xd9db0c051e31691c; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0xc7ff90f8ac42c141; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0xd57920beb567bc74; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0x703d35865026f3e9; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0xfee00f216866f009; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x432273ebba42bcd9; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0x6e98c442f7fd51cf; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x77b4a3425d9e0ff5; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x77b4a3425d9e0ff5; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0x6e98c442f7fd51cf; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0xa7f481753c56abf2; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0xddae2cc40d149bed; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0xf201d143a17215fc; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0x43be6253386966c8; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3d0; Value = 0x0000000080008009; PC = 0x555555584020 *)
mov rdx L0x55555559a3d0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3d8; Value = 0x000000008000000a; PC = 0x555555584023 *)
mov rcx L0x55555559a3d8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0xddae2cc40d149bed; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x43be6253386966c8; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0xab5d70deeb76e164; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0xf201d143a17215fc; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0x2de17284c70d476d; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0xa7f481753c56abf2; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0x1ca3cf79ad4aae45; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x7d41fa650ab776b1; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x7d41fa650ab776b1; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0x1ca3cf79ad4aae45; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x4d62329b3a3548b3; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0x24bddc514aae0c0f; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0xe50c05d0fdd2a9d7; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0x1c448a1137c582c2; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3e0; Value = 0x000000008000808b; PC = 0x555555584020 *)
mov rdx L0x55555559a3e0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3e8; Value = 0x800000000000008b; PC = 0x555555584023 *)
mov rcx L0x55555559a3e8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x24bddc514aae0c0f; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x1c448a1137c582c2; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x7864b511e5615abf; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0xe50c05d0fdd2a9d7; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0x6ab9f36c7cb024e8; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0x4d62329b3a3548b3; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0xdb86bc4622b64d19; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x1f424c454a013a7b; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x1f424c454a013a7b; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0xdb86bc4622b64d19; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0xec9739a635885281; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0x08c0a53cc9b69710; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0x067966ca5bc2a38c; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0x2f02eaa59ccdd9a0; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3f0; Value = 0x8000000000008089; PC = 0x555555584020 *)
mov rdx L0x55555559a3f0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3f8; Value = 0x8000000000008003; PC = 0x555555584023 *)
mov rcx L0x55555559a3f8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x08c0a53cc9b69710; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x2f02eaa59ccdd9a0; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0xbbbfc6f880c5a110; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0x067966ca5bc2a38c; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0x50b1162f288338f9; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0xec9739a635885281; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0xd24c7cd3ccdb37e4; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0xf1e53755b04425f0; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0xf1e53755b04425f0; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0xd24c7cd3ccdb37e4; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0xb4efd156213e34e9; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0xedafa47ca52489ef; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0x7f27c708d9b0c7aa; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0xeb5ab9ed2ac6b7f5; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a400; Value = 0x8000000000008002; PC = 0x555555584020 *)
mov rdx L0x55555559a400;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a408; Value = 0x8000000000000080; PC = 0x555555584023 *)
mov rcx L0x55555559a408;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0xedafa47ca52489ef; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0xeb5ab9ed2ac6b7f5; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x55363a209f7d87e8; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0x7f27c708d9b0c7aa; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0xa3ac69ce5fa75a74; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0xb4efd156213e34e9; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0x9cf892f358761a2a; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0xe9f0c5b9e698ec51; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0xe9f0c5b9e698ec51; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0x9cf892f358761a2a; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0xb2a7c00457e981d4; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0xf51419c1975dd664; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0xfc912990a250b9c9; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0x0621e272504bc792; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a410; Value = 0x000000000000800a; PC = 0x555555584020 *)
mov rdx L0x55555559a410;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a418; Value = 0x800000008000000a; PC = 0x555555584023 *)
mov rcx L0x55555559a418;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0xf51419c1975dd664; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x0621e272504bc792; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0xd1d864226126b977; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0xfc912990a250b9c9; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0x46451df7cb00cf0a; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0xb2a7c00457e981d4; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0x9fc4a349c88de95b; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0x88a80a84465d6239; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0x88a80a84465d6239; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0x9fc4a349c88de95b; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0xd28090883d1d1e5d; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0x6f0818b8f507ce47; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0xdabfebb6b2769b88; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0x88e87350a8a4237c; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a420; Value = 0x8000000080008081; PC = 0x555555584020 *)
mov rdx L0x55555559a420;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a428; Value = 0x8000000000008080; PC = 0x555555584023 *)
mov rcx L0x55555559a428;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x6f0818b8f507ce47; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x88e87350a8a4237c; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0xab8fd4730b4c8f8c; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0xdabfebb6b2769b88; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0xc5c5dd890600554a; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0xd28090883d1d1e5d; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0x919105dc6a2eb192; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0xceea07042a08b67f; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0xceea07042a08b67f; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0x919105dc6a2eb192; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0xb44f92e62bdddbc0; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0x2442010d393d6152; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0x869bce6379f17611; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0x77df33c2b2f499a9; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a430; Value = 0x0000000080000001; PC = 0x555555584020 *)
mov rdx L0x55555559a430;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a438; Value = 0x8000000080008008; PC = 0x555555584023 *)
mov rcx L0x55555559a438;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x5555555840ec *)
mov L0x7fffffffd6c0 xmm0_0;
mov L0x7fffffffd6c8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6b0; PC = 0x555555584121 *)
mov L0x7fffffffd6b0 xmm2_0;
mov L0x7fffffffd6b8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6a0; Value = 0x2442010d393d6152; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6a0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6a8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd690; Value = 0x77df33c2b2f499a9; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd690;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd698;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6d0; Value = 0x7a12243931a710a9; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6d0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6d8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd670; Value = 0x869bce6379f17611; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd670;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd678;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6e0; Value = 0x4fcc9ae60c297ac5; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6e0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6e8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd680; Value = 0xb44f92e62bdddbc0; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd680;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd688;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6c0; Value = 0x11657a980f9c0a30; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6c0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6c8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6b0; Value = 0xc9960c1247c44dce; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6b0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6b8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584444 *)
mov L0x7fffffffd690 xmm13_0;
mov L0x7fffffffd698 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd670; PC = 0x55555558449f *)
mov L0x7fffffffd670 xmm10_0;
mov L0x7fffffffd678 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6b0; Value = 0xc9960c1247c44dce; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6b0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6b8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6c0; Value = 0x11657a980f9c0a30; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6c0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6c8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x555555584542 *)
mov L0x7fffffffd680 xmm12_0;
mov L0x7fffffffd688 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd680; Value = 0x2db2e653ca7d5762; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd680;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd688;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6a0 xmm26_0;
mov L0x7fffffffd6a8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6a0; Value = 0x863a535ea4132326; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6a0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6a8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd670; Value = 0x1664ba63775e5faa; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd670;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd678;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd690; Value = 0xe0bc5740a0dd2cc7; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd690;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd698;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6e0; PC = 0x555555584650 *)
mov L0x7fffffffd6e0 xmm17_0;
mov L0x7fffffffd6e8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6d0; PC = 0x55555558465b *)
mov L0x7fffffffd6d0 xmm23_0;
mov L0x7fffffffd6d8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* vmovdqa -0x48(%rsp),%xmm1                       #! EA = L0x7fffffffd6a0; Value = 0x863a535ea4132326; PC = 0x55555558468a *)
mov xmm1_0 L0x7fffffffd6a0;
mov xmm1_1 L0x7fffffffd6a8;
(* vmovdqa %xmm7,0xd0(%rdi)                        #! EA = L0x7fffffffdb00; PC = 0x555555584690 *)
mov L0x7fffffffdb00 xmm7_0;
mov L0x7fffffffdb08 xmm7_1;
(* vmovdqa %xmm1,0x10(%rdi)                        #! EA = L0x7fffffffda40; PC = 0x555555584698 *)
mov L0x7fffffffda40 xmm1_0;
mov L0x7fffffffda48 xmm1_1;
(* vmovdqa -0x8(%rsp),%xmm1                        #! EA = L0x7fffffffd6e0; Value = 0x2edb52edf09d4de1; PC = 0x55555558469d *)
mov xmm1_0 L0x7fffffffd6e0;
mov xmm1_1 L0x7fffffffd6e8;
(* vmovdqa -0x18(%rsp),%xmm7                       #! EA = L0x7fffffffd6d0; Value = 0xfa5ab212105e8eea; PC = 0x5555555846a3 *)
mov xmm7_0 L0x7fffffffd6d0;
mov xmm7_1 L0x7fffffffd6d8;
(* vmovdqa %xmm1,0x20(%rdi)                        #! EA = L0x7fffffffda50; PC = 0x5555555846a9 *)
mov L0x7fffffffda50 xmm1_0;
mov L0x7fffffffda58 xmm1_1;
(* vmovdqa -0x58(%rsp),%xmm1                       #! EA = L0x7fffffffd690; Value = 0xe0bc5740a0dd2cc7; PC = 0x5555555846ae *)
mov xmm1_0 L0x7fffffffd690;
mov xmm1_1 L0x7fffffffd698;
(* vmovdqa %xmm7,0xe0(%rdi)                        #! EA = L0x7fffffffdb10; PC = 0x5555555846b4 *)
mov L0x7fffffffdb10 xmm7_0;
mov L0x7fffffffdb18 xmm7_1;
(* vmovdqa %xmm1,0x40(%rdi)                        #! EA = L0x7fffffffda70; PC = 0x5555555846bc *)
mov L0x7fffffffda70 xmm1_0;
mov L0x7fffffffda78 xmm1_1;
(* vmovdqa -0x68(%rsp),%xmm7                       #! EA = L0x7fffffffd680; Value = 0x2db2e653ca7d5762; PC = 0x5555555846c1 *)
mov xmm7_0 L0x7fffffffd680;
mov xmm7_1 L0x7fffffffd688;
(* vmovdqa -0x78(%rsp),%xmm1                       #! EA = L0x7fffffffd670; Value = 0x1664ba63775e5faa; PC = 0x5555555846c7 *)
mov xmm1_0 L0x7fffffffd670;
mov xmm1_1 L0x7fffffffd678;
(* vmovdqa64 %xmm27,(%rdi)                         #! EA = L0x7fffffffda30; PC = 0x5555555846cd *)
mov L0x7fffffffda30 xmm27_0;
mov L0x7fffffffda38 xmm27_1;
(* vmovdqa64 %xmm24,0x30(%rdi)                     #! EA = L0x7fffffffda60; PC = 0x5555555846d3 *)
mov L0x7fffffffda60 xmm24_0;
mov L0x7fffffffda68 xmm24_1;
(* vmovdqa64 %xmm29,0x50(%rdi)                     #! EA = L0x7fffffffda80; PC = 0x5555555846da *)
mov L0x7fffffffda80 xmm29_0;
mov L0x7fffffffda88 xmm29_1;
(* vmovdqa %xmm9,0x60(%rdi)                        #! EA = L0x7fffffffda90; PC = 0x5555555846e1 *)
mov L0x7fffffffda90 xmm9_0;
mov L0x7fffffffda98 xmm9_1;
(* vmovdqa64 %xmm22,0x70(%rdi)                     #! EA = L0x7fffffffdaa0; PC = 0x5555555846e6 *)
mov L0x7fffffffdaa0 xmm22_0;
mov L0x7fffffffdaa8 xmm22_1;
(* vmovdqa %xmm1,0x80(%rdi)                        #! EA = L0x7fffffffdab0; PC = 0x5555555846ed *)
mov L0x7fffffffdab0 xmm1_0;
mov L0x7fffffffdab8 xmm1_1;
(* vmovdqa %xmm5,0x90(%rdi)                        #! EA = L0x7fffffffdac0; PC = 0x5555555846f5 *)
mov L0x7fffffffdac0 xmm5_0;
mov L0x7fffffffdac8 xmm5_1;
(* vmovdqa64 %xmm21,0xa0(%rdi)                     #! EA = L0x7fffffffdad0; PC = 0x5555555846fd *)
mov L0x7fffffffdad0 xmm21_0;
mov L0x7fffffffdad8 xmm21_1;
(* vmovdqa64 %xmm28,0xb0(%rdi)                     #! EA = L0x7fffffffdae0; PC = 0x555555584704 *)
mov L0x7fffffffdae0 xmm28_0;
mov L0x7fffffffdae8 xmm28_1;
(* vmovdqa %xmm8,0xc0(%rdi)                        #! EA = L0x7fffffffdaf0; PC = 0x55555558470b *)
mov L0x7fffffffdaf0 xmm8_0;
mov L0x7fffffffdaf8 xmm8_1;
(* vmovdqa %xmm7,0xf0(%rdi)                        #! EA = L0x7fffffffdb20; PC = 0x555555584713 *)
mov L0x7fffffffdb20 xmm7_0;
mov L0x7fffffffdb28 xmm7_1;
(* vmovdqa64 %xmm19,0x100(%rdi)                    #! EA = L0x7fffffffdb30; PC = 0x55555558471b *)
mov L0x7fffffffdb30 xmm19_0;
mov L0x7fffffffdb38 xmm19_1;
(* vmovdqa64 %xmm18,0x110(%rdi)                    #! EA = L0x7fffffffdb40; PC = 0x555555584722 *)
mov L0x7fffffffdb40 xmm18_0;
mov L0x7fffffffdb48 xmm18_1;
(* vmovdqa %xmm12,0x120(%rdi)                      #! EA = L0x7fffffffdb50; PC = 0x555555584729 *)
mov L0x7fffffffdb50 xmm12_0;
mov L0x7fffffffdb58 xmm12_1;
(* vmovdqa %xmm4,0x130(%rdi)                       #! EA = L0x7fffffffdb60; PC = 0x555555584731 *)
mov L0x7fffffffdb60 xmm4_0;
mov L0x7fffffffdb68 xmm4_1;
(* vmovdqa64 %xmm16,0x140(%rdi)                    #! EA = L0x7fffffffdb70; PC = 0x555555584739 *)
mov L0x7fffffffdb70 xmm16_0;
mov L0x7fffffffdb78 xmm16_1;
(* vmovdqa64 %xmm26,0x150(%rdi)                    #! EA = L0x7fffffffdb80; PC = 0x555555584740 *)
mov L0x7fffffffdb80 xmm26_0;
mov L0x7fffffffdb88 xmm26_1;
(* vmovdqa %xmm14,0x160(%rdi)                      #! EA = L0x7fffffffdb90; PC = 0x555555584747 *)
mov L0x7fffffffdb90 xmm14_0;
mov L0x7fffffffdb98 xmm14_1;
(* vmovdqa %xmm3,0x170(%rdi)                       #! EA = L0x7fffffffdba0; PC = 0x55555558474f *)
mov L0x7fffffffdba0 xmm3_0;
mov L0x7fffffffdba8 xmm3_1;
(* vmovdqa %xmm15,0x180(%rdi)                      #! EA = L0x7fffffffdbb0; PC = 0x555555584757 *)
mov L0x7fffffffdbb0 xmm15_0;
mov L0x7fffffffdbb8 xmm15_1;
(* add    $0x10,%rsp                               #! PC = 0x55555558475f *)
adds carry rsp rsp 0x10@uint64;
(* #! <- SP = 0x7fffffffd6f8 *)
#! 0x7fffffffd6f8 = 0x7fffffffd6f8;
(* #ret                                            #! PC = 0x555555584763 *)
#ret                                            #! 0x555555584763 = 0x555555584763;
(* #jmp    0x555555583ea0 <KeccakP1600times2_PermuteAll_24rounds>#! PC = 0x55555558697d *)
#jmp    0x555555583ea0 <KeccakP1600times2_PermuteAll_24rounds>#! 0x55555558697d = 0x55555558697d;
(* sub    $0x10,%rsp                               #! PC = 0x555555583ea4 *)
subb carry rsp rsp 0x10@uint64;
(* vmovdqa 0xf0(%rdi),%xmm3                        #! EA = L0x7fffffffdcb0; Value = 0x00007fffffffdff8; PC = 0x555555583ea8 *)
mov xmm3_0 L0x7fffffffdcb0;
mov xmm3_1 L0x7fffffffdcb8;
(* vmovdqa64 0x50(%rdi),%xmm29                     #! EA = L0x7fffffffdc10; Value = 0x00007fffffffdff8; PC = 0x555555583eb0 *)
mov xmm29_0 L0x7fffffffdc10;
mov xmm29_1 L0x7fffffffdc18;
(* vmovdqa64 0xa0(%rdi),%xmm21                     #! EA = L0x7fffffffdc60; Value = 0x0000000000000000; PC = 0x555555583eb7 *)
mov xmm21_0 L0x7fffffffdc60;
mov xmm21_1 L0x7fffffffdc68;
(* vmovdqa64 0x140(%rdi),%xmm16                    #! EA = L0x7fffffffdd00; Value = 0x0000000000000000; PC = 0x555555583ebe *)
mov xmm16_0 L0x7fffffffdd00;
mov xmm16_1 L0x7fffffffdd08;
(* vmovdqa %xmm3,%xmm1                             #! PC = 0x555555583ec5 *)
mov xmm1_0 xmm3_0;
mov xmm1_1 xmm3_1;
(* vmovdqa 0x60(%rdi),%xmm9                        #! EA = L0x7fffffffdc20; Value = 0x0000006000000018; PC = 0x555555583ec9 *)
mov xmm9_0 L0x7fffffffdc20;
mov xmm9_1 L0x7fffffffdc28;
(* vmovdqa64 0xb0(%rdi),%xmm28                     #! EA = L0x7fffffffdc70; Value = 0x4009c409ba5e353f; PC = 0x555555583ece *)
mov xmm28_0 L0x7fffffffdc70;
mov xmm28_1 L0x7fffffffdc78;
(* vmovdqa64 0x100(%rdi),%xmm19                    #! EA = L0x7fffffffdcc0; Value = 0x0000003000000018; PC = 0x555555583ed5 *)
mov xmm19_0 L0x7fffffffdcc0;
mov xmm19_1 L0x7fffffffdcc8;
(* vmovdqa64 0x150(%rdi),%xmm26                    #! EA = L0x7fffffffdd10; Value = 0x00007ffff7eb2780; PC = 0x555555583edc *)
mov xmm26_0 L0x7fffffffdd10;
mov xmm26_1 L0x7fffffffdd18;
(* vpxorq %xmm1,%xmm16,%xmm0                       #! PC = 0x555555583ee3 *)
xor xmm0_0@uint64 xmm16_0 xmm1_0;
xor xmm0_1@uint64 xmm16_1 xmm1_1;
(* vmovdqa64 0x70(%rdi),%xmm22                     #! EA = L0x7fffffffdc30; Value = 0x00007fffffffdc40; PC = 0x555555583ee9 *)
mov xmm22_0 L0x7fffffffdc30;
mov xmm22_1 L0x7fffffffdc38;
(* vmovdqa 0xc0(%rdi),%xmm8                        #! EA = L0x7fffffffdc80; Value = 0x40077efa567f7538; PC = 0x555555583ef0 *)
mov xmm8_0 L0x7fffffffdc80;
mov xmm8_1 L0x7fffffffdc88;
(* vmovdqa64 0x110(%rdi),%xmm18                    #! EA = L0x7fffffffdcd0; Value = 0x00007fffffffdce0; PC = 0x555555583ef8 *)
mov xmm18_0 L0x7fffffffdcd0;
mov xmm18_1 L0x7fffffffdcd8;
(* vmovdqa 0x160(%rdi),%xmm14                      #! EA = L0x7fffffffdd20; Value = 0x00007ffff7c95740; PC = 0x555555583eff *)
mov xmm14_0 L0x7fffffffdd20;
mov xmm14_1 L0x7fffffffdd28;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x555555583f07 *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vmovdqa 0x80(%rdi),%xmm6                        #! EA = L0x7fffffffdc40; Value = 0x7dfacdd0c1163cb3; PC = 0x555555583f0d *)
mov xmm6_0 L0x7fffffffdc40;
mov xmm6_1 L0x7fffffffdc48;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x555555583f15 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm19,%xmm26,%xmm1                      #! PC = 0x555555583f19 *)
xor xmm1_0@uint64 xmm26_0 xmm19_0;
xor xmm1_1@uint64 xmm26_1 xmm19_1;
(* vmovdqa 0xd0(%rdi),%xmm7                        #! EA = L0x7fffffffdc90; Value = 0x000055555559bad0; PC = 0x555555583f1f *)
mov xmm7_0 L0x7fffffffdc90;
mov xmm7_1 L0x7fffffffdc98;
(* vmovdqa 0x120(%rdi),%xmm12                      #! EA = L0x7fffffffdce0; Value = 0x0000000000000001; PC = 0x555555583f27 *)
mov xmm12_0 L0x7fffffffdce0;
mov xmm12_1 L0x7fffffffdce8;
(* vmovdqa %xmm3,-0x68(%rsp)                       #! EA = L0x7fffffffd690; PC = 0x555555583f2f *)
mov L0x7fffffffd690 xmm3_0;
mov L0x7fffffffd698 xmm3_1;
(* vpxorq %xmm9,%xmm28,%xmm0                       #! PC = 0x555555583f35 *)
xor xmm0_0@uint64 xmm28_0 xmm9_0;
xor xmm0_1@uint64 xmm28_1 xmm9_1;
(* vmovdqa 0x170(%rdi),%xmm3                       #! EA = L0x7fffffffdd30; Value = 0x00007fffffffdff8; PC = 0x555555583f3b *)
mov xmm3_0 L0x7fffffffdd30;
mov xmm3_1 L0x7fffffffdd38;
(* vmovdqa64 0xe0(%rdi),%xmm25                     #! EA = L0x7fffffffdca0; Value = 0x00007fffffffdda0; PC = 0x555555583f43 *)
mov xmm25_0 L0x7fffffffdca0;
mov xmm25_1 L0x7fffffffdca8;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x555555583f4a *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxorq %xmm18,%xmm14,%xmm2                      #! PC = 0x555555583f4e *)
xor xmm2_0@uint64 xmm14_0 xmm18_0;
xor xmm2_1@uint64 xmm14_1 xmm18_1;
(* vpxorq %xmm22,%xmm8,%xmm1                       #! PC = 0x555555583f54 *)
xor xmm1_0@uint64 xmm8_0 xmm22_0;
xor xmm1_1@uint64 xmm8_1 xmm22_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x555555583f5a *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpxorq %xmm6,%xmm7,%xmm17                       #! PC = 0x555555583f5e *)
xor xmm17_0@uint64 xmm7_0 xmm6_0;
xor xmm17_1@uint64 xmm7_1 xmm6_1;
(* vpxor  %xmm12,%xmm3,%xmm2                       #! PC = 0x555555583f64 *)
xor xmm2_0@uint64 xmm3_0 xmm12_0;
xor xmm2_1@uint64 xmm3_1 xmm12_1;
(* vmovdqa 0x90(%rdi),%xmm5                        #! EA = L0x7fffffffdc50; Value = 0x000055555559c380; PC = 0x555555583f69 *)
mov xmm5_0 L0x7fffffffdc50;
mov xmm5_1 L0x7fffffffdc58;
(* vmovdqa 0x130(%rdi),%xmm4                       #! EA = L0x7fffffffdcf0; Value = 0x000055555559bad0; PC = 0x555555583f71 *)
mov xmm4_0 L0x7fffffffdcf0;
mov xmm4_1 L0x7fffffffdcf8;
(* vmovdqa 0x180(%rdi),%xmm15                      #! EA = L0x7fffffffdd40; Value = 0x00007ffff7eb2780; PC = 0x555555583f79 *)
mov xmm15_0 L0x7fffffffdd40;
mov xmm15_1 L0x7fffffffdd48;
(* vmovdqa64 0x40(%rdi),%xmm31                     #! EA = L0x7fffffffdc00; Value = 0x0000000000000000; PC = 0x555555583f81 *)
mov xmm31_0 L0x7fffffffdc00;
mov xmm31_1 L0x7fffffffdc08;
(* vpxorq %xmm2,%xmm17,%xmm17                      #! PC = 0x555555583f88 *)
xor xmm17_0@uint64 xmm17_0 xmm2_0;
xor xmm17_1@uint64 xmm17_1 xmm2_1;
(* vmovdqa64 0x30(%rdi),%xmm24                     #! EA = L0x7fffffffdbf0; Value = 0x00007fffffffdd20; PC = 0x555555583f8e *)
mov xmm24_0 L0x7fffffffdbf0;
mov xmm24_1 L0x7fffffffdbf8;
(* vmovdqa64 %xmm25,%xmm2                          #! PC = 0x555555583f95 *)
mov xmm2_0 xmm25_0;
mov xmm2_1 xmm25_1;
(* vmovdqa 0x10(%rdi),%xmm11                       #! EA = L0x7fffffffdbd0; Value = 0x34363938303631ba; PC = 0x555555583f9b *)
mov xmm11_0 L0x7fffffffdbd0;
mov xmm11_1 L0x7fffffffdbd8;
(* vmovdqa 0x20(%rdi),%xmm10                       #! EA = L0x7fffffffdbe0; Value = 0x529a560d8dbaf494; PC = 0x555555583fa0 *)
mov xmm10_0 L0x7fffffffdbe0;
mov xmm10_1 L0x7fffffffdbe8;
(* vmovdqa64 (%rdi),%xmm27                         #! EA = L0x7fffffffdbc0; Value = 0xbd9b8ffcbdb431ae; PC = 0x555555583fa5 *)
mov xmm27_0 L0x7fffffffdbc0;
mov xmm27_1 L0x7fffffffdbc8;
(* vmovdqa %xmm6,-0x78(%rsp)                       #! EA = L0x7fffffffd680; PC = 0x555555583fab *)
mov L0x7fffffffd680 xmm6_0;
mov L0x7fffffffd688 xmm6_1;
(* vpxor  %xmm2,%xmm5,%xmm2                        #! PC = 0x555555583fb1 *)
xor xmm2_0@uint64 xmm5_0 xmm2_0;
xor xmm2_1@uint64 xmm5_1 xmm2_1;
(* vpxor  %xmm4,%xmm15,%xmm6                       #! PC = 0x555555583fb5 *)
xor xmm6_0@uint64 xmm15_0 xmm4_0;
xor xmm6_1@uint64 xmm15_1 xmm4_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x555555583fb9 *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vpxorq %xmm24,%xmm17,%xmm17                     #! PC = 0x555555583fbd *)
xor xmm17_0@uint64 xmm17_0 xmm24_0;
xor xmm17_1@uint64 xmm17_1 xmm24_1;
(* vmovdqa64 %xmm31,%xmm6                          #! PC = 0x555555583fc3 *)
mov xmm6_0 xmm31_0;
mov xmm6_1 xmm31_1;
(* vmovdqa %xmm11,-0x48(%rsp)                      #! EA = L0x7fffffffd6b0; PC = 0x555555583fd0 *)
mov L0x7fffffffd6b0 xmm11_0;
mov L0x7fffffffd6b8 xmm11_1;
(* vmovdqa %xmm10,-0x8(%rsp)                       #! EA = L0x7fffffffd6f0; PC = 0x555555583fd6 *)
mov L0x7fffffffd6f0 xmm10_0;
mov L0x7fffffffd6f8 xmm10_1;
(* vmovdqa64 %xmm31,-0x58(%rsp)                    #! EA = L0x7fffffffd6a0; PC = 0x555555583fdc *)
mov L0x7fffffffd6a0 xmm31_0;
mov L0x7fffffffd6a8 xmm31_1;
(* vmovdqa64 %xmm25,-0x18(%rsp)                    #! EA = L0x7fffffffd6e0; PC = 0x555555583fe7 *)
mov L0x7fffffffd6e0 xmm25_0;
mov L0x7fffffffd6e8 xmm25_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555583ff2 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxor  %xmm11,%xmm0,%xmm0                       #! PC = 0x555555583ff8 *)
xor xmm0_0@uint64 xmm0_0 xmm11_0;
xor xmm0_1@uint64 xmm0_1 xmm11_1;
(* vpxor  %xmm10,%xmm1,%xmm1                       #! PC = 0x555555583ffd *)
xor xmm1_0@uint64 xmm1_0 xmm10_0;
xor xmm1_1@uint64 xmm1_1 xmm10_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x555555584002 *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* mov    $0x8082,%ecx                             #! PC = 0x55555558400d *)
mov rcx 0x8082@uint64;
(* mov    $0x1,%edx                                #! PC = 0x555555584012 *)
mov rdx 0x1@uint64;
(* vmovdqa64 %xmm17,%xmm20                         #! PC = 0x555555584017 *)
mov xmm20_0 xmm17_0;
mov xmm20_1 xmm17_1;
(* #jmp    0x55555558402b <KeccakP1600times2_PermuteAll_24rounds+395>#! PC = 0x55555558401d *)
#jmp    0x55555558402b <KeccakP1600times2_PermuteAll_24rounds+395>#! 0x55555558401d = 0x55555558401d;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6d0; PC = 0x5555555840ec *)
mov L0x7fffffffd6d0 xmm0_0;
mov L0x7fffffffd6d8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x555555584121 *)
mov L0x7fffffffd6c0 xmm2_0;
mov L0x7fffffffd6c8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6b0; Value = 0x34363938303631ba; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6b0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6b8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd6a0; Value = 0x0000000000000000; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd6a0;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd6a8;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6e0; Value = 0x00007fffffffdda0; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6e0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6e8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd680; Value = 0x7dfacdd0c1163cb3; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd680;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd688;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6f0; Value = 0x529a560d8dbaf494; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6f0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6f8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd690; Value = 0x00007fffffffdff8; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd690;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd698;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6d0; Value = 0x04da0c7f4e72f80a; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6d0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6d8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6c0; Value = 0xf632a7392001cbb9; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6c0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6c8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd6a0; PC = 0x555555584444 *)
mov L0x7fffffffd6a0 xmm13_0;
mov L0x7fffffffd6a8 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x55555558449f *)
mov L0x7fffffffd680 xmm10_0;
mov L0x7fffffffd688 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6c0; Value = 0xf632a7392001cbb9; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6c0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6c8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6d0; Value = 0x04da0c7f4e72f80a; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6d0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6d8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584542 *)
mov L0x7fffffffd690 xmm12_0;
mov L0x7fffffffd698 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd690; Value = 0x1185c898e5c3df94; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd690;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd698;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6b0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6b0 xmm26_0;
mov L0x7fffffffd6b8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6b0; Value = 0x756715348bdb5d77; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6b0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6b8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd680; Value = 0x2615d0b7d08b7ee4; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd680;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd688;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd6a0; Value = 0xd0d63e89a6de1b87; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd6a0;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd6a8;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6f0; PC = 0x555555584650 *)
mov L0x7fffffffd6f0 xmm17_0;
mov L0x7fffffffd6f8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6e0; PC = 0x55555558465b *)
mov L0x7fffffffd6e0 xmm23_0;
mov L0x7fffffffd6e8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a390; Value = 0x800000000000808a; PC = 0x555555584020 *)
mov rdx L0x55555559a390;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a398; Value = 0x8000000080008000; PC = 0x555555584023 *)
mov rcx L0x55555559a398;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6d0; PC = 0x5555555840ec *)
mov L0x7fffffffd6d0 xmm0_0;
mov L0x7fffffffd6d8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x555555584121 *)
mov L0x7fffffffd6c0 xmm2_0;
mov L0x7fffffffd6c8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6b0; Value = 0x756715348bdb5d77; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6b0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6b8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd6a0; Value = 0xd0d63e89a6de1b87; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd6a0;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd6a8;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6e0; Value = 0x42c0171ff06f9fff; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6e0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6e8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd680; Value = 0x2615d0b7d08b7ee4; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd680;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd688;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6f0; Value = 0x40f7ca1626adb1ba; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6f0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6f8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd690; Value = 0x1185c898e5c3df94; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd690;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd698;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6d0; Value = 0xc3d022a523850c4d; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6d0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6d8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6c0; Value = 0x1477a68f2c98366f; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6c0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6c8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd6a0; PC = 0x555555584444 *)
mov L0x7fffffffd6a0 xmm13_0;
mov L0x7fffffffd6a8 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x55555558449f *)
mov L0x7fffffffd680 xmm10_0;
mov L0x7fffffffd688 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6c0; Value = 0x1477a68f2c98366f; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6c0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6c8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6d0; Value = 0xc3d022a523850c4d; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6d0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6d8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584542 *)
mov L0x7fffffffd690 xmm12_0;
mov L0x7fffffffd698 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd690; Value = 0x5303f7b146d2c20f; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd690;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd698;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6b0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6b0 xmm26_0;
mov L0x7fffffffd6b8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6b0; Value = 0xe442974965db03fb; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6b0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6b8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd680; Value = 0x32b5d10245eba5ea; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd680;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd688;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd6a0; Value = 0xa444674be7e38c11; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd6a0;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd6a8;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6f0; PC = 0x555555584650 *)
mov L0x7fffffffd6f0 xmm17_0;
mov L0x7fffffffd6f8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6e0; PC = 0x55555558465b *)
mov L0x7fffffffd6e0 xmm23_0;
mov L0x7fffffffd6e8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3a0; Value = 0x000000000000808b; PC = 0x555555584020 *)
mov rdx L0x55555559a3a0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3a8; Value = 0x0000000080000001; PC = 0x555555584023 *)
mov rcx L0x55555559a3a8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6d0; PC = 0x5555555840ec *)
mov L0x7fffffffd6d0 xmm0_0;
mov L0x7fffffffd6d8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x555555584121 *)
mov L0x7fffffffd6c0 xmm2_0;
mov L0x7fffffffd6c8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6b0; Value = 0xe442974965db03fb; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6b0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6b8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd6a0; Value = 0xa444674be7e38c11; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd6a0;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd6a8;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6e0; Value = 0x1020eac2c9f7ef78; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6e0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6e8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd680; Value = 0x32b5d10245eba5ea; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd680;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd688;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6f0; Value = 0x9b7d48acd9f49567; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6f0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6f8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd690; Value = 0x5303f7b146d2c20f; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd690;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd698;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6d0; Value = 0x3f4f87fad6d4076c; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6d0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6d8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6c0; Value = 0x5a3b1863f93d58cd; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6c0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6c8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd6a0; PC = 0x555555584444 *)
mov L0x7fffffffd6a0 xmm13_0;
mov L0x7fffffffd6a8 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x55555558449f *)
mov L0x7fffffffd680 xmm10_0;
mov L0x7fffffffd688 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6c0; Value = 0x5a3b1863f93d58cd; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6c0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6c8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6d0; Value = 0x3f4f87fad6d4076c; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6d0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6d8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584542 *)
mov L0x7fffffffd690 xmm12_0;
mov L0x7fffffffd698 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd690; Value = 0x35684442efd886c1; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd690;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd698;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6b0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6b0 xmm26_0;
mov L0x7fffffffd6b8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6b0; Value = 0xa23f60c4b006d5fd; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6b0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6b8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd680; Value = 0x9d85485437ef15a7; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd680;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd688;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd6a0; Value = 0xd5df0f5e272876ca; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd6a0;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd6a8;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6f0; PC = 0x555555584650 *)
mov L0x7fffffffd6f0 xmm17_0;
mov L0x7fffffffd6f8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6e0; PC = 0x55555558465b *)
mov L0x7fffffffd6e0 xmm23_0;
mov L0x7fffffffd6e8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3b0; Value = 0x8000000080008081; PC = 0x555555584020 *)
mov rdx L0x55555559a3b0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3b8; Value = 0x8000000000008009; PC = 0x555555584023 *)
mov rcx L0x55555559a3b8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6d0; PC = 0x5555555840ec *)
mov L0x7fffffffd6d0 xmm0_0;
mov L0x7fffffffd6d8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x555555584121 *)
mov L0x7fffffffd6c0 xmm2_0;
mov L0x7fffffffd6c8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6b0; Value = 0xa23f60c4b006d5fd; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6b0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6b8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd6a0; Value = 0xd5df0f5e272876ca; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd6a0;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd6a8;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6e0; Value = 0xbb0c383c091739d1; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6e0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6e8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd680; Value = 0x9d85485437ef15a7; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd680;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd688;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6f0; Value = 0x68817a49d4c96927; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6f0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6f8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd690; Value = 0x35684442efd886c1; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd690;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd698;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6d0; Value = 0xc4aa5ff54ec73006; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6d0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6d8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6c0; Value = 0x793b67b9660f80a1; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6c0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6c8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd6a0; PC = 0x555555584444 *)
mov L0x7fffffffd6a0 xmm13_0;
mov L0x7fffffffd6a8 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x55555558449f *)
mov L0x7fffffffd680 xmm10_0;
mov L0x7fffffffd688 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6c0; Value = 0x793b67b9660f80a1; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6c0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6c8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6d0; Value = 0xc4aa5ff54ec73006; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6d0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6d8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584542 *)
mov L0x7fffffffd690 xmm12_0;
mov L0x7fffffffd698 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd690; Value = 0x0ad3eabecfeb7db0; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd690;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd698;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6b0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6b0 xmm26_0;
mov L0x7fffffffd6b8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6b0; Value = 0x7e724d50c299c584; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6b0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6b8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd680; Value = 0xad267893f0aa21ee; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd680;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd688;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd6a0; Value = 0x47aff6a4d54eb991; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd6a0;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd6a8;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6f0; PC = 0x555555584650 *)
mov L0x7fffffffd6f0 xmm17_0;
mov L0x7fffffffd6f8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6e0; PC = 0x55555558465b *)
mov L0x7fffffffd6e0 xmm23_0;
mov L0x7fffffffd6e8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3c0; Value = 0x000000000000008a; PC = 0x555555584020 *)
mov rdx L0x55555559a3c0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3c8; Value = 0x0000000000000088; PC = 0x555555584023 *)
mov rcx L0x55555559a3c8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6d0; PC = 0x5555555840ec *)
mov L0x7fffffffd6d0 xmm0_0;
mov L0x7fffffffd6d8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x555555584121 *)
mov L0x7fffffffd6c0 xmm2_0;
mov L0x7fffffffd6c8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6b0; Value = 0x7e724d50c299c584; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6b0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6b8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd6a0; Value = 0x47aff6a4d54eb991; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd6a0;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd6a8;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6e0; Value = 0x1cb4bd83b59e1bac; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6e0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6e8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd680; Value = 0xad267893f0aa21ee; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd680;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd688;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6f0; Value = 0x741797ab955ab43d; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6f0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6f8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd690; Value = 0x0ad3eabecfeb7db0; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd690;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd698;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6d0; Value = 0x1e2f7f992f9801ae; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6d0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6d8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6c0; Value = 0x7c77605011e2781d; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6c0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6c8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd6a0; PC = 0x555555584444 *)
mov L0x7fffffffd6a0 xmm13_0;
mov L0x7fffffffd6a8 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x55555558449f *)
mov L0x7fffffffd680 xmm10_0;
mov L0x7fffffffd688 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6c0; Value = 0x7c77605011e2781d; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6c0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6c8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6d0; Value = 0x1e2f7f992f9801ae; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6d0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6d8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584542 *)
mov L0x7fffffffd690 xmm12_0;
mov L0x7fffffffd698 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd690; Value = 0xd26acd69d45a2eae; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd690;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd698;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6b0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6b0 xmm26_0;
mov L0x7fffffffd6b8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6b0; Value = 0xb7eb97ae96ae25cc; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6b0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6b8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd680; Value = 0x15048a73bfd206fc; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd680;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd688;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd6a0; Value = 0xab143dadbfb73bb8; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd6a0;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd6a8;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6f0; PC = 0x555555584650 *)
mov L0x7fffffffd6f0 xmm17_0;
mov L0x7fffffffd6f8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6e0; PC = 0x55555558465b *)
mov L0x7fffffffd6e0 xmm23_0;
mov L0x7fffffffd6e8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3d0; Value = 0x0000000080008009; PC = 0x555555584020 *)
mov rdx L0x55555559a3d0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3d8; Value = 0x000000008000000a; PC = 0x555555584023 *)
mov rcx L0x55555559a3d8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6d0; PC = 0x5555555840ec *)
mov L0x7fffffffd6d0 xmm0_0;
mov L0x7fffffffd6d8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x555555584121 *)
mov L0x7fffffffd6c0 xmm2_0;
mov L0x7fffffffd6c8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6b0; Value = 0xb7eb97ae96ae25cc; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6b0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6b8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd6a0; Value = 0xab143dadbfb73bb8; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd6a0;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd6a8;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6e0; Value = 0x036dc5bf566dde1b; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6e0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6e8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd680; Value = 0x15048a73bfd206fc; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd680;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd688;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6f0; Value = 0xb20a1688740dc3e4; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6f0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6f8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd690; Value = 0xd26acd69d45a2eae; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd690;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd698;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6d0; Value = 0x32ba2d5e7da01ac6; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6d0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6d8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6c0; Value = 0x96ac1cf581f7c276; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6c0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6c8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd6a0; PC = 0x555555584444 *)
mov L0x7fffffffd6a0 xmm13_0;
mov L0x7fffffffd6a8 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x55555558449f *)
mov L0x7fffffffd680 xmm10_0;
mov L0x7fffffffd688 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6c0; Value = 0x96ac1cf581f7c276; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6c0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6c8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6d0; Value = 0x32ba2d5e7da01ac6; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6d0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6d8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584542 *)
mov L0x7fffffffd690 xmm12_0;
mov L0x7fffffffd698 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd690; Value = 0x5a305be748e471d9; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd690;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd698;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6b0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6b0 xmm26_0;
mov L0x7fffffffd6b8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6b0; Value = 0x98ae2a7ff15e7141; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6b0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6b8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd680; Value = 0x53de3d1edc588e54; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd680;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd688;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd6a0; Value = 0x7bfe0e07c177138e; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd6a0;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd6a8;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6f0; PC = 0x555555584650 *)
mov L0x7fffffffd6f0 xmm17_0;
mov L0x7fffffffd6f8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6e0; PC = 0x55555558465b *)
mov L0x7fffffffd6e0 xmm23_0;
mov L0x7fffffffd6e8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3e0; Value = 0x000000008000808b; PC = 0x555555584020 *)
mov rdx L0x55555559a3e0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3e8; Value = 0x800000000000008b; PC = 0x555555584023 *)
mov rcx L0x55555559a3e8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6d0; PC = 0x5555555840ec *)
mov L0x7fffffffd6d0 xmm0_0;
mov L0x7fffffffd6d8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x555555584121 *)
mov L0x7fffffffd6c0 xmm2_0;
mov L0x7fffffffd6c8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6b0; Value = 0x98ae2a7ff15e7141; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6b0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6b8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd6a0; Value = 0x7bfe0e07c177138e; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd6a0;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd6a8;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6e0; Value = 0xcfcd0d376ba1dd21; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6e0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6e8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd680; Value = 0x53de3d1edc588e54; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd680;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd688;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6f0; Value = 0x5f9d7d798ae292c6; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6f0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6f8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd690; Value = 0x5a305be748e471d9; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd690;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd698;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6d0; Value = 0x89740cc012015f7b; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6d0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6d8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6c0; Value = 0x283150bc53e61204; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6c0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6c8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd6a0; PC = 0x555555584444 *)
mov L0x7fffffffd6a0 xmm13_0;
mov L0x7fffffffd6a8 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x55555558449f *)
mov L0x7fffffffd680 xmm10_0;
mov L0x7fffffffd688 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6c0; Value = 0x283150bc53e61204; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6c0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6c8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6d0; Value = 0x89740cc012015f7b; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6d0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6d8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584542 *)
mov L0x7fffffffd690 xmm12_0;
mov L0x7fffffffd698 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd690; Value = 0xacf7c330d7acdac9; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd690;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd698;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6b0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6b0 xmm26_0;
mov L0x7fffffffd6b8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6b0; Value = 0xb5cb47b836981fa3; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6b0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6b8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd680; Value = 0xee23acbe0286d97c; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd680;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd688;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd6a0; Value = 0x0ec287b121d8f562; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd6a0;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd6a8;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6f0; PC = 0x555555584650 *)
mov L0x7fffffffd6f0 xmm17_0;
mov L0x7fffffffd6f8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6e0; PC = 0x55555558465b *)
mov L0x7fffffffd6e0 xmm23_0;
mov L0x7fffffffd6e8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a3f0; Value = 0x8000000000008089; PC = 0x555555584020 *)
mov rdx L0x55555559a3f0;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a3f8; Value = 0x8000000000008003; PC = 0x555555584023 *)
mov rcx L0x55555559a3f8;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6d0; PC = 0x5555555840ec *)
mov L0x7fffffffd6d0 xmm0_0;
mov L0x7fffffffd6d8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x555555584121 *)
mov L0x7fffffffd6c0 xmm2_0;
mov L0x7fffffffd6c8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6b0; Value = 0xb5cb47b836981fa3; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6b0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6b8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd6a0; Value = 0x0ec287b121d8f562; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd6a0;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd6a8;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6e0; Value = 0x6ef9a1af8dc7f49c; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6e0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6e8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd680; Value = 0xee23acbe0286d97c; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd680;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd688;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6f0; Value = 0x3b619aa8e63f52bd; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6f0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6f8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd690; Value = 0xacf7c330d7acdac9; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd690;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd698;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6d0; Value = 0x8d19460692e4c388; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6d0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6d8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6c0; Value = 0xe15ffe97aa51e664; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6c0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6c8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd6a0; PC = 0x555555584444 *)
mov L0x7fffffffd6a0 xmm13_0;
mov L0x7fffffffd6a8 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x55555558449f *)
mov L0x7fffffffd680 xmm10_0;
mov L0x7fffffffd688 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6c0; Value = 0xe15ffe97aa51e664; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6c0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6c8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6d0; Value = 0x8d19460692e4c388; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6d0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6d8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584542 *)
mov L0x7fffffffd690 xmm12_0;
mov L0x7fffffffd698 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd690; Value = 0x53ce6a798e4c98de; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd690;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd698;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6b0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6b0 xmm26_0;
mov L0x7fffffffd6b8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6b0; Value = 0xc017e401865fd29d; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6b0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6b8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd680; Value = 0xe9a48f1631cb0936; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd680;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd688;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd6a0; Value = 0xfe76b85f5e5b5fa7; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd6a0;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd6a8;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6f0; PC = 0x555555584650 *)
mov L0x7fffffffd6f0 xmm17_0;
mov L0x7fffffffd6f8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6e0; PC = 0x55555558465b *)
mov L0x7fffffffd6e0 xmm23_0;
mov L0x7fffffffd6e8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a400; Value = 0x8000000000008002; PC = 0x555555584020 *)
mov rdx L0x55555559a400;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a408; Value = 0x8000000000000080; PC = 0x555555584023 *)
mov rcx L0x55555559a408;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6d0; PC = 0x5555555840ec *)
mov L0x7fffffffd6d0 xmm0_0;
mov L0x7fffffffd6d8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x555555584121 *)
mov L0x7fffffffd6c0 xmm2_0;
mov L0x7fffffffd6c8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6b0; Value = 0xc017e401865fd29d; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6b0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6b8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd6a0; Value = 0xfe76b85f5e5b5fa7; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd6a0;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd6a8;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6e0; Value = 0x008dcd22c6773ffb; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6e0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6e8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd680; Value = 0xe9a48f1631cb0936; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd680;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd688;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6f0; Value = 0xbb541cb651508b32; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6f0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6f8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd690; Value = 0x53ce6a798e4c98de; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd690;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd698;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6d0; Value = 0xaa8cd52f8a106eb7; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6d0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6d8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6c0; Value = 0xe6e4f1758f6dc192; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6c0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6c8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd6a0; PC = 0x555555584444 *)
mov L0x7fffffffd6a0 xmm13_0;
mov L0x7fffffffd6a8 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x55555558449f *)
mov L0x7fffffffd680 xmm10_0;
mov L0x7fffffffd688 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6c0; Value = 0xe6e4f1758f6dc192; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6c0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6c8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6d0; Value = 0xaa8cd52f8a106eb7; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6d0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6d8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584542 *)
mov L0x7fffffffd690 xmm12_0;
mov L0x7fffffffd698 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd690; Value = 0x87a211bd65575967; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd690;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd698;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6b0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6b0 xmm26_0;
mov L0x7fffffffd6b8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6b0; Value = 0xcf3f36da84b9f2cc; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6b0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6b8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd680; Value = 0x0d20e03b086053db; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd680;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd688;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd6a0; Value = 0xbbcf582f88d77372; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd6a0;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd6a8;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6f0; PC = 0x555555584650 *)
mov L0x7fffffffd6f0 xmm17_0;
mov L0x7fffffffd6f8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6e0; PC = 0x55555558465b *)
mov L0x7fffffffd6e0 xmm23_0;
mov L0x7fffffffd6e8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a410; Value = 0x000000000000800a; PC = 0x555555584020 *)
mov rdx L0x55555559a410;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a418; Value = 0x800000008000000a; PC = 0x555555584023 *)
mov rcx L0x55555559a418;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6d0; PC = 0x5555555840ec *)
mov L0x7fffffffd6d0 xmm0_0;
mov L0x7fffffffd6d8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x555555584121 *)
mov L0x7fffffffd6c0 xmm2_0;
mov L0x7fffffffd6c8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6b0; Value = 0xcf3f36da84b9f2cc; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6b0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6b8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd6a0; Value = 0xbbcf582f88d77372; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd6a0;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd6a8;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6e0; Value = 0xa69b0c36f4453220; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6e0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6e8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd680; Value = 0x0d20e03b086053db; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd680;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd688;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6f0; Value = 0x9f6a04c0cc12c338; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6f0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6f8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd690; Value = 0x87a211bd65575967; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd690;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd698;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6d0; Value = 0x98270e828a6f0ac3; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6d0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6d8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6c0; Value = 0x7bd180ef354553a0; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6c0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6c8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd6a0; PC = 0x555555584444 *)
mov L0x7fffffffd6a0 xmm13_0;
mov L0x7fffffffd6a8 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x55555558449f *)
mov L0x7fffffffd680 xmm10_0;
mov L0x7fffffffd688 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6c0; Value = 0x7bd180ef354553a0; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6c0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6c8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6d0; Value = 0x98270e828a6f0ac3; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6d0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6d8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584542 *)
mov L0x7fffffffd690 xmm12_0;
mov L0x7fffffffd698 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd690; Value = 0x448c7cdecfa178e0; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd690;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd698;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6b0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6b0 xmm26_0;
mov L0x7fffffffd6b8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6b0; Value = 0xb0c3006620df22cb; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6b0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6b8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd680; Value = 0x6af45ee6d61f1165; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd680;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd688;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd6a0; Value = 0x0addc575a445f534; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd6a0;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd6a8;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6f0; PC = 0x555555584650 *)
mov L0x7fffffffd6f0 xmm17_0;
mov L0x7fffffffd6f8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6e0; PC = 0x55555558465b *)
mov L0x7fffffffd6e0 xmm23_0;
mov L0x7fffffffd6e8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a420; Value = 0x8000000080008081; PC = 0x555555584020 *)
mov rdx L0x55555559a420;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a428; Value = 0x8000000000008080; PC = 0x555555584023 *)
mov rcx L0x55555559a428;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6d0; PC = 0x5555555840ec *)
mov L0x7fffffffd6d0 xmm0_0;
mov L0x7fffffffd6d8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x555555584121 *)
mov L0x7fffffffd6c0 xmm2_0;
mov L0x7fffffffd6c8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6b0; Value = 0xb0c3006620df22cb; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6b0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6b8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd6a0; Value = 0x0addc575a445f534; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd6a0;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd6a8;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6e0; Value = 0x7b06d69ec00372db; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6e0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6e8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd680; Value = 0x6af45ee6d61f1165; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd680;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd688;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6f0; Value = 0xa7673464e44567a0; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6f0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6f8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd690; Value = 0x448c7cdecfa178e0; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd690;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd698;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6d0; Value = 0x1a0b62c60ce9d385; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6d0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6d8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6c0; Value = 0x9de30336015df3b8; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6c0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6c8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd6a0; PC = 0x555555584444 *)
mov L0x7fffffffd6a0 xmm13_0;
mov L0x7fffffffd6a8 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x55555558449f *)
mov L0x7fffffffd680 xmm10_0;
mov L0x7fffffffd688 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6c0; Value = 0x9de30336015df3b8; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6c0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6c8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6d0; Value = 0x1a0b62c60ce9d385; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6d0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6d8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584542 *)
mov L0x7fffffffd690 xmm12_0;
mov L0x7fffffffd698 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd690; Value = 0xd2ca1f82d8c8a755; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd690;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd698;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6b0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6b0 xmm26_0;
mov L0x7fffffffd6b8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6b0; Value = 0xa28273cbf9a82eab; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6b0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6b8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd680; Value = 0x6f0f57655fd9b782; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd680;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd688;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd6a0; Value = 0x3b6802636f849316; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd6a0;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd6a8;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6f0; PC = 0x555555584650 *)
mov L0x7fffffffd6f0 xmm17_0;
mov L0x7fffffffd6f8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6e0; PC = 0x55555558465b *)
mov L0x7fffffffd6e0 xmm23_0;
mov L0x7fffffffd6e8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* mov    (%rax),%rdx                              #! EA = L0x55555559a430; Value = 0x0000000080000001; PC = 0x555555584020 *)
mov rdx L0x55555559a430;
(* mov    0x8(%rax),%rcx                           #! EA = L0x55555559a438; Value = 0x8000000080008008; PC = 0x555555584023 *)
mov rcx L0x55555559a438;
(* add    $0x10,%rax                               #! PC = 0x555555584027 *)
adds carry rax rax 0x10@uint64;
(* vprolq $0x1,%xmm0,%xmm11                        #! PC = 0x55555558402b *)
rol xmm11_0 xmm0_0 0x1;
rol xmm11_1 xmm0_1 0x1;
(* vprolq $0x1,%xmm1,%xmm10                        #! PC = 0x555555584032 *)
rol xmm10_0 xmm1_0 0x1;
rol xmm10_1 xmm1_1 0x1;
(* vprolq $0x1,%xmm20,%xmm6                        #! PC = 0x555555584039 *)
rol xmm6_0 xmm20_0 0x1;
rol xmm6_1 xmm20_1 0x1;
(* vprolq $0x1,%xmm13,%xmm17                       #! PC = 0x555555584040 *)
rol xmm17_0 xmm13_0 0x1;
rol xmm17_1 xmm13_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584047 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558404b *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxor  %xmm13,%xmm10,%xmm10                     #! PC = 0x555555584052 *)
xor xmm10_0@uint64 xmm10_0 xmm13_0;
xor xmm10_1@uint64 xmm10_1 xmm13_1;
(* vpxor  %xmm0,%xmm6,%xmm6                        #! PC = 0x555555584057 *)
xor xmm6_0@uint64 xmm6_0 xmm0_0;
xor xmm6_1@uint64 xmm6_1 xmm0_1;
(* vpxorq %xmm20,%xmm17,%xmm17                     #! PC = 0x55555558405b *)
xor xmm17_0@uint64 xmm17_0 xmm20_0;
xor xmm17_1@uint64 xmm17_1 xmm20_1;
(* vpxor  %xmm10,%xmm9,%xmm9                       #! PC = 0x555555584061 *)
xor xmm9_0@uint64 xmm9_0 xmm10_0;
xor xmm9_1@uint64 xmm9_1 xmm10_1;
(* vpxor  %xmm6,%xmm8,%xmm8                        #! PC = 0x555555584066 *)
xor xmm8_0@uint64 xmm8_0 xmm6_0;
xor xmm8_1@uint64 xmm8_1 xmm6_1;
(* vpxor  %xmm1,%xmm2,%xmm1                        #! PC = 0x55555558406a *)
xor xmm1_0@uint64 xmm2_0 xmm1_0;
xor xmm1_1@uint64 xmm2_1 xmm1_1;
(* vprorq $0x15,%xmm8,%xmm8                        #! PC = 0x55555558406e *)
ror xmm8_0 xmm8_0 0x15;
ror xmm8_1 xmm8_1 0x15;
(* vprorq $0x14,%xmm9,%xmm2                        #! PC = 0x555555584075 *)
ror xmm2_0 xmm9_0 0x14;
ror xmm2_1 xmm9_1 0x14;
(* vpxor  %xmm1,%xmm12,%xmm12                      #! PC = 0x55555558407c *)
xor xmm12_0@uint64 xmm12_0 xmm1_0;
xor xmm12_1@uint64 xmm12_1 xmm1_1;
(* vpxorq %xmm17,%xmm15,%xmm15                     #! PC = 0x555555584080 *)
xor xmm15_0@uint64 xmm15_0 xmm17_0;
xor xmm15_1@uint64 xmm15_1 xmm17_1;
(* vprolq $0x15,%xmm12,%xmm20                      #! PC = 0x555555584086 *)
rol xmm20_0 xmm12_0 0x15;
rol xmm20_1 xmm12_1 0x15;
(* vprolq $0xe,%xmm15,%xmm15                       #! PC = 0x55555558408d *)
rol xmm15_0 xmm15_0 0xe;
rol xmm15_1 xmm15_1 0xe;
(* vpbroadcastq %rdx,%xmm13                        #! PC = 0x555555584094 *)
mov xmm13_0 rdx;
mov xmm13_1 rdx;
(* vpandn %xmm8,%xmm2,%xmm0                        #! PC = 0x55555558409a *)
not xmm2_0n@uint64 xmm2_0;
and xmm0_0@uint64 xmm2_0n xmm8_0;
not xmm2_1n@uint64 xmm2_1;
and xmm0_1@uint64 xmm2_1n xmm8_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x55555558409f *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm10,%xmm19,%xmm19                     #! PC = 0x5555555840a3 *)
xor xmm19_0@uint64 xmm19_0 xmm10_0;
xor xmm19_1@uint64 xmm19_1 xmm10_1;
(* vpxor  %xmm6,%xmm14,%xmm14                      #! PC = 0x5555555840a9 *)
xor xmm14_0@uint64 xmm14_0 xmm6_0;
xor xmm14_1@uint64 xmm14_1 xmm6_1;
(* vpandnq %xmm15,%xmm20,%xmm0                     #! PC = 0x5555555840ad *)
not xmm20_0n@uint64 xmm20_0;
and xmm0_0@uint64 xmm20_0n xmm15_0;
not xmm20_1n@uint64 xmm20_1;
and xmm0_1@uint64 xmm20_1n xmm15_1;
(* vprorq $0x13,%xmm19,%xmm19                      #! PC = 0x5555555840b3 *)
ror xmm19_0 xmm19_0 0x13;
ror xmm19_1 xmm19_1 0x13;
(* vprorq $0x3,%xmm14,%xmm14                       #! PC = 0x5555555840ba *)
ror xmm14_0 xmm14_0 0x3;
ror xmm14_1 xmm14_1 0x3;
(* vpandnq %xmm20,%xmm8,%xmm12                     #! PC = 0x5555555840c1 *)
not xmm8_0n@uint64 xmm8_0;
and xmm12_0@uint64 xmm8_0n xmm20_0;
not xmm8_1n@uint64 xmm8_1;
and xmm12_1@uint64 xmm8_1n xmm20_1;
(* vpxor  %xmm8,%xmm0,%xmm0                        #! PC = 0x5555555840c7 *)
xor xmm0_0@uint64 xmm0_0 xmm8_0;
xor xmm0_1@uint64 xmm0_1 xmm8_1;
(* vpxorq %xmm11,%xmm21,%xmm21                     #! PC = 0x5555555840cc *)
xor xmm21_0@uint64 xmm21_0 xmm11_0;
xor xmm21_1@uint64 xmm21_1 xmm11_1;
(* vpxorq %xmm1,%xmm24,%xmm8                       #! PC = 0x5555555840d2 *)
xor xmm8_0@uint64 xmm24_0 xmm1_0;
xor xmm8_1@uint64 xmm24_1 xmm1_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555840d8 *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vprolq $0x14,%xmm5,%xmm5                        #! PC = 0x5555555840de *)
rol xmm5_0 xmm5_0 0x14;
rol xmm5_1 xmm5_1 0x14;
(* vprolq $0x1c,%xmm8,%xmm8                        #! PC = 0x5555555840e5 *)
rol xmm8_0 xmm8_0 0x1c;
rol xmm8_1 xmm8_1 0x1c;
(* vmovdqa %xmm0,-0x28(%rsp)                       #! EA = L0x7fffffffd6d0; PC = 0x5555555840ec *)
mov L0x7fffffffd6d0 xmm0_0;
mov L0x7fffffffd6d8 xmm0_1;
(* vprolq $0x3,%xmm21,%xmm0                        #! PC = 0x5555555840f2 *)
rol xmm0_0 xmm21_0 0x3;
rol xmm0_1 xmm21_1 0x3;
(* vpxorq %xmm11,%xmm27,%xmm27                     #! PC = 0x5555555840f9 *)
xor xmm27_0@uint64 xmm27_0 xmm11_0;
xor xmm27_1@uint64 xmm27_1 xmm11_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x5555555840ff *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpandnq %xmm27,%xmm15,%xmm9                     #! PC = 0x555555584105 *)
not xmm15_0n@uint64 xmm15_0;
and xmm9_0@uint64 xmm15_0n xmm27_0;
not xmm15_1n@uint64 xmm15_1;
and xmm9_1@uint64 xmm15_1n xmm27_1;
(* vpandnq %xmm2,%xmm27,%xmm27                     #! PC = 0x55555558410b *)
not xmm27_0n@uint64 xmm27_0;
and xmm27_0@uint64 xmm27_0n xmm2_0;
not xmm27_1n@uint64 xmm27_1;
and xmm27_1@uint64 xmm27_1n xmm2_1;
(* vpxor  %xmm2,%xmm12,%xmm12                      #! PC = 0x555555584111 *)
xor xmm12_0@uint64 xmm12_0 xmm2_0;
xor xmm12_1@uint64 xmm12_1 xmm2_1;
(* vpandnq %xmm14,%xmm19,%xmm21                    #! PC = 0x555555584115 *)
not xmm19_0n@uint64 xmm19_0;
and xmm21_0@uint64 xmm19_0n xmm14_0;
not xmm19_1n@uint64 xmm19_1;
and xmm21_1@uint64 xmm19_1n xmm14_1;
(* vpxorq %xmm15,%xmm27,%xmm2                      #! PC = 0x55555558411b *)
xor xmm2_0@uint64 xmm27_0 xmm15_0;
xor xmm2_1@uint64 xmm27_1 xmm15_1;
(* vmovdqa %xmm2,-0x38(%rsp)                       #! EA = L0x7fffffffd6c0; PC = 0x555555584121 *)
mov L0x7fffffffd6c0 xmm2_0;
mov L0x7fffffffd6c8 xmm2_1;
(* vpandnq %xmm19,%xmm0,%xmm24                     #! PC = 0x555555584127 *)
not xmm0_0n@uint64 xmm0_0;
and xmm24_0@uint64 xmm0_0n xmm19_0;
not xmm0_1n@uint64 xmm0_1;
and xmm24_1@uint64 xmm0_1n xmm19_1;
(* vpandn %xmm0,%xmm5,%xmm2                        #! PC = 0x55555558412d *)
not xmm5_0n@uint64 xmm5_0;
and xmm2_0@uint64 xmm5_0n xmm0_0;
not xmm5_1n@uint64 xmm5_1;
and xmm2_1@uint64 xmm5_1n xmm0_1;
(* vpxorq %xmm0,%xmm21,%xmm21                      #! PC = 0x555555584131 *)
xor xmm21_0@uint64 xmm21_0 xmm0_0;
xor xmm21_1@uint64 xmm21_1 xmm0_1;
(* vpxorq %xmm11,%xmm16,%xmm16                     #! PC = 0x555555584137 *)
xor xmm16_0@uint64 xmm16_0 xmm11_0;
xor xmm16_1@uint64 xmm16_1 xmm11_1;
(* vpandn %xmm8,%xmm14,%xmm0                       #! PC = 0x55555558413d *)
not xmm14_0n@uint64 xmm14_0;
and xmm0_0@uint64 xmm14_0n xmm8_0;
not xmm14_1n@uint64 xmm14_1;
and xmm0_1@uint64 xmm14_1n xmm8_1;
(* vpxorq %xmm19,%xmm0,%xmm31                      #! PC = 0x555555584142 *)
xor xmm31_0@uint64 xmm0_0 xmm19_0;
xor xmm31_1@uint64 xmm0_1 xmm19_1;
(* vpxorq %xmm6,%xmm22,%xmm22                      #! PC = 0x555555584148 *)
xor xmm22_0@uint64 xmm22_0 xmm6_0;
xor xmm22_1@uint64 xmm22_1 xmm6_1;
(* vprolq $0x12,%xmm16,%xmm0                       #! PC = 0x55555558414e *)
rol xmm0_0 xmm16_0 0x12;
rol xmm0_1 xmm16_1 0x12;
(* vpxor  %xmm1,%xmm7,%xmm7                        #! PC = 0x555555584155 *)
xor xmm7_0@uint64 xmm7_0 xmm1_0;
xor xmm7_1@uint64 xmm7_1 xmm1_1;
(* vprolq $0x6,%xmm22,%xmm22                       #! PC = 0x555555584159 *)
rol xmm22_0 xmm22_0 0x6;
rol xmm22_1 xmm22_1 0x6;
(* vprolq $0x19,%xmm7,%xmm7                        #! PC = 0x555555584160 *)
rol xmm7_0 xmm7_0 0x19;
rol xmm7_1 xmm7_1 0x19;
(* vpxorq %xmm17,%xmm4,%xmm4                       #! PC = 0x555555584167 *)
xor xmm4_0@uint64 xmm4_0 xmm17_0;
xor xmm4_1@uint64 xmm4_1 xmm17_1;
(* vpshufb 0x162ca(%rip),%xmm4,%xmm4        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x55555558416d *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxorq %xmm8,%xmm2,%xmm30                       #! PC = 0x555555584176 *)
xor xmm30_0@uint64 xmm2_0 xmm8_0;
xor xmm30_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm0,%xmm4,%xmm27                      #! PC = 0x55555558417c *)
not xmm4_0n@uint64 xmm4_0;
and xmm27_0@uint64 xmm4_0n xmm0_0;
not xmm4_1n@uint64 xmm4_1;
and xmm27_1@uint64 xmm4_1n xmm0_1;
(* vpandn %xmm5,%xmm8,%xmm8                        #! PC = 0x555555584182 *)
not xmm8_0n@uint64 xmm8_0;
and xmm8_0@uint64 xmm8_0n xmm5_0;
not xmm8_1n@uint64 xmm8_1;
and xmm8_1@uint64 xmm8_1n xmm5_1;
(* vpxor  %xmm14,%xmm8,%xmm8                       #! PC = 0x555555584186 *)
xor xmm8_0@uint64 xmm8_0 xmm14_0;
xor xmm8_1@uint64 xmm8_1 xmm14_1;
(* vpandnq %xmm7,%xmm22,%xmm23                     #! PC = 0x55555558418b *)
not xmm22_0n@uint64 xmm22_0;
and xmm23_0@uint64 xmm22_0n xmm7_0;
not xmm22_1n@uint64 xmm22_1;
and xmm23_1@uint64 xmm22_1n xmm7_1;
(* vpxor  -0x48(%rsp),%xmm10,%xmm14                #! EA = L0x7fffffffd6b0; Value = 0xa28273cbf9a82eab; PC = 0x555555584191 *)
xor xmm14_0@uint64 xmm10_0 L0x7fffffffd6b0;
xor xmm14_1@uint64 xmm10_1 L0x7fffffffd6b8;
(* vpandnq %xmm4,%xmm7,%xmm19                      #! PC = 0x555555584197 *)
not xmm7_0n@uint64 xmm7_0;
and xmm19_0@uint64 xmm7_0n xmm4_0;
not xmm7_1n@uint64 xmm7_1;
and xmm19_1@uint64 xmm7_1n xmm4_1;
(* vpxorq %xmm7,%xmm27,%xmm27                      #! PC = 0x55555558419d *)
xor xmm27_0@uint64 xmm27_0 xmm7_0;
xor xmm27_1@uint64 xmm27_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x5555555841a3 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxorq -0x58(%rsp),%xmm17,%xmm7                 #! EA = L0x7fffffffd6a0; Value = 0x3b6802636f849316; PC = 0x5555555841a9 *)
xor xmm7_0@uint64 xmm17_0 L0x7fffffffd6a0;
xor xmm7_1@uint64 xmm17_1 L0x7fffffffd6a8;
(* vprolq $0xf,%xmm18,%xmm18                       #! PC = 0x5555555841b4 *)
rol xmm18_0 xmm18_0 0xf;
rol xmm18_1 xmm18_1 0xf;
(* vpxorq -0x18(%rsp),%xmm17,%xmm17                #! EA = L0x7fffffffd6e0; Value = 0xa33aa782264ef4bb; PC = 0x5555555841bb *)
xor xmm17_0@uint64 xmm17_0 L0x7fffffffd6e0;
xor xmm17_1@uint64 xmm17_1 L0x7fffffffd6e8;
(* vpxorq %xmm11,%xmm29,%xmm29                     #! PC = 0x5555555841c6 *)
xor xmm29_0@uint64 xmm29_0 xmm11_0;
xor xmm29_1@uint64 xmm29_1 xmm11_1;
(* vpxorq %xmm10,%xmm28,%xmm28                     #! PC = 0x5555555841cc *)
xor xmm28_0@uint64 xmm28_0 xmm10_0;
xor xmm28_1@uint64 xmm28_1 xmm10_1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x5555555841d2 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  -0x78(%rsp),%xmm1,%xmm1                  #! EA = L0x7fffffffd680; Value = 0x6f0f57655fd9b782; PC = 0x5555555841d6 *)
xor xmm1_0@uint64 xmm1_0 L0x7fffffffd680;
xor xmm1_1@uint64 xmm1_1 L0x7fffffffd688;
(* vprorq $0x1c,%xmm29,%xmm29                      #! PC = 0x5555555841dc *)
ror xmm29_0 xmm29_0 0x1c;
ror xmm29_1 xmm29_1 0x1c;
(* vprolq $0xa,%xmm28,%xmm28                       #! PC = 0x5555555841e3 *)
rol xmm28_0 xmm28_0 0xa;
rol xmm28_1 xmm28_1 0xa;
(* vprolq $0x1,%xmm14,%xmm14                       #! PC = 0x5555555841ea *)
rol xmm14_0 xmm14_0 0x1;
rol xmm14_1 xmm14_1 0x1;
(* vprolq $0x1b,%xmm7,%xmm7                        #! PC = 0x5555555841f1 *)
rol xmm7_0 xmm7_0 0x1b;
rol xmm7_1 xmm7_1 0x1b;
(* vpxor  -0x8(%rsp),%xmm6,%xmm6                   #! EA = L0x7fffffffd6f0; Value = 0x34072f3d6b65b715; PC = 0x5555555841f8 *)
xor xmm6_0@uint64 xmm6_0 L0x7fffffffd6f0;
xor xmm6_1@uint64 xmm6_1 L0x7fffffffd6f8;
(* vprorq $0x9,%xmm1,%xmm1                         #! PC = 0x5555555841fe *)
ror xmm1_0 xmm1_0 0x9;
ror xmm1_1 xmm1_1 0x9;
(* vprorq $0x19,%xmm17,%xmm17                      #! PC = 0x555555584205 *)
ror xmm17_0 xmm17_0 0x19;
ror xmm17_1 xmm17_1 0x19;
(* vprorq $0x2,%xmm6,%xmm6                         #! PC = 0x55555558420c *)
ror xmm6_0 xmm6_0 0x2;
ror xmm6_1 xmm6_1 0x2;
(* vpxor  -0x68(%rsp),%xmm11,%xmm11                #! EA = L0x7fffffffd690; Value = 0xd2ca1f82d8c8a755; PC = 0x555555584213 *)
xor xmm11_0@uint64 xmm11_0 L0x7fffffffd690;
xor xmm11_1@uint64 xmm11_1 L0x7fffffffd698;
(* vpshufb 0x1622e(%rip),%xmm3,%xmm3        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584219 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm3_0, xmm3_1, tmp_0, tmp_1);
mov xmm3_0 tmp_0;
mov xmm3_1 tmp_1;
(* vprorq $0x17,%xmm11,%xmm11                      #! PC = 0x555555584222 *)
ror xmm11_0 xmm11_0 0x17;
ror xmm11_1 xmm11_1 0x17;
(* vpxorq %xmm10,%xmm26,%xmm10                     #! PC = 0x555555584229 *)
xor xmm10_0@uint64 xmm26_0 xmm10_0;
xor xmm10_1@uint64 xmm26_1 xmm10_1;
(* vprolq $0x2,%xmm10,%xmm10                       #! PC = 0x55555558422f *)
rol xmm10_0 xmm10_0 0x2;
rol xmm10_1 xmm10_1 0x2;
(* vpandnq %xmm3,%xmm18,%xmm15                     #! PC = 0x555555584236 *)
not xmm18_0n@uint64 xmm18_0;
and xmm15_0@uint64 xmm18_0n xmm3_0;
not xmm18_1n@uint64 xmm18_1;
and xmm15_1@uint64 xmm18_1n xmm3_1;
(* vpxorq %xmm20,%xmm9,%xmm9                       #! PC = 0x55555558423c *)
xor xmm9_0@uint64 xmm9_0 xmm20_0;
xor xmm9_1@uint64 xmm9_1 xmm20_1;
(* vpxorq %xmm14,%xmm23,%xmm23                     #! PC = 0x555555584242 *)
xor xmm23_0@uint64 xmm23_0 xmm14_0;
xor xmm23_1@uint64 xmm23_1 xmm14_1;
(* vpxorq %xmm22,%xmm19,%xmm19                     #! PC = 0x555555584248 *)
xor xmm19_0@uint64 xmm19_0 xmm22_0;
xor xmm19_1@uint64 xmm19_1 xmm22_1;
(* vpandnq %xmm14,%xmm0,%xmm16                     #! PC = 0x55555558424e *)
not xmm0_0n@uint64 xmm0_0;
and xmm16_0@uint64 xmm0_0n xmm14_0;
not xmm0_1n@uint64 xmm0_1;
and xmm16_1@uint64 xmm0_1n xmm14_1;
(* vpandnq %xmm28,%xmm29,%xmm20                    #! PC = 0x555555584254 *)
not xmm29_0n@uint64 xmm29_0;
and xmm20_0@uint64 xmm29_0n xmm28_0;
not xmm29_1n@uint64 xmm29_1;
and xmm20_1@uint64 xmm29_1n xmm28_1;
(* vpxorq %xmm28,%xmm15,%xmm15                     #! PC = 0x55555558425a *)
xor xmm15_0@uint64 xmm15_0 xmm28_0;
xor xmm15_1@uint64 xmm15_1 xmm28_1;
(* vpandnq %xmm22,%xmm14,%xmm14                    #! PC = 0x555555584260 *)
not xmm14_0n@uint64 xmm14_0;
and xmm14_0@uint64 xmm14_0n xmm22_0;
not xmm14_1n@uint64 xmm14_1;
and xmm14_1@uint64 xmm14_1n xmm22_1;
(* vpandnq %xmm18,%xmm28,%xmm22                    #! PC = 0x555555584266 *)
not xmm28_0n@uint64 xmm28_0;
and xmm22_0@uint64 xmm28_0n xmm18_0;
not xmm28_1n@uint64 xmm28_1;
and xmm22_1@uint64 xmm28_1n xmm18_1;
(* vpandnq %xmm7,%xmm3,%xmm28                      #! PC = 0x55555558426c *)
not xmm3_0n@uint64 xmm3_0;
and xmm28_0@uint64 xmm3_0n xmm7_0;
not xmm3_1n@uint64 xmm3_1;
and xmm28_1@uint64 xmm3_1n xmm7_1;
(* vpxorq %xmm18,%xmm28,%xmm28                     #! PC = 0x555555584272 *)
xor xmm28_0@uint64 xmm28_0 xmm18_0;
xor xmm28_1@uint64 xmm28_1 xmm18_1;
(* vpandnq %xmm17,%xmm1,%xmm18                     #! PC = 0x555555584278 *)
not xmm1_0n@uint64 xmm1_0;
and xmm18_0@uint64 xmm1_0n xmm17_0;
not xmm1_1n@uint64 xmm1_1;
and xmm18_1@uint64 xmm1_1n xmm17_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x55555558427e *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxorq %xmm6,%xmm18,%xmm18                      #! PC = 0x555555584284 *)
xor xmm18_0@uint64 xmm18_0 xmm6_0;
xor xmm18_1@uint64 xmm18_1 xmm6_1;
(* vpxor  %xmm0,%xmm14,%xmm14                      #! PC = 0x55555558428a *)
xor xmm14_0@uint64 xmm14_0 xmm0_0;
xor xmm14_1@uint64 xmm14_1 xmm0_1;
(* vpxorq %xmm5,%xmm24,%xmm24                      #! PC = 0x55555558428e *)
xor xmm24_0@uint64 xmm24_0 xmm5_0;
xor xmm24_1@uint64 xmm24_1 xmm5_1;
(* vpxorq %xmm20,%xmm18,%xmm0                      #! PC = 0x555555584294 *)
xor xmm0_0@uint64 xmm18_0 xmm20_0;
xor xmm0_1@uint64 xmm18_1 xmm20_1;
(* vpxorq %xmm29,%xmm22,%xmm22                     #! PC = 0x55555558429a *)
xor xmm22_0@uint64 xmm22_0 xmm29_0;
xor xmm22_1@uint64 xmm22_1 xmm29_1;
(* vpxorq %xmm30,%xmm23,%xmm2                      #! PC = 0x5555555842a0 *)
xor xmm2_0@uint64 xmm23_0 xmm30_0;
xor xmm2_1@uint64 xmm23_1 xmm30_1;
(* vpandnq %xmm11,%xmm17,%xmm25                    #! PC = 0x5555555842a6 *)
not xmm17_0n@uint64 xmm17_0;
and xmm25_0@uint64 xmm17_0n xmm11_0;
not xmm17_1n@uint64 xmm17_1;
and xmm25_1@uint64 xmm17_1n xmm11_1;
(* vpxorq -0x28(%rsp),%xmm21,%xmm26                #! EA = L0x7fffffffd6d0; Value = 0x0198f98638098df5; PC = 0x5555555842ac *)
xor xmm26_0@uint64 xmm21_0 L0x7fffffffd6d0;
xor xmm26_1@uint64 xmm21_1 L0x7fffffffd6d8;
(* vpandnq %xmm29,%xmm7,%xmm7                      #! PC = 0x5555555842b7 *)
not xmm7_0n@uint64 xmm7_0;
and xmm7_0@uint64 xmm7_0n xmm29_0;
not xmm7_1n@uint64 xmm7_1;
and xmm7_1@uint64 xmm7_1n xmm29_1;
(* vpxor  %xmm0,%xmm2,%xmm2                        #! PC = 0x5555555842bd *)
xor xmm2_0@uint64 xmm2_0 xmm0_0;
xor xmm2_1@uint64 xmm2_1 xmm0_1;
(* vpxorq %xmm12,%xmm24,%xmm29                     #! PC = 0x5555555842c1 *)
xor xmm29_0@uint64 xmm24_0 xmm12_0;
xor xmm29_1@uint64 xmm24_1 xmm12_1;
(* vpxorq %xmm19,%xmm22,%xmm0                      #! PC = 0x5555555842c7 *)
xor xmm0_0@uint64 xmm22_0 xmm19_0;
xor xmm0_1@uint64 xmm22_1 xmm19_1;
(* vpxorq %xmm1,%xmm25,%xmm25                      #! PC = 0x5555555842cd *)
xor xmm25_0@uint64 xmm25_0 xmm1_0;
xor xmm25_1@uint64 xmm25_1 xmm1_1;
(* vpandn %xmm10,%xmm11,%xmm5                      #! PC = 0x5555555842d3 *)
not xmm11_0n@uint64 xmm11_0;
and xmm5_0@uint64 xmm11_0n xmm10_0;
not xmm11_1n@uint64 xmm11_1;
and xmm5_1@uint64 xmm11_1n xmm10_1;
(* vpxorq %xmm0,%xmm29,%xmm29                      #! PC = 0x5555555842d8 *)
xor xmm29_0@uint64 xmm29_0 xmm0_0;
xor xmm29_1@uint64 xmm29_1 xmm0_1;
(* vpxorq %xmm27,%xmm15,%xmm0                      #! PC = 0x5555555842de *)
xor xmm0_0@uint64 xmm15_0 xmm27_0;
xor xmm0_1@uint64 xmm15_1 xmm27_1;
(* vpxorq %xmm25,%xmm29,%xmm29                     #! PC = 0x5555555842e4 *)
xor xmm29_0@uint64 xmm29_0 xmm25_0;
xor xmm29_1@uint64 xmm29_1 xmm25_1;
(* vpxorq %xmm17,%xmm5,%xmm5                       #! PC = 0x5555555842ea *)
xor xmm5_0@uint64 xmm5_0 xmm17_0;
xor xmm5_1@uint64 xmm5_1 xmm17_1;
(* vpxorq %xmm0,%xmm26,%xmm26                      #! PC = 0x5555555842f0 *)
xor xmm26_0@uint64 xmm26_0 xmm0_0;
xor xmm26_1@uint64 xmm26_1 xmm0_1;
(* vpxorq %xmm5,%xmm26,%xmm26                      #! PC = 0x5555555842f6 *)
xor xmm26_0@uint64 xmm26_0 xmm5_0;
xor xmm26_1@uint64 xmm26_1 xmm5_1;
(* vpxorq %xmm4,%xmm16,%xmm16                      #! PC = 0x5555555842fc *)
xor xmm16_0@uint64 xmm16_0 xmm4_0;
xor xmm16_1@uint64 xmm16_1 xmm4_1;
(* vpxor  %xmm3,%xmm7,%xmm7                        #! PC = 0x555555584302 *)
xor xmm7_0@uint64 xmm7_0 xmm3_0;
xor xmm7_1@uint64 xmm7_1 xmm3_1;
(* vpandn %xmm6,%xmm10,%xmm4                       #! PC = 0x555555584306 *)
not xmm10_0n@uint64 xmm10_0;
and xmm4_0@uint64 xmm10_0n xmm6_0;
not xmm10_1n@uint64 xmm10_1;
and xmm4_1@uint64 xmm10_1n xmm6_1;
(* vprolq $0x1,%xmm29,%xmm3                        #! PC = 0x55555558430a *)
rol xmm3_0 xmm29_0 0x1;
rol xmm3_1 xmm29_1 0x1;
(* vpxor  %xmm11,%xmm4,%xmm4                       #! PC = 0x555555584311 *)
xor xmm4_0@uint64 xmm4_0 xmm11_0;
xor xmm4_1@uint64 xmm4_1 xmm11_1;
(* vpandn %xmm1,%xmm6,%xmm6                        #! PC = 0x555555584316 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm1_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm1_1;
(* vprolq $0x1,%xmm26,%xmm11                       #! PC = 0x55555558431a *)
rol xmm11_0 xmm26_0 0x1;
rol xmm11_1 xmm26_1 0x1;
(* vpxorq %xmm28,%xmm4,%xmm0                       #! PC = 0x555555584321 *)
xor xmm0_0@uint64 xmm4_0 xmm28_0;
xor xmm0_1@uint64 xmm4_1 xmm28_1;
(* vpxor  %xmm10,%xmm6,%xmm6                       #! PC = 0x555555584327 *)
xor xmm6_0@uint64 xmm6_0 xmm10_0;
xor xmm6_1@uint64 xmm6_1 xmm10_1;
(* vpxor  -0x38(%rsp),%xmm8,%xmm1                  #! EA = L0x7fffffffd6c0; Value = 0xe8483c2c8a146583; PC = 0x55555558432c *)
xor xmm1_0@uint64 xmm8_0 L0x7fffffffd6c0;
xor xmm1_1@uint64 xmm8_1 L0x7fffffffd6c8;
(* vpxorq %xmm9,%xmm31,%xmm17                      #! PC = 0x555555584332 *)
xor xmm17_0@uint64 xmm31_0 xmm9_0;
xor xmm17_1@uint64 xmm31_1 xmm9_1;
(* vpxorq %xmm0,%xmm17,%xmm17                      #! PC = 0x555555584338 *)
xor xmm17_0@uint64 xmm17_0 xmm0_0;
xor xmm17_1@uint64 xmm17_1 xmm0_1;
(* vpxor  %xmm14,%xmm6,%xmm0                       #! PC = 0x55555558433e *)
xor xmm0_0@uint64 xmm6_0 xmm14_0;
xor xmm0_1@uint64 xmm6_1 xmm14_1;
(* vpxorq %xmm16,%xmm17,%xmm17                     #! PC = 0x555555584343 *)
xor xmm17_0@uint64 xmm17_0 xmm16_0;
xor xmm17_1@uint64 xmm17_1 xmm16_1;
(* vpxor  %xmm0,%xmm1,%xmm1                        #! PC = 0x555555584349 *)
xor xmm1_0@uint64 xmm1_0 xmm0_0;
xor xmm1_1@uint64 xmm1_1 xmm0_1;
(* vpxor  %xmm7,%xmm1,%xmm1                        #! PC = 0x55555558434d *)
xor xmm1_0@uint64 xmm1_0 xmm7_0;
xor xmm1_1@uint64 xmm1_1 xmm7_1;
(* vprolq $0x1,%xmm17,%xmm0                        #! PC = 0x555555584351 *)
rol xmm0_0 xmm17_0 0x1;
rol xmm0_1 xmm17_1 0x1;
(* vpxor  %xmm1,%xmm3,%xmm3                        #! PC = 0x555555584358 *)
xor xmm3_0@uint64 xmm3_0 xmm1_0;
xor xmm3_1@uint64 xmm3_1 xmm1_1;
(* vpxor  %xmm13,%xmm2,%xmm2                       #! PC = 0x55555558435c *)
xor xmm2_0@uint64 xmm2_0 xmm13_0;
xor xmm2_1@uint64 xmm2_1 xmm13_1;
(* vprolq $0x1,%xmm1,%xmm1                         #! PC = 0x555555584361 *)
rol xmm1_0 xmm1_0 0x1;
rol xmm1_1 xmm1_1 0x1;
(* vpxor  %xmm2,%xmm11,%xmm11                      #! PC = 0x555555584368 *)
xor xmm11_0@uint64 xmm11_0 xmm2_0;
xor xmm11_1@uint64 xmm11_1 xmm2_1;
(* vprolq $0x1,%xmm2,%xmm2                         #! PC = 0x55555558436c *)
rol xmm2_0 xmm2_0 0x1;
rol xmm2_1 xmm2_1 0x1;
(* vpxorq %xmm29,%xmm0,%xmm0                       #! PC = 0x555555584373 *)
xor xmm0_0@uint64 xmm0_0 xmm29_0;
xor xmm0_1@uint64 xmm0_1 xmm29_1;
(* vpxorq %xmm24,%xmm11,%xmm24                     #! PC = 0x555555584379 *)
xor xmm24_0@uint64 xmm11_0 xmm24_0;
xor xmm24_1@uint64 xmm11_1 xmm24_1;
(* vpxorq %xmm27,%xmm0,%xmm27                      #! PC = 0x55555558437f *)
xor xmm27_0@uint64 xmm0_0 xmm27_0;
xor xmm27_1@uint64 xmm0_1 xmm27_1;
(* vpxorq %xmm26,%xmm1,%xmm1                       #! PC = 0x555555584385 *)
xor xmm1_0@uint64 xmm1_0 xmm26_0;
xor xmm1_1@uint64 xmm1_1 xmm26_1;
(* vprorq $0x14,%xmm24,%xmm10                      #! PC = 0x55555558438b *)
ror xmm10_0 xmm24_0 0x14;
ror xmm10_1 xmm24_1 0x14;
(* vpxorq %xmm17,%xmm2,%xmm2                       #! PC = 0x555555584392 *)
xor xmm2_0@uint64 xmm2_0 xmm17_0;
xor xmm2_1@uint64 xmm2_1 xmm17_1;
(* vpxorq %xmm28,%xmm1,%xmm28                      #! PC = 0x555555584398 *)
xor xmm28_0@uint64 xmm1_0 xmm28_0;
xor xmm28_1@uint64 xmm1_1 xmm28_1;
(* vprorq $0x15,%xmm27,%xmm17                      #! PC = 0x55555558439e *)
ror xmm17_0 xmm27_0 0x15;
ror xmm17_1 xmm27_1 0x15;
(* vprolq $0x15,%xmm28,%xmm28                      #! PC = 0x5555555843a5 *)
rol xmm28_0 xmm28_0 0x15;
rol xmm28_1 xmm28_1 0x15;
(* vpxor  %xmm6,%xmm2,%xmm6                        #! PC = 0x5555555843ac *)
xor xmm6_0@uint64 xmm2_0 xmm6_0;
xor xmm6_1@uint64 xmm2_1 xmm6_1;
(* vprolq $0xe,%xmm6,%xmm6                         #! PC = 0x5555555843b0 *)
rol xmm6_0 xmm6_0 0xe;
rol xmm6_1 xmm6_1 0xe;
(* vpbroadcastq %rcx,%xmm27                        #! PC = 0x5555555843b7 *)
mov xmm27_0 rcx;
mov xmm27_1 rcx;
(* vpxor  %xmm13,%xmm3,%xmm13                      #! PC = 0x5555555843bd *)
xor xmm13_0@uint64 xmm3_0 xmm13_0;
xor xmm13_1@uint64 xmm3_1 xmm13_1;
(* vpandnq %xmm17,%xmm10,%xmm24                    #! PC = 0x5555555843c2 *)
not xmm10_0n@uint64 xmm10_0;
and xmm24_0@uint64 xmm10_0n xmm17_0;
not xmm10_1n@uint64 xmm10_1;
and xmm24_1@uint64 xmm10_1n xmm17_1;
(* vpxorq %xmm24,%xmm27,%xmm27                     #! PC = 0x5555555843c8 *)
xor xmm27_0@uint64 xmm27_0 xmm24_0;
xor xmm27_1@uint64 xmm27_1 xmm24_1;
(* vpandnq %xmm28,%xmm17,%xmm24                    #! PC = 0x5555555843ce *)
not xmm17_0n@uint64 xmm17_0;
and xmm24_0@uint64 xmm17_0n xmm28_0;
not xmm17_1n@uint64 xmm17_1;
and xmm24_1@uint64 xmm17_1n xmm28_1;
(* vpxorq %xmm10,%xmm24,%xmm26                     #! PC = 0x5555555843d4 *)
xor xmm26_0@uint64 xmm24_0 xmm10_0;
xor xmm26_1@uint64 xmm24_1 xmm10_1;
(* vpxor  %xmm8,%xmm2,%xmm8                        #! PC = 0x5555555843da *)
xor xmm8_0@uint64 xmm2_0 xmm8_0;
xor xmm8_1@uint64 xmm2_1 xmm8_1;
(* vpandnq %xmm6,%xmm28,%xmm24                     #! PC = 0x5555555843df *)
not xmm28_0n@uint64 xmm28_0;
and xmm24_0@uint64 xmm28_0n xmm6_0;
not xmm28_1n@uint64 xmm28_1;
and xmm24_1@uint64 xmm28_1n xmm6_1;
(* vpxorq %xmm23,%xmm3,%xmm23                      #! PC = 0x5555555843e5 *)
xor xmm23_0@uint64 xmm3_0 xmm23_0;
xor xmm23_1@uint64 xmm3_1 xmm23_1;
(* vprolq $0x14,%xmm8,%xmm8                        #! PC = 0x5555555843eb *)
rol xmm8_0 xmm8_0 0x14;
rol xmm8_1 xmm8_1 0x14;
(* vprolq $0x3,%xmm23,%xmm23                       #! PC = 0x5555555843f2 *)
rol xmm23_0 xmm23_0 0x3;
rol xmm23_1 xmm23_1 0x3;
(* vpxorq %xmm17,%xmm24,%xmm17                     #! PC = 0x5555555843f9 *)
xor xmm17_0@uint64 xmm24_0 xmm17_0;
xor xmm17_1@uint64 xmm24_1 xmm17_1;
(* vpxor  %xmm9,%xmm1,%xmm9                        #! PC = 0x5555555843ff *)
xor xmm9_0@uint64 xmm1_0 xmm9_0;
xor xmm9_1@uint64 xmm1_1 xmm9_1;
(* vpandnq %xmm13,%xmm6,%xmm24                     #! PC = 0x555555584404 *)
not xmm6_0n@uint64 xmm6_0;
and xmm24_0@uint64 xmm6_0n xmm13_0;
not xmm6_1n@uint64 xmm6_1;
and xmm24_1@uint64 xmm6_1n xmm13_1;
(* vpxorq %xmm13,%xmm27,%xmm27                     #! PC = 0x55555558440a *)
xor xmm27_0@uint64 xmm27_0 xmm13_0;
xor xmm27_1@uint64 xmm27_1 xmm13_1;
(* vpxor  %xmm5,%xmm0,%xmm5                        #! PC = 0x555555584410 *)
xor xmm5_0@uint64 xmm0_0 xmm5_0;
xor xmm5_1@uint64 xmm0_1 xmm5_1;
(* vpandn %xmm10,%xmm13,%xmm13                     #! PC = 0x555555584414 *)
not xmm13_0n@uint64 xmm13_0;
and xmm13_0@uint64 xmm13_0n xmm10_0;
not xmm13_1n@uint64 xmm13_1;
and xmm13_1@uint64 xmm13_1n xmm10_1;
(* vprorq $0x3,%xmm5,%xmm5                         #! PC = 0x555555584419 *)
ror xmm5_0 xmm5_0 0x3;
ror xmm5_1 xmm5_1 0x3;
(* vpxor  %xmm6,%xmm13,%xmm13                      #! PC = 0x555555584420 *)
xor xmm13_0@uint64 xmm13_0 xmm6_0;
xor xmm13_1@uint64 xmm13_1 xmm6_1;
(* vpxorq %xmm22,%xmm11,%xmm22                     #! PC = 0x555555584424 *)
xor xmm22_0@uint64 xmm11_0 xmm22_0;
xor xmm22_1@uint64 xmm11_1 xmm22_1;
(* vprolq $0x1c,%xmm9,%xmm6                        #! PC = 0x55555558442a *)
rol xmm6_0 xmm9_0 0x1c;
rol xmm6_1 xmm9_1 0x1c;
(* vprorq $0x13,%xmm22,%xmm10                      #! PC = 0x555555584431 *)
ror xmm10_0 xmm22_0 0x13;
ror xmm10_1 xmm22_1 0x13;
(* vpandnq %xmm23,%xmm8,%xmm29                     #! PC = 0x555555584438 *)
not xmm8_0n@uint64 xmm8_0;
and xmm29_0@uint64 xmm8_0n xmm23_0;
not xmm8_1n@uint64 xmm8_1;
and xmm29_1@uint64 xmm8_1n xmm23_1;
(* vpxorq %xmm21,%xmm0,%xmm21                      #! PC = 0x55555558443e *)
xor xmm21_0@uint64 xmm0_0 xmm21_0;
xor xmm21_1@uint64 xmm0_1 xmm21_1;
(* vmovdqa %xmm13,-0x58(%rsp)                      #! EA = L0x7fffffffd6a0; PC = 0x555555584444 *)
mov L0x7fffffffd6a0 xmm13_0;
mov L0x7fffffffd6a8 xmm13_1;
(* vpxorq %xmm16,%xmm1,%xmm16                      #! PC = 0x55555558444a *)
xor xmm16_0@uint64 xmm1_0 xmm16_0;
xor xmm16_1@uint64 xmm1_1 xmm16_1;
(* vpandn %xmm6,%xmm5,%xmm13                       #! PC = 0x555555584450 *)
not xmm5_0n@uint64 xmm5_0;
and xmm13_0@uint64 xmm5_0n xmm6_0;
not xmm5_1n@uint64 xmm5_1;
and xmm13_1@uint64 xmm5_1n xmm6_1;
(* vpxorq %xmm6,%xmm29,%xmm29                      #! PC = 0x555555584454 *)
xor xmm29_0@uint64 xmm29_0 xmm6_0;
xor xmm29_1@uint64 xmm29_1 xmm6_1;
(* vpandn %xmm8,%xmm6,%xmm6                        #! PC = 0x55555558445a *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm8_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm8_1;
(* vprolq $0x19,%xmm16,%xmm16                      #! PC = 0x55555558445f *)
rol xmm16_0 xmm16_0 0x19;
rol xmm16_1 xmm16_1 0x19;
(* vpandnq %xmm5,%xmm10,%xmm22                     #! PC = 0x555555584466 *)
not xmm10_0n@uint64 xmm10_0;
and xmm22_0@uint64 xmm10_0n xmm5_0;
not xmm10_1n@uint64 xmm10_1;
and xmm22_1@uint64 xmm10_1n xmm5_1;
(* vpxor  %xmm12,%xmm11,%xmm12                     #! PC = 0x55555558446c *)
xor xmm12_0@uint64 xmm11_0 xmm12_0;
xor xmm12_1@uint64 xmm11_1 xmm12_1;
(* vpxor  %xmm5,%xmm6,%xmm5                        #! PC = 0x555555584471 *)
xor xmm5_0@uint64 xmm6_0 xmm5_0;
xor xmm5_1@uint64 xmm6_1 xmm5_1;
(* vpxorq %xmm18,%xmm3,%xmm18                      #! PC = 0x555555584475 *)
xor xmm18_0@uint64 xmm3_0 xmm18_0;
xor xmm18_1@uint64 xmm3_1 xmm18_1;
(* vprolq $0x6,%xmm21,%xmm6                        #! PC = 0x55555558447b *)
rol xmm6_0 xmm21_0 0x6;
rol xmm6_1 xmm21_1 0x6;
(* vprolq $0x12,%xmm18,%xmm18                      #! PC = 0x555555584482 *)
rol xmm18_0 xmm18_0 0x12;
rol xmm18_1 xmm18_1 0x12;
(* vprolq $0x1,%xmm12,%xmm12                       #! PC = 0x555555584489 *)
rol xmm12_0 xmm12_0 0x1;
rol xmm12_1 xmm12_1 0x1;
(* vpandnq %xmm10,%xmm23,%xmm9                     #! PC = 0x555555584490 *)
not xmm23_0n@uint64 xmm23_0;
and xmm9_0@uint64 xmm23_0n xmm10_0;
not xmm23_1n@uint64 xmm23_1;
and xmm9_1@uint64 xmm23_1n xmm10_1;
(* vpxor  %xmm7,%xmm2,%xmm7                        #! PC = 0x555555584496 *)
xor xmm7_0@uint64 xmm2_0 xmm7_0;
xor xmm7_1@uint64 xmm2_1 xmm7_1;
(* vpxor  %xmm10,%xmm13,%xmm10                     #! PC = 0x55555558449a *)
xor xmm10_0@uint64 xmm13_0 xmm10_0;
xor xmm10_1@uint64 xmm13_1 xmm10_1;
(* vmovdqa %xmm10,-0x78(%rsp)                      #! EA = L0x7fffffffd680; PC = 0x55555558449f *)
mov L0x7fffffffd680 xmm10_0;
mov L0x7fffffffd688 xmm10_1;
(* vpshufb 0x15f92(%rip),%xmm7,%xmm10        # 0x55555559a440#! EA = L0x55555559a440; Value = 0x0605040302010007; PC = 0x5555555844a5 *)
inline vpshufb128(L0x55555559a440, L0x55555559a448, xmm7_0, xmm7_1, tmp_0, tmp_1);
mov xmm10_0 tmp_0;
mov xmm10_1 tmp_1;
(* vpandnq %xmm16,%xmm6,%xmm21                     #! PC = 0x5555555844ae *)
not xmm6_0n@uint64 xmm6_0;
and xmm21_0@uint64 xmm6_0n xmm16_0;
not xmm6_1n@uint64 xmm6_1;
and xmm21_1@uint64 xmm6_1n xmm16_1;
(* vpxorq %xmm28,%xmm24,%xmm24                     #! PC = 0x5555555844b4 *)
xor xmm24_0@uint64 xmm24_0 xmm28_0;
xor xmm24_1@uint64 xmm24_1 xmm28_1;
(* vpandnq %xmm10,%xmm16,%xmm28                    #! PC = 0x5555555844ba *)
not xmm16_0n@uint64 xmm16_0;
and xmm28_0@uint64 xmm16_0n xmm10_0;
not xmm16_1n@uint64 xmm16_1;
and xmm28_1@uint64 xmm16_1n xmm10_1;
(* vpandnq %xmm12,%xmm18,%xmm7                     #! PC = 0x5555555844c0 *)
not xmm18_0n@uint64 xmm18_0;
and xmm7_0@uint64 xmm18_0n xmm12_0;
not xmm18_1n@uint64 xmm18_1;
and xmm7_1@uint64 xmm18_1n xmm12_1;
(* vpxorq %xmm19,%xmm11,%xmm19                     #! PC = 0x5555555844c6 *)
xor xmm19_0@uint64 xmm11_0 xmm19_0;
xor xmm19_1@uint64 xmm11_1 xmm19_1;
(* vpxorq %xmm12,%xmm21,%xmm21                     #! PC = 0x5555555844cc *)
xor xmm21_0@uint64 xmm21_0 xmm12_0;
xor xmm21_1@uint64 xmm21_1 xmm12_1;
(* vpxorq %xmm6,%xmm28,%xmm28                      #! PC = 0x5555555844d2 *)
xor xmm28_0@uint64 xmm28_0 xmm6_0;
xor xmm28_1@uint64 xmm28_1 xmm6_1;
(* vpandn %xmm6,%xmm12,%xmm12                      #! PC = 0x5555555844d8 *)
not xmm12_0n@uint64 xmm12_0;
and xmm12_0@uint64 xmm12_0n xmm6_0;
not xmm12_1n@uint64 xmm12_1;
and xmm12_1@uint64 xmm12_1n xmm6_1;
(* vpxorq %xmm30,%xmm3,%xmm30                      #! PC = 0x5555555844dc *)
xor xmm30_0@uint64 xmm3_0 xmm30_0;
xor xmm30_1@uint64 xmm3_1 xmm30_1;
(* vpxor  -0x38(%rsp),%xmm2,%xmm6                  #! EA = L0x7fffffffd6c0; Value = 0xe8483c2c8a146583; PC = 0x5555555844e2 *)
xor xmm6_0@uint64 xmm2_0 L0x7fffffffd6c0;
xor xmm6_1@uint64 xmm2_1 L0x7fffffffd6c8;
(* vprorq $0x1c,%xmm30,%xmm30                      #! PC = 0x5555555844e8 *)
ror xmm30_0 xmm30_0 0x1c;
ror xmm30_1 xmm30_1 0x1c;
(* vpxor  %xmm8,%xmm9,%xmm9                        #! PC = 0x5555555844ef *)
xor xmm9_0@uint64 xmm9_0 xmm8_0;
xor xmm9_1@uint64 xmm9_1 xmm8_1;
(* vpxor  %xmm10,%xmm7,%xmm7                       #! PC = 0x5555555844f4 *)
xor xmm7_0@uint64 xmm7_0 xmm10_0;
xor xmm7_1@uint64 xmm7_1 xmm10_1;
(* vpandnq %xmm18,%xmm10,%xmm8                     #! PC = 0x5555555844f9 *)
not xmm10_0n@uint64 xmm10_0;
and xmm8_0@uint64 xmm10_0n xmm18_0;
not xmm10_1n@uint64 xmm10_1;
and xmm8_1@uint64 xmm10_1n xmm18_1;
(* vprolq $0xa,%xmm19,%xmm10                       #! PC = 0x5555555844ff *)
rol xmm10_0 xmm19_0 0xa;
rol xmm10_1 xmm19_1 0xa;
(* vprolq $0x1b,%xmm6,%xmm6                        #! PC = 0x555555584506 *)
rol xmm6_0 xmm6_0 0x1b;
rol xmm6_1 xmm6_1 0x1b;
(* vpxor  %xmm15,%xmm0,%xmm15                      #! PC = 0x55555558450d *)
xor xmm15_0@uint64 xmm0_0 xmm15_0;
xor xmm15_1@uint64 xmm0_1 xmm15_1;
(* vprolq $0xf,%xmm15,%xmm15                       #! PC = 0x555555584512 *)
rol xmm15_0 xmm15_0 0xf;
rol xmm15_1 xmm15_1 0xf;
(* vpxorq %xmm23,%xmm22,%xmm22                     #! PC = 0x555555584519 *)
xor xmm22_0@uint64 xmm22_0 xmm23_0;
xor xmm22_1@uint64 xmm22_1 xmm23_1;
(* vpxor  %xmm4,%xmm1,%xmm4                        #! PC = 0x55555558451f *)
xor xmm4_0@uint64 xmm1_0 xmm4_0;
xor xmm4_1@uint64 xmm1_1 xmm4_1;
(* vpxorq %xmm18,%xmm12,%xmm23                     #! PC = 0x555555584523 *)
xor xmm23_0@uint64 xmm12_0 xmm18_0;
xor xmm23_1@uint64 xmm12_1 xmm18_1;
(* vpandnq %xmm10,%xmm30,%xmm12                    #! PC = 0x555555584529 *)
not xmm30_0n@uint64 xmm30_0;
and xmm12_0@uint64 xmm30_0n xmm10_0;
not xmm30_1n@uint64 xmm30_1;
and xmm12_1@uint64 xmm30_1n xmm10_1;
(* vpxor  %xmm6,%xmm12,%xmm12                      #! PC = 0x55555558452f *)
xor xmm12_0@uint64 xmm12_0 xmm6_0;
xor xmm12_1@uint64 xmm12_1 xmm6_1;
(* vpshufb 0x15f14(%rip),%xmm4,%xmm4        # 0x55555559a450#! EA = L0x55555559a450; Value = 0x0007060504030201; PC = 0x555555584533 *)
inline vpshufb128(L0x55555559a450, L0x55555559a458, xmm4_0, xmm4_1, tmp_0, tmp_1);
mov xmm4_0 tmp_0;
mov xmm4_1 tmp_1;
(* vpxor  -0x28(%rsp),%xmm0,%xmm0                  #! EA = L0x7fffffffd6d0; Value = 0x0198f98638098df5; PC = 0x55555558453c *)
xor xmm0_0@uint64 xmm0_0 L0x7fffffffd6d0;
xor xmm0_1@uint64 xmm0_1 L0x7fffffffd6d8;
(* vmovdqa %xmm12,-0x68(%rsp)                      #! EA = L0x7fffffffd690; PC = 0x555555584542 *)
mov L0x7fffffffd690 xmm12_0;
mov L0x7fffffffd698 xmm12_1;
(* vpxorq %xmm31,%xmm1,%xmm1                       #! PC = 0x555555584548 *)
xor xmm1_0@uint64 xmm1_0 xmm31_0;
xor xmm1_1@uint64 xmm1_1 xmm31_1;
(* vpandn %xmm6,%xmm4,%xmm12                       #! PC = 0x55555558454e *)
not xmm4_0n@uint64 xmm4_0;
and xmm12_0@uint64 xmm4_0n xmm6_0;
not xmm4_1n@uint64 xmm4_1;
and xmm12_1@uint64 xmm4_1n xmm6_1;
(* vpxor  %xmm14,%xmm2,%xmm2                       #! PC = 0x555555584552 *)
xor xmm2_0@uint64 xmm2_0 xmm14_0;
xor xmm2_1@uint64 xmm2_1 xmm14_1;
(* vprorq $0x19,%xmm2,%xmm2                        #! PC = 0x555555584557 *)
ror xmm2_0 xmm2_0 0x19;
ror xmm2_1 xmm2_1 0x19;
(* vpandnq %xmm15,%xmm10,%xmm19                    #! PC = 0x55555558455e *)
not xmm10_0n@uint64 xmm10_0;
and xmm19_0@uint64 xmm10_0n xmm15_0;
not xmm10_1n@uint64 xmm10_1;
and xmm19_1@uint64 xmm10_1n xmm15_1;
(* vpandnq %xmm4,%xmm15,%xmm18                     #! PC = 0x555555584564 *)
not xmm15_0n@uint64 xmm15_0;
and xmm18_0@uint64 xmm15_0n xmm4_0;
not xmm15_1n@uint64 xmm15_1;
and xmm18_1@uint64 xmm15_1n xmm4_1;
(* vpxor  %xmm15,%xmm12,%xmm12                     #! PC = 0x55555558456a *)
xor xmm12_0@uint64 xmm12_0 xmm15_0;
xor xmm12_1@uint64 xmm12_1 xmm15_1;
(* vpandnq %xmm30,%xmm6,%xmm6                      #! PC = 0x55555558456f *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm30_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm30_1;
(* vprorq $0x9,%xmm1,%xmm15                        #! PC = 0x555555584575 *)
ror xmm15_0 xmm1_0 0x9;
ror xmm15_1 xmm1_1 0x9;
(* vpxor  %xmm4,%xmm6,%xmm4                        #! PC = 0x55555558457c *)
xor xmm4_0@uint64 xmm6_0 xmm4_0;
xor xmm4_1@uint64 xmm6_1 xmm4_1;
(* vprorq $0x2,%xmm0,%xmm6                         #! PC = 0x555555584580 *)
ror xmm6_0 xmm0_0 0x2;
ror xmm6_1 xmm0_1 0x2;
(* vpxorq %xmm20,%xmm3,%xmm3                       #! PC = 0x555555584587 *)
xor xmm3_0@uint64 xmm3_0 xmm20_0;
xor xmm3_1@uint64 xmm3_1 xmm20_1;
(* vpxorq %xmm25,%xmm11,%xmm11                     #! PC = 0x55555558458d *)
xor xmm11_0@uint64 xmm11_0 xmm25_0;
xor xmm11_1@uint64 xmm11_1 xmm25_1;
(* vprolq $0x2,%xmm11,%xmm11                       #! PC = 0x555555584593 *)
rol xmm11_0 xmm11_0 0x2;
rol xmm11_1 xmm11_1 0x2;
(* vpxorq %xmm10,%xmm18,%xmm18                     #! PC = 0x55555558459a *)
xor xmm18_0@uint64 xmm18_0 xmm10_0;
xor xmm18_1@uint64 xmm18_1 xmm10_1;
(* vprorq $0x17,%xmm3,%xmm10                       #! PC = 0x5555555845a0 *)
ror xmm10_0 xmm3_0 0x17;
ror xmm10_1 xmm3_1 0x17;
(* vpxorq %xmm16,%xmm8,%xmm8                       #! PC = 0x5555555845a7 *)
xor xmm8_0@uint64 xmm8_0 xmm16_0;
xor xmm8_1@uint64 xmm8_1 xmm16_1;
(* vpandnq %xmm2,%xmm15,%xmm16                     #! PC = 0x5555555845ad *)
not xmm15_0n@uint64 xmm15_0;
and xmm16_0@uint64 xmm15_0n xmm2_0;
not xmm15_1n@uint64 xmm15_1;
and xmm16_1@uint64 xmm15_1n xmm2_1;
(* vpxorq %xmm6,%xmm16,%xmm16                      #! PC = 0x5555555845b3 *)
xor xmm16_0@uint64 xmm16_0 xmm6_0;
xor xmm16_1@uint64 xmm16_1 xmm6_1;
(* vpxorq -0x68(%rsp),%xmm16,%xmm0                 #! EA = L0x7fffffffd690; Value = 0x99457d3767cbc1e2; PC = 0x5555555845b9 *)
xor xmm0_0@uint64 xmm16_0 L0x7fffffffd690;
xor xmm0_1@uint64 xmm16_1 L0x7fffffffd698;
(* vmovdqa64 %xmm26,-0x48(%rsp)                    #! EA = L0x7fffffffd6b0; PC = 0x5555555845c4 *)
mov L0x7fffffffd6b0 xmm26_0;
mov L0x7fffffffd6b8 xmm26_1;
(* vpxorq %xmm29,%xmm21,%xmm13                     #! PC = 0x5555555845cf *)
xor xmm13_0@uint64 xmm21_0 xmm29_0;
xor xmm13_1@uint64 xmm21_1 xmm29_1;
(* vpxor  %xmm0,%xmm13,%xmm13                      #! PC = 0x5555555845d5 *)
xor xmm13_0@uint64 xmm13_0 xmm0_0;
xor xmm13_1@uint64 xmm13_1 xmm0_1;
(* vpxorq %xmm30,%xmm19,%xmm19                     #! PC = 0x5555555845d9 *)
xor xmm19_0@uint64 xmm19_0 xmm30_0;
xor xmm19_1@uint64 xmm19_1 xmm30_1;
(* vpxor  -0x48(%rsp),%xmm9,%xmm0                  #! EA = L0x7fffffffd6b0; Value = 0xcbae03109d51739e; PC = 0x5555555845df *)
xor xmm0_0@uint64 xmm9_0 L0x7fffffffd6b0;
xor xmm0_1@uint64 xmm9_1 L0x7fffffffd6b8;
(* vpxorq %xmm19,%xmm28,%xmm1                      #! PC = 0x5555555845e5 *)
xor xmm1_0@uint64 xmm28_0 xmm19_0;
xor xmm1_1@uint64 xmm28_1 xmm19_1;
(* vpandn %xmm6,%xmm11,%xmm3                       #! PC = 0x5555555845eb *)
not xmm11_0n@uint64 xmm11_0;
and xmm3_0@uint64 xmm11_0n xmm6_0;
not xmm11_1n@uint64 xmm11_1;
and xmm3_1@uint64 xmm11_1n xmm6_1;
(* vpandn %xmm11,%xmm10,%xmm14                     #! PC = 0x5555555845ef *)
not xmm10_0n@uint64 xmm10_0;
and xmm14_0@uint64 xmm10_0n xmm11_0;
not xmm10_1n@uint64 xmm10_1;
and xmm14_1@uint64 xmm10_1n xmm11_1;
(* vpandnq %xmm10,%xmm2,%xmm26                     #! PC = 0x5555555845f4 *)
not xmm2_0n@uint64 xmm2_0;
and xmm26_0@uint64 xmm2_0n xmm10_0;
not xmm2_1n@uint64 xmm2_1;
and xmm26_1@uint64 xmm2_1n xmm10_1;
(* vpxor  %xmm1,%xmm0,%xmm0                        #! PC = 0x5555555845fa *)
xor xmm0_0@uint64 xmm0_0 xmm1_0;
xor xmm0_1@uint64 xmm0_1 xmm1_1;
(* vpxor  %xmm2,%xmm14,%xmm14                      #! PC = 0x5555555845fe *)
xor xmm14_0@uint64 xmm14_0 xmm2_0;
xor xmm14_1@uint64 xmm14_1 xmm2_1;
(* vpxorq -0x78(%rsp),%xmm24,%xmm20                #! EA = L0x7fffffffd680; Value = 0x96501608577028da; PC = 0x555555584602 *)
xor xmm20_0@uint64 xmm24_0 L0x7fffffffd680;
xor xmm20_1@uint64 xmm24_1 L0x7fffffffd688;
(* vpxorq %xmm18,%xmm8,%xmm2                       #! PC = 0x55555558460d *)
xor xmm2_0@uint64 xmm8_0 xmm18_0;
xor xmm2_1@uint64 xmm8_1 xmm18_1;
(* vpxorq %xmm17,%xmm22,%xmm1                      #! PC = 0x555555584613 *)
xor xmm1_0@uint64 xmm22_0 xmm17_0;
xor xmm1_1@uint64 xmm22_1 xmm17_1;
(* vpxor  %xmm10,%xmm3,%xmm3                       #! PC = 0x555555584619 *)
xor xmm3_0@uint64 xmm3_0 xmm10_0;
xor xmm3_1@uint64 xmm3_1 xmm10_1;
(* vpxor  %xmm2,%xmm1,%xmm1                        #! PC = 0x55555558461e *)
xor xmm1_0@uint64 xmm1_0 xmm2_0;
xor xmm1_1@uint64 xmm1_1 xmm2_1;
(* vpandn %xmm15,%xmm6,%xmm6                       #! PC = 0x555555584622 *)
not xmm6_0n@uint64 xmm6_0;
and xmm6_0@uint64 xmm6_0n xmm15_0;
not xmm6_1n@uint64 xmm6_1;
and xmm6_1@uint64 xmm6_1n xmm15_1;
(* vpxor  %xmm3,%xmm12,%xmm2                       #! PC = 0x555555584627 *)
xor xmm2_0@uint64 xmm12_0 xmm3_0;
xor xmm2_1@uint64 xmm12_1 xmm3_1;
(* vpxorq %xmm15,%xmm26,%xmm26                     #! PC = 0x55555558462b *)
xor xmm26_0@uint64 xmm26_0 xmm15_0;
xor xmm26_1@uint64 xmm26_1 xmm15_1;
(* vpxorq %xmm2,%xmm20,%xmm20                      #! PC = 0x555555584631 *)
xor xmm20_0@uint64 xmm20_0 xmm2_0;
xor xmm20_1@uint64 xmm20_1 xmm2_1;
(* vpxor  %xmm11,%xmm6,%xmm15                      #! PC = 0x555555584637 *)
xor xmm15_0@uint64 xmm6_0 xmm11_0;
xor xmm15_1@uint64 xmm6_1 xmm11_1;
(* vpxor  -0x58(%rsp),%xmm5,%xmm2                  #! EA = L0x7fffffffd6a0; Value = 0x239fdc1fb2538760; PC = 0x55555558463c *)
xor xmm2_0@uint64 xmm5_0 L0x7fffffffd6a0;
xor xmm2_1@uint64 xmm5_1 L0x7fffffffd6a8;
(* vmovdqa64 %xmm23,%xmm6                          #! PC = 0x555555584642 *)
mov xmm6_0 xmm23_0;
mov xmm6_1 xmm23_1;
(* vpxor  %xmm6,%xmm15,%xmm6                       #! PC = 0x555555584648 *)
xor xmm6_0@uint64 xmm15_0 xmm6_0;
xor xmm6_1@uint64 xmm15_1 xmm6_1;
(* vpxor  %xmm6,%xmm2,%xmm2                        #! PC = 0x55555558464c *)
xor xmm2_0@uint64 xmm2_0 xmm6_0;
xor xmm2_1@uint64 xmm2_1 xmm6_1;
(* vmovdqa64 %xmm17,-0x8(%rsp)                     #! EA = L0x7fffffffd6f0; PC = 0x555555584650 *)
mov L0x7fffffffd6f0 xmm17_0;
mov L0x7fffffffd6f8 xmm17_1;
(* vmovdqa64 %xmm23,-0x18(%rsp)                    #! EA = L0x7fffffffd6e0; PC = 0x55555558465b *)
mov L0x7fffffffd6e0 xmm23_0;
mov L0x7fffffffd6e8 xmm23_1;
(* vpxorq %xmm27,%xmm13,%xmm13                     #! PC = 0x555555584666 *)
xor xmm13_0@uint64 xmm13_0 xmm27_0;
xor xmm13_1@uint64 xmm13_1 xmm27_1;
(* vpxorq %xmm26,%xmm0,%xmm0                       #! PC = 0x55555558466c *)
xor xmm0_0@uint64 xmm0_0 xmm26_0;
xor xmm0_1@uint64 xmm0_1 xmm26_1;
(* vpxor  %xmm14,%xmm1,%xmm1                       #! PC = 0x555555584672 *)
xor xmm1_0@uint64 xmm1_0 xmm14_0;
xor xmm1_1@uint64 xmm1_1 xmm14_1;
(* vpxorq %xmm7,%xmm20,%xmm20                      #! PC = 0x555555584677 *)
xor xmm20_0@uint64 xmm20_0 xmm7_0;
xor xmm20_1@uint64 xmm20_1 xmm7_1;
(* vpxor  %xmm4,%xmm2,%xmm2                        #! PC = 0x55555558467d *)
xor xmm2_0@uint64 xmm2_0 xmm4_0;
xor xmm2_1@uint64 xmm2_1 xmm4_1;
(* #jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! PC = 0x555555584684 *)
#jne    0x555555584020 <KeccakP1600times2_PermuteAll_24rounds+384>#! 0x555555584684 = 0x555555584684;
(* vmovdqa -0x48(%rsp),%xmm1                       #! EA = L0x7fffffffd6b0; Value = 0xcbae03109d51739e; PC = 0x55555558468a *)
mov xmm1_0 L0x7fffffffd6b0;
mov xmm1_1 L0x7fffffffd6b8;
(* vmovdqa %xmm7,0xd0(%rdi)                        #! EA = L0x7fffffffdc90; PC = 0x555555584690 *)
mov L0x7fffffffdc90 xmm7_0;
mov L0x7fffffffdc98 xmm7_1;
(* vmovdqa %xmm1,0x10(%rdi)                        #! EA = L0x7fffffffdbd0; PC = 0x555555584698 *)
mov L0x7fffffffdbd0 xmm1_0;
mov L0x7fffffffdbd8 xmm1_1;
(* vmovdqa -0x8(%rsp),%xmm1                        #! EA = L0x7fffffffd6f0; Value = 0x6fc97456f8d3a246; PC = 0x55555558469d *)
mov xmm1_0 L0x7fffffffd6f0;
mov xmm1_1 L0x7fffffffd6f8;
(* vmovdqa -0x18(%rsp),%xmm7                       #! EA = L0x7fffffffd6e0; Value = 0xcd2c753c63f9a2db; PC = 0x5555555846a3 *)
mov xmm7_0 L0x7fffffffd6e0;
mov xmm7_1 L0x7fffffffd6e8;
(* vmovdqa %xmm1,0x20(%rdi)                        #! EA = L0x7fffffffdbe0; PC = 0x5555555846a9 *)
mov L0x7fffffffdbe0 xmm1_0;
mov L0x7fffffffdbe8 xmm1_1;
(* vmovdqa -0x58(%rsp),%xmm1                       #! EA = L0x7fffffffd6a0; Value = 0x239fdc1fb2538760; PC = 0x5555555846ae *)
mov xmm1_0 L0x7fffffffd6a0;
mov xmm1_1 L0x7fffffffd6a8;
(* vmovdqa %xmm7,0xe0(%rdi)                        #! EA = L0x7fffffffdca0; PC = 0x5555555846b4 *)
mov L0x7fffffffdca0 xmm7_0;
mov L0x7fffffffdca8 xmm7_1;
(* vmovdqa %xmm1,0x40(%rdi)                        #! EA = L0x7fffffffdc00; PC = 0x5555555846bc *)
mov L0x7fffffffdc00 xmm1_0;
mov L0x7fffffffdc08 xmm1_1;
(* vmovdqa -0x68(%rsp),%xmm7                       #! EA = L0x7fffffffd690; Value = 0x99457d3767cbc1e2; PC = 0x5555555846c1 *)
mov xmm7_0 L0x7fffffffd690;
mov xmm7_1 L0x7fffffffd698;
(* vmovdqa -0x78(%rsp),%xmm1                       #! EA = L0x7fffffffd680; Value = 0x96501608577028da; PC = 0x5555555846c7 *)
mov xmm1_0 L0x7fffffffd680;
mov xmm1_1 L0x7fffffffd688;
(* vmovdqa64 %xmm27,(%rdi)                         #! EA = L0x7fffffffdbc0; PC = 0x5555555846cd *)
mov L0x7fffffffdbc0 xmm27_0;
mov L0x7fffffffdbc8 xmm27_1;
(* vmovdqa64 %xmm24,0x30(%rdi)                     #! EA = L0x7fffffffdbf0; PC = 0x5555555846d3 *)
mov L0x7fffffffdbf0 xmm24_0;
mov L0x7fffffffdbf8 xmm24_1;
(* vmovdqa64 %xmm29,0x50(%rdi)                     #! EA = L0x7fffffffdc10; PC = 0x5555555846da *)
mov L0x7fffffffdc10 xmm29_0;
mov L0x7fffffffdc18 xmm29_1;
(* vmovdqa %xmm9,0x60(%rdi)                        #! EA = L0x7fffffffdc20; PC = 0x5555555846e1 *)
mov L0x7fffffffdc20 xmm9_0;
mov L0x7fffffffdc28 xmm9_1;
(* vmovdqa64 %xmm22,0x70(%rdi)                     #! EA = L0x7fffffffdc30; PC = 0x5555555846e6 *)
mov L0x7fffffffdc30 xmm22_0;
mov L0x7fffffffdc38 xmm22_1;
(* vmovdqa %xmm1,0x80(%rdi)                        #! EA = L0x7fffffffdc40; PC = 0x5555555846ed *)
mov L0x7fffffffdc40 xmm1_0;
mov L0x7fffffffdc48 xmm1_1;
(* vmovdqa %xmm5,0x90(%rdi)                        #! EA = L0x7fffffffdc50; PC = 0x5555555846f5 *)
mov L0x7fffffffdc50 xmm5_0;
mov L0x7fffffffdc58 xmm5_1;
(* vmovdqa64 %xmm21,0xa0(%rdi)                     #! EA = L0x7fffffffdc60; PC = 0x5555555846fd *)
mov L0x7fffffffdc60 xmm21_0;
mov L0x7fffffffdc68 xmm21_1;
(* vmovdqa64 %xmm28,0xb0(%rdi)                     #! EA = L0x7fffffffdc70; PC = 0x555555584704 *)
mov L0x7fffffffdc70 xmm28_0;
mov L0x7fffffffdc78 xmm28_1;
(* vmovdqa %xmm8,0xc0(%rdi)                        #! EA = L0x7fffffffdc80; PC = 0x55555558470b *)
mov L0x7fffffffdc80 xmm8_0;
mov L0x7fffffffdc88 xmm8_1;
(* vmovdqa %xmm7,0xf0(%rdi)                        #! EA = L0x7fffffffdcb0; PC = 0x555555584713 *)
mov L0x7fffffffdcb0 xmm7_0;
mov L0x7fffffffdcb8 xmm7_1;
(* vmovdqa64 %xmm19,0x100(%rdi)                    #! EA = L0x7fffffffdcc0; PC = 0x55555558471b *)
mov L0x7fffffffdcc0 xmm19_0;
mov L0x7fffffffdcc8 xmm19_1;
(* vmovdqa64 %xmm18,0x110(%rdi)                    #! EA = L0x7fffffffdcd0; PC = 0x555555584722 *)
mov L0x7fffffffdcd0 xmm18_0;
mov L0x7fffffffdcd8 xmm18_1;
(* vmovdqa %xmm12,0x120(%rdi)                      #! EA = L0x7fffffffdce0; PC = 0x555555584729 *)
mov L0x7fffffffdce0 xmm12_0;
mov L0x7fffffffdce8 xmm12_1;
(* vmovdqa %xmm4,0x130(%rdi)                       #! EA = L0x7fffffffdcf0; PC = 0x555555584731 *)
mov L0x7fffffffdcf0 xmm4_0;
mov L0x7fffffffdcf8 xmm4_1;
(* vmovdqa64 %xmm16,0x140(%rdi)                    #! EA = L0x7fffffffdd00; PC = 0x555555584739 *)
mov L0x7fffffffdd00 xmm16_0;
mov L0x7fffffffdd08 xmm16_1;
(* vmovdqa64 %xmm26,0x150(%rdi)                    #! EA = L0x7fffffffdd10; PC = 0x555555584740 *)
mov L0x7fffffffdd10 xmm26_0;
mov L0x7fffffffdd18 xmm26_1;
(* vmovdqa %xmm14,0x160(%rdi)                      #! EA = L0x7fffffffdd20; PC = 0x555555584747 *)
mov L0x7fffffffdd20 xmm14_0;
mov L0x7fffffffdd28 xmm14_1;
(* vmovdqa %xmm3,0x170(%rdi)                       #! EA = L0x7fffffffdd30; PC = 0x55555558474f *)
mov L0x7fffffffdd30 xmm3_0;
mov L0x7fffffffdd38 xmm3_1;
(* vmovdqa %xmm15,0x180(%rdi)                      #! EA = L0x7fffffffdd40; PC = 0x555555584757 *)
mov L0x7fffffffdd40 xmm15_0;
mov L0x7fffffffdd48 xmm15_1;
(* add    $0x10,%rsp                               #! PC = 0x55555558475f *)
adds carry rsp rsp 0x10@uint64;
(* #! <- SP = 0x7fffffffd708 *)
#! 0x7fffffffd708 = 0x7fffffffd708;
(* #ret                                            #! PC = 0x555555584763 *)
#ret                                            #! 0x555555584763 = 0x555555584763;

(* ===== Outputs ===== *)

mov a00 L0x7fffffffd710;
mov b00 L0x7fffffffd718;
mov a01 L0x7fffffffd720;
mov b01 L0x7fffffffd728;
mov a02 L0x7fffffffd730;
mov b02 L0x7fffffffd738;
mov a03 L0x7fffffffd740;
mov b03 L0x7fffffffd748;
mov a04 L0x7fffffffd750;
mov b04 L0x7fffffffd758;
mov a05 L0x7fffffffd760;
mov b05 L0x7fffffffd768;
mov a06 L0x7fffffffd770;
mov b06 L0x7fffffffd778;
mov a07 L0x7fffffffd780;
mov b07 L0x7fffffffd788;
mov a08 L0x7fffffffd790;
mov b08 L0x7fffffffd798;
mov a09 L0x7fffffffd7a0;
mov b09 L0x7fffffffd7a8;
mov a10 L0x7fffffffd7b0;
mov b10 L0x7fffffffd7b8;
mov a11 L0x7fffffffd7c0;
mov b11 L0x7fffffffd7c8;
mov a12 L0x7fffffffd7d0;
mov b12 L0x7fffffffd7d8;
mov a13 L0x7fffffffd7e0;
mov b13 L0x7fffffffd7e8;
mov a14 L0x7fffffffd7f0;
mov b14 L0x7fffffffd7f8;
mov a15 L0x7fffffffd800;
mov b15 L0x7fffffffd808;
mov a16 L0x7fffffffd810;
mov b16 L0x7fffffffd818;
mov a17 L0x7fffffffd820;
mov b17 L0x7fffffffd828;
mov a18 L0x7fffffffd830;
mov b18 L0x7fffffffd838;
mov a19 L0x7fffffffd840;
mov b19 L0x7fffffffd848;
mov a20 L0x7fffffffd850;
mov b20 L0x7fffffffd858;
mov a21 L0x7fffffffd860;
mov b21 L0x7fffffffd868;
mov a22 L0x7fffffffd870;
mov b22 L0x7fffffffd878;
mov a23 L0x7fffffffd880;
mov b23 L0x7fffffffd888;
mov a24 L0x7fffffffd890;
mov b24 L0x7fffffffd898;

mov c00 L0x7fffffffd8a0;
mov d00 L0x7fffffffd8a8;
mov c01 L0x7fffffffd8b0;
mov d01 L0x7fffffffd8b8;
mov c02 L0x7fffffffd8c0;
mov d02 L0x7fffffffd8c8;
mov c03 L0x7fffffffd8d0;
mov d03 L0x7fffffffd8d8;
mov c04 L0x7fffffffd8e0;
mov d04 L0x7fffffffd8e8;
mov c05 L0x7fffffffd8f0;
mov d05 L0x7fffffffd8f8;
mov c06 L0x7fffffffd900;
mov d06 L0x7fffffffd908;
mov c07 L0x7fffffffd910;
mov d07 L0x7fffffffd918;
mov c08 L0x7fffffffd920;
mov d08 L0x7fffffffd928;
mov c09 L0x7fffffffd930;
mov d09 L0x7fffffffd938;
mov c10 L0x7fffffffd940;
mov d10 L0x7fffffffd948;
mov c11 L0x7fffffffd950;
mov d11 L0x7fffffffd958;
mov c12 L0x7fffffffd960;
mov d12 L0x7fffffffd968;
mov c13 L0x7fffffffd970;
mov d13 L0x7fffffffd978;
mov c14 L0x7fffffffd980;
mov d14 L0x7fffffffd988;
mov c15 L0x7fffffffd990;
mov d15 L0x7fffffffd998;
mov c16 L0x7fffffffd9a0;
mov d16 L0x7fffffffd9a8;
mov c17 L0x7fffffffd9b0;
mov d17 L0x7fffffffd9b8;
mov c18 L0x7fffffffd9c0;
mov d18 L0x7fffffffd9c8;
mov c19 L0x7fffffffd9d0;
mov d19 L0x7fffffffd9d8;
mov c20 L0x7fffffffd9e0;
mov d20 L0x7fffffffd9e8;
mov c21 L0x7fffffffd9f0;
mov d21 L0x7fffffffd9f8;
mov c22 L0x7fffffffda00;
mov d22 L0x7fffffffda08;
mov c23 L0x7fffffffda10;
mov d23 L0x7fffffffda18;
mov c24 L0x7fffffffda20;
mov d24 L0x7fffffffda28;

mov e00 L0x7fffffffda30;
mov f00 L0x7fffffffda38;
mov e01 L0x7fffffffda40;
mov f01 L0x7fffffffda48;
mov e02 L0x7fffffffda50;
mov f02 L0x7fffffffda58;
mov e03 L0x7fffffffda60;
mov f03 L0x7fffffffda68;
mov e04 L0x7fffffffda70;
mov f04 L0x7fffffffda78;
mov e05 L0x7fffffffda80;
mov f05 L0x7fffffffda88;
mov e06 L0x7fffffffda90;
mov f06 L0x7fffffffda98;
mov e07 L0x7fffffffdaa0;
mov f07 L0x7fffffffdaa8;
mov e08 L0x7fffffffdab0;
mov f08 L0x7fffffffdab8;
mov e09 L0x7fffffffdac0;
mov f09 L0x7fffffffdac8;
mov e10 L0x7fffffffdad0;
mov f10 L0x7fffffffdad8;
mov e11 L0x7fffffffdae0;
mov f11 L0x7fffffffdae8;
mov e12 L0x7fffffffdaf0;
mov f12 L0x7fffffffdaf8;
mov e13 L0x7fffffffdb00;
mov f13 L0x7fffffffdb08;
mov e14 L0x7fffffffdb10;
mov f14 L0x7fffffffdb18;
mov e15 L0x7fffffffdb20;
mov f15 L0x7fffffffdb28;
mov e16 L0x7fffffffdb30;
mov f16 L0x7fffffffdb38;
mov e17 L0x7fffffffdb40;
mov f17 L0x7fffffffdb48;
mov e18 L0x7fffffffdb50;
mov f18 L0x7fffffffdb58;
mov e19 L0x7fffffffdb60;
mov f19 L0x7fffffffdb68;
mov e20 L0x7fffffffdb70;
mov f20 L0x7fffffffdb78;
mov e21 L0x7fffffffdb80;
mov f21 L0x7fffffffdb88;
mov e22 L0x7fffffffdb90;
mov f22 L0x7fffffffdb98;
mov e23 L0x7fffffffdba0;
mov f23 L0x7fffffffdba8;
mov e24 L0x7fffffffdbb0;
mov f24 L0x7fffffffdbb8;

mov g00 L0x7fffffffdbc0;
mov h00 L0x7fffffffdbc8;
mov g01 L0x7fffffffdbd0;
mov h01 L0x7fffffffdbd8;
mov g02 L0x7fffffffdbe0;
mov h02 L0x7fffffffdbe8;
mov g03 L0x7fffffffdbf0;
mov h03 L0x7fffffffdbf8;
mov g04 L0x7fffffffdc00;
mov h04 L0x7fffffffdc08;
mov g05 L0x7fffffffdc10;
mov h05 L0x7fffffffdc18;
mov g06 L0x7fffffffdc20;
mov h06 L0x7fffffffdc28;
mov g07 L0x7fffffffdc30;
mov h07 L0x7fffffffdc38;
mov g08 L0x7fffffffdc40;
mov h08 L0x7fffffffdc48;
mov g09 L0x7fffffffdc50;
mov h09 L0x7fffffffdc58;
mov g10 L0x7fffffffdc60;
mov h10 L0x7fffffffdc68;
mov g11 L0x7fffffffdc70;
mov h11 L0x7fffffffdc78;
mov g12 L0x7fffffffdc80;
mov h12 L0x7fffffffdc88;
mov g13 L0x7fffffffdc90;
mov h13 L0x7fffffffdc98;
mov g14 L0x7fffffffdca0;
mov h14 L0x7fffffffdca8;
mov g15 L0x7fffffffdcb0;
mov h15 L0x7fffffffdcb8;
mov g16 L0x7fffffffdcc0;
mov h16 L0x7fffffffdcc8;
mov g17 L0x7fffffffdcd0;
mov h17 L0x7fffffffdcd8;
mov g18 L0x7fffffffdce0;
mov h18 L0x7fffffffdce8;
mov g19 L0x7fffffffdcf0;
mov h19 L0x7fffffffdcf8;
mov g20 L0x7fffffffdd00;
mov h20 L0x7fffffffdd08;
mov g21 L0x7fffffffdd10;
mov h21 L0x7fffffffdd18;
mov g22 L0x7fffffffdd20;
mov h22 L0x7fffffffdd28;
mov g23 L0x7fffffffdd30;
mov h23 L0x7fffffffdd38;
mov g24 L0x7fffffffdd40;
mov h24 L0x7fffffffdd48;

{
  true
  &&
  true
}

