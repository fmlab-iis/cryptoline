(* popper: cv_cec.exe -v -ov a00,a01,a02,a03,a04,a05,a06,a07,a08,a09,a10,a11,a12,a13,a14,a15,a16,a17,a18,a19,a20,a21,a22,a23,a24#b00,b01,b02,b03,b04,b05,b06,b07,b08,b09,b10,b11,b12,b13,b14,b15,b16,b17,b18,b19,b20,b21,b22,b23,b24#c00,c01,c02,c03,c04,c05,c06,c07,c08,c09,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20,c21,c22,c23,c24#d00,d01,d02,d03,d04,d05,d06,d07,d08,d09,d10,d11,d12,d13,d14,d15,d16,d17,d18,d19,d20,d21,c22,d23 ../KeccakP-1600-reference_KeccakP1600times4_Permute_24rounds.cl KeccakP1600times4_PermuteAll_24rounds.cl
Parsing CryptoLine file:                [OK]            0.024968 seconds
Checking well-formedness:               [OK]            0.027401 seconds
Parsing CryptoLine file:                [OK]            0.116929 seconds
Checking well-formedness:               [OK]            0.041611 seconds
Equivalence of output group #0:         [OK]            93.094295 seconds
Equivalence of output group #1:         [OK]            93.529900 seconds
Equivalence of output group #2:         [OK]            93.141929 seconds
Equivalence of output group #3:         [OK]            199.457738 seconds
Final result:                           [OK]            205.456031 seconds
*)
(*
on popper, equivalence against XKCP Keccak-p[1600,24]x4 AVX512, 4 threads:
Parsing CryptoLine file:		[OK]		2.895210 seconds
Checking well-formedness:		[OK]		3.359791 seconds
Parsing CryptoLine file:		[OK]		0.110778 seconds
Checking well-formedness:		[OK]		0.041320 seconds
Equivalence of output group #2:	[OK]		786.860711 seconds
Equivalence of output group #1:		[OK]		804.964053 seconds
Equivalence of output group #0:		[OK]		825.556764 seconds
Equivalence of output group #3:		[OK]		797.175656 seconds
Final result:				[OK]		836.518487 seconds

on popper, equivalence against XKCP Keccak-p[1600,24]x4 SSSE3, 4 threads:
Parsing CryptoLine file:		[OK]		0.121863 seconds
Checking well-formedness:		[OK]		0.040133 seconds
Parsing CryptoLine file:		[OK]		0.082785 seconds
Checking well-formedness:		[OK]		0.035312 seconds
Equivalence of output group #2:		[OK]		1.501116 seconds
Equivalence of output group #1:		[OK]		2.940568 seconds
Equivalence of output group #0:		[OK]		4.405514 seconds
Equivalence of output group #3:		[OK]		0.531462 seconds
Final result:				[OK]		6.355069 seconds
*)

const KeccakF1600RoundConstants_00 = 0x0000000000000001
const KeccakF1600RoundConstants_01 = 0x0000000000008082
const KeccakF1600RoundConstants_02 = 0x800000000000808a
const KeccakF1600RoundConstants_03 = 0x8000000080008000
const KeccakF1600RoundConstants_04 = 0x000000000000808b
const KeccakF1600RoundConstants_05 = 0x0000000080000001
const KeccakF1600RoundConstants_06 = 0x8000000080008081
const KeccakF1600RoundConstants_07 = 0x8000000000008009
const KeccakF1600RoundConstants_08 = 0x000000000000008a
const KeccakF1600RoundConstants_09 = 0x0000000000000088
const KeccakF1600RoundConstants_10 = 0x0000000080008009
const KeccakF1600RoundConstants_11 = 0x000000008000000a
const KeccakF1600RoundConstants_12 = 0x000000008000808b
const KeccakF1600RoundConstants_13 = 0x800000000000008b
const KeccakF1600RoundConstants_14 = 0x8000000000008089
const KeccakF1600RoundConstants_15 = 0x8000000000008003
const KeccakF1600RoundConstants_16 = 0x8000000000008002
const KeccakF1600RoundConstants_17 = 0x8000000000000080
const KeccakF1600RoundConstants_18 = 0x000000000000800a
const KeccakF1600RoundConstants_19 = 0x800000008000000a
const KeccakF1600RoundConstants_20 = 0x8000000080008081
const KeccakF1600RoundConstants_21 = 0x8000000000008080
const KeccakF1600RoundConstants_22 = 0x0000000080000001
const KeccakF1600RoundConstants_23 = 0x8000000080008008

const rho8_0 = 0x0605040302010007
const rho8_1 = 0x0E0D0C0B0A09080F
const rho8_2 = 0x1615141312111017
const rho8_3 = 0x1E1D1C1B1A19181F
const rho56_0 = 0x0007060504030201
const rho56_1 = 0x080F0E0D0C0B0A09
const rho56_2 = 0x1017161514131211
const rho56_3 = 0x181F1E1D1C1B1A19

proc stb64(uint64 x; uint8 x_0, uint8 x_1, uint8 x_2, uint8 x_3, uint8 x_4, uint8 x_5, uint8 x_6, uint8 x_7) =
{ true && true }
spl tmp x_0 x 8;
spl tmp x_1 tmp 8;
spl tmp x_2 tmp 8;
spl tmp x_3 tmp 8;
spl tmp x_4 tmp 8;
spl tmp x_5 tmp 8;
spl x_7 x_6 tmp 8;
{ true && true }

proc sel128(uint8 idx, uint128 src; uint8 dst) =
{ true && true }
spl idx_7 dontcare idx 7;
cast idx_7@bit idx_7;
spl dontcare idx_0t4 idx 4;
cast idx_0t4@uint128 idx_0t4;
shr tmp src idx_0t4;
shr tmp tmp idx_0t4;
shr tmp tmp idx_0t4;
shr tmp tmp idx_0t4;
shr tmp tmp idx_0t4;
shr tmp tmp idx_0t4;
shr tmp tmp idx_0t4;
shr tmp tmp idx_0t4;
spl dontcare dst tmp 8;
cmov dst idx_7 0@uint8 dst;
{ true && true }

proc jb64(uint8 src_0, uint8 src_1, uint8 src_2, uint8 src_3, uint8 src_4, uint8 src_5, uint8 src_6, uint8 src_7; uint8 dst) =
{ true && true }
join tmp src_1 src_0;
join tmp src_2 tmp;
join tmp src_3 tmp;
join tmp src_4 tmp;
join tmp src_5 tmp;
join tmp src_6 tmp;
join dst src_7 tmp;
{ true && true }

proc vpshufb128(uint64 idx_0, uint64 idx_1, uint64 src_0, uint64 src_1; uint64 dst_0, uint64 dst_1) =
{ true && true }
inline stb64(idx_0, idx_00, idx_01, idx_02, idx_03, idx_04, idx_05, idx_06, idx_07);
inline stb64(idx_1, idx_08, idx_09, idx_10, idx_11, idx_12, idx_13, idx_14, idx_15);
join src src_1 src_0;
inline sel128(idx_00, src, dst_00);
inline sel128(idx_01, src, dst_01);
inline sel128(idx_02, src, dst_02);
inline sel128(idx_03, src, dst_03);
inline sel128(idx_04, src, dst_04);
inline sel128(idx_05, src, dst_05);
inline sel128(idx_06, src, dst_06);
inline sel128(idx_07, src, dst_07);
inline sel128(idx_08, src, dst_08);
inline sel128(idx_09, src, dst_09);
inline sel128(idx_10, src, dst_10);
inline sel128(idx_11, src, dst_11);
inline sel128(idx_12, src, dst_12);
inline sel128(idx_13, src, dst_13);
inline sel128(idx_14, src, dst_14);
inline sel128(idx_15, src, dst_15);
inline jb64(dst_00, dst_01, dst_02, dst_03, dst_04, dst_05, dst_06, dst_07, dst_0);
inline jb64(dst_08, dst_09, dst_10, dst_11, dst_12, dst_13, dst_14, dst_15, dst_1);
{ true && true }


proc main (uint64 A00, uint64 A01, uint64 A02, uint64 A03, uint64 A04,
           uint64 A05, uint64 A06, uint64 A07, uint64 A08, uint64 A09,
           uint64 A10, uint64 A11, uint64 A12, uint64 A13, uint64 A14,
           uint64 A15, uint64 A16, uint64 A17, uint64 A18, uint64 A19,
           uint64 A20, uint64 A21, uint64 A22, uint64 A23, uint64 A24,
           uint64 B00, uint64 B01, uint64 B02, uint64 B03, uint64 B04,
           uint64 B05, uint64 B06, uint64 B07, uint64 B08, uint64 B09,
           uint64 B10, uint64 B11, uint64 B12, uint64 B13, uint64 B14,
           uint64 B15, uint64 B16, uint64 B17, uint64 B18, uint64 B19,
           uint64 B20, uint64 B21, uint64 B22, uint64 B23, uint64 B24,
           uint64 C00, uint64 C01, uint64 C02, uint64 C03, uint64 C04,
           uint64 C05, uint64 C06, uint64 C07, uint64 C08, uint64 C09,
           uint64 C10, uint64 C11, uint64 C12, uint64 C13, uint64 C14,
           uint64 C15, uint64 C16, uint64 C17, uint64 C18, uint64 C19,
           uint64 C20, uint64 C21, uint64 C22, uint64 C23, uint64 C24,
           uint64 D00, uint64 D01, uint64 D02, uint64 D03, uint64 D04,
           uint64 D05, uint64 D06, uint64 D07, uint64 D08, uint64 D09,
           uint64 D10, uint64 D11, uint64 D12, uint64 D13, uint64 D14,
           uint64 D15, uint64 D16, uint64 D17, uint64 D18, uint64 D19,
           uint64 D20, uint64 D21, uint64 D22, uint64 D23, uint64 D24) =
{
  true
  &&
  true
}

(* ===== Initialization ===== *)

mov L0x5555555cdf00 $rho8_0@uint64;
mov L0x5555555cdf08 $rho8_1@uint64;
mov L0x5555555cdf10 $rho8_2@uint64;
mov L0x5555555cdf18 $rho8_3@uint64;
mov L0x5555555cdee0 $rho56_0@uint64;
mov L0x5555555cdee8 $rho56_1@uint64;
mov L0x5555555cdef0 $rho56_2@uint64;
mov L0x5555555cdef8 $rho56_3@uint64;

mov L0x5555555cde80 $KeccakF1600RoundConstants_12@uint64;
mov L0x5555555cde88 $KeccakF1600RoundConstants_13@uint64;
mov L0x5555555cde90 $KeccakF1600RoundConstants_14@uint64;
mov L0x5555555cde98 $KeccakF1600RoundConstants_15@uint64;
mov L0x5555555cdea0 $KeccakF1600RoundConstants_16@uint64;
mov L0x5555555cdea8 $KeccakF1600RoundConstants_17@uint64;
mov L0x5555555cdeb0 $KeccakF1600RoundConstants_18@uint64;
mov L0x5555555cdeb8 $KeccakF1600RoundConstants_19@uint64;
mov L0x5555555cdec0 $KeccakF1600RoundConstants_20@uint64;
mov L0x5555555cdec8 $KeccakF1600RoundConstants_21@uint64;
mov L0x5555555cded0 $KeccakF1600RoundConstants_22@uint64;
mov L0x5555555cded8 $KeccakF1600RoundConstants_23@uint64;

mov L0x7fffffffc320 A00;
mov L0x7fffffffc328 B00;
mov L0x7fffffffc330 C00;
mov L0x7fffffffc338 D00;
mov L0x7fffffffc340 A01;
mov L0x7fffffffc348 B01;
mov L0x7fffffffc350 C01;
mov L0x7fffffffc358 D01;
mov L0x7fffffffc360 A02;
mov L0x7fffffffc368 B02;
mov L0x7fffffffc370 C02;
mov L0x7fffffffc378 D02;
mov L0x7fffffffc380 A03;
mov L0x7fffffffc388 B03;
mov L0x7fffffffc390 C03;
mov L0x7fffffffc398 D03;
mov L0x7fffffffc3a0 A04;
mov L0x7fffffffc3a8 B04;
mov L0x7fffffffc3b0 C04;
mov L0x7fffffffc3b8 D04;
mov L0x7fffffffc3c0 A05;
mov L0x7fffffffc3c8 B05;
mov L0x7fffffffc3d0 C05;
mov L0x7fffffffc3d8 D05;
mov L0x7fffffffc3e0 A06;
mov L0x7fffffffc3e8 B06;
mov L0x7fffffffc3f0 C06;
mov L0x7fffffffc3f8 D06;
mov L0x7fffffffc400 A07;
mov L0x7fffffffc408 B07;
mov L0x7fffffffc410 C07;
mov L0x7fffffffc418 D07;
mov L0x7fffffffc420 A08;
mov L0x7fffffffc428 B08;
mov L0x7fffffffc430 C08;
mov L0x7fffffffc438 D08;
mov L0x7fffffffc440 A09;
mov L0x7fffffffc448 B09;
mov L0x7fffffffc450 C09;
mov L0x7fffffffc458 D09;
mov L0x7fffffffc460 A10;
mov L0x7fffffffc468 B10;
mov L0x7fffffffc470 C10;
mov L0x7fffffffc478 D10;
mov L0x7fffffffc480 A11;
mov L0x7fffffffc488 B11;
mov L0x7fffffffc490 C11;
mov L0x7fffffffc498 D11;
mov L0x7fffffffc4a0 A12;
mov L0x7fffffffc4a8 B12;
mov L0x7fffffffc4b0 C12;
mov L0x7fffffffc4b8 D12;
mov L0x7fffffffc4c0 A13;
mov L0x7fffffffc4c8 B13;
mov L0x7fffffffc4d0 C13;
mov L0x7fffffffc4d8 D13;
mov L0x7fffffffc4e0 A14;
mov L0x7fffffffc4e8 B14;
mov L0x7fffffffc4f0 C14;
mov L0x7fffffffc4f8 D14;
mov L0x7fffffffc500 A15;
mov L0x7fffffffc508 B15;
mov L0x7fffffffc510 C15;
mov L0x7fffffffc518 D15;
mov L0x7fffffffc520 A16;
mov L0x7fffffffc528 B16;
mov L0x7fffffffc530 C16;
mov L0x7fffffffc538 D16;
mov L0x7fffffffc540 A17;
mov L0x7fffffffc548 B17;
mov L0x7fffffffc550 C17;
mov L0x7fffffffc558 D17;
mov L0x7fffffffc560 A18;
mov L0x7fffffffc568 B18;
mov L0x7fffffffc570 C18;
mov L0x7fffffffc578 D18;
mov L0x7fffffffc580 A19;
mov L0x7fffffffc588 B19;
mov L0x7fffffffc590 C19;
mov L0x7fffffffc598 D19;
mov L0x7fffffffc5a0 A20;
mov L0x7fffffffc5a8 B20;
mov L0x7fffffffc5b0 C20;
mov L0x7fffffffc5b8 D20;
mov L0x7fffffffc5c0 A21;
mov L0x7fffffffc5c8 B21;
mov L0x7fffffffc5d0 C21;
mov L0x7fffffffc5d8 D21;
mov L0x7fffffffc5e0 A22;
mov L0x7fffffffc5e8 B22;
mov L0x7fffffffc5f0 C22;
mov L0x7fffffffc5f8 D22;
mov L0x7fffffffc600 A23;
mov L0x7fffffffc608 B23;
mov L0x7fffffffc610 C23;
mov L0x7fffffffc618 D23;
mov L0x7fffffffc620 A24;
mov L0x7fffffffc628 B24;
mov L0x7fffffffc630 C24;
mov L0x7fffffffc638 D24;

nondet rax@uint64;
nondet rsp@uint64;

(* ===== Program ===== *)

(* #! -> SP = 0x7fffffffbf68 *)
#! 0x7fffffffbf68 = 0x7fffffffbf68;
(* and    $0xffffffffffffffe0,%rsp                 #! PC = 0x555555577175 *)
and rsp@uint64 rsp 0xffffffffffffffe0@uint64;
(* movabs $0x8000000000008009,%r11                 #! PC = 0x555555577180 *)
mov r11 0x8000000000008009@uint64;
(* mov    $0x80000001,%r9d                         #! PC = 0x55555557718e *)
mov r9 0x80000001@uint64;
(* mov    $0x808b,%r8d                             #! PC = 0x555555577194 *)
mov r8 0x808b@uint64;
(* movabs $0x8000000080008000,%rsi                 #! PC = 0x55555557719a *)
mov rsi 0x8000000080008000@uint64;
(* movabs $0x800000000000808a,%rcx                 #! PC = 0x5555555771a4 *)
mov rcx 0x800000000000808a@uint64;
(* mov    $0x8082,%edx                             #! PC = 0x5555555771af *)
mov rdx 0x8082@uint64;
(* mov    %rsp,%rbp                                #! PC = 0x5555555771b4 *)
mov rbp rsp;
(* mov    $0x8000000a,%r14d                        #! PC = 0x5555555771c2 *)
mov r14 0x8000000a@uint64;
(* mov    $0x80008009,%r13d                        #! PC = 0x5555555771ca *)
mov r13 0x80008009@uint64;
(* mov    $0x88,%r12d                              #! PC = 0x5555555771d2 *)
mov r12 0x88@uint64;
(* movabs $0x8000000080008081,%r10                 #! PC = 0x5555555771da *)
mov r10 0x8000000080008081@uint64;
(* mov    $0x8a,%ebx                               #! PC = 0x5555555771e5 *)
mov rbx 0x8a@uint64;
(* sub    $0x328,%rsp                              #! PC = 0x5555555771ea *)
subb carry rsp rsp 0x328@uint64;
(* vmovdqa 0x20(%rdi),%ymm5                        #! EA = L0x7fffffffc340; Value = 0x0000000000000000; PC = 0x5555555771f1 *)
mov ymm5_0 L0x7fffffffc340;
mov ymm5_1 L0x7fffffffc348;
mov ymm5_2 L0x7fffffffc350;
mov ymm5_3 L0x7fffffffc358;
(* vmovdqa 0xa0(%rdi),%ymm3                        #! EA = L0x7fffffffc3c0; Value = 0x0000000000000000; PC = 0x5555555771f6 *)
mov ymm3_0 L0x7fffffffc3c0;
mov ymm3_1 L0x7fffffffc3c8;
mov ymm3_2 L0x7fffffffc3d0;
mov ymm3_3 L0x7fffffffc3d8;
(* vmovdqa 0xc0(%rdi),%ymm0                        #! EA = L0x7fffffffc3e0; Value = 0x0000000000000000; PC = 0x5555555771fe *)
mov ymm0_0 L0x7fffffffc3e0;
mov ymm0_1 L0x7fffffffc3e8;
mov ymm0_2 L0x7fffffffc3f0;
mov ymm0_3 L0x7fffffffc3f8;
(* vmovdqa 0xe0(%rdi),%ymm11                       #! EA = L0x7fffffffc400; Value = 0x0000000000000000; PC = 0x555555577206 *)
mov ymm11_0 L0x7fffffffc400;
mov ymm11_1 L0x7fffffffc408;
mov ymm11_2 L0x7fffffffc410;
mov ymm11_3 L0x7fffffffc418;
(* vmovdqa (%rdi),%ymm6                            #! EA = L0x7fffffffc320; Value = 0x0000000000000000; PC = 0x55555557720e *)
mov ymm6_0 L0x7fffffffc320;
mov ymm6_1 L0x7fffffffc328;
mov ymm6_2 L0x7fffffffc330;
mov ymm6_3 L0x7fffffffc338;
(* vmovdqa 0x40(%rdi),%ymm4                        #! EA = L0x7fffffffc360; Value = 0x0000000000000000; PC = 0x555555577212 *)
mov ymm4_0 L0x7fffffffc360;
mov ymm4_1 L0x7fffffffc368;
mov ymm4_2 L0x7fffffffc370;
mov ymm4_3 L0x7fffffffc378;
(* vmovdqa %ymm5,-0x3b0(%rbp)                      #! EA = L0x7fffffffbba0; PC = 0x555555577217 *)
mov L0x7fffffffbba0 ymm5_0;
mov L0x7fffffffbba8 ymm5_1;
mov L0x7fffffffbbb0 ymm5_2;
mov L0x7fffffffbbb8 ymm5_3;
(* vmovdqa 0x60(%rdi),%ymm1                        #! EA = L0x7fffffffc380; Value = 0x0000000000000000; PC = 0x55555557721f *)
mov ymm1_0 L0x7fffffffc380;
mov ymm1_1 L0x7fffffffc388;
mov ymm1_2 L0x7fffffffc390;
mov ymm1_3 L0x7fffffffc398;
(* vmovdqa 0x80(%rdi),%ymm7                        #! EA = L0x7fffffffc3a0; Value = 0x0000000000000000; PC = 0x555555577224 *)
mov ymm7_0 L0x7fffffffc3a0;
mov ymm7_1 L0x7fffffffc3a8;
mov ymm7_2 L0x7fffffffc3b0;
mov ymm7_3 L0x7fffffffc3b8;
(* vmovdqa %ymm3,-0x150(%rbp)                      #! EA = L0x7fffffffbe00; PC = 0x55555557722c *)
mov L0x7fffffffbe00 ymm3_0;
mov L0x7fffffffbe08 ymm3_1;
mov L0x7fffffffbe10 ymm3_2;
mov L0x7fffffffbe18 ymm3_3;
(* vmovdqa 0x100(%rdi),%ymm2                       #! EA = L0x7fffffffc420; Value = 0x0000000000000000; PC = 0x555555577234 *)
mov ymm2_0 L0x7fffffffc420;
mov ymm2_1 L0x7fffffffc428;
mov ymm2_2 L0x7fffffffc430;
mov ymm2_3 L0x7fffffffc438;
(* vmovdqa 0x120(%rdi),%ymm14                      #! EA = L0x7fffffffc440; Value = 0x0000000000000000; PC = 0x55555557723c *)
mov ymm14_0 L0x7fffffffc440;
mov ymm14_1 L0x7fffffffc448;
mov ymm14_2 L0x7fffffffc450;
mov ymm14_3 L0x7fffffffc458;
(* vmovdqa %ymm0,-0x170(%rbp)                      #! EA = L0x7fffffffbde0; PC = 0x555555577244 *)
mov L0x7fffffffbde0 ymm0_0;
mov L0x7fffffffbde8 ymm0_1;
mov L0x7fffffffbdf0 ymm0_2;
mov L0x7fffffffbdf8 ymm0_3;
(* vmovdqa %ymm7,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x55555557724c *)
mov L0x7fffffffbf00 ymm7_0;
mov L0x7fffffffbf08 ymm7_1;
mov L0x7fffffffbf10 ymm7_2;
mov L0x7fffffffbf18 ymm7_3;
(* vmovdqa %ymm11,-0x190(%rbp)                     #! EA = L0x7fffffffbdc0; PC = 0x555555577251 *)
mov L0x7fffffffbdc0 ymm11_0;
mov L0x7fffffffbdc8 ymm11_1;
mov L0x7fffffffbdd0 ymm11_2;
mov L0x7fffffffbdd8 ymm11_3;
(* vmovdqa %ymm2,-0x70(%rbp)                       #! EA = L0x7fffffffbee0; PC = 0x555555577259 *)
mov L0x7fffffffbee0 ymm2_0;
mov L0x7fffffffbee8 ymm2_1;
mov L0x7fffffffbef0 ymm2_2;
mov L0x7fffffffbef8 ymm2_3;
(* vmovdqa %ymm14,-0x90(%rbp)                      #! EA = L0x7fffffffbec0; PC = 0x55555557725e *)
mov L0x7fffffffbec0 ymm14_0;
mov L0x7fffffffbec8 ymm14_1;
mov L0x7fffffffbed0 ymm14_2;
mov L0x7fffffffbed8 ymm14_3;
(* vmovdqa %ymm6,-0x1f0(%rbp)                      #! EA = L0x7fffffffbd60; PC = 0x555555577266 *)
mov L0x7fffffffbd60 ymm6_0;
mov L0x7fffffffbd68 ymm6_1;
mov L0x7fffffffbd70 ymm6_2;
mov L0x7fffffffbd78 ymm6_3;
(* vmovdqa %ymm4,-0x110(%rbp)                      #! EA = L0x7fffffffbe40; PC = 0x55555557726e *)
mov L0x7fffffffbe40 ymm4_0;
mov L0x7fffffffbe48 ymm4_1;
mov L0x7fffffffbe50 ymm4_2;
mov L0x7fffffffbe58 ymm4_3;
(* vmovdqa %ymm1,-0x130(%rbp)                      #! EA = L0x7fffffffbe20; PC = 0x555555577276 *)
mov L0x7fffffffbe20 ymm1_0;
mov L0x7fffffffbe28 ymm1_1;
mov L0x7fffffffbe30 ymm1_2;
mov L0x7fffffffbe38 ymm1_3;
(* vmovdqa 0x140(%rdi),%ymm8                       #! EA = L0x7fffffffc460; Value = 0x0000000000000000; PC = 0x55555557727e *)
mov ymm8_0 L0x7fffffffc460;
mov ymm8_1 L0x7fffffffc468;
mov ymm8_2 L0x7fffffffc470;
mov ymm8_3 L0x7fffffffc478;
(* vmovdqa 0x1a0(%rdi),%ymm15                      #! EA = L0x7fffffffc4c0; Value = 0x0000000000000000; PC = 0x555555577286 *)
mov ymm15_0 L0x7fffffffc4c0;
mov ymm15_1 L0x7fffffffc4c8;
mov ymm15_2 L0x7fffffffc4d0;
mov ymm15_3 L0x7fffffffc4d8;
(* vmovdqa 0x220(%rdi),%ymm10                      #! EA = L0x7fffffffc540; Value = 0x0000000000000000; PC = 0x55555557728e *)
mov ymm10_0 L0x7fffffffc540;
mov ymm10_1 L0x7fffffffc548;
mov ymm10_2 L0x7fffffffc550;
mov ymm10_3 L0x7fffffffc558;
(* vmovdqa 0x1c0(%rdi),%ymm12                      #! EA = L0x7fffffffc4e0; Value = 0x0000000000000000; PC = 0x555555577296 *)
mov ymm12_0 L0x7fffffffc4e0;
mov ymm12_1 L0x7fffffffc4e8;
mov ymm12_2 L0x7fffffffc4f0;
mov ymm12_3 L0x7fffffffc4f8;
(* vmovdqa %ymm15,-0xd0(%rbp)                      #! EA = L0x7fffffffbe80; PC = 0x55555557729e *)
mov L0x7fffffffbe80 ymm15_0;
mov L0x7fffffffbe88 ymm15_1;
mov L0x7fffffffbe90 ymm15_2;
mov L0x7fffffffbe98 ymm15_3;
(* vmovdqa 0x200(%rdi),%ymm9                       #! EA = L0x7fffffffc520; Value = 0x0000000000000000; PC = 0x5555555772a6 *)
mov ymm9_0 L0x7fffffffc520;
mov ymm9_1 L0x7fffffffc528;
mov ymm9_2 L0x7fffffffc530;
mov ymm9_3 L0x7fffffffc538;
(* vmovdqa 0x280(%rdi),%ymm15                      #! EA = L0x7fffffffc5a0; Value = 0x0000000000000000; PC = 0x5555555772ae *)
mov ymm15_0 L0x7fffffffc5a0;
mov ymm15_1 L0x7fffffffc5a8;
mov ymm15_2 L0x7fffffffc5b0;
mov ymm15_3 L0x7fffffffc5b8;
(* vmovdqa %ymm10,-0x230(%rbp)                     #! EA = L0x7fffffffbd20; PC = 0x5555555772b6 *)
mov L0x7fffffffbd20 ymm10_0;
mov L0x7fffffffbd28 ymm10_1;
mov L0x7fffffffbd30 ymm10_2;
mov L0x7fffffffbd38 ymm10_3;
(* vmovdqa 0x2e0(%rdi),%ymm2                       #! EA = L0x7fffffffc600; Value = 0x0000000000000000; PC = 0x5555555772be *)
mov ymm2_0 L0x7fffffffc600;
mov ymm2_1 L0x7fffffffc608;
mov ymm2_2 L0x7fffffffc610;
mov ymm2_3 L0x7fffffffc618;
(* vmovdqa 0x2a0(%rdi),%ymm10                      #! EA = L0x7fffffffc5c0; Value = 0x0000000000000000; PC = 0x5555555772c6 *)
mov ymm10_0 L0x7fffffffc5c0;
mov ymm10_1 L0x7fffffffc5c8;
mov ymm10_2 L0x7fffffffc5d0;
mov ymm10_3 L0x7fffffffc5d8;
(* vmovdqa 0x160(%rdi),%ymm7                       #! EA = L0x7fffffffc480; Value = 0x0000000000000000; PC = 0x5555555772ce *)
mov ymm7_0 L0x7fffffffc480;
mov ymm7_1 L0x7fffffffc488;
mov ymm7_2 L0x7fffffffc490;
mov ymm7_3 L0x7fffffffc498;
(* vmovdqa %ymm9,-0x210(%rbp)                      #! EA = L0x7fffffffbd40; PC = 0x5555555772d6 *)
mov L0x7fffffffbd40 ymm9_0;
mov L0x7fffffffbd48 ymm9_1;
mov L0x7fffffffbd50 ymm9_2;
mov L0x7fffffffbd58 ymm9_3;
(* vmovdqa 0x240(%rdi),%ymm9                       #! EA = L0x7fffffffc560; Value = 0x0000000000000000; PC = 0x5555555772de *)
mov ymm9_0 L0x7fffffffc560;
mov ymm9_1 L0x7fffffffc568;
mov ymm9_2 L0x7fffffffc570;
mov ymm9_3 L0x7fffffffc578;
(* vmovdqa %ymm15,-0x250(%rbp)                     #! EA = L0x7fffffffbd00; PC = 0x5555555772e6 *)
mov L0x7fffffffbd00 ymm15_0;
mov L0x7fffffffbd08 ymm15_1;
mov L0x7fffffffbd10 ymm15_2;
mov L0x7fffffffbd18 ymm15_3;
(* vmovdqa 0x180(%rdi),%ymm13                      #! EA = L0x7fffffffc4a0; Value = 0x0000000000000000; PC = 0x5555555772ee *)
mov ymm13_0 L0x7fffffffc4a0;
mov ymm13_1 L0x7fffffffc4a8;
mov ymm13_2 L0x7fffffffc4b0;
mov ymm13_3 L0x7fffffffc4b8;
(* vmovdqa 0x260(%rdi),%ymm14                      #! EA = L0x7fffffffc580; Value = 0x0000000000000000; PC = 0x5555555772f6 *)
mov ymm14_0 L0x7fffffffc580;
mov ymm14_1 L0x7fffffffc588;
mov ymm14_2 L0x7fffffffc590;
mov ymm14_3 L0x7fffffffc598;
(* vmovdqa %ymm10,-0xf0(%rbp)                      #! EA = L0x7fffffffbe60; PC = 0x5555555772fe *)
mov L0x7fffffffbe60 ymm10_0;
mov L0x7fffffffbe68 ymm10_1;
mov L0x7fffffffbe70 ymm10_2;
mov L0x7fffffffbe78 ymm10_3;
(* vmovdqa 0x2c0(%rdi),%ymm15                      #! EA = L0x7fffffffc5e0; Value = 0x0000000000000000; PC = 0x555555577306 *)
mov ymm15_0 L0x7fffffffc5e0;
mov ymm15_1 L0x7fffffffc5e8;
mov ymm15_2 L0x7fffffffc5f0;
mov ymm15_3 L0x7fffffffc5f8;
(* vmovdqa 0x300(%rdi),%ymm10                      #! EA = L0x7fffffffc620; Value = 0x0000000000000000; PC = 0x55555557730e *)
mov ymm10_0 L0x7fffffffc620;
mov ymm10_1 L0x7fffffffc628;
mov ymm10_2 L0x7fffffffc630;
mov ymm10_3 L0x7fffffffc638;
(* vmovdqa %ymm2,-0x270(%rbp)                      #! EA = L0x7fffffffbce0; PC = 0x555555577316 *)
mov L0x7fffffffbce0 ymm2_0;
mov L0x7fffffffbce8 ymm2_1;
mov L0x7fffffffbcf0 ymm2_2;
mov L0x7fffffffbcf8 ymm2_3;
(* vpxor  %ymm3,%ymm8,%ymm2                        #! PC = 0x55555557731e *)
xor ymm2_0@uint64 ymm8_0 ymm3_0;
xor ymm2_1@uint64 ymm8_1 ymm3_1;
xor ymm2_2@uint64 ymm8_2 ymm3_2;
xor ymm2_3@uint64 ymm8_3 ymm3_3;
(* vmovdqa -0x250(%rbp),%ymm3                      #! EA = L0x7fffffffbd00; Value = 0x0000000000000000; PC = 0x555555577322 *)
mov ymm3_0 L0x7fffffffbd00;
mov ymm3_1 L0x7fffffffbd08;
mov ymm3_2 L0x7fffffffbd10;
mov ymm3_3 L0x7fffffffbd18;
(* vmovdqa %ymm8,-0x1b0(%rbp)                      #! EA = L0x7fffffffbda0; PC = 0x55555557732a *)
mov L0x7fffffffbda0 ymm8_0;
mov L0x7fffffffbda8 ymm8_1;
mov L0x7fffffffbdb0 ymm8_2;
mov L0x7fffffffbdb8 ymm8_3;
(* vmovdqa -0xf0(%rbp),%ymm8                       #! EA = L0x7fffffffbe60; Value = 0x0000000000000000; PC = 0x555555577332 *)
mov ymm8_0 L0x7fffffffbe60;
mov ymm8_1 L0x7fffffffbe68;
mov ymm8_2 L0x7fffffffbe70;
mov ymm8_3 L0x7fffffffbe78;
(* vpxor  -0x210(%rbp),%ymm8,%ymm8                 #! EA = L0x7fffffffbd40; Value = 0x0000000000000000; PC = 0x55555557733a *)
xor ymm8_0@uint64 ymm8_0 L0x7fffffffbd40;
xor ymm8_1@uint64 ymm8_1 L0x7fffffffbd48;
xor ymm8_2@uint64 ymm8_2 L0x7fffffffbd50;
xor ymm8_3@uint64 ymm8_3 L0x7fffffffbd58;
(* vmovdqa %ymm12,-0xb0(%rbp)                      #! EA = L0x7fffffffbea0; PC = 0x555555577342 *)
mov L0x7fffffffbea0 ymm12_0;
mov L0x7fffffffbea8 ymm12_1;
mov L0x7fffffffbeb0 ymm12_2;
mov L0x7fffffffbeb8 ymm12_3;
(* vmovdqa 0x1e0(%rdi),%ymm12                      #! EA = L0x7fffffffc500; Value = 0x0000000000000000; PC = 0x55555557734a *)
mov ymm12_0 L0x7fffffffc500;
mov ymm12_1 L0x7fffffffc508;
mov ymm12_2 L0x7fffffffc510;
mov ymm12_3 L0x7fffffffc518;
(* vmovdqa %ymm7,-0x3d0(%rbp)                      #! EA = L0x7fffffffbb80; PC = 0x555555577352 *)
mov L0x7fffffffbb80 ymm7_0;
mov L0x7fffffffbb88 ymm7_1;
mov L0x7fffffffbb90 ymm7_2;
mov L0x7fffffffbb98 ymm7_3;
(* vpxor  %ymm0,%ymm7,%ymm7                        #! PC = 0x55555557735a *)
xor ymm7_0@uint64 ymm7_0 ymm0_0;
xor ymm7_1@uint64 ymm7_1 ymm0_1;
xor ymm7_2@uint64 ymm7_2 ymm0_2;
xor ymm7_3@uint64 ymm7_3 ymm0_3;
(* vpxor  %ymm11,%ymm13,%ymm0                      #! PC = 0x55555557735e *)
xor ymm0_0@uint64 ymm13_0 ymm11_0;
xor ymm0_1@uint64 ymm13_1 ymm11_1;
xor ymm0_2@uint64 ymm13_2 ymm11_2;
xor ymm0_3@uint64 ymm13_3 ymm11_3;
(* vmovdqa %ymm12,-0x1d0(%rbp)                     #! EA = L0x7fffffffbd80; PC = 0x555555577363 *)
mov L0x7fffffffbd80 ymm12_0;
mov L0x7fffffffbd88 ymm12_1;
mov L0x7fffffffbd90 ymm12_2;
mov L0x7fffffffbd98 ymm12_3;
(* vpxor  %ymm8,%ymm7,%ymm7                        #! PC = 0x55555557736b *)
xor ymm7_0@uint64 ymm7_0 ymm8_0;
xor ymm7_1@uint64 ymm7_1 ymm8_1;
xor ymm7_2@uint64 ymm7_2 ymm8_2;
xor ymm7_3@uint64 ymm7_3 ymm8_3;
(* vpxor  %ymm12,%ymm3,%ymm12                      #! PC = 0x555555577370 *)
xor ymm12_0@uint64 ymm3_0 ymm12_0;
xor ymm12_1@uint64 ymm3_1 ymm12_1;
xor ymm12_2@uint64 ymm3_2 ymm12_2;
xor ymm12_3@uint64 ymm3_3 ymm12_3;
(* vpxor  -0x270(%rbp),%ymm9,%ymm8                 #! EA = L0x7fffffffbce0; Value = 0x0000000000000000; PC = 0x555555577375 *)
xor ymm8_0@uint64 ymm9_0 L0x7fffffffbce0;
xor ymm8_1@uint64 ymm9_1 L0x7fffffffbce8;
xor ymm8_2@uint64 ymm9_2 L0x7fffffffbcf0;
xor ymm8_3@uint64 ymm9_3 L0x7fffffffbcf8;
(* vmovdqa -0xd0(%rbp),%ymm3                       #! EA = L0x7fffffffbe80; Value = 0x0000000000000000; PC = 0x55555557737d *)
mov ymm3_0 L0x7fffffffbe80;
mov ymm3_1 L0x7fffffffbe88;
mov ymm3_2 L0x7fffffffbe90;
mov ymm3_3 L0x7fffffffbe98;
(* vpxor  %ymm5,%ymm7,%ymm7                        #! PC = 0x555555577385 *)
xor ymm7_0@uint64 ymm7_0 ymm5_0;
xor ymm7_1@uint64 ymm7_1 ymm5_1;
xor ymm7_2@uint64 ymm7_2 ymm5_2;
xor ymm7_3@uint64 ymm7_3 ymm5_3;
(* vpxor  %ymm12,%ymm2,%ymm2                       #! PC = 0x555555577389 *)
xor ymm2_0@uint64 ymm2_0 ymm12_0;
xor ymm2_1@uint64 ymm2_1 ymm12_1;
xor ymm2_2@uint64 ymm2_2 ymm12_2;
xor ymm2_3@uint64 ymm2_3 ymm12_3;
(* vpxor  -0x70(%rbp),%ymm3,%ymm3                  #! EA = L0x7fffffffbee0; Value = 0x0000000000000000; PC = 0x55555557738e *)
xor ymm3_0@uint64 ymm3_0 L0x7fffffffbee0;
xor ymm3_1@uint64 ymm3_1 L0x7fffffffbee8;
xor ymm3_2@uint64 ymm3_2 L0x7fffffffbef0;
xor ymm3_3@uint64 ymm3_3 L0x7fffffffbef8;
(* vpxor  -0x230(%rbp),%ymm15,%ymm5                #! EA = L0x7fffffffbd20; Value = 0x0000000000000000; PC = 0x555555577393 *)
xor ymm5_0@uint64 ymm15_0 L0x7fffffffbd20;
xor ymm5_1@uint64 ymm15_1 L0x7fffffffbd28;
xor ymm5_2@uint64 ymm15_2 L0x7fffffffbd30;
xor ymm5_3@uint64 ymm15_3 L0x7fffffffbd38;
(* vpxor  %ymm6,%ymm2,%ymm12                       #! PC = 0x55555557739b *)
xor ymm12_0@uint64 ymm2_0 ymm6_0;
xor ymm12_1@uint64 ymm2_1 ymm6_1;
xor ymm12_2@uint64 ymm2_2 ymm6_2;
xor ymm12_3@uint64 ymm2_3 ymm6_3;
(* vpxor  %ymm8,%ymm3,%ymm3                        #! PC = 0x55555557739f *)
xor ymm3_0@uint64 ymm3_0 ymm8_0;
xor ymm3_1@uint64 ymm3_1 ymm8_1;
xor ymm3_2@uint64 ymm3_2 ymm8_2;
xor ymm3_3@uint64 ymm3_3 ymm8_3;
(* vpxor  %ymm1,%ymm3,%ymm8                        #! PC = 0x5555555773a4 *)
xor ymm8_0@uint64 ymm3_0 ymm1_0;
xor ymm8_1@uint64 ymm3_1 ymm1_1;
xor ymm8_2@uint64 ymm3_2 ymm1_2;
xor ymm8_3@uint64 ymm3_3 ymm1_3;
(* vmovdqa -0xb0(%rbp),%ymm3                       #! EA = L0x7fffffffbea0; Value = 0x0000000000000000; PC = 0x5555555773a8 *)
mov ymm3_0 L0x7fffffffbea0;
mov ymm3_1 L0x7fffffffbea8;
mov ymm3_2 L0x7fffffffbeb0;
mov ymm3_3 L0x7fffffffbeb8;
(* vpxor  %ymm5,%ymm0,%ymm5                        #! PC = 0x5555555773b0 *)
xor ymm5_0@uint64 ymm0_0 ymm5_0;
xor ymm5_1@uint64 ymm0_1 ymm5_1;
xor ymm5_2@uint64 ymm0_2 ymm5_2;
xor ymm5_3@uint64 ymm0_3 ymm5_3;
(* vpxor  -0x90(%rbp),%ymm3,%ymm11                 #! EA = L0x7fffffffbec0; Value = 0x0000000000000000; PC = 0x5555555773b4 *)
xor ymm11_0@uint64 ymm3_0 L0x7fffffffbec0;
xor ymm11_1@uint64 ymm3_1 L0x7fffffffbec8;
xor ymm11_2@uint64 ymm3_2 L0x7fffffffbed0;
xor ymm11_3@uint64 ymm3_3 L0x7fffffffbed8;
(* vpxor  %ymm14,%ymm10,%ymm0                      #! PC = 0x5555555773bc *)
xor ymm0_0@uint64 ymm10_0 ymm14_0;
xor ymm0_1@uint64 ymm10_1 ymm14_1;
xor ymm0_2@uint64 ymm10_2 ymm14_2;
xor ymm0_3@uint64 ymm10_3 ymm14_3;
(* vpxor  %ymm4,%ymm5,%ymm5                        #! PC = 0x5555555773c1 *)
xor ymm5_0@uint64 ymm5_0 ymm4_0;
xor ymm5_1@uint64 ymm5_1 ymm4_1;
xor ymm5_2@uint64 ymm5_2 ymm4_2;
xor ymm5_3@uint64 ymm5_3 ymm4_3;
(* movq   $0x1,-0x290(%rbp)                        #! EA = L0x7fffffffbcc0; PC = 0x5555555773c5 *)
mov L0x7fffffffbcc0 0x1@uint64;
(* vpxor  %ymm0,%ymm11,%ymm11                      #! PC = 0x5555555773d0 *)
xor ymm11_0@uint64 ymm11_0 ymm0_0;
xor ymm11_1@uint64 ymm11_1 ymm0_1;
xor ymm11_2@uint64 ymm11_2 ymm0_2;
xor ymm11_3@uint64 ymm11_3 ymm0_3;
(* vpxor  -0x50(%rbp),%ymm11,%ymm11                #! EA = L0x7fffffffbf00; Value = 0x0000000000000000; PC = 0x5555555773d4 *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbf00;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbf08;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbf10;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbf18;
(* vpsrlq $0x3f,%ymm7,%ymm1                        #! PC = 0x5555555773d9 *)
shr ymm1_0 ymm7_0 0x3f@uint64;
shr ymm1_1 ymm7_1 0x3f@uint64;
shr ymm1_2 ymm7_2 0x3f@uint64;
shr ymm1_3 ymm7_3 0x3f@uint64;
(* vpsllq $0x1,%ymm7,%ymm0                         #! PC = 0x5555555773de *)
shl ymm0_0 ymm7_0 0x1@uint64;
shl ymm0_1 ymm7_1 0x1@uint64;
shl ymm0_2 ymm7_2 0x1@uint64;
shl ymm0_3 ymm7_3 0x1@uint64;
(* add    $0x60,%rax                               #! PC = 0x5555555773e3 *)
adds carry rax rax 0x60@uint64;
(* vpsllq $0x1,%ymm5,%ymm2                         #! PC = 0x5555555773e7 *)
shl ymm2_0 ymm5_0 0x1@uint64;
shl ymm2_1 ymm5_1 0x1@uint64;
shl ymm2_2 ymm5_2 0x1@uint64;
shl ymm2_3 ymm5_3 0x1@uint64;
(* vpsllq $0x1,%ymm8,%ymm3                         #! PC = 0x5555555773ec *)
shl ymm3_0 ymm8_0 0x1@uint64;
shl ymm3_1 ymm8_1 0x1@uint64;
shl ymm3_2 ymm8_2 0x1@uint64;
shl ymm3_3 ymm8_3 0x1@uint64;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x5555555773f2 *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpsrlq $0x3f,%ymm5,%ymm1                        #! PC = 0x5555555773f6 *)
shr ymm1_0 ymm5_0 0x3f@uint64;
shr ymm1_1 ymm5_1 0x3f@uint64;
shr ymm1_2 ymm5_2 0x3f@uint64;
shr ymm1_3 ymm5_3 0x3f@uint64;
(* vpxor  %ymm11,%ymm0,%ymm0                       #! PC = 0x5555555773fb *)
xor ymm0_0@uint64 ymm0_0 ymm11_0;
xor ymm0_1@uint64 ymm0_1 ymm11_1;
xor ymm0_2@uint64 ymm0_2 ymm11_2;
xor ymm0_3@uint64 ymm0_3 ymm11_3;
(* vpor   %ymm1,%ymm2,%ymm2                        #! PC = 0x555555577400 *)
or ymm2_0@uint64 ymm2_0 ymm1_0;
or ymm2_1@uint64 ymm2_1 ymm1_1;
or ymm2_2@uint64 ymm2_2 ymm1_2;
or ymm2_3@uint64 ymm2_3 ymm1_3;
(* vpsrlq $0x3f,%ymm8,%ymm1                        #! PC = 0x555555577404 *)
shr ymm1_0 ymm8_0 0x3f@uint64;
shr ymm1_1 ymm8_1 0x3f@uint64;
shr ymm1_2 ymm8_2 0x3f@uint64;
shr ymm1_3 ymm8_3 0x3f@uint64;
(* vpor   %ymm1,%ymm3,%ymm1                        #! PC = 0x55555557740a *)
or ymm1_0@uint64 ymm3_0 ymm1_0;
or ymm1_1@uint64 ymm3_1 ymm1_1;
or ymm1_2@uint64 ymm3_2 ymm1_2;
or ymm1_3@uint64 ymm3_3 ymm1_3;
(* vpsrlq $0x3f,%ymm11,%ymm3                       #! PC = 0x55555557740e *)
shr ymm3_0 ymm11_0 0x3f@uint64;
shr ymm3_1 ymm11_1 0x3f@uint64;
shr ymm3_2 ymm11_2 0x3f@uint64;
shr ymm3_3 ymm11_3 0x3f@uint64;
(* vpxor  %ymm12,%ymm2,%ymm2                       #! PC = 0x555555577414 *)
xor ymm2_0@uint64 ymm2_0 ymm12_0;
xor ymm2_1@uint64 ymm2_1 ymm12_1;
xor ymm2_2@uint64 ymm2_2 ymm12_2;
xor ymm2_3@uint64 ymm2_3 ymm12_3;
(* vpsllq $0x1,%ymm11,%ymm11                       #! PC = 0x555555577419 *)
shl ymm11_0 ymm11_0 0x1@uint64;
shl ymm11_1 ymm11_1 0x1@uint64;
shl ymm11_2 ymm11_2 0x1@uint64;
shl ymm11_3 ymm11_3 0x1@uint64;
(* vpxor  %ymm7,%ymm1,%ymm1                        #! PC = 0x55555557741f *)
xor ymm1_0@uint64 ymm1_0 ymm7_0;
xor ymm1_1@uint64 ymm1_1 ymm7_1;
xor ymm1_2@uint64 ymm1_2 ymm7_2;
xor ymm1_3@uint64 ymm1_3 ymm7_3;
(* vpor   %ymm3,%ymm11,%ymm11                      #! PC = 0x555555577423 *)
or ymm11_0@uint64 ymm11_0 ymm3_0;
or ymm11_1@uint64 ymm11_1 ymm3_1;
or ymm11_2@uint64 ymm11_2 ymm3_2;
or ymm11_3@uint64 ymm11_3 ymm3_3;
(* vpxor  %ymm1,%ymm13,%ymm13                      #! PC = 0x555555577427 *)
xor ymm13_0@uint64 ymm13_0 ymm1_0;
xor ymm13_1@uint64 ymm13_1 ymm1_1;
xor ymm13_2@uint64 ymm13_2 ymm1_2;
xor ymm13_3@uint64 ymm13_3 ymm1_3;
(* vpxor  %ymm1,%ymm15,%ymm15                      #! PC = 0x55555557742b *)
xor ymm15_0@uint64 ymm15_0 ymm1_0;
xor ymm15_1@uint64 ymm15_1 ymm1_1;
xor ymm15_2@uint64 ymm15_2 ymm1_2;
xor ymm15_3@uint64 ymm15_3 ymm1_3;
(* vpxor  %ymm5,%ymm11,%ymm6                       #! PC = 0x55555557742f *)
xor ymm6_0@uint64 ymm11_0 ymm5_0;
xor ymm6_1@uint64 ymm11_1 ymm5_1;
xor ymm6_2@uint64 ymm11_2 ymm5_2;
xor ymm6_3@uint64 ymm11_3 ymm5_3;
(* vpxor  -0x170(%rbp),%ymm2,%ymm5                 #! EA = L0x7fffffffbde0; Value = 0x0000000000000000; PC = 0x555555577433 *)
xor ymm5_0@uint64 ymm2_0 L0x7fffffffbde0;
xor ymm5_1@uint64 ymm2_1 L0x7fffffffbde8;
xor ymm5_2@uint64 ymm2_2 L0x7fffffffbdf0;
xor ymm5_3@uint64 ymm2_3 L0x7fffffffbdf8;
(* vpsrlq $0x3f,%ymm12,%ymm3                       #! PC = 0x55555557743b *)
shr ymm3_0 ymm12_0 0x3f@uint64;
shr ymm3_1 ymm12_1 0x3f@uint64;
shr ymm3_2 ymm12_2 0x3f@uint64;
shr ymm3_3 ymm12_3 0x3f@uint64;
(* vpsllq $0x1,%ymm12,%ymm12                       #! PC = 0x555555577441 *)
shl ymm12_0 ymm12_0 0x1@uint64;
shl ymm12_1 ymm12_1 0x1@uint64;
shl ymm12_2 ymm12_2 0x1@uint64;
shl ymm12_3 ymm12_3 0x1@uint64;
(* vpxor  %ymm6,%ymm9,%ymm9                        #! PC = 0x555555577447 *)
xor ymm9_0@uint64 ymm9_0 ymm6_0;
xor ymm9_1@uint64 ymm9_1 ymm6_1;
xor ymm9_2@uint64 ymm9_2 ymm6_2;
xor ymm9_3@uint64 ymm9_3 ymm6_3;
(* vpsrlq $0x14,%ymm5,%ymm7                        #! PC = 0x55555557744b *)
shr ymm7_0 ymm5_0 0x14@uint64;
shr ymm7_1 ymm5_1 0x14@uint64;
shr ymm7_2 ymm5_2 0x14@uint64;
shr ymm7_3 ymm5_3 0x14@uint64;
(* vpsllq $0x2c,%ymm5,%ymm5                        #! PC = 0x555555577450 *)
shl ymm5_0 ymm5_0 0x2c@uint64;
shl ymm5_1 ymm5_1 0x2c@uint64;
shl ymm5_2 ymm5_2 0x2c@uint64;
shl ymm5_3 ymm5_3 0x2c@uint64;
(* vpor   %ymm3,%ymm12,%ymm12                      #! PC = 0x555555577455 *)
or ymm12_0@uint64 ymm12_0 ymm3_0;
or ymm12_1@uint64 ymm12_1 ymm3_1;
or ymm12_2@uint64 ymm12_2 ymm3_2;
or ymm12_3@uint64 ymm12_3 ymm3_3;
(* vpor   %ymm7,%ymm5,%ymm5                        #! PC = 0x555555577459 *)
or ymm5_0@uint64 ymm5_0 ymm7_0;
or ymm5_1@uint64 ymm5_1 ymm7_1;
or ymm5_2@uint64 ymm5_2 ymm7_2;
or ymm5_3@uint64 ymm5_3 ymm7_3;
(* vpsrlq $0x15,%ymm13,%ymm7                       #! PC = 0x55555557745d *)
shr ymm7_0 ymm13_0 0x15@uint64;
shr ymm7_1 ymm13_1 0x15@uint64;
shr ymm7_2 ymm13_2 0x15@uint64;
shr ymm7_3 ymm13_3 0x15@uint64;
(* vpxor  %ymm8,%ymm12,%ymm4                       #! PC = 0x555555577463 *)
xor ymm4_0@uint64 ymm12_0 ymm8_0;
xor ymm4_1@uint64 ymm12_1 ymm8_1;
xor ymm4_2@uint64 ymm12_2 ymm8_2;
xor ymm4_3@uint64 ymm12_3 ymm8_3;
(* vpsllq $0x2b,%ymm13,%ymm13                      #! PC = 0x555555577468 *)
shl ymm13_0 ymm13_0 0x2b@uint64;
shl ymm13_1 ymm13_1 0x2b@uint64;
shl ymm13_2 ymm13_2 0x2b@uint64;
shl ymm13_3 ymm13_3 0x2b@uint64;
(* vpxor  -0x1f0(%rbp),%ymm0,%ymm3                 #! EA = L0x7fffffffbd60; Value = 0x0000000000000000; PC = 0x55555557746e *)
xor ymm3_0@uint64 ymm0_0 L0x7fffffffbd60;
xor ymm3_1@uint64 ymm0_1 L0x7fffffffbd68;
xor ymm3_2@uint64 ymm0_2 L0x7fffffffbd70;
xor ymm3_3@uint64 ymm0_3 L0x7fffffffbd78;
(* vpxor  %ymm4,%ymm10,%ymm10                      #! PC = 0x555555577476 *)
xor ymm10_0@uint64 ymm10_0 ymm4_0;
xor ymm10_1@uint64 ymm10_1 ymm4_1;
xor ymm10_2@uint64 ymm10_2 ymm4_2;
xor ymm10_3@uint64 ymm10_3 ymm4_3;
(* vpor   %ymm7,%ymm13,%ymm13                      #! PC = 0x55555557747a *)
or ymm13_0@uint64 ymm13_0 ymm7_0;
or ymm13_1@uint64 ymm13_1 ymm7_1;
or ymm13_2@uint64 ymm13_2 ymm7_2;
or ymm13_3@uint64 ymm13_3 ymm7_3;
(* vpxor  %ymm4,%ymm14,%ymm14                      #! PC = 0x55555557747e *)
xor ymm14_0@uint64 ymm14_0 ymm4_0;
xor ymm14_1@uint64 ymm14_1 ymm4_1;
xor ymm14_2@uint64 ymm14_2 ymm4_2;
xor ymm14_3@uint64 ymm14_3 ymm4_3;
(* vpbroadcastq -0x290(%rbp),%ymm7                 #! EA = L0x7fffffffbcc0; Value = 0x0000000000000001; PC = 0x555555577482 *)
mov ymm7_0 L0x7fffffffbcc0;
mov ymm7_1 L0x7fffffffbcc0;
mov ymm7_2 L0x7fffffffbcc0;
mov ymm7_3 L0x7fffffffbcc0;
(* vpshufb 0x56a6c(%rip),%ymm14,%ymm14        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x55555557748b *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm14_0, ymm14_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm14_2, ymm14_3, tmp_2, tmp_3);
mov ymm14_0 tmp_0;
mov ymm14_1 tmp_1;
mov ymm14_2 tmp_2;
mov ymm14_3 tmp_3;
(* vpandn %ymm13,%ymm5,%ymm8                       #! PC = 0x555555577494 *)
not ymm5_0n@uint64 ymm5_0;
and ymm8_0@uint64 ymm5_0n ymm13_0;
not ymm5_1n@uint64 ymm5_1;
and ymm8_1@uint64 ymm5_1n ymm13_1;
not ymm5_2n@uint64 ymm5_2;
and ymm8_2@uint64 ymm5_2n ymm13_2;
not ymm5_3n@uint64 ymm5_3;
and ymm8_3@uint64 ymm5_3n ymm13_3;
(* vpxor  %ymm8,%ymm7,%ymm7                        #! PC = 0x555555577499 *)
xor ymm7_0@uint64 ymm7_0 ymm8_0;
xor ymm7_1@uint64 ymm7_1 ymm8_1;
xor ymm7_2@uint64 ymm7_2 ymm8_2;
xor ymm7_3@uint64 ymm7_3 ymm8_3;
(* vpxor  %ymm3,%ymm7,%ymm7                        #! PC = 0x55555557749e *)
xor ymm7_0@uint64 ymm7_0 ymm3_0;
xor ymm7_1@uint64 ymm7_1 ymm3_1;
xor ymm7_2@uint64 ymm7_2 ymm3_2;
xor ymm7_3@uint64 ymm7_3 ymm3_3;
(* vmovdqa %ymm7,-0x390(%rbp)                      #! EA = L0x7fffffffbbc0; PC = 0x5555555774a2 *)
mov L0x7fffffffbbc0 ymm7_0;
mov L0x7fffffffbbc8 ymm7_1;
mov L0x7fffffffbbd0 ymm7_2;
mov L0x7fffffffbbd8 ymm7_3;
(* vpsrlq $0x2b,%ymm9,%ymm7                        #! PC = 0x5555555774aa *)
shr ymm7_0 ymm9_0 0x2b@uint64;
shr ymm7_1 ymm9_1 0x2b@uint64;
shr ymm7_2 ymm9_2 0x2b@uint64;
shr ymm7_3 ymm9_3 0x2b@uint64;
(* vpsllq $0x15,%ymm9,%ymm9                        #! PC = 0x5555555774b0 *)
shl ymm9_0 ymm9_0 0x15@uint64;
shl ymm9_1 ymm9_1 0x15@uint64;
shl ymm9_2 ymm9_2 0x15@uint64;
shl ymm9_3 ymm9_3 0x15@uint64;
(* vpor   %ymm7,%ymm9,%ymm9                        #! PC = 0x5555555774b6 *)
or ymm9_0@uint64 ymm9_0 ymm7_0;
or ymm9_1@uint64 ymm9_1 ymm7_1;
or ymm9_2@uint64 ymm9_2 ymm7_2;
or ymm9_3@uint64 ymm9_3 ymm7_3;
(* vpandn %ymm9,%ymm13,%ymm7                       #! PC = 0x5555555774ba *)
not ymm13_0n@uint64 ymm13_0;
and ymm7_0@uint64 ymm13_0n ymm9_0;
not ymm13_1n@uint64 ymm13_1;
and ymm7_1@uint64 ymm13_1n ymm9_1;
not ymm13_2n@uint64 ymm13_2;
and ymm7_2@uint64 ymm13_2n ymm9_2;
not ymm13_3n@uint64 ymm13_3;
and ymm7_3@uint64 ymm13_3n ymm9_3;
(* vpxor  %ymm5,%ymm7,%ymm8                        #! PC = 0x5555555774bf *)
xor ymm8_0@uint64 ymm7_0 ymm5_0;
xor ymm8_1@uint64 ymm7_1 ymm5_1;
xor ymm8_2@uint64 ymm7_2 ymm5_2;
xor ymm8_3@uint64 ymm7_3 ymm5_3;
(* vpsrlq $0x32,%ymm10,%ymm7                       #! PC = 0x5555555774c3 *)
shr ymm7_0 ymm10_0 0x32@uint64;
shr ymm7_1 ymm10_1 0x32@uint64;
shr ymm7_2 ymm10_2 0x32@uint64;
shr ymm7_3 ymm10_3 0x32@uint64;
(* vpsllq $0xe,%ymm10,%ymm10                       #! PC = 0x5555555774c9 *)
shl ymm10_0 ymm10_0 0xe@uint64;
shl ymm10_1 ymm10_1 0xe@uint64;
shl ymm10_2 ymm10_2 0xe@uint64;
shl ymm10_3 ymm10_3 0xe@uint64;
(* vmovdqa %ymm8,-0x370(%rbp)                      #! EA = L0x7fffffffbbe0; PC = 0x5555555774cf *)
mov L0x7fffffffbbe0 ymm8_0;
mov L0x7fffffffbbe8 ymm8_1;
mov L0x7fffffffbbf0 ymm8_2;
mov L0x7fffffffbbf8 ymm8_3;
(* vpor   %ymm7,%ymm10,%ymm10                      #! PC = 0x5555555774d7 *)
or ymm10_0@uint64 ymm10_0 ymm7_0;
or ymm10_1@uint64 ymm10_1 ymm7_1;
or ymm10_2@uint64 ymm10_2 ymm7_2;
or ymm10_3@uint64 ymm10_3 ymm7_3;
(* vpandn %ymm10,%ymm9,%ymm7                       #! PC = 0x5555555774db *)
not ymm9_0n@uint64 ymm9_0;
and ymm7_0@uint64 ymm9_0n ymm10_0;
not ymm9_1n@uint64 ymm9_1;
and ymm7_1@uint64 ymm9_1n ymm10_1;
not ymm9_2n@uint64 ymm9_2;
and ymm7_2@uint64 ymm9_2n ymm10_2;
not ymm9_3n@uint64 ymm9_3;
and ymm7_3@uint64 ymm9_3n ymm10_3;
(* vpxor  %ymm13,%ymm7,%ymm13                      #! PC = 0x5555555774e0 *)
xor ymm13_0@uint64 ymm7_0 ymm13_0;
xor ymm13_1@uint64 ymm7_1 ymm13_1;
xor ymm13_2@uint64 ymm7_2 ymm13_2;
xor ymm13_3@uint64 ymm7_3 ymm13_3;
(* vpandn %ymm3,%ymm10,%ymm7                       #! PC = 0x5555555774e5 *)
not ymm10_0n@uint64 ymm10_0;
and ymm7_0@uint64 ymm10_0n ymm3_0;
not ymm10_1n@uint64 ymm10_1;
and ymm7_1@uint64 ymm10_1n ymm3_1;
not ymm10_2n@uint64 ymm10_2;
and ymm7_2@uint64 ymm10_2n ymm3_2;
not ymm10_3n@uint64 ymm10_3;
and ymm7_3@uint64 ymm10_3n ymm3_3;
(* vpandn %ymm5,%ymm3,%ymm3                        #! PC = 0x5555555774e9 *)
not ymm3_0n@uint64 ymm3_0;
and ymm3_0@uint64 ymm3_0n ymm5_0;
not ymm3_1n@uint64 ymm3_1;
and ymm3_1@uint64 ymm3_1n ymm5_1;
not ymm3_2n@uint64 ymm3_2;
and ymm3_2@uint64 ymm3_2n ymm5_2;
not ymm3_3n@uint64 ymm3_3;
and ymm3_3@uint64 ymm3_3n ymm5_3;
(* vpxor  %ymm9,%ymm7,%ymm12                       #! PC = 0x5555555774ed *)
xor ymm12_0@uint64 ymm7_0 ymm9_0;
xor ymm12_1@uint64 ymm7_1 ymm9_1;
xor ymm12_2@uint64 ymm7_2 ymm9_2;
xor ymm12_3@uint64 ymm7_3 ymm9_3;
(* vpxor  -0x130(%rbp),%ymm6,%ymm7                 #! EA = L0x7fffffffbe20; Value = 0x0000000000000000; PC = 0x5555555774f2 *)
xor ymm7_0@uint64 ymm6_0 L0x7fffffffbe20;
xor ymm7_1@uint64 ymm6_1 L0x7fffffffbe28;
xor ymm7_2@uint64 ymm6_2 L0x7fffffffbe30;
xor ymm7_3@uint64 ymm6_3 L0x7fffffffbe38;
(* vpxor  %ymm10,%ymm3,%ymm5                       #! PC = 0x5555555774fa *)
xor ymm5_0@uint64 ymm3_0 ymm10_0;
xor ymm5_1@uint64 ymm3_1 ymm10_1;
xor ymm5_2@uint64 ymm3_2 ymm10_2;
xor ymm5_3@uint64 ymm3_3 ymm10_3;
(* vmovdqa %ymm13,-0x350(%rbp)                     #! EA = L0x7fffffffbc00; PC = 0x5555555774ff *)
mov L0x7fffffffbc00 ymm13_0;
mov L0x7fffffffbc08 ymm13_1;
mov L0x7fffffffbc10 ymm13_2;
mov L0x7fffffffbc18 ymm13_3;
(* vmovdqa %ymm12,-0x330(%rbp)                     #! EA = L0x7fffffffbc20; PC = 0x555555577507 *)
mov L0x7fffffffbc20 ymm12_0;
mov L0x7fffffffbc28 ymm12_1;
mov L0x7fffffffbc30 ymm12_2;
mov L0x7fffffffbc38 ymm12_3;
(* vpsrlq $0x24,%ymm7,%ymm3                        #! PC = 0x55555557750f *)
shr ymm3_0 ymm7_0 0x24@uint64;
shr ymm3_1 ymm7_1 0x24@uint64;
shr ymm3_2 ymm7_2 0x24@uint64;
shr ymm3_3 ymm7_3 0x24@uint64;
(* vpsllq $0x1c,%ymm7,%ymm12                       #! PC = 0x555555577514 *)
shl ymm12_0 ymm7_0 0x1c@uint64;
shl ymm12_1 ymm7_1 0x1c@uint64;
shl ymm12_2 ymm7_2 0x1c@uint64;
shl ymm12_3 ymm7_3 0x1c@uint64;
(* vpxor  -0x90(%rbp),%ymm4,%ymm7                  #! EA = L0x7fffffffbec0; Value = 0x0000000000000000; PC = 0x555555577519 *)
xor ymm7_0@uint64 ymm4_0 L0x7fffffffbec0;
xor ymm7_1@uint64 ymm4_1 L0x7fffffffbec8;
xor ymm7_2@uint64 ymm4_2 L0x7fffffffbed0;
xor ymm7_3@uint64 ymm4_3 L0x7fffffffbed8;
(* vmovdqa %ymm5,-0x310(%rbp)                      #! EA = L0x7fffffffbc40; PC = 0x555555577521 *)
mov L0x7fffffffbc40 ymm5_0;
mov L0x7fffffffbc48 ymm5_1;
mov L0x7fffffffbc50 ymm5_2;
mov L0x7fffffffbc58 ymm5_3;
(* vpxor  -0x1b0(%rbp),%ymm0,%ymm5                 #! EA = L0x7fffffffbda0; Value = 0x0000000000000000; PC = 0x555555577529 *)
xor ymm5_0@uint64 ymm0_0 L0x7fffffffbda0;
xor ymm5_1@uint64 ymm0_1 L0x7fffffffbda8;
xor ymm5_2@uint64 ymm0_2 L0x7fffffffbdb0;
xor ymm5_3@uint64 ymm0_3 L0x7fffffffbdb8;
(* vpor   %ymm3,%ymm12,%ymm12                      #! PC = 0x555555577531 *)
or ymm12_0@uint64 ymm12_0 ymm3_0;
or ymm12_1@uint64 ymm12_1 ymm3_1;
or ymm12_2@uint64 ymm12_2 ymm3_2;
or ymm12_3@uint64 ymm12_3 ymm3_3;
(* vpsrlq $0x2c,%ymm7,%ymm3                        #! PC = 0x555555577535 *)
shr ymm3_0 ymm7_0 0x2c@uint64;
shr ymm3_1 ymm7_1 0x2c@uint64;
shr ymm3_2 ymm7_2 0x2c@uint64;
shr ymm3_3 ymm7_3 0x2c@uint64;
(* vpsllq $0x14,%ymm7,%ymm7                        #! PC = 0x55555557753a *)
shl ymm7_0 ymm7_0 0x14@uint64;
shl ymm7_1 ymm7_1 0x14@uint64;
shl ymm7_2 ymm7_2 0x14@uint64;
shl ymm7_3 ymm7_3 0x14@uint64;
(* vpor   %ymm3,%ymm7,%ymm7                        #! PC = 0x55555557753f *)
or ymm7_0@uint64 ymm7_0 ymm3_0;
or ymm7_1@uint64 ymm7_1 ymm3_1;
or ymm7_2@uint64 ymm7_2 ymm3_2;
or ymm7_3@uint64 ymm7_3 ymm3_3;
(* vpsrlq $0x3d,%ymm5,%ymm3                        #! PC = 0x555555577543 *)
shr ymm3_0 ymm5_0 0x3d@uint64;
shr ymm3_1 ymm5_1 0x3d@uint64;
shr ymm3_2 ymm5_2 0x3d@uint64;
shr ymm3_3 ymm5_3 0x3d@uint64;
(* vpsllq $0x3,%ymm5,%ymm5                         #! PC = 0x555555577548 *)
shl ymm5_0 ymm5_0 0x3@uint64;
shl ymm5_1 ymm5_1 0x3@uint64;
shl ymm5_2 ymm5_2 0x3@uint64;
shl ymm5_3 ymm5_3 0x3@uint64;
(* vpor   %ymm3,%ymm5,%ymm5                        #! PC = 0x55555557754d *)
or ymm5_0@uint64 ymm5_0 ymm3_0;
or ymm5_1@uint64 ymm5_1 ymm3_1;
or ymm5_2@uint64 ymm5_2 ymm3_2;
or ymm5_3@uint64 ymm5_3 ymm3_3;
(* vpandn %ymm5,%ymm7,%ymm3                        #! PC = 0x555555577551 *)
not ymm7_0n@uint64 ymm7_0;
and ymm3_0@uint64 ymm7_0n ymm5_0;
not ymm7_1n@uint64 ymm7_1;
and ymm3_1@uint64 ymm7_1n ymm5_1;
not ymm7_2n@uint64 ymm7_2;
and ymm3_2@uint64 ymm7_2n ymm5_2;
not ymm7_3n@uint64 ymm7_3;
and ymm3_3@uint64 ymm7_3n ymm5_3;
(* vpxor  %ymm12,%ymm3,%ymm11                      #! PC = 0x555555577555 *)
xor ymm11_0@uint64 ymm3_0 ymm12_0;
xor ymm11_1@uint64 ymm3_1 ymm12_1;
xor ymm11_2@uint64 ymm3_2 ymm12_2;
xor ymm11_3@uint64 ymm3_3 ymm12_3;
(* vpxor  -0x210(%rbp),%ymm2,%ymm3                 #! EA = L0x7fffffffbd40; Value = 0x0000000000000000; PC = 0x55555557755a *)
xor ymm3_0@uint64 ymm2_0 L0x7fffffffbd40;
xor ymm3_1@uint64 ymm2_1 L0x7fffffffbd48;
xor ymm3_2@uint64 ymm2_2 L0x7fffffffbd50;
xor ymm3_3@uint64 ymm2_3 L0x7fffffffbd58;
(* vmovdqa %ymm11,-0x2f0(%rbp)                     #! EA = L0x7fffffffbc60; PC = 0x555555577562 *)
mov L0x7fffffffbc60 ymm11_0;
mov L0x7fffffffbc68 ymm11_1;
mov L0x7fffffffbc70 ymm11_2;
mov L0x7fffffffbc78 ymm11_3;
(* vpxor  -0x230(%rbp),%ymm1,%ymm11                #! EA = L0x7fffffffbd20; Value = 0x0000000000000000; PC = 0x55555557756a *)
xor ymm11_0@uint64 ymm1_0 L0x7fffffffbd20;
xor ymm11_1@uint64 ymm1_1 L0x7fffffffbd28;
xor ymm11_2@uint64 ymm1_2 L0x7fffffffbd30;
xor ymm11_3@uint64 ymm1_3 L0x7fffffffbd38;
(* vpsrlq $0x13,%ymm3,%ymm8                        #! PC = 0x555555577572 *)
shr ymm8_0 ymm3_0 0x13@uint64;
shr ymm8_1 ymm3_1 0x13@uint64;
shr ymm8_2 ymm3_2 0x13@uint64;
shr ymm8_3 ymm3_3 0x13@uint64;
(* vpsllq $0x2d,%ymm3,%ymm13                       #! PC = 0x555555577577 *)
shl ymm13_0 ymm3_0 0x2d@uint64;
shl ymm13_1 ymm3_1 0x2d@uint64;
shl ymm13_2 ymm3_2 0x2d@uint64;
shl ymm13_3 ymm3_3 0x2d@uint64;
(* vpor   %ymm8,%ymm13,%ymm13                      #! PC = 0x55555557757c *)
or ymm13_0@uint64 ymm13_0 ymm8_0;
or ymm13_1@uint64 ymm13_1 ymm8_1;
or ymm13_2@uint64 ymm13_2 ymm8_2;
or ymm13_3@uint64 ymm13_3 ymm8_3;
(* vpsrlq $0x3,%ymm15,%ymm8                        #! PC = 0x555555577581 *)
shr ymm8_0 ymm15_0 0x3@uint64;
shr ymm8_1 ymm15_1 0x3@uint64;
shr ymm8_2 ymm15_2 0x3@uint64;
shr ymm8_3 ymm15_3 0x3@uint64;
(* vpsllq $0x3d,%ymm15,%ymm15                      #! PC = 0x555555577587 *)
shl ymm15_0 ymm15_0 0x3d@uint64;
shl ymm15_1 ymm15_1 0x3d@uint64;
shl ymm15_2 ymm15_2 0x3d@uint64;
shl ymm15_3 ymm15_3 0x3d@uint64;
(* vpandn %ymm13,%ymm5,%ymm10                      #! PC = 0x55555557758d *)
not ymm5_0n@uint64 ymm5_0;
and ymm10_0@uint64 ymm5_0n ymm13_0;
not ymm5_1n@uint64 ymm5_1;
and ymm10_1@uint64 ymm5_1n ymm13_1;
not ymm5_2n@uint64 ymm5_2;
and ymm10_2@uint64 ymm5_2n ymm13_2;
not ymm5_3n@uint64 ymm5_3;
and ymm10_3@uint64 ymm5_3n ymm13_3;
(* vpor   %ymm8,%ymm15,%ymm3                       #! PC = 0x555555577592 *)
or ymm3_0@uint64 ymm15_0 ymm8_0;
or ymm3_1@uint64 ymm15_1 ymm8_1;
or ymm3_2@uint64 ymm15_2 ymm8_2;
or ymm3_3@uint64 ymm15_3 ymm8_3;
(* vpxor  %ymm7,%ymm10,%ymm10                      #! PC = 0x555555577597 *)
xor ymm10_0@uint64 ymm10_0 ymm7_0;
xor ymm10_1@uint64 ymm10_1 ymm7_1;
xor ymm10_2@uint64 ymm10_2 ymm7_2;
xor ymm10_3@uint64 ymm10_3 ymm7_3;
(* vpandn %ymm3,%ymm13,%ymm8                       #! PC = 0x55555557759b *)
not ymm13_0n@uint64 ymm13_0;
and ymm8_0@uint64 ymm13_0n ymm3_0;
not ymm13_1n@uint64 ymm13_1;
and ymm8_1@uint64 ymm13_1n ymm3_1;
not ymm13_2n@uint64 ymm13_2;
and ymm8_2@uint64 ymm13_2n ymm3_2;
not ymm13_3n@uint64 ymm13_3;
and ymm8_3@uint64 ymm13_3n ymm3_3;
(* vpxor  %ymm5,%ymm8,%ymm15                       #! PC = 0x55555557759f *)
xor ymm15_0@uint64 ymm8_0 ymm5_0;
xor ymm15_1@uint64 ymm8_1 ymm5_1;
xor ymm15_2@uint64 ymm8_2 ymm5_2;
xor ymm15_3@uint64 ymm8_3 ymm5_3;
(* vpandn %ymm12,%ymm3,%ymm5                       #! PC = 0x5555555775a3 *)
not ymm3_0n@uint64 ymm3_0;
and ymm5_0@uint64 ymm3_0n ymm12_0;
not ymm3_1n@uint64 ymm3_1;
and ymm5_1@uint64 ymm3_1n ymm12_1;
not ymm3_2n@uint64 ymm3_2;
and ymm5_2@uint64 ymm3_2n ymm12_2;
not ymm3_3n@uint64 ymm3_3;
and ymm5_3@uint64 ymm3_3n ymm12_3;
(* vpandn %ymm7,%ymm12,%ymm12                      #! PC = 0x5555555775a8 *)
not ymm12_0n@uint64 ymm12_0;
and ymm12_0@uint64 ymm12_0n ymm7_0;
not ymm12_1n@uint64 ymm12_1;
and ymm12_1@uint64 ymm12_1n ymm7_1;
not ymm12_2n@uint64 ymm12_2;
and ymm12_2@uint64 ymm12_2n ymm7_2;
not ymm12_3n@uint64 ymm12_3;
and ymm12_3@uint64 ymm12_3n ymm7_3;
(* vpxor  %ymm13,%ymm5,%ymm9                       #! PC = 0x5555555775ac *)
xor ymm9_0@uint64 ymm5_0 ymm13_0;
xor ymm9_1@uint64 ymm5_1 ymm13_1;
xor ymm9_2@uint64 ymm5_2 ymm13_2;
xor ymm9_3@uint64 ymm5_3 ymm13_3;
(* vpxor  -0x190(%rbp),%ymm1,%ymm8                 #! EA = L0x7fffffffbdc0; Value = 0x0000000000000000; PC = 0x5555555775b1 *)
xor ymm8_0@uint64 ymm1_0 L0x7fffffffbdc0;
xor ymm8_1@uint64 ymm1_1 L0x7fffffffbdc8;
xor ymm8_2@uint64 ymm1_2 L0x7fffffffbdd0;
xor ymm8_3@uint64 ymm1_3 L0x7fffffffbdd8;
(* vpxor  -0x3b0(%rbp),%ymm2,%ymm13                #! EA = L0x7fffffffbba0; Value = 0x0000000000000000; PC = 0x5555555775b9 *)
xor ymm13_0@uint64 ymm2_0 L0x7fffffffbba0;
xor ymm13_1@uint64 ymm2_1 L0x7fffffffbba8;
xor ymm13_2@uint64 ymm2_2 L0x7fffffffbbb0;
xor ymm13_3@uint64 ymm2_3 L0x7fffffffbbb8;
(* vmovdqa %ymm15,-0x2d0(%rbp)                     #! EA = L0x7fffffffbc80; PC = 0x5555555775c1 *)
mov L0x7fffffffbc80 ymm15_0;
mov L0x7fffffffbc88 ymm15_1;
mov L0x7fffffffbc90 ymm15_2;
mov L0x7fffffffbc98 ymm15_3;
(* vpxor  %ymm3,%ymm12,%ymm3                       #! PC = 0x5555555775c9 *)
xor ymm3_0@uint64 ymm12_0 ymm3_0;
xor ymm3_1@uint64 ymm12_1 ymm3_1;
xor ymm3_2@uint64 ymm12_2 ymm3_2;
xor ymm3_3@uint64 ymm12_3 ymm3_3;
(* vpxor  -0xd0(%rbp),%ymm6,%ymm12                 #! EA = L0x7fffffffbe80; Value = 0x0000000000000000; PC = 0x5555555775cd *)
xor ymm12_0@uint64 ymm6_0 L0x7fffffffbe80;
xor ymm12_1@uint64 ymm6_1 L0x7fffffffbe88;
xor ymm12_2@uint64 ymm6_2 L0x7fffffffbe90;
xor ymm12_3@uint64 ymm6_3 L0x7fffffffbe98;
(* vmovdqa %ymm9,-0x2b0(%rbp)                      #! EA = L0x7fffffffbca0; PC = 0x5555555775d5 *)
mov L0x7fffffffbca0 ymm9_0;
mov L0x7fffffffbca8 ymm9_1;
mov L0x7fffffffbcb0 ymm9_2;
mov L0x7fffffffbcb8 ymm9_3;
(* vpsrlq $0x3a,%ymm8,%ymm5                        #! PC = 0x5555555775dd *)
shr ymm5_0 ymm8_0 0x3a@uint64;
shr ymm5_1 ymm8_1 0x3a@uint64;
shr ymm5_2 ymm8_2 0x3a@uint64;
shr ymm5_3 ymm8_3 0x3a@uint64;
(* vmovdqa %ymm3,-0x210(%rbp)                      #! EA = L0x7fffffffbd40; PC = 0x5555555775e3 *)
mov L0x7fffffffbd40 ymm3_0;
mov L0x7fffffffbd48 ymm3_1;
mov L0x7fffffffbd50 ymm3_2;
mov L0x7fffffffbd58 ymm3_3;
(* vpsllq $0x6,%ymm8,%ymm8                         #! PC = 0x5555555775eb *)
shl ymm8_0 ymm8_0 0x6@uint64;
shl ymm8_1 ymm8_1 0x6@uint64;
shl ymm8_2 ymm8_2 0x6@uint64;
shl ymm8_3 ymm8_3 0x6@uint64;
(* vpsrlq $0x3f,%ymm13,%ymm3                       #! PC = 0x5555555775f1 *)
shr ymm3_0 ymm13_0 0x3f@uint64;
shr ymm3_1 ymm13_1 0x3f@uint64;
shr ymm3_2 ymm13_2 0x3f@uint64;
shr ymm3_3 ymm13_3 0x3f@uint64;
(* vpsrlq $0x27,%ymm12,%ymm7                       #! PC = 0x5555555775f7 *)
shr ymm7_0 ymm12_0 0x27@uint64;
shr ymm7_1 ymm12_1 0x27@uint64;
shr ymm7_2 ymm12_2 0x27@uint64;
shr ymm7_3 ymm12_3 0x27@uint64;
(* vpsllq $0x1,%ymm13,%ymm13                       #! PC = 0x5555555775fd *)
shl ymm13_0 ymm13_0 0x1@uint64;
shl ymm13_1 ymm13_1 0x1@uint64;
shl ymm13_2 ymm13_2 0x1@uint64;
shl ymm13_3 ymm13_3 0x1@uint64;
(* vpsllq $0x19,%ymm12,%ymm12                      #! PC = 0x555555577603 *)
shl ymm12_0 ymm12_0 0x19@uint64;
shl ymm12_1 ymm12_1 0x19@uint64;
shl ymm12_2 ymm12_2 0x19@uint64;
shl ymm12_3 ymm12_3 0x19@uint64;
(* vpor   %ymm3,%ymm13,%ymm13                      #! PC = 0x555555577609 *)
or ymm13_0@uint64 ymm13_0 ymm3_0;
or ymm13_1@uint64 ymm13_1 ymm3_1;
or ymm13_2@uint64 ymm13_2 ymm3_2;
or ymm13_3@uint64 ymm13_3 ymm3_3;
(* vpor   %ymm5,%ymm8,%ymm3                        #! PC = 0x55555557760d *)
or ymm3_0@uint64 ymm8_0 ymm5_0;
or ymm3_1@uint64 ymm8_1 ymm5_1;
or ymm3_2@uint64 ymm8_2 ymm5_2;
or ymm3_3@uint64 ymm8_3 ymm5_3;
(* vpor   %ymm7,%ymm12,%ymm5                       #! PC = 0x555555577611 *)
or ymm5_0@uint64 ymm12_0 ymm7_0;
or ymm5_1@uint64 ymm12_1 ymm7_1;
or ymm5_2@uint64 ymm12_2 ymm7_2;
or ymm5_3@uint64 ymm12_3 ymm7_3;
(* vpandn %ymm5,%ymm3,%ymm7                        #! PC = 0x555555577615 *)
not ymm3_0n@uint64 ymm3_0;
and ymm7_0@uint64 ymm3_0n ymm5_0;
not ymm3_1n@uint64 ymm3_1;
and ymm7_1@uint64 ymm3_1n ymm5_1;
not ymm3_2n@uint64 ymm3_2;
and ymm7_2@uint64 ymm3_2n ymm5_2;
not ymm3_3n@uint64 ymm3_3;
and ymm7_3@uint64 ymm3_3n ymm5_3;
(* vpxor  %ymm13,%ymm7,%ymm15                      #! PC = 0x555555577619 *)
xor ymm15_0@uint64 ymm7_0 ymm13_0;
xor ymm15_1@uint64 ymm7_1 ymm13_1;
xor ymm15_2@uint64 ymm7_2 ymm13_2;
xor ymm15_3@uint64 ymm7_3 ymm13_3;
(* vpandn %ymm14,%ymm5,%ymm7                       #! PC = 0x55555557761e *)
not ymm5_0n@uint64 ymm5_0;
and ymm7_0@uint64 ymm5_0n ymm14_0;
not ymm5_1n@uint64 ymm5_1;
and ymm7_1@uint64 ymm5_1n ymm14_1;
not ymm5_2n@uint64 ymm5_2;
and ymm7_2@uint64 ymm5_2n ymm14_2;
not ymm5_3n@uint64 ymm5_3;
and ymm7_3@uint64 ymm5_3n ymm14_3;
(* vpxor  %ymm3,%ymm7,%ymm8                        #! PC = 0x555555577623 *)
xor ymm8_0@uint64 ymm7_0 ymm3_0;
xor ymm8_1@uint64 ymm7_1 ymm3_1;
xor ymm8_2@uint64 ymm7_2 ymm3_2;
xor ymm8_3@uint64 ymm7_3 ymm3_3;
(* vpxor  -0x250(%rbp),%ymm0,%ymm7                 #! EA = L0x7fffffffbd00; Value = 0x0000000000000000; PC = 0x555555577627 *)
xor ymm7_0@uint64 ymm0_0 L0x7fffffffbd00;
xor ymm7_1@uint64 ymm0_1 L0x7fffffffbd08;
xor ymm7_2@uint64 ymm0_2 L0x7fffffffbd10;
xor ymm7_3@uint64 ymm0_3 L0x7fffffffbd18;
(* vmovdqa %ymm15,-0xd0(%rbp)                      #! EA = L0x7fffffffbe80; PC = 0x55555557762f *)
mov L0x7fffffffbe80 ymm15_0;
mov L0x7fffffffbe88 ymm15_1;
mov L0x7fffffffbe90 ymm15_2;
mov L0x7fffffffbe98 ymm15_3;
(* vmovdqa %ymm8,-0x290(%rbp)                      #! EA = L0x7fffffffbcc0; PC = 0x555555577637 *)
mov L0x7fffffffbcc0 ymm8_0;
mov L0x7fffffffbcc8 ymm8_1;
mov L0x7fffffffbcd0 ymm8_2;
mov L0x7fffffffbcd8 ymm8_3;
(* vpsrlq $0x2e,%ymm7,%ymm9                        #! PC = 0x55555557763f *)
shr ymm9_0 ymm7_0 0x2e@uint64;
shr ymm9_1 ymm7_1 0x2e@uint64;
shr ymm9_2 ymm7_2 0x2e@uint64;
shr ymm9_3 ymm7_3 0x2e@uint64;
(* vpsllq $0x12,%ymm7,%ymm8                        #! PC = 0x555555577644 *)
shl ymm8_0 ymm7_0 0x12@uint64;
shl ymm8_1 ymm7_1 0x12@uint64;
shl ymm8_2 ymm7_2 0x12@uint64;
shl ymm8_3 ymm7_3 0x12@uint64;
(* vpor   %ymm9,%ymm8,%ymm8                        #! PC = 0x555555577649 *)
or ymm8_0@uint64 ymm8_0 ymm9_0;
or ymm8_1@uint64 ymm8_1 ymm9_1;
or ymm8_2@uint64 ymm8_2 ymm9_2;
or ymm8_3@uint64 ymm8_3 ymm9_3;
(* vpandn %ymm13,%ymm8,%ymm9                       #! PC = 0x55555557764e *)
not ymm8_0n@uint64 ymm8_0;
and ymm9_0@uint64 ymm8_0n ymm13_0;
not ymm8_1n@uint64 ymm8_1;
and ymm9_1@uint64 ymm8_1n ymm13_1;
not ymm8_2n@uint64 ymm8_2;
and ymm9_2@uint64 ymm8_2n ymm13_2;
not ymm8_3n@uint64 ymm8_3;
and ymm9_3@uint64 ymm8_3n ymm13_3;
(* vpandn %ymm8,%ymm14,%ymm12                      #! PC = 0x555555577653 *)
not ymm14_0n@uint64 ymm14_0;
and ymm12_0@uint64 ymm14_0n ymm8_0;
not ymm14_1n@uint64 ymm14_1;
and ymm12_1@uint64 ymm14_1n ymm8_1;
not ymm14_2n@uint64 ymm14_2;
and ymm12_2@uint64 ymm14_2n ymm8_2;
not ymm14_3n@uint64 ymm14_3;
and ymm12_3@uint64 ymm14_3n ymm8_3;
(* vpandn %ymm3,%ymm13,%ymm13                      #! PC = 0x555555577658 *)
not ymm13_0n@uint64 ymm13_0;
and ymm13_0@uint64 ymm13_0n ymm3_0;
not ymm13_1n@uint64 ymm13_1;
and ymm13_1@uint64 ymm13_1n ymm3_1;
not ymm13_2n@uint64 ymm13_2;
and ymm13_2@uint64 ymm13_2n ymm3_2;
not ymm13_3n@uint64 ymm13_3;
and ymm13_3@uint64 ymm13_3n ymm3_3;
(* vpxor  %ymm14,%ymm9,%ymm14                      #! PC = 0x55555557765c *)
xor ymm14_0@uint64 ymm9_0 ymm14_0;
xor ymm14_1@uint64 ymm9_1 ymm14_1;
xor ymm14_2@uint64 ymm9_2 ymm14_2;
xor ymm14_3@uint64 ymm9_3 ymm14_3;
(* vpxor  -0x50(%rbp),%ymm4,%ymm9                  #! EA = L0x7fffffffbf00; Value = 0x0000000000000000; PC = 0x555555577661 *)
xor ymm9_0@uint64 ymm4_0 L0x7fffffffbf00;
xor ymm9_1@uint64 ymm4_1 L0x7fffffffbf08;
xor ymm9_2@uint64 ymm4_2 L0x7fffffffbf10;
xor ymm9_3@uint64 ymm4_3 L0x7fffffffbf18;
(* vpxor  %ymm5,%ymm12,%ymm12                      #! PC = 0x555555577666 *)
xor ymm12_0@uint64 ymm12_0 ymm5_0;
xor ymm12_1@uint64 ymm12_1 ymm5_1;
xor ymm12_2@uint64 ymm12_2 ymm5_2;
xor ymm12_3@uint64 ymm12_3 ymm5_3;
(* vpxor  %ymm8,%ymm13,%ymm13                      #! PC = 0x55555557766a *)
xor ymm13_0@uint64 ymm13_0 ymm8_0;
xor ymm13_1@uint64 ymm13_1 ymm8_1;
xor ymm13_2@uint64 ymm13_2 ymm8_2;
xor ymm13_3@uint64 ymm13_3 ymm8_3;
(* vpxor  -0x3d0(%rbp),%ymm2,%ymm8                 #! EA = L0x7fffffffbb80; Value = 0x0000000000000000; PC = 0x55555557766f *)
xor ymm8_0@uint64 ymm2_0 L0x7fffffffbb80;
xor ymm8_1@uint64 ymm2_1 L0x7fffffffbb88;
xor ymm8_2@uint64 ymm2_2 L0x7fffffffbb90;
xor ymm8_3@uint64 ymm2_3 L0x7fffffffbb98;
(* vmovdqa %ymm14,-0x170(%rbp)                     #! EA = L0x7fffffffbde0; PC = 0x555555577677 *)
mov L0x7fffffffbde0 ymm14_0;
mov L0x7fffffffbde8 ymm14_1;
mov L0x7fffffffbdf0 ymm14_2;
mov L0x7fffffffbdf8 ymm14_3;
(* vpsrlq $0x25,%ymm9,%ymm5                        #! PC = 0x55555557767f *)
shr ymm5_0 ymm9_0 0x25@uint64;
shr ymm5_1 ymm9_1 0x25@uint64;
shr ymm5_2 ymm9_2 0x25@uint64;
shr ymm5_3 ymm9_3 0x25@uint64;
(* vpsllq $0x1b,%ymm9,%ymm9                        #! PC = 0x555555577685 *)
shl ymm9_0 ymm9_0 0x1b@uint64;
shl ymm9_1 ymm9_1 0x1b@uint64;
shl ymm9_2 ymm9_2 0x1b@uint64;
shl ymm9_3 ymm9_3 0x1b@uint64;
(* vmovdqa %ymm13,-0x1f0(%rbp)                     #! EA = L0x7fffffffbd60; PC = 0x55555557768b *)
mov L0x7fffffffbd60 ymm13_0;
mov L0x7fffffffbd68 ymm13_1;
mov L0x7fffffffbd70 ymm13_2;
mov L0x7fffffffbd78 ymm13_3;
(* vpxor  -0x270(%rbp),%ymm6,%ymm13                #! EA = L0x7fffffffbce0; Value = 0x0000000000000000; PC = 0x555555577693 *)
xor ymm13_0@uint64 ymm6_0 L0x7fffffffbce0;
xor ymm13_1@uint64 ymm6_1 L0x7fffffffbce8;
xor ymm13_2@uint64 ymm6_2 L0x7fffffffbcf0;
xor ymm13_3@uint64 ymm6_3 L0x7fffffffbcf8;
(* vpor   %ymm5,%ymm9,%ymm3                        #! PC = 0x55555557769b *)
or ymm3_0@uint64 ymm9_0 ymm5_0;
or ymm3_1@uint64 ymm9_1 ymm5_1;
or ymm3_2@uint64 ymm9_2 ymm5_2;
or ymm3_3@uint64 ymm9_3 ymm5_3;
(* vpxor  -0x150(%rbp),%ymm0,%ymm9                 #! EA = L0x7fffffffbe00; Value = 0x0000000000000000; PC = 0x55555557769f *)
xor ymm9_0@uint64 ymm0_0 L0x7fffffffbe00;
xor ymm9_1@uint64 ymm0_1 L0x7fffffffbe08;
xor ymm9_2@uint64 ymm0_2 L0x7fffffffbe10;
xor ymm9_3@uint64 ymm0_3 L0x7fffffffbe18;
(* vpshufb 0x56830(%rip),%ymm13,%ymm13        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x5555555776a7 *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm13_0, ymm13_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm13_2, ymm13_3, tmp_2, tmp_3);
mov ymm13_0 tmp_0;
mov ymm13_1 tmp_1;
mov ymm13_2 tmp_2;
mov ymm13_3 tmp_3;
(* vpsrlq $0x1c,%ymm9,%ymm5                        #! PC = 0x5555555776b0 *)
shr ymm5_0 ymm9_0 0x1c@uint64;
shr ymm5_1 ymm9_1 0x1c@uint64;
shr ymm5_2 ymm9_2 0x1c@uint64;
shr ymm5_3 ymm9_3 0x1c@uint64;
(* vpsllq $0x24,%ymm9,%ymm9                        #! PC = 0x5555555776b6 *)
shl ymm9_0 ymm9_0 0x24@uint64;
shl ymm9_1 ymm9_1 0x24@uint64;
shl ymm9_2 ymm9_2 0x24@uint64;
shl ymm9_3 ymm9_3 0x24@uint64;
(* vpor   %ymm5,%ymm9,%ymm7                        #! PC = 0x5555555776bc *)
or ymm7_0@uint64 ymm9_0 ymm5_0;
or ymm7_1@uint64 ymm9_1 ymm5_1;
or ymm7_2@uint64 ymm9_2 ymm5_2;
or ymm7_3@uint64 ymm9_3 ymm5_3;
(* vpsrlq $0x36,%ymm8,%ymm5                        #! PC = 0x5555555776c0 *)
shr ymm5_0 ymm8_0 0x36@uint64;
shr ymm5_1 ymm8_1 0x36@uint64;
shr ymm5_2 ymm8_2 0x36@uint64;
shr ymm5_3 ymm8_3 0x36@uint64;
(* vpsllq $0xa,%ymm8,%ymm8                         #! PC = 0x5555555776c6 *)
shl ymm8_0 ymm8_0 0xa@uint64;
shl ymm8_1 ymm8_1 0xa@uint64;
shl ymm8_2 ymm8_2 0xa@uint64;
shl ymm8_3 ymm8_3 0xa@uint64;
(* vpsrlq $0x31,%ymm11,%ymm9                       #! PC = 0x5555555776cc *)
shr ymm9_0 ymm11_0 0x31@uint64;
shr ymm9_1 ymm11_1 0x31@uint64;
shr ymm9_2 ymm11_2 0x31@uint64;
shr ymm9_3 ymm11_3 0x31@uint64;
(* vpor   %ymm5,%ymm8,%ymm8                        #! PC = 0x5555555776d2 *)
or ymm8_0@uint64 ymm8_0 ymm5_0;
or ymm8_1@uint64 ymm8_1 ymm5_1;
or ymm8_2@uint64 ymm8_2 ymm5_2;
or ymm8_3@uint64 ymm8_3 ymm5_3;
(* vpsllq $0xf,%ymm11,%ymm11                       #! PC = 0x5555555776d6 *)
shl ymm11_0 ymm11_0 0xf@uint64;
shl ymm11_1 ymm11_1 0xf@uint64;
shl ymm11_2 ymm11_2 0xf@uint64;
shl ymm11_3 ymm11_3 0xf@uint64;
(* vpandn %ymm8,%ymm7,%ymm5                        #! PC = 0x5555555776dc *)
not ymm7_0n@uint64 ymm7_0;
and ymm5_0@uint64 ymm7_0n ymm8_0;
not ymm7_1n@uint64 ymm7_1;
and ymm5_1@uint64 ymm7_1n ymm8_1;
not ymm7_2n@uint64 ymm7_2;
and ymm5_2@uint64 ymm7_2n ymm8_2;
not ymm7_3n@uint64 ymm7_3;
and ymm5_3@uint64 ymm7_3n ymm8_3;
(* vpxor  %ymm3,%ymm5,%ymm14                       #! PC = 0x5555555776e1 *)
xor ymm14_0@uint64 ymm5_0 ymm3_0;
xor ymm14_1@uint64 ymm5_1 ymm3_1;
xor ymm14_2@uint64 ymm5_2 ymm3_2;
xor ymm14_3@uint64 ymm5_3 ymm3_3;
(* vpor   %ymm9,%ymm11,%ymm5                       #! PC = 0x5555555776e5 *)
or ymm5_0@uint64 ymm11_0 ymm9_0;
or ymm5_1@uint64 ymm11_1 ymm9_1;
or ymm5_2@uint64 ymm11_2 ymm9_2;
or ymm5_3@uint64 ymm11_3 ymm9_3;
(* vpandn %ymm13,%ymm5,%ymm11                      #! PC = 0x5555555776ea *)
not ymm5_0n@uint64 ymm5_0;
and ymm11_0@uint64 ymm5_0n ymm13_0;
not ymm5_1n@uint64 ymm5_1;
and ymm11_1@uint64 ymm5_1n ymm13_1;
not ymm5_2n@uint64 ymm5_2;
and ymm11_2@uint64 ymm5_2n ymm13_2;
not ymm5_3n@uint64 ymm5_3;
and ymm11_3@uint64 ymm5_3n ymm13_3;
(* vpandn %ymm5,%ymm8,%ymm9                        #! PC = 0x5555555776ef *)
not ymm8_0n@uint64 ymm8_0;
and ymm9_0@uint64 ymm8_0n ymm5_0;
not ymm8_1n@uint64 ymm8_1;
and ymm9_1@uint64 ymm8_1n ymm5_1;
not ymm8_2n@uint64 ymm8_2;
and ymm9_2@uint64 ymm8_2n ymm5_2;
not ymm8_3n@uint64 ymm8_3;
and ymm9_3@uint64 ymm8_3n ymm5_3;
(* vpxor  %ymm8,%ymm11,%ymm11                      #! PC = 0x5555555776f3 *)
xor ymm11_0@uint64 ymm11_0 ymm8_0;
xor ymm11_1@uint64 ymm11_1 ymm8_1;
xor ymm11_2@uint64 ymm11_2 ymm8_2;
xor ymm11_3@uint64 ymm11_3 ymm8_3;
(* vpxor  %ymm7,%ymm9,%ymm9                        #! PC = 0x5555555776f8 *)
xor ymm9_0@uint64 ymm9_0 ymm7_0;
xor ymm9_1@uint64 ymm9_1 ymm7_1;
xor ymm9_2@uint64 ymm9_2 ymm7_2;
xor ymm9_3@uint64 ymm9_3 ymm7_3;
(* vmovdqa %ymm11,-0x130(%rbp)                     #! EA = L0x7fffffffbe20; PC = 0x5555555776fc *)
mov L0x7fffffffbe20 ymm11_0;
mov L0x7fffffffbe28 ymm11_1;
mov L0x7fffffffbe30 ymm11_2;
mov L0x7fffffffbe38 ymm11_3;
(* vpandn %ymm3,%ymm13,%ymm11                      #! PC = 0x555555577704 *)
not ymm13_0n@uint64 ymm13_0;
and ymm11_0@uint64 ymm13_0n ymm3_0;
not ymm13_1n@uint64 ymm13_1;
and ymm11_1@uint64 ymm13_1n ymm3_1;
not ymm13_2n@uint64 ymm13_2;
and ymm11_2@uint64 ymm13_2n ymm3_2;
not ymm13_3n@uint64 ymm13_3;
and ymm11_3@uint64 ymm13_3n ymm3_3;
(* vpandn %ymm7,%ymm3,%ymm3                        #! PC = 0x555555577708 *)
not ymm3_0n@uint64 ymm3_0;
and ymm3_0@uint64 ymm3_0n ymm7_0;
not ymm3_1n@uint64 ymm3_1;
and ymm3_1@uint64 ymm3_1n ymm7_1;
not ymm3_2n@uint64 ymm3_2;
and ymm3_2@uint64 ymm3_2n ymm7_2;
not ymm3_3n@uint64 ymm3_3;
and ymm3_3@uint64 ymm3_3n ymm7_3;
(* vpxor  %ymm5,%ymm11,%ymm11                      #! PC = 0x55555557770c *)
xor ymm11_0@uint64 ymm11_0 ymm5_0;
xor ymm11_1@uint64 ymm11_1 ymm5_1;
xor ymm11_2@uint64 ymm11_2 ymm5_2;
xor ymm11_3@uint64 ymm11_3 ymm5_3;
(* vpxor  %ymm13,%ymm3,%ymm5                       #! PC = 0x555555577710 *)
xor ymm5_0@uint64 ymm3_0 ymm13_0;
xor ymm5_1@uint64 ymm3_1 ymm13_1;
xor ymm5_2@uint64 ymm3_2 ymm13_2;
xor ymm5_3@uint64 ymm3_3 ymm13_3;
(* vmovdqa %ymm5,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x555555577715 *)
mov L0x7fffffffbf00 ymm5_0;
mov L0x7fffffffbf08 ymm5_1;
mov L0x7fffffffbf10 ymm5_2;
mov L0x7fffffffbf18 ymm5_3;
(* vpxor  -0x110(%rbp),%ymm1,%ymm1                 #! EA = L0x7fffffffbe40; Value = 0x0000000000000000; PC = 0x55555557771a *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbe40;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbe48;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbe50;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbe58;
(* vpxor  -0x70(%rbp),%ymm6,%ymm6                  #! EA = L0x7fffffffbee0; Value = 0x0000000000000000; PC = 0x555555577722 *)
xor ymm6_0@uint64 ymm6_0 L0x7fffffffbee0;
xor ymm6_1@uint64 ymm6_1 L0x7fffffffbee8;
xor ymm6_2@uint64 ymm6_2 L0x7fffffffbef0;
xor ymm6_3@uint64 ymm6_3 L0x7fffffffbef8;
(* vpxor  -0xb0(%rbp),%ymm4,%ymm4                  #! EA = L0x7fffffffbea0; Value = 0x0000000000000000; PC = 0x555555577727 *)
xor ymm4_0@uint64 ymm4_0 L0x7fffffffbea0;
xor ymm4_1@uint64 ymm4_1 L0x7fffffffbea8;
xor ymm4_2@uint64 ymm4_2 L0x7fffffffbeb0;
xor ymm4_3@uint64 ymm4_3 L0x7fffffffbeb8;
(* vpxor  -0x1d0(%rbp),%ymm0,%ymm0                 #! EA = L0x7fffffffbd80; Value = 0x0000000000000000; PC = 0x55555557772f *)
xor ymm0_0@uint64 ymm0_0 L0x7fffffffbd80;
xor ymm0_1@uint64 ymm0_1 L0x7fffffffbd88;
xor ymm0_2@uint64 ymm0_2 L0x7fffffffbd90;
xor ymm0_3@uint64 ymm0_3 L0x7fffffffbd98;
(* vmovdqa %ymm14,-0x3b0(%rbp)                     #! EA = L0x7fffffffbba0; PC = 0x555555577737 *)
mov L0x7fffffffbba0 ymm14_0;
mov L0x7fffffffbba8 ymm14_1;
mov L0x7fffffffbbb0 ymm14_2;
mov L0x7fffffffbbb8 ymm14_3;
(* vpsrlq $0x2,%ymm1,%ymm3                         #! PC = 0x55555557773f *)
shr ymm3_0 ymm1_0 0x2@uint64;
shr ymm3_1 ymm1_1 0x2@uint64;
shr ymm3_2 ymm1_2 0x2@uint64;
shr ymm3_3 ymm1_3 0x2@uint64;
(* vpsllq $0x3e,%ymm1,%ymm1                        #! PC = 0x555555577744 *)
shl ymm1_0 ymm1_0 0x3e@uint64;
shl ymm1_1 ymm1_1 0x3e@uint64;
shl ymm1_2 ymm1_2 0x3e@uint64;
shl ymm1_3 ymm1_3 0x3e@uint64;
(* vpxor  -0xf0(%rbp),%ymm2,%ymm2                  #! EA = L0x7fffffffbe60; Value = 0x0000000000000000; PC = 0x555555577749 *)
xor ymm2_0@uint64 ymm2_0 L0x7fffffffbe60;
xor ymm2_1@uint64 ymm2_1 L0x7fffffffbe68;
xor ymm2_2@uint64 ymm2_2 L0x7fffffffbe70;
xor ymm2_3@uint64 ymm2_3 L0x7fffffffbe78;
(* vpor   %ymm3,%ymm1,%ymm15                       #! PC = 0x555555577751 *)
or ymm15_0@uint64 ymm1_0 ymm3_0;
or ymm15_1@uint64 ymm1_1 ymm3_1;
or ymm15_2@uint64 ymm1_2 ymm3_2;
or ymm15_3@uint64 ymm1_3 ymm3_3;
(* vpsrlq $0x9,%ymm6,%ymm3                         #! PC = 0x555555577755 *)
shr ymm3_0 ymm6_0 0x9@uint64;
shr ymm3_1 ymm6_1 0x9@uint64;
shr ymm3_2 ymm6_2 0x9@uint64;
shr ymm3_3 ymm6_3 0x9@uint64;
(* vpsrlq $0x19,%ymm4,%ymm1                        #! PC = 0x55555557775a *)
shr ymm1_0 ymm4_0 0x19@uint64;
shr ymm1_1 ymm4_1 0x19@uint64;
shr ymm1_2 ymm4_2 0x19@uint64;
shr ymm1_3 ymm4_3 0x19@uint64;
(* vpsllq $0x37,%ymm6,%ymm6                        #! PC = 0x55555557775f *)
shl ymm6_0 ymm6_0 0x37@uint64;
shl ymm6_1 ymm6_1 0x37@uint64;
shl ymm6_2 ymm6_2 0x37@uint64;
shl ymm6_3 ymm6_3 0x37@uint64;
(* vpsllq $0x27,%ymm4,%ymm4                        #! PC = 0x555555577764 *)
shl ymm4_0 ymm4_0 0x27@uint64;
shl ymm4_1 ymm4_1 0x27@uint64;
shl ymm4_2 ymm4_2 0x27@uint64;
shl ymm4_3 ymm4_3 0x27@uint64;
(* vpsrlq $0x3e,%ymm2,%ymm7                        #! PC = 0x555555577769 *)
shr ymm7_0 ymm2_0 0x3e@uint64;
shr ymm7_1 ymm2_1 0x3e@uint64;
shr ymm7_2 ymm2_2 0x3e@uint64;
shr ymm7_3 ymm2_3 0x3e@uint64;
(* vpor   %ymm3,%ymm6,%ymm5                        #! PC = 0x55555557776e *)
or ymm5_0@uint64 ymm6_0 ymm3_0;
or ymm5_1@uint64 ymm6_1 ymm3_1;
or ymm5_2@uint64 ymm6_2 ymm3_2;
or ymm5_3@uint64 ymm6_3 ymm3_3;
(* vpor   %ymm1,%ymm4,%ymm4                        #! PC = 0x555555577772 *)
or ymm4_0@uint64 ymm4_0 ymm1_0;
or ymm4_1@uint64 ymm4_1 ymm1_1;
or ymm4_2@uint64 ymm4_2 ymm1_2;
or ymm4_3@uint64 ymm4_3 ymm1_3;
(* vmovdqa -0xd0(%rbp),%ymm1                       #! EA = L0x7fffffffbe80; Value = 0x0000000000000000; PC = 0x555555577776 *)
mov ymm1_0 L0x7fffffffbe80;
mov ymm1_1 L0x7fffffffbe88;
mov ymm1_2 L0x7fffffffbe90;
mov ymm1_3 L0x7fffffffbe98;
(* vpsllq $0x2,%ymm2,%ymm2                         #! PC = 0x55555557777e *)
shl ymm2_0 ymm2_0 0x2@uint64;
shl ymm2_1 ymm2_1 0x2@uint64;
shl ymm2_2 ymm2_2 0x2@uint64;
shl ymm2_3 ymm2_3 0x2@uint64;
(* vpxor  -0x2f0(%rbp),%ymm1,%ymm1                 #! EA = L0x7fffffffbc60; Value = 0x0000000000000000; PC = 0x555555577783 *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbc60;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbc68;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbc70;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbc78;
(* vpandn %ymm4,%ymm5,%ymm6                        #! PC = 0x55555557778b *)
not ymm5_0n@uint64 ymm5_0;
and ymm6_0@uint64 ymm5_0n ymm4_0;
not ymm5_1n@uint64 ymm5_1;
and ymm6_1@uint64 ymm5_1n ymm4_1;
not ymm5_2n@uint64 ymm5_2;
and ymm6_2@uint64 ymm5_2n ymm4_2;
not ymm5_3n@uint64 ymm5_3;
and ymm6_3@uint64 ymm5_3n ymm4_3;
(* vpxor  %ymm15,%ymm6,%ymm6                       #! PC = 0x55555557778f *)
xor ymm6_0@uint64 ymm6_0 ymm15_0;
xor ymm6_1@uint64 ymm6_1 ymm15_1;
xor ymm6_2@uint64 ymm6_2 ymm15_2;
xor ymm6_3@uint64 ymm6_3 ymm15_3;
(* vpxor  %ymm14,%ymm6,%ymm3                       #! PC = 0x555555577794 *)
xor ymm3_0@uint64 ymm6_0 ymm14_0;
xor ymm3_1@uint64 ymm6_1 ymm14_1;
xor ymm3_2@uint64 ymm6_2 ymm14_2;
xor ymm3_3@uint64 ymm6_3 ymm14_3;
(* vpxor  -0x290(%rbp),%ymm9,%ymm14                #! EA = L0x7fffffffbcc0; Value = 0x0000000000000000; PC = 0x555555577799 *)
xor ymm14_0@uint64 ymm9_0 L0x7fffffffbcc0;
xor ymm14_1@uint64 ymm9_1 L0x7fffffffbcc8;
xor ymm14_2@uint64 ymm9_2 L0x7fffffffbcd0;
xor ymm14_3@uint64 ymm9_3 L0x7fffffffbcd8;
(* vpxor  %ymm1,%ymm3,%ymm3                        #! PC = 0x5555555777a1 *)
xor ymm3_0@uint64 ymm3_0 ymm1_0;
xor ymm3_1@uint64 ymm3_1 ymm1_1;
xor ymm3_2@uint64 ymm3_2 ymm1_2;
xor ymm3_3@uint64 ymm3_3 ymm1_3;
(* vpsrlq $0x17,%ymm0,%ymm1                        #! PC = 0x5555555777a5 *)
shr ymm1_0 ymm0_0 0x17@uint64;
shr ymm1_1 ymm0_1 0x17@uint64;
shr ymm1_2 ymm0_2 0x17@uint64;
shr ymm1_3 ymm0_3 0x17@uint64;
(* vpxor  -0x390(%rbp),%ymm3,%ymm3                 #! EA = L0x7fffffffbbc0; Value = 0x0000000000000001; PC = 0x5555555777aa *)
xor ymm3_0@uint64 ymm3_0 L0x7fffffffbbc0;
xor ymm3_1@uint64 ymm3_1 L0x7fffffffbbc8;
xor ymm3_2@uint64 ymm3_2 L0x7fffffffbbd0;
xor ymm3_3@uint64 ymm3_3 L0x7fffffffbbd8;
(* vpsllq $0x29,%ymm0,%ymm0                        #! PC = 0x5555555777b2 *)
shl ymm0_0 ymm0_0 0x29@uint64;
shl ymm0_1 ymm0_1 0x29@uint64;
shl ymm0_2 ymm0_2 0x29@uint64;
shl ymm0_3 ymm0_3 0x29@uint64;
(* vpor   %ymm1,%ymm0,%ymm8                        #! PC = 0x5555555777b7 *)
or ymm8_0@uint64 ymm0_0 ymm1_0;
or ymm8_1@uint64 ymm0_1 ymm1_1;
or ymm8_2@uint64 ymm0_2 ymm1_2;
or ymm8_3@uint64 ymm0_3 ymm1_3;
(* vpandn %ymm8,%ymm4,%ymm0                        #! PC = 0x5555555777bb *)
not ymm4_0n@uint64 ymm4_0;
and ymm0_0@uint64 ymm4_0n ymm8_0;
not ymm4_1n@uint64 ymm4_1;
and ymm0_1@uint64 ymm4_1n ymm8_1;
not ymm4_2n@uint64 ymm4_2;
and ymm0_2@uint64 ymm4_2n ymm8_2;
not ymm4_3n@uint64 ymm4_3;
and ymm0_3@uint64 ymm4_3n ymm8_3;
(* vpxor  %ymm5,%ymm0,%ymm0                        #! PC = 0x5555555777c0 *)
xor ymm0_0@uint64 ymm0_0 ymm5_0;
xor ymm0_1@uint64 ymm0_1 ymm5_1;
xor ymm0_2@uint64 ymm0_2 ymm5_2;
xor ymm0_3@uint64 ymm0_3 ymm5_3;
(* vmovdqa %ymm0,%ymm13                            #! PC = 0x5555555777c4 *)
mov ymm13_0 ymm0_0;
mov ymm13_1 ymm0_1;
mov ymm13_2 ymm0_2;
mov ymm13_3 ymm0_3;
(* vpxor  -0x370(%rbp),%ymm10,%ymm0                #! EA = L0x7fffffffbbe0; Value = 0x0000000000000000; PC = 0x5555555777c8 *)
xor ymm0_0@uint64 ymm10_0 L0x7fffffffbbe0;
xor ymm0_1@uint64 ymm10_1 L0x7fffffffbbe8;
xor ymm0_2@uint64 ymm10_2 L0x7fffffffbbf0;
xor ymm0_3@uint64 ymm10_3 L0x7fffffffbbf8;
(* vmovdqa %ymm13,-0x270(%rbp)                     #! EA = L0x7fffffffbce0; PC = 0x5555555777d0 *)
mov L0x7fffffffbce0 ymm13_0;
mov L0x7fffffffbce8 ymm13_1;
mov L0x7fffffffbcf0 ymm13_2;
mov L0x7fffffffbcf8 ymm13_3;
(* vpxor  %ymm0,%ymm14,%ymm14                      #! PC = 0x5555555777d8 *)
xor ymm14_0@uint64 ymm14_0 ymm0_0;
xor ymm14_1@uint64 ymm14_1 ymm0_1;
xor ymm14_2@uint64 ymm14_2 ymm0_2;
xor ymm14_3@uint64 ymm14_3 ymm0_3;
(* vpor   %ymm7,%ymm2,%ymm0                        #! PC = 0x5555555777dc *)
or ymm0_0@uint64 ymm2_0 ymm7_0;
or ymm0_1@uint64 ymm2_1 ymm7_1;
or ymm0_2@uint64 ymm2_2 ymm7_2;
or ymm0_3@uint64 ymm2_3 ymm7_3;
(* vmovdqa -0x2d0(%rbp),%ymm2                      #! EA = L0x7fffffffbc80; Value = 0x0000000000000000; PC = 0x5555555777e0 *)
mov ymm2_0 L0x7fffffffbc80;
mov ymm2_1 L0x7fffffffbc88;
mov ymm2_2 L0x7fffffffbc90;
mov ymm2_3 L0x7fffffffbc98;
(* vpxor  -0x350(%rbp),%ymm2,%ymm1                 #! EA = L0x7fffffffbc00; Value = 0x0000000000000000; PC = 0x5555555777e8 *)
xor ymm1_0@uint64 ymm2_0 L0x7fffffffbc00;
xor ymm1_1@uint64 ymm2_1 L0x7fffffffbc08;
xor ymm1_2@uint64 ymm2_2 L0x7fffffffbc10;
xor ymm1_3@uint64 ymm2_3 L0x7fffffffbc18;
(* vpxor  %ymm13,%ymm14,%ymm14                     #! PC = 0x5555555777f0 *)
xor ymm14_0@uint64 ymm14_0 ymm13_0;
xor ymm14_1@uint64 ymm14_1 ymm13_1;
xor ymm14_2@uint64 ymm14_2 ymm13_2;
xor ymm14_3@uint64 ymm14_3 ymm13_3;
(* vpxor  -0x130(%rbp),%ymm12,%ymm13               #! EA = L0x7fffffffbe20; Value = 0x0000000000000000; PC = 0x5555555777f5 *)
xor ymm13_0@uint64 ymm12_0 L0x7fffffffbe20;
xor ymm13_1@uint64 ymm12_1 L0x7fffffffbe28;
xor ymm13_2@uint64 ymm12_2 L0x7fffffffbe30;
xor ymm13_3@uint64 ymm12_3 L0x7fffffffbe38;
(* vpandn %ymm0,%ymm8,%ymm7                        #! PC = 0x5555555777fd *)
not ymm8_0n@uint64 ymm8_0;
and ymm7_0@uint64 ymm8_0n ymm0_0;
not ymm8_1n@uint64 ymm8_1;
and ymm7_1@uint64 ymm8_1n ymm0_1;
not ymm8_2n@uint64 ymm8_2;
and ymm7_2@uint64 ymm8_2n ymm0_2;
not ymm8_3n@uint64 ymm8_3;
and ymm7_3@uint64 ymm8_3n ymm0_3;
(* vpxor  %ymm4,%ymm7,%ymm7                        #! PC = 0x555555577801 *)
xor ymm7_0@uint64 ymm7_0 ymm4_0;
xor ymm7_1@uint64 ymm7_1 ymm4_1;
xor ymm7_2@uint64 ymm7_2 ymm4_2;
xor ymm7_3@uint64 ymm7_3 ymm4_3;
(* vmovdqa -0x2b0(%rbp),%ymm4                      #! EA = L0x7fffffffbca0; Value = 0x0000000000000000; PC = 0x555555577805 *)
mov ymm4_0 L0x7fffffffbca0;
mov ymm4_1 L0x7fffffffbca8;
mov ymm4_2 L0x7fffffffbcb0;
mov ymm4_3 L0x7fffffffbcb8;
(* vpxor  %ymm1,%ymm13,%ymm13                      #! PC = 0x55555557780d *)
xor ymm13_0@uint64 ymm13_0 ymm1_0;
xor ymm13_1@uint64 ymm13_1 ymm1_1;
xor ymm13_2@uint64 ymm13_2 ymm1_2;
xor ymm13_3@uint64 ymm13_3 ymm1_3;
(* vpandn %ymm15,%ymm0,%ymm1                       #! PC = 0x555555577811 *)
not ymm0_0n@uint64 ymm0_0;
and ymm1_0@uint64 ymm0_0n ymm15_0;
not ymm0_1n@uint64 ymm0_1;
and ymm1_1@uint64 ymm0_1n ymm15_1;
not ymm0_2n@uint64 ymm0_2;
and ymm1_2@uint64 ymm0_2n ymm15_2;
not ymm0_3n@uint64 ymm0_3;
and ymm1_3@uint64 ymm0_3n ymm15_3;
(* vpxor  %ymm8,%ymm1,%ymm8                        #! PC = 0x555555577816 *)
xor ymm8_0@uint64 ymm1_0 ymm8_0;
xor ymm8_1@uint64 ymm1_1 ymm8_1;
xor ymm8_2@uint64 ymm1_2 ymm8_2;
xor ymm8_3@uint64 ymm1_3 ymm8_3;
(* vpxor  -0x330(%rbp),%ymm4,%ymm1                 #! EA = L0x7fffffffbc20; Value = 0x0000000000000000; PC = 0x55555557781b *)
xor ymm1_0@uint64 ymm4_0 L0x7fffffffbc20;
xor ymm1_1@uint64 ymm4_1 L0x7fffffffbc28;
xor ymm1_2@uint64 ymm4_2 L0x7fffffffbc30;
xor ymm1_3@uint64 ymm4_3 L0x7fffffffbc38;
(* vpxor  %ymm7,%ymm13,%ymm13                      #! PC = 0x555555577823 *)
xor ymm13_0@uint64 ymm13_0 ymm7_0;
xor ymm13_1@uint64 ymm13_1 ymm7_1;
xor ymm13_2@uint64 ymm13_2 ymm7_2;
xor ymm13_3@uint64 ymm13_3 ymm7_3;
(* vmovdqa %ymm8,-0x70(%rbp)                       #! EA = L0x7fffffffbee0; PC = 0x555555577827 *)
mov L0x7fffffffbee0 ymm8_0;
mov L0x7fffffffbee8 ymm8_1;
mov L0x7fffffffbef0 ymm8_2;
mov L0x7fffffffbef8 ymm8_3;
(* vpxor  %ymm8,%ymm11,%ymm8                       #! PC = 0x55555557782c *)
xor ymm8_0@uint64 ymm11_0 ymm8_0;
xor ymm8_1@uint64 ymm11_1 ymm8_1;
xor ymm8_2@uint64 ymm11_2 ymm8_2;
xor ymm8_3@uint64 ymm11_3 ymm8_3;
(* vpsllq $0x1,%ymm13,%ymm4                        #! PC = 0x555555577831 *)
shl ymm4_0 ymm13_0 0x1@uint64;
shl ymm4_1 ymm13_1 0x1@uint64;
shl ymm4_2 ymm13_2 0x1@uint64;
shl ymm4_3 ymm13_3 0x1@uint64;
(* vpxor  %ymm1,%ymm8,%ymm8                        #! PC = 0x555555577837 *)
xor ymm8_0@uint64 ymm8_0 ymm1_0;
xor ymm8_1@uint64 ymm8_1 ymm1_1;
xor ymm8_2@uint64 ymm8_2 ymm1_2;
xor ymm8_3@uint64 ymm8_3 ymm1_3;
(* vpandn %ymm5,%ymm15,%ymm1                       #! PC = 0x55555557783b *)
not ymm15_0n@uint64 ymm15_0;
and ymm1_0@uint64 ymm15_0n ymm5_0;
not ymm15_1n@uint64 ymm15_1;
and ymm1_1@uint64 ymm15_1n ymm5_1;
not ymm15_2n@uint64 ymm15_2;
and ymm1_2@uint64 ymm15_2n ymm5_2;
not ymm15_3n@uint64 ymm15_3;
and ymm1_3@uint64 ymm15_3n ymm5_3;
(* vpxor  -0x170(%rbp),%ymm8,%ymm8                 #! EA = L0x7fffffffbde0; Value = 0x0000000000000000; PC = 0x55555557783f *)
xor ymm8_0@uint64 ymm8_0 L0x7fffffffbde0;
xor ymm8_1@uint64 ymm8_1 L0x7fffffffbde8;
xor ymm8_2@uint64 ymm8_2 L0x7fffffffbdf0;
xor ymm8_3@uint64 ymm8_3 L0x7fffffffbdf8;
(* vpxor  %ymm0,%ymm1,%ymm1                        #! PC = 0x555555577847 *)
xor ymm1_0@uint64 ymm1_0 ymm0_0;
xor ymm1_1@uint64 ymm1_1 ymm0_1;
xor ymm1_2@uint64 ymm1_2 ymm0_2;
xor ymm1_3@uint64 ymm1_3 ymm0_3;
(* vmovdqa -0x210(%rbp),%ymm0                      #! EA = L0x7fffffffbd40; Value = 0x0000000000000000; PC = 0x55555557784b *)
mov ymm0_0 L0x7fffffffbd40;
mov ymm0_1 L0x7fffffffbd48;
mov ymm0_2 L0x7fffffffbd50;
mov ymm0_3 L0x7fffffffbd58;
(* vpsllq $0x1,%ymm14,%ymm5                        #! PC = 0x555555577853 *)
shl ymm5_0 ymm14_0 0x1@uint64;
shl ymm5_1 ymm14_1 0x1@uint64;
shl ymm5_2 ymm14_2 0x1@uint64;
shl ymm5_3 ymm14_3 0x1@uint64;
(* vpxor  -0x1f0(%rbp),%ymm1,%ymm2                 #! EA = L0x7fffffffbd60; Value = 0x0000000000000000; PC = 0x555555577859 *)
xor ymm2_0@uint64 ymm1_0 L0x7fffffffbd60;
xor ymm2_1@uint64 ymm1_1 L0x7fffffffbd68;
xor ymm2_2@uint64 ymm1_2 L0x7fffffffbd70;
xor ymm2_3@uint64 ymm1_3 L0x7fffffffbd78;
(* vpxor  -0x310(%rbp),%ymm0,%ymm0                 #! EA = L0x7fffffffbc40; Value = 0x0000000000000000; PC = 0x555555577861 *)
xor ymm0_0@uint64 ymm0_0 L0x7fffffffbc40;
xor ymm0_1@uint64 ymm0_1 L0x7fffffffbc48;
xor ymm0_2@uint64 ymm0_2 L0x7fffffffbc50;
xor ymm0_3@uint64 ymm0_3 L0x7fffffffbc58;
(* vpsrlq $0x3f,%ymm8,%ymm15                       #! PC = 0x555555577869 *)
shr ymm15_0 ymm8_0 0x3f@uint64;
shr ymm15_1 ymm8_1 0x3f@uint64;
shr ymm15_2 ymm8_2 0x3f@uint64;
shr ymm15_3 ymm8_3 0x3f@uint64;
(* vpxor  %ymm0,%ymm2,%ymm2                        #! PC = 0x55555557786f *)
xor ymm2_0@uint64 ymm2_0 ymm0_0;
xor ymm2_1@uint64 ymm2_1 ymm0_1;
xor ymm2_2@uint64 ymm2_2 ymm0_2;
xor ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x3f,%ymm14,%ymm0                       #! PC = 0x555555577873 *)
shr ymm0_0 ymm14_0 0x3f@uint64;
shr ymm0_1 ymm14_1 0x3f@uint64;
shr ymm0_2 ymm14_2 0x3f@uint64;
shr ymm0_3 ymm14_3 0x3f@uint64;
(* vpxor  -0x50(%rbp),%ymm2,%ymm2                  #! EA = L0x7fffffffbf00; Value = 0x0000000000000000; PC = 0x555555577879 *)
xor ymm2_0@uint64 ymm2_0 L0x7fffffffbf00;
xor ymm2_1@uint64 ymm2_1 L0x7fffffffbf08;
xor ymm2_2@uint64 ymm2_2 L0x7fffffffbf10;
xor ymm2_3@uint64 ymm2_3 L0x7fffffffbf18;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x55555557787e *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpsrlq $0x3f,%ymm13,%ymm0                       #! PC = 0x555555577882 *)
shr ymm0_0 ymm13_0 0x3f@uint64;
shr ymm0_1 ymm13_1 0x3f@uint64;
shr ymm0_2 ymm13_2 0x3f@uint64;
shr ymm0_3 ymm13_3 0x3f@uint64;
(* vpor   %ymm0,%ymm4,%ymm4                        #! PC = 0x555555577888 *)
or ymm4_0@uint64 ymm4_0 ymm0_0;
or ymm4_1@uint64 ymm4_1 ymm0_1;
or ymm4_2@uint64 ymm4_2 ymm0_2;
or ymm4_3@uint64 ymm4_3 ymm0_3;
(* vpsllq $0x1,%ymm8,%ymm0                         #! PC = 0x55555557788c *)
shl ymm0_0 ymm8_0 0x1@uint64;
shl ymm0_1 ymm8_1 0x1@uint64;
shl ymm0_2 ymm8_2 0x1@uint64;
shl ymm0_3 ymm8_3 0x1@uint64;
(* vpxor  %ymm2,%ymm5,%ymm5                        #! PC = 0x555555577892 *)
xor ymm5_0@uint64 ymm5_0 ymm2_0;
xor ymm5_1@uint64 ymm5_1 ymm2_1;
xor ymm5_2@uint64 ymm5_2 ymm2_2;
xor ymm5_3@uint64 ymm5_3 ymm2_3;
(* vpor   %ymm15,%ymm0,%ymm0                       #! PC = 0x555555577896 *)
or ymm0_0@uint64 ymm0_0 ymm15_0;
or ymm0_1@uint64 ymm0_1 ymm15_1;
or ymm0_2@uint64 ymm0_2 ymm15_2;
or ymm0_3@uint64 ymm0_3 ymm15_3;
(* vpxor  %ymm3,%ymm4,%ymm4                        #! PC = 0x55555557789b *)
xor ymm4_0@uint64 ymm4_0 ymm3_0;
xor ymm4_1@uint64 ymm4_1 ymm3_1;
xor ymm4_2@uint64 ymm4_2 ymm3_2;
xor ymm4_3@uint64 ymm4_3 ymm3_3;
(* vmovq  %rdx,%xmm15                              #! PC = 0x55555557789f *)
mov xmm15_0 rdx;
mov xmm15_1 0@uint64;
(* vpxor  %ymm14,%ymm0,%ymm14                      #! PC = 0x5555555778a4 *)
xor ymm14_0@uint64 ymm0_0 ymm14_0;
xor ymm14_1@uint64 ymm0_1 ymm14_1;
xor ymm14_2@uint64 ymm0_2 ymm14_2;
xor ymm14_3@uint64 ymm0_3 ymm14_3;
(* vpsrlq $0x3f,%ymm2,%ymm0                        #! PC = 0x5555555778a9 *)
shr ymm0_0 ymm2_0 0x3f@uint64;
shr ymm0_1 ymm2_1 0x3f@uint64;
shr ymm0_2 ymm2_2 0x3f@uint64;
shr ymm0_3 ymm2_3 0x3f@uint64;
(* vpxor  %ymm10,%ymm4,%ymm10                      #! PC = 0x5555555778ae *)
xor ymm10_0@uint64 ymm4_0 ymm10_0;
xor ymm10_1@uint64 ymm4_1 ymm10_1;
xor ymm10_2@uint64 ymm4_2 ymm10_2;
xor ymm10_3@uint64 ymm4_3 ymm10_3;
(* vpsllq $0x1,%ymm2,%ymm2                         #! PC = 0x5555555778b3 *)
shl ymm2_0 ymm2_0 0x1@uint64;
shl ymm2_1 ymm2_1 0x1@uint64;
shl ymm2_2 ymm2_2 0x1@uint64;
shl ymm2_3 ymm2_3 0x1@uint64;
(* vpxor  %ymm12,%ymm14,%ymm12                     #! PC = 0x5555555778b8 *)
xor ymm12_0@uint64 ymm14_0 ymm12_0;
xor ymm12_1@uint64 ymm14_1 ymm12_1;
xor ymm12_2@uint64 ymm14_2 ymm12_2;
xor ymm12_3@uint64 ymm14_3 ymm12_3;
(* vpxor  %ymm9,%ymm4,%ymm9                        #! PC = 0x5555555778bd *)
xor ymm9_0@uint64 ymm4_0 ymm9_0;
xor ymm9_1@uint64 ymm4_1 ymm9_1;
xor ymm9_2@uint64 ymm4_2 ymm9_2;
xor ymm9_3@uint64 ymm4_3 ymm9_3;
(* vpor   %ymm0,%ymm2,%ymm2                        #! PC = 0x5555555778c2 *)
or ymm2_0@uint64 ymm2_0 ymm0_0;
or ymm2_1@uint64 ymm2_1 ymm0_1;
or ymm2_2@uint64 ymm2_2 ymm0_2;
or ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x3f,%ymm3,%ymm0                        #! PC = 0x5555555778c6 *)
shr ymm0_0 ymm3_0 0x3f@uint64;
shr ymm0_1 ymm3_1 0x3f@uint64;
shr ymm0_2 ymm3_2 0x3f@uint64;
shr ymm0_3 ymm3_3 0x3f@uint64;
(* vpxor  %ymm7,%ymm14,%ymm7                       #! PC = 0x5555555778cb *)
xor ymm7_0@uint64 ymm14_0 ymm7_0;
xor ymm7_1@uint64 ymm14_1 ymm7_1;
xor ymm7_2@uint64 ymm14_2 ymm7_2;
xor ymm7_3@uint64 ymm14_3 ymm7_3;
(* vpxor  %ymm13,%ymm2,%ymm13                      #! PC = 0x5555555778cf *)
xor ymm13_0@uint64 ymm2_0 ymm13_0;
xor ymm13_1@uint64 ymm2_1 ymm13_1;
xor ymm13_2@uint64 ymm2_2 ymm13_2;
xor ymm13_3@uint64 ymm2_3 ymm13_3;
(* vpsrlq $0x14,%ymm10,%ymm2                       #! PC = 0x5555555778d4 *)
shr ymm2_0 ymm10_0 0x14@uint64;
shr ymm2_1 ymm10_1 0x14@uint64;
shr ymm2_2 ymm10_2 0x14@uint64;
shr ymm2_3 ymm10_3 0x14@uint64;
(* vpxor  %ymm6,%ymm5,%ymm6                        #! PC = 0x5555555778da *)
xor ymm6_0@uint64 ymm5_0 ymm6_0;
xor ymm6_1@uint64 ymm5_1 ymm6_1;
xor ymm6_2@uint64 ymm5_2 ymm6_2;
xor ymm6_3@uint64 ymm5_3 ymm6_3;
(* vpsllq $0x2c,%ymm10,%ymm10                      #! PC = 0x5555555778de *)
shl ymm10_0 ymm10_0 0x2c@uint64;
shl ymm10_1 ymm10_1 0x2c@uint64;
shl ymm10_2 ymm10_2 0x2c@uint64;
shl ymm10_3 ymm10_3 0x2c@uint64;
(* vpsllq $0x1,%ymm3,%ymm3                         #! PC = 0x5555555778e4 *)
shl ymm3_0 ymm3_0 0x1@uint64;
shl ymm3_1 ymm3_1 0x1@uint64;
shl ymm3_2 ymm3_2 0x1@uint64;
shl ymm3_3 ymm3_3 0x1@uint64;
(* vpxor  %ymm11,%ymm13,%ymm11                     #! PC = 0x5555555778e9 *)
xor ymm11_0@uint64 ymm13_0 ymm11_0;
xor ymm11_1@uint64 ymm13_1 ymm11_1;
xor ymm11_2@uint64 ymm13_2 ymm11_2;
xor ymm11_3@uint64 ymm13_3 ymm11_3;
(* vpor   %ymm2,%ymm10,%ymm10                      #! PC = 0x5555555778ee *)
or ymm10_0@uint64 ymm10_0 ymm2_0;
or ymm10_1@uint64 ymm10_1 ymm2_1;
or ymm10_2@uint64 ymm10_2 ymm2_2;
or ymm10_3@uint64 ymm10_3 ymm2_3;
(* vpsrlq $0x15,%ymm12,%ymm2                       #! PC = 0x5555555778f2 *)
shr ymm2_0 ymm12_0 0x15@uint64;
shr ymm2_1 ymm12_1 0x15@uint64;
shr ymm2_2 ymm12_2 0x15@uint64;
shr ymm2_3 ymm12_3 0x15@uint64;
(* vpor   %ymm0,%ymm3,%ymm3                        #! PC = 0x5555555778f8 *)
or ymm3_0@uint64 ymm3_0 ymm0_0;
or ymm3_1@uint64 ymm3_1 ymm0_1;
or ymm3_2@uint64 ymm3_2 ymm0_2;
or ymm3_3@uint64 ymm3_3 ymm0_3;
(* vpsllq $0x2b,%ymm12,%ymm12                      #! PC = 0x5555555778fc *)
shl ymm12_0 ymm12_0 0x2b@uint64;
shl ymm12_1 ymm12_1 0x2b@uint64;
shl ymm12_2 ymm12_2 0x2b@uint64;
shl ymm12_3 ymm12_3 0x2b@uint64;
(* vpxor  %ymm8,%ymm3,%ymm8                        #! PC = 0x555555577902 *)
xor ymm8_0@uint64 ymm3_0 ymm8_0;
xor ymm8_1@uint64 ymm3_1 ymm8_1;
xor ymm8_2@uint64 ymm3_2 ymm8_2;
xor ymm8_3@uint64 ymm3_3 ymm8_3;
(* vpxor  -0x390(%rbp),%ymm5,%ymm0                 #! EA = L0x7fffffffbbc0; Value = 0x0000000000000001; PC = 0x555555577907 *)
xor ymm0_0@uint64 ymm5_0 L0x7fffffffbbc0;
xor ymm0_1@uint64 ymm5_1 L0x7fffffffbbc8;
xor ymm0_2@uint64 ymm5_2 L0x7fffffffbbd0;
xor ymm0_3@uint64 ymm5_3 L0x7fffffffbbd8;
(* vpor   %ymm2,%ymm12,%ymm12                      #! PC = 0x55555557790f *)
or ymm12_0@uint64 ymm12_0 ymm2_0;
or ymm12_1@uint64 ymm12_1 ymm2_1;
or ymm12_2@uint64 ymm12_2 ymm2_2;
or ymm12_3@uint64 ymm12_3 ymm2_3;
(* vpbroadcastq %xmm15,%ymm2                       #! PC = 0x555555577913 *)
mov ymm2_0 xmm15_0;
mov ymm2_1 xmm15_0;
mov ymm2_2 xmm15_0;
mov ymm2_3 xmm15_0;
(* vpxor  %ymm1,%ymm8,%ymm1                        #! PC = 0x555555577918 *)
xor ymm1_0@uint64 ymm8_0 ymm1_0;
xor ymm1_1@uint64 ymm8_1 ymm1_1;
xor ymm1_2@uint64 ymm8_2 ymm1_2;
xor ymm1_3@uint64 ymm8_3 ymm1_3;
(* vpandn %ymm12,%ymm10,%ymm3                      #! PC = 0x55555557791c *)
not ymm10_0n@uint64 ymm10_0;
and ymm3_0@uint64 ymm10_0n ymm12_0;
not ymm10_1n@uint64 ymm10_1;
and ymm3_1@uint64 ymm10_1n ymm12_1;
not ymm10_2n@uint64 ymm10_2;
and ymm3_2@uint64 ymm10_2n ymm12_2;
not ymm10_3n@uint64 ymm10_3;
and ymm3_3@uint64 ymm10_3n ymm12_3;
(* vpxor  %ymm3,%ymm2,%ymm2                        #! PC = 0x555555577921 *)
xor ymm2_0@uint64 ymm2_0 ymm3_0;
xor ymm2_1@uint64 ymm2_1 ymm3_1;
xor ymm2_2@uint64 ymm2_2 ymm3_2;
xor ymm2_3@uint64 ymm2_3 ymm3_3;
(* vpxor  %ymm0,%ymm2,%ymm15                       #! PC = 0x555555577925 *)
xor ymm15_0@uint64 ymm2_0 ymm0_0;
xor ymm15_1@uint64 ymm2_1 ymm0_1;
xor ymm15_2@uint64 ymm2_2 ymm0_2;
xor ymm15_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x2b,%ymm11,%ymm2                       #! PC = 0x555555577929 *)
shr ymm2_0 ymm11_0 0x2b@uint64;
shr ymm2_1 ymm11_1 0x2b@uint64;
shr ymm2_2 ymm11_2 0x2b@uint64;
shr ymm2_3 ymm11_3 0x2b@uint64;
(* vpsllq $0x15,%ymm11,%ymm11                      #! PC = 0x55555557792f *)
shl ymm11_0 ymm11_0 0x15@uint64;
shl ymm11_1 ymm11_1 0x15@uint64;
shl ymm11_2 ymm11_2 0x15@uint64;
shl ymm11_3 ymm11_3 0x15@uint64;
(* vmovdqa %ymm15,-0x90(%rbp)                      #! EA = L0x7fffffffbec0; PC = 0x555555577935 *)
mov L0x7fffffffbec0 ymm15_0;
mov L0x7fffffffbec8 ymm15_1;
mov L0x7fffffffbed0 ymm15_2;
mov L0x7fffffffbed8 ymm15_3;
(* vpor   %ymm2,%ymm11,%ymm11                      #! PC = 0x55555557793d *)
or ymm11_0@uint64 ymm11_0 ymm2_0;
or ymm11_1@uint64 ymm11_1 ymm2_1;
or ymm11_2@uint64 ymm11_2 ymm2_2;
or ymm11_3@uint64 ymm11_3 ymm2_3;
(* vpandn %ymm11,%ymm12,%ymm2                      #! PC = 0x555555577941 *)
not ymm12_0n@uint64 ymm12_0;
and ymm2_0@uint64 ymm12_0n ymm11_0;
not ymm12_1n@uint64 ymm12_1;
and ymm2_1@uint64 ymm12_1n ymm11_1;
not ymm12_2n@uint64 ymm12_2;
and ymm2_2@uint64 ymm12_2n ymm11_2;
not ymm12_3n@uint64 ymm12_3;
and ymm2_3@uint64 ymm12_3n ymm11_3;
(* vpxor  %ymm10,%ymm2,%ymm15                      #! PC = 0x555555577946 *)
xor ymm15_0@uint64 ymm2_0 ymm10_0;
xor ymm15_1@uint64 ymm2_1 ymm10_1;
xor ymm15_2@uint64 ymm2_2 ymm10_2;
xor ymm15_3@uint64 ymm2_3 ymm10_3;
(* vpsrlq $0x32,%ymm1,%ymm2                        #! PC = 0x55555557794b *)
shr ymm2_0 ymm1_0 0x32@uint64;
shr ymm2_1 ymm1_1 0x32@uint64;
shr ymm2_2 ymm1_2 0x32@uint64;
shr ymm2_3 ymm1_3 0x32@uint64;
(* vpandn %ymm10,%ymm0,%ymm10                      #! PC = 0x555555577950 *)
not ymm0_0n@uint64 ymm0_0;
and ymm10_0@uint64 ymm0_0n ymm10_0;
not ymm0_1n@uint64 ymm0_1;
and ymm10_1@uint64 ymm0_1n ymm10_1;
not ymm0_2n@uint64 ymm0_2;
and ymm10_2@uint64 ymm0_2n ymm10_2;
not ymm0_3n@uint64 ymm0_3;
and ymm10_3@uint64 ymm0_3n ymm10_3;
(* vpsllq $0xe,%ymm1,%ymm1                         #! PC = 0x555555577955 *)
shl ymm1_0 ymm1_0 0xe@uint64;
shl ymm1_1 ymm1_1 0xe@uint64;
shl ymm1_2 ymm1_2 0xe@uint64;
shl ymm1_3 ymm1_3 0xe@uint64;
(* vmovdqa %ymm15,-0x1b0(%rbp)                     #! EA = L0x7fffffffbda0; PC = 0x55555557795a *)
mov L0x7fffffffbda0 ymm15_0;
mov L0x7fffffffbda8 ymm15_1;
mov L0x7fffffffbdb0 ymm15_2;
mov L0x7fffffffbdb8 ymm15_3;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x555555577962 *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpandn %ymm1,%ymm11,%ymm2                       #! PC = 0x555555577966 *)
not ymm11_0n@uint64 ymm11_0;
and ymm2_0@uint64 ymm11_0n ymm1_0;
not ymm11_1n@uint64 ymm11_1;
and ymm2_1@uint64 ymm11_1n ymm1_1;
not ymm11_2n@uint64 ymm11_2;
and ymm2_2@uint64 ymm11_2n ymm1_2;
not ymm11_3n@uint64 ymm11_3;
and ymm2_3@uint64 ymm11_3n ymm1_3;
(* vpxor  %ymm1,%ymm10,%ymm10                      #! PC = 0x55555557796a *)
xor ymm10_0@uint64 ymm10_0 ymm1_0;
xor ymm10_1@uint64 ymm10_1 ymm1_1;
xor ymm10_2@uint64 ymm10_2 ymm1_2;
xor ymm10_3@uint64 ymm10_3 ymm1_3;
(* vpxor  %ymm12,%ymm2,%ymm12                      #! PC = 0x55555557796e *)
xor ymm12_0@uint64 ymm2_0 ymm12_0;
xor ymm12_1@uint64 ymm2_1 ymm12_1;
xor ymm12_2@uint64 ymm2_2 ymm12_2;
xor ymm12_3@uint64 ymm2_3 ymm12_3;
(* vpandn %ymm0,%ymm1,%ymm2                        #! PC = 0x555555577973 *)
not ymm1_0n@uint64 ymm1_0;
and ymm2_0@uint64 ymm1_0n ymm0_0;
not ymm1_1n@uint64 ymm1_1;
and ymm2_1@uint64 ymm1_1n ymm0_1;
not ymm1_2n@uint64 ymm1_2;
and ymm2_2@uint64 ymm1_2n ymm0_2;
not ymm1_3n@uint64 ymm1_3;
and ymm2_3@uint64 ymm1_3n ymm0_3;
(* vpxor  -0x330(%rbp),%ymm13,%ymm0                #! EA = L0x7fffffffbc20; Value = 0x0000000000000000; PC = 0x555555577977 *)
xor ymm0_0@uint64 ymm13_0 L0x7fffffffbc20;
xor ymm0_1@uint64 ymm13_1 L0x7fffffffbc28;
xor ymm0_2@uint64 ymm13_2 L0x7fffffffbc30;
xor ymm0_3@uint64 ymm13_3 L0x7fffffffbc38;
(* vmovdqa %ymm10,-0x230(%rbp)                     #! EA = L0x7fffffffbd20; PC = 0x55555557797f *)
mov L0x7fffffffbd20 ymm10_0;
mov L0x7fffffffbd28 ymm10_1;
mov L0x7fffffffbd30 ymm10_2;
mov L0x7fffffffbd38 ymm10_3;
(* vmovdqa %ymm12,-0x250(%rbp)                     #! EA = L0x7fffffffbd00; PC = 0x555555577987 *)
mov L0x7fffffffbd00 ymm12_0;
mov L0x7fffffffbd08 ymm12_1;
mov L0x7fffffffbd10 ymm12_2;
mov L0x7fffffffbd18 ymm12_3;
(* vpxor  %ymm11,%ymm2,%ymm12                      #! PC = 0x55555557798f *)
xor ymm12_0@uint64 ymm2_0 ymm11_0;
xor ymm12_1@uint64 ymm2_1 ymm11_1;
xor ymm12_2@uint64 ymm2_2 ymm11_2;
xor ymm12_3@uint64 ymm2_3 ymm11_3;
(* vpsrlq $0x24,%ymm0,%ymm1                        #! PC = 0x555555577994 *)
shr ymm1_0 ymm0_0 0x24@uint64;
shr ymm1_1 ymm0_1 0x24@uint64;
shr ymm1_2 ymm0_2 0x24@uint64;
shr ymm1_3 ymm0_3 0x24@uint64;
(* vpsllq $0x1c,%ymm0,%ymm0                        #! PC = 0x555555577999 *)
shl ymm0_0 ymm0_0 0x1c@uint64;
shl ymm0_1 ymm0_1 0x1c@uint64;
shl ymm0_2 ymm0_2 0x1c@uint64;
shl ymm0_3 ymm0_3 0x1c@uint64;
(* vmovdqa %ymm12,-0x110(%rbp)                     #! EA = L0x7fffffffbe40; PC = 0x55555557799e *)
mov L0x7fffffffbe40 ymm12_0;
mov L0x7fffffffbe48 ymm12_1;
mov L0x7fffffffbe50 ymm12_2;
mov L0x7fffffffbe58 ymm12_3;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x5555555779a6 *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x210(%rbp),%ymm8,%ymm1                 #! EA = L0x7fffffffbd40; Value = 0x0000000000000000; PC = 0x5555555779aa *)
xor ymm1_0@uint64 ymm8_0 L0x7fffffffbd40;
xor ymm1_1@uint64 ymm8_1 L0x7fffffffbd48;
xor ymm1_2@uint64 ymm8_2 L0x7fffffffbd50;
xor ymm1_3@uint64 ymm8_3 L0x7fffffffbd58;
(* vpsrlq $0x2c,%ymm1,%ymm2                        #! PC = 0x5555555779b2 *)
shr ymm2_0 ymm1_0 0x2c@uint64;
shr ymm2_1 ymm1_1 0x2c@uint64;
shr ymm2_2 ymm1_2 0x2c@uint64;
shr ymm2_3 ymm1_3 0x2c@uint64;
(* vpsllq $0x14,%ymm1,%ymm1                        #! PC = 0x5555555779b7 *)
shl ymm1_0 ymm1_0 0x14@uint64;
shl ymm1_1 ymm1_1 0x14@uint64;
shl ymm1_2 ymm1_2 0x14@uint64;
shl ymm1_3 ymm1_3 0x14@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x5555555779bc *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpxor  -0xd0(%rbp),%ymm5,%ymm2                  #! EA = L0x7fffffffbe80; Value = 0x0000000000000000; PC = 0x5555555779c0 *)
xor ymm2_0@uint64 ymm5_0 L0x7fffffffbe80;
xor ymm2_1@uint64 ymm5_1 L0x7fffffffbe88;
xor ymm2_2@uint64 ymm5_2 L0x7fffffffbe90;
xor ymm2_3@uint64 ymm5_3 L0x7fffffffbe98;
(* vpsrlq $0x3d,%ymm2,%ymm3                        #! PC = 0x5555555779c8 *)
shr ymm3_0 ymm2_0 0x3d@uint64;
shr ymm3_1 ymm2_1 0x3d@uint64;
shr ymm3_2 ymm2_2 0x3d@uint64;
shr ymm3_3 ymm2_3 0x3d@uint64;
(* vpsllq $0x3,%ymm2,%ymm2                         #! PC = 0x5555555779cd *)
shl ymm2_0 ymm2_0 0x3@uint64;
shl ymm2_1 ymm2_1 0x3@uint64;
shl ymm2_2 ymm2_2 0x3@uint64;
shl ymm2_3 ymm2_3 0x3@uint64;
(* vpor   %ymm3,%ymm2,%ymm2                        #! PC = 0x5555555779d2 *)
or ymm2_0@uint64 ymm2_0 ymm3_0;
or ymm2_1@uint64 ymm2_1 ymm3_1;
or ymm2_2@uint64 ymm2_2 ymm3_2;
or ymm2_3@uint64 ymm2_3 ymm3_3;
(* vpandn %ymm2,%ymm1,%ymm3                        #! PC = 0x5555555779d6 *)
not ymm1_0n@uint64 ymm1_0;
and ymm3_0@uint64 ymm1_0n ymm2_0;
not ymm1_1n@uint64 ymm1_1;
and ymm3_1@uint64 ymm1_1n ymm2_1;
not ymm1_2n@uint64 ymm1_2;
and ymm3_2@uint64 ymm1_2n ymm2_2;
not ymm1_3n@uint64 ymm1_3;
and ymm3_3@uint64 ymm1_3n ymm2_3;
(* vpxor  %ymm0,%ymm3,%ymm10                       #! PC = 0x5555555779da *)
xor ymm10_0@uint64 ymm3_0 ymm0_0;
xor ymm10_1@uint64 ymm3_1 ymm0_1;
xor ymm10_2@uint64 ymm3_2 ymm0_2;
xor ymm10_3@uint64 ymm3_3 ymm0_3;
(* vpsrlq $0x13,%ymm9,%ymm3                        #! PC = 0x5555555779de *)
shr ymm3_0 ymm9_0 0x13@uint64;
shr ymm3_1 ymm9_1 0x13@uint64;
shr ymm3_2 ymm9_2 0x13@uint64;
shr ymm3_3 ymm9_3 0x13@uint64;
(* vpsllq $0x2d,%ymm9,%ymm9                        #! PC = 0x5555555779e4 *)
shl ymm9_0 ymm9_0 0x2d@uint64;
shl ymm9_1 ymm9_1 0x2d@uint64;
shl ymm9_2 ymm9_2 0x2d@uint64;
shl ymm9_3 ymm9_3 0x2d@uint64;
(* vmovdqa %ymm10,-0x210(%rbp)                     #! EA = L0x7fffffffbd40; PC = 0x5555555779ea *)
mov L0x7fffffffbd40 ymm10_0;
mov L0x7fffffffbd48 ymm10_1;
mov L0x7fffffffbd50 ymm10_2;
mov L0x7fffffffbd58 ymm10_3;
(* vpxor  -0x130(%rbp),%ymm14,%ymm11               #! EA = L0x7fffffffbe20; Value = 0x0000000000000000; PC = 0x5555555779f2 *)
xor ymm11_0@uint64 ymm14_0 L0x7fffffffbe20;
xor ymm11_1@uint64 ymm14_1 L0x7fffffffbe28;
xor ymm11_2@uint64 ymm14_2 L0x7fffffffbe30;
xor ymm11_3@uint64 ymm14_3 L0x7fffffffbe38;
(* vpor   %ymm3,%ymm9,%ymm15                       #! PC = 0x5555555779fa *)
or ymm15_0@uint64 ymm9_0 ymm3_0;
or ymm15_1@uint64 ymm9_1 ymm3_1;
or ymm15_2@uint64 ymm9_2 ymm3_2;
or ymm15_3@uint64 ymm9_3 ymm3_3;
(* vpsrlq $0x3,%ymm7,%ymm3                         #! PC = 0x5555555779fe *)
shr ymm3_0 ymm7_0 0x3@uint64;
shr ymm3_1 ymm7_1 0x3@uint64;
shr ymm3_2 ymm7_2 0x3@uint64;
shr ymm3_3 ymm7_3 0x3@uint64;
(* vpsllq $0x3d,%ymm7,%ymm7                        #! PC = 0x555555577a03 *)
shl ymm7_0 ymm7_0 0x3d@uint64;
shl ymm7_1 ymm7_1 0x3d@uint64;
shl ymm7_2 ymm7_2 0x3d@uint64;
shl ymm7_3 ymm7_3 0x3d@uint64;
(* vpandn %ymm15,%ymm2,%ymm10                      #! PC = 0x555555577a08 *)
not ymm2_0n@uint64 ymm2_0;
and ymm10_0@uint64 ymm2_0n ymm15_0;
not ymm2_1n@uint64 ymm2_1;
and ymm10_1@uint64 ymm2_1n ymm15_1;
not ymm2_2n@uint64 ymm2_2;
and ymm10_2@uint64 ymm2_2n ymm15_2;
not ymm2_3n@uint64 ymm2_3;
and ymm10_3@uint64 ymm2_3n ymm15_3;
(* vpor   %ymm3,%ymm7,%ymm7                        #! PC = 0x555555577a0d *)
or ymm7_0@uint64 ymm7_0 ymm3_0;
or ymm7_1@uint64 ymm7_1 ymm3_1;
or ymm7_2@uint64 ymm7_2 ymm3_2;
or ymm7_3@uint64 ymm7_3 ymm3_3;
(* vpxor  %ymm1,%ymm10,%ymm10                      #! PC = 0x555555577a11 *)
xor ymm10_0@uint64 ymm10_0 ymm1_0;
xor ymm10_1@uint64 ymm10_1 ymm1_1;
xor ymm10_2@uint64 ymm10_2 ymm1_2;
xor ymm10_3@uint64 ymm10_3 ymm1_3;
(* vpandn %ymm7,%ymm15,%ymm3                       #! PC = 0x555555577a15 *)
not ymm15_0n@uint64 ymm15_0;
and ymm3_0@uint64 ymm15_0n ymm7_0;
not ymm15_1n@uint64 ymm15_1;
and ymm3_1@uint64 ymm15_1n ymm7_1;
not ymm15_2n@uint64 ymm15_2;
and ymm3_2@uint64 ymm15_2n ymm7_2;
not ymm15_3n@uint64 ymm15_3;
and ymm3_3@uint64 ymm15_3n ymm7_3;
(* vpxor  %ymm2,%ymm3,%ymm9                        #! PC = 0x555555577a19 *)
xor ymm9_0@uint64 ymm3_0 ymm2_0;
xor ymm9_1@uint64 ymm3_1 ymm2_1;
xor ymm9_2@uint64 ymm3_2 ymm2_2;
xor ymm9_3@uint64 ymm3_3 ymm2_3;
(* vpandn %ymm0,%ymm7,%ymm2                        #! PC = 0x555555577a1d *)
not ymm7_0n@uint64 ymm7_0;
and ymm2_0@uint64 ymm7_0n ymm0_0;
not ymm7_1n@uint64 ymm7_1;
and ymm2_1@uint64 ymm7_1n ymm0_1;
not ymm7_2n@uint64 ymm7_2;
and ymm2_2@uint64 ymm7_2n ymm0_2;
not ymm7_3n@uint64 ymm7_3;
and ymm2_3@uint64 ymm7_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x555555577a21 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm7,%ymm0,%ymm7                        #! PC = 0x555555577a25 *)
xor ymm7_0@uint64 ymm0_0 ymm7_0;
xor ymm7_1@uint64 ymm0_1 ymm7_1;
xor ymm7_2@uint64 ymm0_2 ymm7_2;
xor ymm7_3@uint64 ymm0_3 ymm7_3;
(* vpxor  -0x370(%rbp),%ymm4,%ymm0                 #! EA = L0x7fffffffbbe0; Value = 0x0000000000000000; PC = 0x555555577a29 *)
xor ymm0_0@uint64 ymm4_0 L0x7fffffffbbe0;
xor ymm0_1@uint64 ymm4_1 L0x7fffffffbbe8;
xor ymm0_2@uint64 ymm4_2 L0x7fffffffbbf0;
xor ymm0_3@uint64 ymm4_3 L0x7fffffffbbf8;
(* vmovdqa %ymm9,-0xf0(%rbp)                       #! EA = L0x7fffffffbe60; PC = 0x555555577a31 *)
mov L0x7fffffffbe60 ymm9_0;
mov L0x7fffffffbe68 ymm9_1;
mov L0x7fffffffbe70 ymm9_2;
mov L0x7fffffffbe78 ymm9_3;
(* vpxor  %ymm15,%ymm2,%ymm9                       #! PC = 0x555555577a39 *)
xor ymm9_0@uint64 ymm2_0 ymm15_0;
xor ymm9_1@uint64 ymm2_1 ymm15_1;
xor ymm9_2@uint64 ymm2_2 ymm15_2;
xor ymm9_3@uint64 ymm2_3 ymm15_3;
(* vmovdqa %ymm9,-0x190(%rbp)                      #! EA = L0x7fffffffbdc0; PC = 0x555555577a3e *)
mov L0x7fffffffbdc0 ymm9_0;
mov L0x7fffffffbdc8 ymm9_1;
mov L0x7fffffffbdd0 ymm9_2;
mov L0x7fffffffbdd8 ymm9_3;
(* vpxor  -0x170(%rbp),%ymm13,%ymm9                #! EA = L0x7fffffffbde0; Value = 0x0000000000000000; PC = 0x555555577a46 *)
xor ymm9_0@uint64 ymm13_0 L0x7fffffffbde0;
xor ymm9_1@uint64 ymm13_1 L0x7fffffffbde8;
xor ymm9_2@uint64 ymm13_2 L0x7fffffffbdf0;
xor ymm9_3@uint64 ymm13_3 L0x7fffffffbdf8;
(* vpsrlq $0x3f,%ymm0,%ymm1                        #! PC = 0x555555577a4e *)
shr ymm1_0 ymm0_0 0x3f@uint64;
shr ymm1_1 ymm0_1 0x3f@uint64;
shr ymm1_2 ymm0_2 0x3f@uint64;
shr ymm1_3 ymm0_3 0x3f@uint64;
(* vpsllq $0x1,%ymm0,%ymm0                         #! PC = 0x555555577a53 *)
shl ymm0_0 ymm0_0 0x1@uint64;
shl ymm0_1 ymm0_1 0x1@uint64;
shl ymm0_2 ymm0_2 0x1@uint64;
shl ymm0_3 ymm0_3 0x1@uint64;
(* vmovdqa %ymm7,-0xd0(%rbp)                       #! EA = L0x7fffffffbe80; PC = 0x555555577a58 *)
mov L0x7fffffffbe80 ymm7_0;
mov L0x7fffffffbe88 ymm7_1;
mov L0x7fffffffbe90 ymm7_2;
mov L0x7fffffffbe98 ymm7_3;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x555555577a60 *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x2d0(%rbp),%ymm14,%ymm1                #! EA = L0x7fffffffbc80; Value = 0x0000000000000000; PC = 0x555555577a64 *)
xor ymm1_0@uint64 ymm14_0 L0x7fffffffbc80;
xor ymm1_1@uint64 ymm14_1 L0x7fffffffbc88;
xor ymm1_2@uint64 ymm14_2 L0x7fffffffbc90;
xor ymm1_3@uint64 ymm14_3 L0x7fffffffbc98;
(* vpxor  -0x350(%rbp),%ymm14,%ymm14               #! EA = L0x7fffffffbc00; Value = 0x0000000000000000; PC = 0x555555577a6c *)
xor ymm14_0@uint64 ymm14_0 L0x7fffffffbc00;
xor ymm14_1@uint64 ymm14_1 L0x7fffffffbc08;
xor ymm14_2@uint64 ymm14_2 L0x7fffffffbc10;
xor ymm14_3@uint64 ymm14_3 L0x7fffffffbc18;
(* vpsrlq $0x27,%ymm9,%ymm3                        #! PC = 0x555555577a74 *)
shr ymm3_0 ymm9_0 0x27@uint64;
shr ymm3_1 ymm9_1 0x27@uint64;
shr ymm3_2 ymm9_2 0x27@uint64;
shr ymm3_3 ymm9_3 0x27@uint64;
(* vpsllq $0x19,%ymm9,%ymm9                        #! PC = 0x555555577a7a *)
shl ymm9_0 ymm9_0 0x19@uint64;
shl ymm9_1 ymm9_1 0x19@uint64;
shl ymm9_2 ymm9_2 0x19@uint64;
shl ymm9_3 ymm9_3 0x19@uint64;
(* vpsrlq $0x3a,%ymm1,%ymm2                        #! PC = 0x555555577a80 *)
shr ymm2_0 ymm1_0 0x3a@uint64;
shr ymm2_1 ymm1_1 0x3a@uint64;
shr ymm2_2 ymm1_2 0x3a@uint64;
shr ymm2_3 ymm1_3 0x3a@uint64;
(* vpsllq $0x6,%ymm1,%ymm1                         #! PC = 0x555555577a85 *)
shl ymm1_0 ymm1_0 0x6@uint64;
shl ymm1_1 ymm1_1 0x6@uint64;
shl ymm1_2 ymm1_2 0x6@uint64;
shl ymm1_3 ymm1_3 0x6@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x555555577a8a *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpor   %ymm3,%ymm9,%ymm2                        #! PC = 0x555555577a8e *)
or ymm2_0@uint64 ymm9_0 ymm3_0;
or ymm2_1@uint64 ymm9_1 ymm3_1;
or ymm2_2@uint64 ymm9_2 ymm3_2;
or ymm2_3@uint64 ymm9_3 ymm3_3;
(* vpandn %ymm2,%ymm1,%ymm3                        #! PC = 0x555555577a92 *)
not ymm1_0n@uint64 ymm1_0;
and ymm3_0@uint64 ymm1_0n ymm2_0;
not ymm1_1n@uint64 ymm1_1;
and ymm3_1@uint64 ymm1_1n ymm2_1;
not ymm1_2n@uint64 ymm1_2;
and ymm3_2@uint64 ymm1_2n ymm2_2;
not ymm1_3n@uint64 ymm1_3;
and ymm3_3@uint64 ymm1_3n ymm2_3;
(* vpxor  %ymm0,%ymm3,%ymm9                        #! PC = 0x555555577a96 *)
xor ymm9_0@uint64 ymm3_0 ymm0_0;
xor ymm9_1@uint64 ymm3_1 ymm0_1;
xor ymm9_2@uint64 ymm3_2 ymm0_2;
xor ymm9_3@uint64 ymm3_3 ymm0_3;
(* vpxor  -0x50(%rbp),%ymm8,%ymm3                  #! EA = L0x7fffffffbf00; Value = 0x0000000000000000; PC = 0x555555577a9a *)
xor ymm3_0@uint64 ymm8_0 L0x7fffffffbf00;
xor ymm3_1@uint64 ymm8_1 L0x7fffffffbf08;
xor ymm3_2@uint64 ymm8_2 L0x7fffffffbf10;
xor ymm3_3@uint64 ymm8_3 L0x7fffffffbf18;
(* vmovdqa %ymm9,%ymm15                            #! PC = 0x555555577a9f *)
mov ymm15_0 ymm9_0;
mov ymm15_1 ymm9_1;
mov ymm15_2 ymm9_2;
mov ymm15_3 ymm9_3;
(* vpshufb 0x56453(%rip),%ymm3,%ymm3        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x555555577aa4 *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm3_0, ymm3_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm3_2, ymm3_3, tmp_2, tmp_3);
mov ymm3_0 tmp_0;
mov ymm3_1 tmp_1;
mov ymm3_2 tmp_2;
mov ymm3_3 tmp_3;
(* vmovdqa %ymm15,-0x1d0(%rbp)                     #! EA = L0x7fffffffbd80; PC = 0x555555577aad *)
mov L0x7fffffffbd80 ymm15_0;
mov L0x7fffffffbd88 ymm15_1;
mov L0x7fffffffbd90 ymm15_2;
mov L0x7fffffffbd98 ymm15_3;
(* vpandn %ymm3,%ymm2,%ymm7                        #! PC = 0x555555577ab5 *)
not ymm2_0n@uint64 ymm2_0;
and ymm7_0@uint64 ymm2_0n ymm3_0;
not ymm2_1n@uint64 ymm2_1;
and ymm7_1@uint64 ymm2_1n ymm3_1;
not ymm2_2n@uint64 ymm2_2;
and ymm7_2@uint64 ymm2_2n ymm3_2;
not ymm2_3n@uint64 ymm2_3;
and ymm7_3@uint64 ymm2_3n ymm3_3;
(* vpxor  %ymm1,%ymm7,%ymm12                       #! PC = 0x555555577ab9 *)
xor ymm12_0@uint64 ymm7_0 ymm1_0;
xor ymm12_1@uint64 ymm7_1 ymm1_1;
xor ymm12_2@uint64 ymm7_2 ymm1_2;
xor ymm12_3@uint64 ymm7_3 ymm1_3;
(* vpsrlq $0x2e,%ymm6,%ymm7                        #! PC = 0x555555577abd *)
shr ymm7_0 ymm6_0 0x2e@uint64;
shr ymm7_1 ymm6_1 0x2e@uint64;
shr ymm7_2 ymm6_2 0x2e@uint64;
shr ymm7_3 ymm6_3 0x2e@uint64;
(* vpsllq $0x12,%ymm6,%ymm6                        #! PC = 0x555555577ac2 *)
shl ymm6_0 ymm6_0 0x12@uint64;
shl ymm6_1 ymm6_1 0x12@uint64;
shl ymm6_2 ymm6_2 0x12@uint64;
shl ymm6_3 ymm6_3 0x12@uint64;
(* vmovdqa %ymm12,-0x170(%rbp)                     #! EA = L0x7fffffffbde0; PC = 0x555555577ac7 *)
mov L0x7fffffffbde0 ymm12_0;
mov L0x7fffffffbde8 ymm12_1;
mov L0x7fffffffbdf0 ymm12_2;
mov L0x7fffffffbdf8 ymm12_3;
(* vpor   %ymm7,%ymm6,%ymm12                       #! PC = 0x555555577acf *)
or ymm12_0@uint64 ymm6_0 ymm7_0;
or ymm12_1@uint64 ymm6_1 ymm7_1;
or ymm12_2@uint64 ymm6_2 ymm7_2;
or ymm12_3@uint64 ymm6_3 ymm7_3;
(* vpandn %ymm12,%ymm3,%ymm9                       #! PC = 0x555555577ad3 *)
not ymm3_0n@uint64 ymm3_0;
and ymm9_0@uint64 ymm3_0n ymm12_0;
not ymm3_1n@uint64 ymm3_1;
and ymm9_1@uint64 ymm3_1n ymm12_1;
not ymm3_2n@uint64 ymm3_2;
and ymm9_2@uint64 ymm3_2n ymm12_2;
not ymm3_3n@uint64 ymm3_3;
and ymm9_3@uint64 ymm3_3n ymm12_3;
(* vpxor  %ymm2,%ymm9,%ymm9                        #! PC = 0x555555577ad8 *)
xor ymm9_0@uint64 ymm9_0 ymm2_0;
xor ymm9_1@uint64 ymm9_1 ymm2_1;
xor ymm9_2@uint64 ymm9_2 ymm2_2;
xor ymm9_3@uint64 ymm9_3 ymm2_3;
(* vpandn %ymm0,%ymm12,%ymm2                       #! PC = 0x555555577adc *)
not ymm12_0n@uint64 ymm12_0;
and ymm2_0@uint64 ymm12_0n ymm0_0;
not ymm12_1n@uint64 ymm12_1;
and ymm2_1@uint64 ymm12_1n ymm0_1;
not ymm12_2n@uint64 ymm12_2;
and ymm2_2@uint64 ymm12_2n ymm0_2;
not ymm12_3n@uint64 ymm12_3;
and ymm2_3@uint64 ymm12_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x555555577ae0 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm12,%ymm0,%ymm1                       #! PC = 0x555555577ae4 *)
xor ymm1_0@uint64 ymm0_0 ymm12_0;
xor ymm1_1@uint64 ymm0_1 ymm12_1;
xor ymm1_2@uint64 ymm0_2 ymm12_2;
xor ymm1_3@uint64 ymm0_3 ymm12_3;
(* vpxor  -0x310(%rbp),%ymm8,%ymm0                 #! EA = L0x7fffffffbc40; Value = 0x0000000000000000; PC = 0x555555577ae9 *)
xor ymm0_0@uint64 ymm8_0 L0x7fffffffbc40;
xor ymm0_1@uint64 ymm8_1 L0x7fffffffbc48;
xor ymm0_2@uint64 ymm8_2 L0x7fffffffbc50;
xor ymm0_3@uint64 ymm8_3 L0x7fffffffbc58;
(* vpxor  %ymm3,%ymm2,%ymm2                        #! PC = 0x555555577af1 *)
xor ymm2_0@uint64 ymm2_0 ymm3_0;
xor ymm2_1@uint64 ymm2_1 ymm3_1;
xor ymm2_2@uint64 ymm2_2 ymm3_2;
xor ymm2_3@uint64 ymm2_3 ymm3_3;
(* vmovdqa %ymm1,-0x150(%rbp)                      #! EA = L0x7fffffffbe00; PC = 0x555555577af5 *)
mov L0x7fffffffbe00 ymm1_0;
mov L0x7fffffffbe08 ymm1_1;
mov L0x7fffffffbe10 ymm1_2;
mov L0x7fffffffbe18 ymm1_3;
(* vpxor  -0x1f0(%rbp),%ymm8,%ymm8                 #! EA = L0x7fffffffbd60; Value = 0x0000000000000000; PC = 0x555555577afd *)
xor ymm8_0@uint64 ymm8_0 L0x7fffffffbd60;
xor ymm8_1@uint64 ymm8_1 L0x7fffffffbd68;
xor ymm8_2@uint64 ymm8_2 L0x7fffffffbd70;
xor ymm8_3@uint64 ymm8_3 L0x7fffffffbd78;
(* vpsrlq $0x25,%ymm0,%ymm1                        #! PC = 0x555555577b05 *)
shr ymm1_0 ymm0_0 0x25@uint64;
shr ymm1_1 ymm0_1 0x25@uint64;
shr ymm1_2 ymm0_2 0x25@uint64;
shr ymm1_3 ymm0_3 0x25@uint64;
(* vpsllq $0x1b,%ymm0,%ymm0                        #! PC = 0x555555577b0a *)
shl ymm0_0 ymm0_0 0x1b@uint64;
shl ymm0_1 ymm0_1 0x1b@uint64;
shl ymm0_2 ymm0_2 0x1b@uint64;
shl ymm0_3 ymm0_3 0x1b@uint64;
(* vmovdqa %ymm2,-0xb0(%rbp)                       #! EA = L0x7fffffffbea0; PC = 0x555555577b0f *)
mov L0x7fffffffbea0 ymm2_0;
mov L0x7fffffffbea8 ymm2_1;
mov L0x7fffffffbeb0 ymm2_2;
mov L0x7fffffffbeb8 ymm2_3;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x555555577b17 *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x2f0(%rbp),%ymm5,%ymm1                 #! EA = L0x7fffffffbc60; Value = 0x0000000000000000; PC = 0x555555577b1b *)
xor ymm1_0@uint64 ymm5_0 L0x7fffffffbc60;
xor ymm1_1@uint64 ymm5_1 L0x7fffffffbc68;
xor ymm1_2@uint64 ymm5_2 L0x7fffffffbc70;
xor ymm1_3@uint64 ymm5_3 L0x7fffffffbc78;
(* vpxor  -0x3b0(%rbp),%ymm5,%ymm5                 #! EA = L0x7fffffffbba0; Value = 0x0000000000000000; PC = 0x555555577b23 *)
xor ymm5_0@uint64 ymm5_0 L0x7fffffffbba0;
xor ymm5_1@uint64 ymm5_1 L0x7fffffffbba8;
xor ymm5_2@uint64 ymm5_2 L0x7fffffffbbb0;
xor ymm5_3@uint64 ymm5_3 L0x7fffffffbbb8;
(* vpsrlq $0x1c,%ymm1,%ymm2                        #! PC = 0x555555577b2b *)
shr ymm2_0 ymm1_0 0x1c@uint64;
shr ymm2_1 ymm1_1 0x1c@uint64;
shr ymm2_2 ymm1_2 0x1c@uint64;
shr ymm2_3 ymm1_3 0x1c@uint64;
(* vpsllq $0x24,%ymm1,%ymm1                        #! PC = 0x555555577b30 *)
shl ymm1_0 ymm1_0 0x24@uint64;
shl ymm1_1 ymm1_1 0x24@uint64;
shl ymm1_2 ymm1_2 0x24@uint64;
shl ymm1_3 ymm1_3 0x24@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x555555577b35 *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpxor  -0x290(%rbp),%ymm4,%ymm2                 #! EA = L0x7fffffffbcc0; Value = 0x0000000000000000; PC = 0x555555577b39 *)
xor ymm2_0@uint64 ymm4_0 L0x7fffffffbcc0;
xor ymm2_1@uint64 ymm4_1 L0x7fffffffbcc8;
xor ymm2_2@uint64 ymm4_2 L0x7fffffffbcd0;
xor ymm2_3@uint64 ymm4_3 L0x7fffffffbcd8;
(* vpxor  -0x270(%rbp),%ymm4,%ymm4                 #! EA = L0x7fffffffbce0; Value = 0x0000000000000000; PC = 0x555555577b41 *)
xor ymm4_0@uint64 ymm4_0 L0x7fffffffbce0;
xor ymm4_1@uint64 ymm4_1 L0x7fffffffbce8;
xor ymm4_2@uint64 ymm4_2 L0x7fffffffbcf0;
xor ymm4_3@uint64 ymm4_3 L0x7fffffffbcf8;
(* vpsrlq $0x36,%ymm2,%ymm3                        #! PC = 0x555555577b49 *)
shr ymm3_0 ymm2_0 0x36@uint64;
shr ymm3_1 ymm2_1 0x36@uint64;
shr ymm3_2 ymm2_2 0x36@uint64;
shr ymm3_3 ymm2_3 0x36@uint64;
(* vpsllq $0xa,%ymm2,%ymm2                         #! PC = 0x555555577b4e *)
shl ymm2_0 ymm2_0 0xa@uint64;
shl ymm2_1 ymm2_1 0xa@uint64;
shl ymm2_2 ymm2_2 0xa@uint64;
shl ymm2_3 ymm2_3 0xa@uint64;
(* vpor   %ymm3,%ymm2,%ymm2                        #! PC = 0x555555577b53 *)
or ymm2_0@uint64 ymm2_0 ymm3_0;
or ymm2_1@uint64 ymm2_1 ymm3_1;
or ymm2_2@uint64 ymm2_2 ymm3_2;
or ymm2_3@uint64 ymm2_3 ymm3_3;
(* vpandn %ymm2,%ymm1,%ymm3                        #! PC = 0x555555577b57 *)
not ymm1_0n@uint64 ymm1_0;
and ymm3_0@uint64 ymm1_0n ymm2_0;
not ymm1_1n@uint64 ymm1_1;
and ymm3_1@uint64 ymm1_1n ymm2_1;
not ymm1_2n@uint64 ymm1_2;
and ymm3_2@uint64 ymm1_2n ymm2_2;
not ymm1_3n@uint64 ymm1_3;
and ymm3_3@uint64 ymm1_3n ymm2_3;
(* vpxor  %ymm0,%ymm3,%ymm7                        #! PC = 0x555555577b5b *)
xor ymm7_0@uint64 ymm3_0 ymm0_0;
xor ymm7_1@uint64 ymm3_1 ymm0_1;
xor ymm7_2@uint64 ymm3_2 ymm0_2;
xor ymm7_3@uint64 ymm3_3 ymm0_3;
(* vpsrlq $0x31,%ymm11,%ymm3                       #! PC = 0x555555577b5f *)
shr ymm3_0 ymm11_0 0x31@uint64;
shr ymm3_1 ymm11_1 0x31@uint64;
shr ymm3_2 ymm11_2 0x31@uint64;
shr ymm3_3 ymm11_3 0x31@uint64;
(* vpsllq $0xf,%ymm11,%ymm11                       #! PC = 0x555555577b65 *)
shl ymm11_0 ymm11_0 0xf@uint64;
shl ymm11_1 ymm11_1 0xf@uint64;
shl ymm11_2 ymm11_2 0xf@uint64;
shl ymm11_3 ymm11_3 0xf@uint64;
(* vmovdqa %ymm7,%ymm12                            #! PC = 0x555555577b6b *)
mov ymm12_0 ymm7_0;
mov ymm12_1 ymm7_1;
mov ymm12_2 ymm7_2;
mov ymm12_3 ymm7_3;
(* vpor   %ymm3,%ymm11,%ymm11                      #! PC = 0x555555577b6f *)
or ymm11_0@uint64 ymm11_0 ymm3_0;
or ymm11_1@uint64 ymm11_1 ymm3_1;
or ymm11_2@uint64 ymm11_2 ymm3_2;
or ymm11_3@uint64 ymm11_3 ymm3_3;
(* vpxor  -0x70(%rbp),%ymm13,%ymm3                 #! EA = L0x7fffffffbee0; Value = 0x0000000000000000; PC = 0x555555577b73 *)
xor ymm3_0@uint64 ymm13_0 L0x7fffffffbee0;
xor ymm3_1@uint64 ymm13_1 L0x7fffffffbee8;
xor ymm3_2@uint64 ymm13_2 L0x7fffffffbef0;
xor ymm3_3@uint64 ymm13_3 L0x7fffffffbef8;
(* vpxor  -0x2b0(%rbp),%ymm13,%ymm13               #! EA = L0x7fffffffbca0; Value = 0x0000000000000000; PC = 0x555555577b78 *)
xor ymm13_0@uint64 ymm13_0 L0x7fffffffbca0;
xor ymm13_1@uint64 ymm13_1 L0x7fffffffbca8;
xor ymm13_2@uint64 ymm13_2 L0x7fffffffbcb0;
xor ymm13_3@uint64 ymm13_3 L0x7fffffffbcb8;
(* vmovdqa %ymm12,-0x350(%rbp)                     #! EA = L0x7fffffffbc00; PC = 0x555555577b80 *)
mov L0x7fffffffbc00 ymm12_0;
mov L0x7fffffffbc08 ymm12_1;
mov L0x7fffffffbc10 ymm12_2;
mov L0x7fffffffbc18 ymm12_3;
(* vpandn %ymm11,%ymm2,%ymm7                       #! PC = 0x555555577b88 *)
not ymm2_0n@uint64 ymm2_0;
and ymm7_0@uint64 ymm2_0n ymm11_0;
not ymm2_1n@uint64 ymm2_1;
and ymm7_1@uint64 ymm2_1n ymm11_1;
not ymm2_2n@uint64 ymm2_2;
and ymm7_2@uint64 ymm2_2n ymm11_2;
not ymm2_3n@uint64 ymm2_3;
and ymm7_3@uint64 ymm2_3n ymm11_3;
(* vpshufb 0x5634a(%rip),%ymm3,%ymm3        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x555555577b8d *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm3_0, ymm3_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm3_2, ymm3_3, tmp_2, tmp_3);
mov ymm3_0 tmp_0;
mov ymm3_1 tmp_1;
mov ymm3_2 tmp_2;
mov ymm3_3 tmp_3;
(* vpxor  %ymm1,%ymm7,%ymm7                        #! PC = 0x555555577b96 *)
xor ymm7_0@uint64 ymm7_0 ymm1_0;
xor ymm7_1@uint64 ymm7_1 ymm1_1;
xor ymm7_2@uint64 ymm7_2 ymm1_2;
xor ymm7_3@uint64 ymm7_3 ymm1_3;
(* vpandn %ymm3,%ymm11,%ymm6                       #! PC = 0x555555577b9a *)
not ymm11_0n@uint64 ymm11_0;
and ymm6_0@uint64 ymm11_0n ymm3_0;
not ymm11_1n@uint64 ymm11_1;
and ymm6_1@uint64 ymm11_1n ymm3_1;
not ymm11_2n@uint64 ymm11_2;
and ymm6_2@uint64 ymm11_2n ymm3_2;
not ymm11_3n@uint64 ymm11_3;
and ymm6_3@uint64 ymm11_3n ymm3_3;
(* vpxor  %ymm2,%ymm6,%ymm2                        #! PC = 0x555555577b9e *)
xor ymm2_0@uint64 ymm6_0 ymm2_0;
xor ymm2_1@uint64 ymm6_1 ymm2_1;
xor ymm2_2@uint64 ymm6_2 ymm2_2;
xor ymm2_3@uint64 ymm6_3 ymm2_3;
(* vmovdqa %ymm2,-0x70(%rbp)                       #! EA = L0x7fffffffbee0; PC = 0x555555577ba2 *)
mov L0x7fffffffbee0 ymm2_0;
mov L0x7fffffffbee8 ymm2_1;
mov L0x7fffffffbef0 ymm2_2;
mov L0x7fffffffbef8 ymm2_3;
(* vpandn %ymm0,%ymm3,%ymm2                        #! PC = 0x555555577ba7 *)
not ymm3_0n@uint64 ymm3_0;
and ymm2_0@uint64 ymm3_0n ymm0_0;
not ymm3_1n@uint64 ymm3_1;
and ymm2_1@uint64 ymm3_1n ymm0_1;
not ymm3_2n@uint64 ymm3_2;
and ymm2_2@uint64 ymm3_2n ymm0_2;
not ymm3_3n@uint64 ymm3_3;
and ymm2_3@uint64 ymm3_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x555555577bab *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm3,%ymm0,%ymm3                        #! PC = 0x555555577baf *)
xor ymm3_0@uint64 ymm0_0 ymm3_0;
xor ymm3_1@uint64 ymm0_1 ymm3_1;
xor ymm3_2@uint64 ymm0_2 ymm3_2;
xor ymm3_3@uint64 ymm0_3 ymm3_3;
(* vpsrlq $0x9,%ymm13,%ymm0                        #! PC = 0x555555577bb3 *)
shr ymm0_0 ymm13_0 0x9@uint64;
shr ymm0_1 ymm13_1 0x9@uint64;
shr ymm0_2 ymm13_2 0x9@uint64;
shr ymm0_3 ymm13_3 0x9@uint64;
(* vpxor  %ymm11,%ymm2,%ymm11                      #! PC = 0x555555577bb9 *)
xor ymm11_0@uint64 ymm2_0 ymm11_0;
xor ymm11_1@uint64 ymm2_1 ymm11_1;
xor ymm11_2@uint64 ymm2_2 ymm11_2;
xor ymm11_3@uint64 ymm2_3 ymm11_3;
(* vpsllq $0x37,%ymm13,%ymm13                      #! PC = 0x555555577bbe *)
shl ymm13_0 ymm13_0 0x37@uint64;
shl ymm13_1 ymm13_1 0x37@uint64;
shl ymm13_2 ymm13_2 0x37@uint64;
shl ymm13_3 ymm13_3 0x37@uint64;
(* vpsrlq $0x2,%ymm14,%ymm2                        #! PC = 0x555555577bc4 *)
shr ymm2_0 ymm14_0 0x2@uint64;
shr ymm2_1 ymm14_1 0x2@uint64;
shr ymm2_2 ymm14_2 0x2@uint64;
shr ymm2_3 ymm14_3 0x2@uint64;
(* vmovdqa %ymm3,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x555555577bca *)
mov L0x7fffffffbf00 ymm3_0;
mov L0x7fffffffbf08 ymm3_1;
mov L0x7fffffffbf10 ymm3_2;
mov L0x7fffffffbf18 ymm3_3;
(* vpor   %ymm0,%ymm13,%ymm13                      #! PC = 0x555555577bcf *)
or ymm13_0@uint64 ymm13_0 ymm0_0;
or ymm13_1@uint64 ymm13_1 ymm0_1;
or ymm13_2@uint64 ymm13_2 ymm0_2;
or ymm13_3@uint64 ymm13_3 ymm0_3;
(* vpsrlq $0x19,%ymm8,%ymm0                        #! PC = 0x555555577bd3 *)
shr ymm0_0 ymm8_0 0x19@uint64;
shr ymm0_1 ymm8_1 0x19@uint64;
shr ymm0_2 ymm8_2 0x19@uint64;
shr ymm0_3 ymm8_3 0x19@uint64;
(* vpsllq $0x27,%ymm8,%ymm8                        #! PC = 0x555555577bd9 *)
shl ymm8_0 ymm8_0 0x27@uint64;
shl ymm8_1 ymm8_1 0x27@uint64;
shl ymm8_2 ymm8_2 0x27@uint64;
shl ymm8_3 ymm8_3 0x27@uint64;
(* vpsllq $0x3e,%ymm14,%ymm14                      #! PC = 0x555555577bdf *)
shl ymm14_0 ymm14_0 0x3e@uint64;
shl ymm14_1 ymm14_1 0x3e@uint64;
shl ymm14_2 ymm14_2 0x3e@uint64;
shl ymm14_3 ymm14_3 0x3e@uint64;
(* vpor   %ymm0,%ymm8,%ymm8                        #! PC = 0x555555577be5 *)
or ymm8_0@uint64 ymm8_0 ymm0_0;
or ymm8_1@uint64 ymm8_1 ymm0_1;
or ymm8_2@uint64 ymm8_2 ymm0_2;
or ymm8_3@uint64 ymm8_3 ymm0_3;
(* vpor   %ymm2,%ymm14,%ymm1                       #! PC = 0x555555577be9 *)
or ymm1_0@uint64 ymm14_0 ymm2_0;
or ymm1_1@uint64 ymm14_1 ymm2_1;
or ymm1_2@uint64 ymm14_2 ymm2_2;
or ymm1_3@uint64 ymm14_3 ymm2_3;
(* vpxor  -0x210(%rbp),%ymm15,%ymm0                #! EA = L0x7fffffffbd40; Value = 0x0000000000000000; PC = 0x555555577bed *)
xor ymm0_0@uint64 ymm15_0 L0x7fffffffbd40;
xor ymm0_1@uint64 ymm15_1 L0x7fffffffbd48;
xor ymm0_2@uint64 ymm15_2 L0x7fffffffbd50;
xor ymm0_3@uint64 ymm15_3 L0x7fffffffbd58;
(* vpandn %ymm8,%ymm13,%ymm6                       #! PC = 0x555555577bf5 *)
not ymm13_0n@uint64 ymm13_0;
and ymm6_0@uint64 ymm13_0n ymm8_0;
not ymm13_1n@uint64 ymm13_1;
and ymm6_1@uint64 ymm13_1n ymm8_1;
not ymm13_2n@uint64 ymm13_2;
and ymm6_2@uint64 ymm13_2n ymm8_2;
not ymm13_3n@uint64 ymm13_3;
and ymm6_3@uint64 ymm13_3n ymm8_3;
(* vpxor  %ymm1,%ymm6,%ymm6                        #! PC = 0x555555577bfa *)
xor ymm6_0@uint64 ymm6_0 ymm1_0;
xor ymm6_1@uint64 ymm6_1 ymm1_1;
xor ymm6_2@uint64 ymm6_2 ymm1_2;
xor ymm6_3@uint64 ymm6_3 ymm1_3;
(* vpxor  %ymm12,%ymm6,%ymm2                       #! PC = 0x555555577bfe *)
xor ymm2_0@uint64 ymm6_0 ymm12_0;
xor ymm2_1@uint64 ymm6_1 ymm12_1;
xor ymm2_2@uint64 ymm6_2 ymm12_2;
xor ymm2_3@uint64 ymm6_3 ymm12_3;
(* vpxor  %ymm0,%ymm2,%ymm2                        #! PC = 0x555555577c03 *)
xor ymm2_0@uint64 ymm2_0 ymm0_0;
xor ymm2_1@uint64 ymm2_1 ymm0_1;
xor ymm2_2@uint64 ymm2_2 ymm0_2;
xor ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x17,%ymm5,%ymm0                        #! PC = 0x555555577c07 *)
shr ymm0_0 ymm5_0 0x17@uint64;
shr ymm0_1 ymm5_1 0x17@uint64;
shr ymm0_2 ymm5_2 0x17@uint64;
shr ymm0_3 ymm5_3 0x17@uint64;
(* vpxor  -0x90(%rbp),%ymm2,%ymm2                  #! EA = L0x7fffffffbec0; Value = 0x0000000000008083; PC = 0x555555577c0c *)
xor ymm2_0@uint64 ymm2_0 L0x7fffffffbec0;
xor ymm2_1@uint64 ymm2_1 L0x7fffffffbec8;
xor ymm2_2@uint64 ymm2_2 L0x7fffffffbed0;
xor ymm2_3@uint64 ymm2_3 L0x7fffffffbed8;
(* vpsllq $0x29,%ymm5,%ymm5                        #! PC = 0x555555577c14 *)
shl ymm5_0 ymm5_0 0x29@uint64;
shl ymm5_1 ymm5_1 0x29@uint64;
shl ymm5_2 ymm5_2 0x29@uint64;
shl ymm5_3 ymm5_3 0x29@uint64;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x555555577c19 *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpxor  -0x1b0(%rbp),%ymm10,%ymm0                #! EA = L0x7fffffffbda0; Value = 0x0000100000000000; PC = 0x555555577c1d *)
xor ymm0_0@uint64 ymm10_0 L0x7fffffffbda0;
xor ymm0_1@uint64 ymm10_1 L0x7fffffffbda8;
xor ymm0_2@uint64 ymm10_2 L0x7fffffffbdb0;
xor ymm0_3@uint64 ymm10_3 L0x7fffffffbdb8;
(* vpandn %ymm5,%ymm8,%ymm3                        #! PC = 0x555555577c25 *)
not ymm8_0n@uint64 ymm8_0;
and ymm3_0@uint64 ymm8_0n ymm5_0;
not ymm8_1n@uint64 ymm8_1;
and ymm3_1@uint64 ymm8_1n ymm5_1;
not ymm8_2n@uint64 ymm8_2;
and ymm3_2@uint64 ymm8_2n ymm5_2;
not ymm8_3n@uint64 ymm8_3;
and ymm3_3@uint64 ymm8_3n ymm5_3;
(* vpxor  %ymm13,%ymm3,%ymm14                      #! PC = 0x555555577c29 *)
xor ymm14_0@uint64 ymm3_0 ymm13_0;
xor ymm14_1@uint64 ymm3_1 ymm13_1;
xor ymm14_2@uint64 ymm3_2 ymm13_2;
xor ymm14_3@uint64 ymm3_3 ymm13_3;
(* vpsrlq $0x3e,%ymm4,%ymm3                        #! PC = 0x555555577c2e *)
shr ymm3_0 ymm4_0 0x3e@uint64;
shr ymm3_1 ymm4_1 0x3e@uint64;
shr ymm3_2 ymm4_2 0x3e@uint64;
shr ymm3_3 ymm4_3 0x3e@uint64;
(* vmovdqa %ymm14,%ymm15                           #! PC = 0x555555577c33 *)
mov ymm15_0 ymm14_0;
mov ymm15_1 ymm14_1;
mov ymm15_2 ymm14_2;
mov ymm15_3 ymm14_3;
(* vpxor  -0x170(%rbp),%ymm7,%ymm14                #! EA = L0x7fffffffbde0; Value = 0x0000000000000200; PC = 0x555555577c38 *)
xor ymm14_0@uint64 ymm7_0 L0x7fffffffbde0;
xor ymm14_1@uint64 ymm7_1 L0x7fffffffbde8;
xor ymm14_2@uint64 ymm7_2 L0x7fffffffbdf0;
xor ymm14_3@uint64 ymm7_3 L0x7fffffffbdf8;
(* vpsllq $0x2,%ymm4,%ymm4                         #! PC = 0x555555577c40 *)
shl ymm4_0 ymm4_0 0x2@uint64;
shl ymm4_1 ymm4_1 0x2@uint64;
shl ymm4_2 ymm4_2 0x2@uint64;
shl ymm4_3 ymm4_3 0x2@uint64;
(* vmovdqa %ymm15,-0x310(%rbp)                     #! EA = L0x7fffffffbc40; PC = 0x555555577c45 *)
mov L0x7fffffffbc40 ymm15_0;
mov L0x7fffffffbc48 ymm15_1;
mov L0x7fffffffbc50 ymm15_2;
mov L0x7fffffffbc58 ymm15_3;
(* vpxor  %ymm0,%ymm14,%ymm14                      #! PC = 0x555555577c4d *)
xor ymm14_0@uint64 ymm14_0 ymm0_0;
xor ymm14_1@uint64 ymm14_1 ymm0_1;
xor ymm14_2@uint64 ymm14_2 ymm0_2;
xor ymm14_3@uint64 ymm14_3 ymm0_3;
(* vpor   %ymm3,%ymm4,%ymm0                        #! PC = 0x555555577c51 *)
or ymm0_0@uint64 ymm4_0 ymm3_0;
or ymm0_1@uint64 ymm4_1 ymm3_1;
or ymm0_2@uint64 ymm4_2 ymm3_2;
or ymm0_3@uint64 ymm4_3 ymm3_3;
(* vpandn %ymm0,%ymm5,%ymm3                        #! PC = 0x555555577c55 *)
not ymm5_0n@uint64 ymm5_0;
and ymm3_0@uint64 ymm5_0n ymm0_0;
not ymm5_1n@uint64 ymm5_1;
and ymm3_1@uint64 ymm5_1n ymm0_1;
not ymm5_2n@uint64 ymm5_2;
and ymm3_2@uint64 ymm5_2n ymm0_2;
not ymm5_3n@uint64 ymm5_3;
and ymm3_3@uint64 ymm5_3n ymm0_3;
(* vpxor  %ymm15,%ymm14,%ymm14                     #! PC = 0x555555577c59 *)
xor ymm14_0@uint64 ymm14_0 ymm15_0;
xor ymm14_1@uint64 ymm14_1 ymm15_1;
xor ymm14_2@uint64 ymm14_2 ymm15_2;
xor ymm14_3@uint64 ymm14_3 ymm15_3;
(* vpxor  -0x70(%rbp),%ymm9,%ymm15                 #! EA = L0x7fffffffbee0; Value = 0x0000000000000400; PC = 0x555555577c5e *)
xor ymm15_0@uint64 ymm9_0 L0x7fffffffbee0;
xor ymm15_1@uint64 ymm9_1 L0x7fffffffbee8;
xor ymm15_2@uint64 ymm9_2 L0x7fffffffbef0;
xor ymm15_3@uint64 ymm9_3 L0x7fffffffbef8;
(* vmovdqa -0xf0(%rbp),%ymm4                       #! EA = L0x7fffffffbe60; Value = 0x0000000000000000; PC = 0x555555577c63 *)
mov ymm4_0 L0x7fffffffbe60;
mov ymm4_1 L0x7fffffffbe68;
mov ymm4_2 L0x7fffffffbe70;
mov ymm4_3 L0x7fffffffbe78;
(* vpxor  %ymm8,%ymm3,%ymm8                        #! PC = 0x555555577c6b *)
xor ymm8_0@uint64 ymm3_0 ymm8_0;
xor ymm8_1@uint64 ymm3_1 ymm8_1;
xor ymm8_2@uint64 ymm3_2 ymm8_2;
xor ymm8_3@uint64 ymm3_3 ymm8_3;
(* vpxor  -0x250(%rbp),%ymm4,%ymm3                 #! EA = L0x7fffffffbd00; Value = 0x0000000000008000; PC = 0x555555577c70 *)
xor ymm3_0@uint64 ymm4_0 L0x7fffffffbd00;
xor ymm3_1@uint64 ymm4_1 L0x7fffffffbd08;
xor ymm3_2@uint64 ymm4_2 L0x7fffffffbd10;
xor ymm3_3@uint64 ymm4_3 L0x7fffffffbd18;
(* vpandn %ymm13,%ymm1,%ymm4                       #! PC = 0x555555577c78 *)
not ymm1_0n@uint64 ymm1_0;
and ymm4_0@uint64 ymm1_0n ymm13_0;
not ymm1_1n@uint64 ymm1_1;
and ymm4_1@uint64 ymm1_1n ymm13_1;
not ymm1_2n@uint64 ymm1_2;
and ymm4_2@uint64 ymm1_2n ymm13_2;
not ymm1_3n@uint64 ymm1_3;
and ymm4_3@uint64 ymm1_3n ymm13_3;
(* vmovdqa -0xd0(%rbp),%ymm13                      #! EA = L0x7fffffffbe80; Value = 0x0000000000200000; PC = 0x555555577c7d *)
mov ymm13_0 L0x7fffffffbe80;
mov ymm13_1 L0x7fffffffbe88;
mov ymm13_2 L0x7fffffffbe90;
mov ymm13_3 L0x7fffffffbe98;
(* vpxor  %ymm0,%ymm4,%ymm4                        #! PC = 0x555555577c85 *)
xor ymm4_0@uint64 ymm4_0 ymm0_0;
xor ymm4_1@uint64 ymm4_1 ymm0_1;
xor ymm4_2@uint64 ymm4_2 ymm0_2;
xor ymm4_3@uint64 ymm4_3 ymm0_3;
(* vpxor  %ymm3,%ymm15,%ymm15                      #! PC = 0x555555577c89 *)
xor ymm15_0@uint64 ymm15_0 ymm3_0;
xor ymm15_1@uint64 ymm15_1 ymm3_1;
xor ymm15_2@uint64 ymm15_2 ymm3_2;
xor ymm15_3@uint64 ymm15_3 ymm3_3;
(* vpandn %ymm1,%ymm0,%ymm3                        #! PC = 0x555555577c8d *)
not ymm0_0n@uint64 ymm0_0;
and ymm3_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm3_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm3_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm3_3@uint64 ymm0_3n ymm1_3;
(* vpxor  -0x230(%rbp),%ymm13,%ymm0                #! EA = L0x7fffffffbd20; Value = 0x0000100000008000; PC = 0x555555577c91 *)
xor ymm0_0@uint64 ymm13_0 L0x7fffffffbd20;
xor ymm0_1@uint64 ymm13_1 L0x7fffffffbd28;
xor ymm0_2@uint64 ymm13_2 L0x7fffffffbd30;
xor ymm0_3@uint64 ymm13_3 L0x7fffffffbd38;
(* vpxor  %ymm5,%ymm3,%ymm5                        #! PC = 0x555555577c99 *)
xor ymm5_0@uint64 ymm3_0 ymm5_0;
xor ymm5_1@uint64 ymm3_1 ymm5_1;
xor ymm5_2@uint64 ymm3_2 ymm5_2;
xor ymm5_3@uint64 ymm3_3 ymm5_3;
(* vpxor  -0x150(%rbp),%ymm4,%ymm1                 #! EA = L0x7fffffffbe00; Value = 0x0000000000000000; PC = 0x555555577c9d *)
xor ymm1_0@uint64 ymm4_0 L0x7fffffffbe00;
xor ymm1_1@uint64 ymm4_1 L0x7fffffffbe08;
xor ymm1_2@uint64 ymm4_2 L0x7fffffffbe10;
xor ymm1_3@uint64 ymm4_3 L0x7fffffffbe18;
(* vmovdqa -0x190(%rbp),%ymm3                      #! EA = L0x7fffffffbdc0; Value = 0x0000200000000000; PC = 0x555555577ca5 *)
mov ymm3_0 L0x7fffffffbdc0;
mov ymm3_1 L0x7fffffffbdc8;
mov ymm3_2 L0x7fffffffbdd0;
mov ymm3_3 L0x7fffffffbdd8;
(* vpxor  %ymm8,%ymm15,%ymm15                      #! PC = 0x555555577cad *)
xor ymm15_0@uint64 ymm15_0 ymm8_0;
xor ymm15_1@uint64 ymm15_1 ymm8_1;
xor ymm15_2@uint64 ymm15_2 ymm8_2;
xor ymm15_3@uint64 ymm15_3 ymm8_3;
(* vpxor  -0x110(%rbp),%ymm3,%ymm3                 #! EA = L0x7fffffffbe40; Value = 0x0000000000000001; PC = 0x555555577cb2 *)
xor ymm3_0@uint64 ymm3_0 L0x7fffffffbe40;
xor ymm3_1@uint64 ymm3_1 L0x7fffffffbe48;
xor ymm3_2@uint64 ymm3_2 L0x7fffffffbe50;
xor ymm3_3@uint64 ymm3_3 L0x7fffffffbe58;
(* vpxor  %ymm5,%ymm11,%ymm12                      #! PC = 0x555555577cba *)
xor ymm12_0@uint64 ymm11_0 ymm5_0;
xor ymm12_1@uint64 ymm11_1 ymm5_1;
xor ymm12_2@uint64 ymm11_2 ymm5_2;
xor ymm12_3@uint64 ymm11_3 ymm5_3;
(* vmovdqa %ymm5,-0x270(%rbp)                      #! EA = L0x7fffffffbce0; PC = 0x555555577cbe *)
mov L0x7fffffffbce0 ymm5_0;
mov L0x7fffffffbce8 ymm5_1;
mov L0x7fffffffbcf0 ymm5_2;
mov L0x7fffffffbcf8 ymm5_3;
(* vpxor  %ymm0,%ymm1,%ymm1                        #! PC = 0x555555577cc6 *)
xor ymm1_0@uint64 ymm1_0 ymm0_0;
xor ymm1_1@uint64 ymm1_1 ymm0_1;
xor ymm1_2@uint64 ymm1_2 ymm0_2;
xor ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsllq $0x1,%ymm14,%ymm5                        #! PC = 0x555555577cca *)
shl ymm5_0 ymm14_0 0x1@uint64;
shl ymm5_1 ymm14_1 0x1@uint64;
shl ymm5_2 ymm14_2 0x1@uint64;
shl ymm5_3 ymm14_3 0x1@uint64;
(* vpxor  -0x50(%rbp),%ymm1,%ymm1                  #! EA = L0x7fffffffbf00; Value = 0x0000000000000000; PC = 0x555555577cd0 *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbf00;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbf08;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbf10;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbf18;
(* vpsrlq $0x3f,%ymm14,%ymm0                       #! PC = 0x555555577cd5 *)
shr ymm0_0 ymm14_0 0x3f@uint64;
shr ymm0_1 ymm14_1 0x3f@uint64;
shr ymm0_2 ymm14_2 0x3f@uint64;
shr ymm0_3 ymm14_3 0x3f@uint64;
(* vpxor  %ymm3,%ymm12,%ymm12                      #! PC = 0x555555577cdb *)
xor ymm12_0@uint64 ymm12_0 ymm3_0;
xor ymm12_1@uint64 ymm12_1 ymm3_1;
xor ymm12_2@uint64 ymm12_2 ymm3_2;
xor ymm12_3@uint64 ymm12_3 ymm3_3;
(* vpxor  -0xb0(%rbp),%ymm12,%ymm12                #! EA = L0x7fffffffbea0; Value = 0x0000000000000202; PC = 0x555555577cdf *)
xor ymm12_0@uint64 ymm12_0 L0x7fffffffbea0;
xor ymm12_1@uint64 ymm12_1 L0x7fffffffbea8;
xor ymm12_2@uint64 ymm12_2 L0x7fffffffbeb0;
xor ymm12_3@uint64 ymm12_3 L0x7fffffffbeb8;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x555555577ce7 *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpsllq $0x1,%ymm15,%ymm3                        #! PC = 0x555555577ceb *)
shl ymm3_0 ymm15_0 0x1@uint64;
shl ymm3_1 ymm15_1 0x1@uint64;
shl ymm3_2 ymm15_2 0x1@uint64;
shl ymm3_3 ymm15_3 0x1@uint64;
(* vpsrlq $0x3f,%ymm15,%ymm0                       #! PC = 0x555555577cf1 *)
shr ymm0_0 ymm15_0 0x3f@uint64;
shr ymm0_1 ymm15_1 0x3f@uint64;
shr ymm0_2 ymm15_2 0x3f@uint64;
shr ymm0_3 ymm15_3 0x3f@uint64;
(* vpsrlq $0x3f,%ymm12,%ymm13                      #! PC = 0x555555577cf7 *)
shr ymm13_0 ymm12_0 0x3f@uint64;
shr ymm13_1 ymm12_1 0x3f@uint64;
shr ymm13_2 ymm12_2 0x3f@uint64;
shr ymm13_3 ymm12_3 0x3f@uint64;
(* vpxor  %ymm1,%ymm5,%ymm5                        #! PC = 0x555555577cfd *)
xor ymm5_0@uint64 ymm5_0 ymm1_0;
xor ymm5_1@uint64 ymm5_1 ymm1_1;
xor ymm5_2@uint64 ymm5_2 ymm1_2;
xor ymm5_3@uint64 ymm5_3 ymm1_3;
(* vpor   %ymm0,%ymm3,%ymm3                        #! PC = 0x555555577d01 *)
or ymm3_0@uint64 ymm3_0 ymm0_0;
or ymm3_1@uint64 ymm3_1 ymm0_1;
or ymm3_2@uint64 ymm3_2 ymm0_2;
or ymm3_3@uint64 ymm3_3 ymm0_3;
(* vpsllq $0x1,%ymm12,%ymm0                        #! PC = 0x555555577d05 *)
shl ymm0_0 ymm12_0 0x1@uint64;
shl ymm0_1 ymm12_1 0x1@uint64;
shl ymm0_2 ymm12_2 0x1@uint64;
shl ymm0_3 ymm12_3 0x1@uint64;
(* vpxor  %ymm6,%ymm5,%ymm6                        #! PC = 0x555555577d0b *)
xor ymm6_0@uint64 ymm5_0 ymm6_0;
xor ymm6_1@uint64 ymm5_1 ymm6_1;
xor ymm6_2@uint64 ymm5_2 ymm6_2;
xor ymm6_3@uint64 ymm5_3 ymm6_3;
(* vpor   %ymm13,%ymm0,%ymm0                       #! PC = 0x555555577d0f *)
or ymm0_0@uint64 ymm0_0 ymm13_0;
or ymm0_1@uint64 ymm0_1 ymm13_1;
or ymm0_2@uint64 ymm0_2 ymm13_2;
or ymm0_3@uint64 ymm0_3 ymm13_3;
(* vpxor  %ymm2,%ymm3,%ymm3                        #! PC = 0x555555577d14 *)
xor ymm3_0@uint64 ymm3_0 ymm2_0;
xor ymm3_1@uint64 ymm3_1 ymm2_1;
xor ymm3_2@uint64 ymm3_2 ymm2_2;
xor ymm3_3@uint64 ymm3_3 ymm2_3;
(* vpxor  %ymm14,%ymm0,%ymm14                      #! PC = 0x555555577d18 *)
xor ymm14_0@uint64 ymm0_0 ymm14_0;
xor ymm14_1@uint64 ymm0_1 ymm14_1;
xor ymm14_2@uint64 ymm0_2 ymm14_2;
xor ymm14_3@uint64 ymm0_3 ymm14_3;
(* vpsrlq $0x3f,%ymm1,%ymm0                        #! PC = 0x555555577d1d *)
shr ymm0_0 ymm1_0 0x3f@uint64;
shr ymm0_1 ymm1_1 0x3f@uint64;
shr ymm0_2 ymm1_2 0x3f@uint64;
shr ymm0_3 ymm1_3 0x3f@uint64;
(* vpxor  %ymm10,%ymm3,%ymm10                      #! PC = 0x555555577d22 *)
xor ymm10_0@uint64 ymm3_0 ymm10_0;
xor ymm10_1@uint64 ymm3_1 ymm10_1;
xor ymm10_2@uint64 ymm3_2 ymm10_2;
xor ymm10_3@uint64 ymm3_3 ymm10_3;
(* vpsllq $0x1,%ymm1,%ymm1                         #! PC = 0x555555577d27 *)
shl ymm1_0 ymm1_0 0x1@uint64;
shl ymm1_1 ymm1_1 0x1@uint64;
shl ymm1_2 ymm1_2 0x1@uint64;
shl ymm1_3 ymm1_3 0x1@uint64;
(* vpxor  %ymm9,%ymm14,%ymm9                       #! PC = 0x555555577d2c *)
xor ymm9_0@uint64 ymm14_0 ymm9_0;
xor ymm9_1@uint64 ymm14_1 ymm9_1;
xor ymm9_2@uint64 ymm14_2 ymm9_2;
xor ymm9_3@uint64 ymm14_3 ymm9_3;
(* vpxor  %ymm7,%ymm3,%ymm7                        #! PC = 0x555555577d31 *)
xor ymm7_0@uint64 ymm3_0 ymm7_0;
xor ymm7_1@uint64 ymm3_1 ymm7_1;
xor ymm7_2@uint64 ymm3_2 ymm7_2;
xor ymm7_3@uint64 ymm3_3 ymm7_3;
(* vpor   %ymm0,%ymm1,%ymm1                        #! PC = 0x555555577d35 *)
or ymm1_0@uint64 ymm1_0 ymm0_0;
or ymm1_1@uint64 ymm1_1 ymm0_1;
or ymm1_2@uint64 ymm1_2 ymm0_2;
or ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3f,%ymm2,%ymm0                        #! PC = 0x555555577d39 *)
shr ymm0_0 ymm2_0 0x3f@uint64;
shr ymm0_1 ymm2_1 0x3f@uint64;
shr ymm0_2 ymm2_2 0x3f@uint64;
shr ymm0_3 ymm2_3 0x3f@uint64;
(* vpxor  %ymm8,%ymm14,%ymm8                       #! PC = 0x555555577d3e *)
xor ymm8_0@uint64 ymm14_0 ymm8_0;
xor ymm8_1@uint64 ymm14_1 ymm8_1;
xor ymm8_2@uint64 ymm14_2 ymm8_2;
xor ymm8_3@uint64 ymm14_3 ymm8_3;
(* vpsllq $0x1,%ymm2,%ymm2                         #! PC = 0x555555577d43 *)
shl ymm2_0 ymm2_0 0x1@uint64;
shl ymm2_1 ymm2_1 0x1@uint64;
shl ymm2_2 ymm2_2 0x1@uint64;
shl ymm2_3 ymm2_3 0x1@uint64;
(* vpxor  %ymm15,%ymm1,%ymm15                      #! PC = 0x555555577d48 *)
xor ymm15_0@uint64 ymm1_0 ymm15_0;
xor ymm15_1@uint64 ymm1_1 ymm15_1;
xor ymm15_2@uint64 ymm1_2 ymm15_2;
xor ymm15_3@uint64 ymm1_3 ymm15_3;
(* vpor   %ymm0,%ymm2,%ymm2                        #! PC = 0x555555577d4d *)
or ymm2_0@uint64 ymm2_0 ymm0_0;
or ymm2_1@uint64 ymm2_1 ymm0_1;
or ymm2_2@uint64 ymm2_2 ymm0_2;
or ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x14,%ymm10,%ymm0                       #! PC = 0x555555577d51 *)
shr ymm0_0 ymm10_0 0x14@uint64;
shr ymm0_1 ymm10_1 0x14@uint64;
shr ymm0_2 ymm10_2 0x14@uint64;
shr ymm0_3 ymm10_3 0x14@uint64;
(* vpxor  %ymm11,%ymm15,%ymm11                     #! PC = 0x555555577d57 *)
xor ymm11_0@uint64 ymm15_0 ymm11_0;
xor ymm11_1@uint64 ymm15_1 ymm11_1;
xor ymm11_2@uint64 ymm15_2 ymm11_2;
xor ymm11_3@uint64 ymm15_3 ymm11_3;
(* vpsllq $0x2c,%ymm10,%ymm10                      #! PC = 0x555555577d5c *)
shl ymm10_0 ymm10_0 0x2c@uint64;
shl ymm10_1 ymm10_1 0x2c@uint64;
shl ymm10_2 ymm10_2 0x2c@uint64;
shl ymm10_3 ymm10_3 0x2c@uint64;
(* vpxor  %ymm12,%ymm2,%ymm12                      #! PC = 0x555555577d62 *)
xor ymm12_0@uint64 ymm2_0 ymm12_0;
xor ymm12_1@uint64 ymm2_1 ymm12_1;
xor ymm12_2@uint64 ymm2_2 ymm12_2;
xor ymm12_3@uint64 ymm2_3 ymm12_3;
(* vpxor  -0x90(%rbp),%ymm5,%ymm2                  #! EA = L0x7fffffffbec0; Value = 0x0000000000008083; PC = 0x555555577d67 *)
xor ymm2_0@uint64 ymm5_0 L0x7fffffffbec0;
xor ymm2_1@uint64 ymm5_1 L0x7fffffffbec8;
xor ymm2_2@uint64 ymm5_2 L0x7fffffffbed0;
xor ymm2_3@uint64 ymm5_3 L0x7fffffffbed8;
(* vpor   %ymm0,%ymm10,%ymm10                      #! PC = 0x555555577d6f *)
or ymm10_0@uint64 ymm10_0 ymm0_0;
or ymm10_1@uint64 ymm10_1 ymm0_1;
or ymm10_2@uint64 ymm10_2 ymm0_2;
or ymm10_3@uint64 ymm10_3 ymm0_3;
(* vpsrlq $0x15,%ymm9,%ymm0                        #! PC = 0x555555577d73 *)
shr ymm0_0 ymm9_0 0x15@uint64;
shr ymm0_1 ymm9_1 0x15@uint64;
shr ymm0_2 ymm9_2 0x15@uint64;
shr ymm0_3 ymm9_3 0x15@uint64;
(* vpxor  %ymm4,%ymm12,%ymm4                       #! PC = 0x555555577d79 *)
xor ymm4_0@uint64 ymm12_0 ymm4_0;
xor ymm4_1@uint64 ymm12_1 ymm4_1;
xor ymm4_2@uint64 ymm12_2 ymm4_2;
xor ymm4_3@uint64 ymm12_3 ymm4_3;
(* vpsllq $0x2b,%ymm9,%ymm9                        #! PC = 0x555555577d7d *)
shl ymm9_0 ymm9_0 0x2b@uint64;
shl ymm9_1 ymm9_1 0x2b@uint64;
shl ymm9_2 ymm9_2 0x2b@uint64;
shl ymm9_3 ymm9_3 0x2b@uint64;
(* vpor   %ymm0,%ymm9,%ymm9                        #! PC = 0x555555577d83 *)
or ymm9_0@uint64 ymm9_0 ymm0_0;
or ymm9_1@uint64 ymm9_1 ymm0_1;
or ymm9_2@uint64 ymm9_2 ymm0_2;
or ymm9_3@uint64 ymm9_3 ymm0_3;
(* vmovq  %rcx,%xmm0                               #! PC = 0x555555577d87 *)
mov xmm0_0 rcx;
mov xmm0_1 0@uint64;
(* vpandn %ymm9,%ymm10,%ymm1                       #! PC = 0x555555577d8c *)
not ymm10_0n@uint64 ymm10_0;
and ymm1_0@uint64 ymm10_0n ymm9_0;
not ymm10_1n@uint64 ymm10_1;
and ymm1_1@uint64 ymm10_1n ymm9_1;
not ymm10_2n@uint64 ymm10_2;
and ymm1_2@uint64 ymm10_2n ymm9_2;
not ymm10_3n@uint64 ymm10_3;
and ymm1_3@uint64 ymm10_3n ymm9_3;
(* vpbroadcastq %xmm0,%ymm0                        #! PC = 0x555555577d91 *)
mov ymm0_0 xmm0_0;
mov ymm0_1 xmm0_0;
mov ymm0_2 xmm0_0;
mov ymm0_3 xmm0_0;
(* vpxor  %ymm1,%ymm0,%ymm0                        #! PC = 0x555555577d96 *)
xor ymm0_0@uint64 ymm0_0 ymm1_0;
xor ymm0_1@uint64 ymm0_1 ymm1_1;
xor ymm0_2@uint64 ymm0_2 ymm1_2;
xor ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  %ymm2,%ymm0,%ymm0                        #! PC = 0x555555577d9a *)
xor ymm0_0@uint64 ymm0_0 ymm2_0;
xor ymm0_1@uint64 ymm0_1 ymm2_1;
xor ymm0_2@uint64 ymm0_2 ymm2_2;
xor ymm0_3@uint64 ymm0_3 ymm2_3;
(* vmovdqa %ymm0,-0x90(%rbp)                       #! EA = L0x7fffffffbec0; PC = 0x555555577d9e *)
mov L0x7fffffffbec0 ymm0_0;
mov L0x7fffffffbec8 ymm0_1;
mov L0x7fffffffbed0 ymm0_2;
mov L0x7fffffffbed8 ymm0_3;
(* vpsrlq $0x2b,%ymm11,%ymm0                       #! PC = 0x555555577da6 *)
shr ymm0_0 ymm11_0 0x2b@uint64;
shr ymm0_1 ymm11_1 0x2b@uint64;
shr ymm0_2 ymm11_2 0x2b@uint64;
shr ymm0_3 ymm11_3 0x2b@uint64;
(* vpsllq $0x15,%ymm11,%ymm11                      #! PC = 0x555555577dac *)
shl ymm11_0 ymm11_0 0x15@uint64;
shl ymm11_1 ymm11_1 0x15@uint64;
shl ymm11_2 ymm11_2 0x15@uint64;
shl ymm11_3 ymm11_3 0x15@uint64;
(* vpor   %ymm0,%ymm11,%ymm11                      #! PC = 0x555555577db2 *)
or ymm11_0@uint64 ymm11_0 ymm0_0;
or ymm11_1@uint64 ymm11_1 ymm0_1;
or ymm11_2@uint64 ymm11_2 ymm0_2;
or ymm11_3@uint64 ymm11_3 ymm0_3;
(* vpandn %ymm11,%ymm9,%ymm0                       #! PC = 0x555555577db6 *)
not ymm9_0n@uint64 ymm9_0;
and ymm0_0@uint64 ymm9_0n ymm11_0;
not ymm9_1n@uint64 ymm9_1;
and ymm0_1@uint64 ymm9_1n ymm11_1;
not ymm9_2n@uint64 ymm9_2;
and ymm0_2@uint64 ymm9_2n ymm11_2;
not ymm9_3n@uint64 ymm9_3;
and ymm0_3@uint64 ymm9_3n ymm11_3;
(* vpxor  %ymm10,%ymm0,%ymm0                       #! PC = 0x555555577dbb *)
xor ymm0_0@uint64 ymm0_0 ymm10_0;
xor ymm0_1@uint64 ymm0_1 ymm10_1;
xor ymm0_2@uint64 ymm0_2 ymm10_2;
xor ymm0_3@uint64 ymm0_3 ymm10_3;
(* vpandn %ymm10,%ymm2,%ymm10                      #! PC = 0x555555577dc0 *)
not ymm2_0n@uint64 ymm2_0;
and ymm10_0@uint64 ymm2_0n ymm10_0;
not ymm2_1n@uint64 ymm2_1;
and ymm10_1@uint64 ymm2_1n ymm10_1;
not ymm2_2n@uint64 ymm2_2;
and ymm10_2@uint64 ymm2_2n ymm10_2;
not ymm2_3n@uint64 ymm2_3;
and ymm10_3@uint64 ymm2_3n ymm10_3;
(* vmovdqa %ymm0,-0x2f0(%rbp)                      #! EA = L0x7fffffffbc60; PC = 0x555555577dc5 *)
mov L0x7fffffffbc60 ymm0_0;
mov L0x7fffffffbc68 ymm0_1;
mov L0x7fffffffbc70 ymm0_2;
mov L0x7fffffffbc78 ymm0_3;
(* vpsrlq $0x32,%ymm4,%ymm0                        #! PC = 0x555555577dcd *)
shr ymm0_0 ymm4_0 0x32@uint64;
shr ymm0_1 ymm4_1 0x32@uint64;
shr ymm0_2 ymm4_2 0x32@uint64;
shr ymm0_3 ymm4_3 0x32@uint64;
(* vpsllq $0xe,%ymm4,%ymm4                         #! PC = 0x555555577dd2 *)
shl ymm4_0 ymm4_0 0xe@uint64;
shl ymm4_1 ymm4_1 0xe@uint64;
shl ymm4_2 ymm4_2 0xe@uint64;
shl ymm4_3 ymm4_3 0xe@uint64;
(* vpor   %ymm0,%ymm4,%ymm4                        #! PC = 0x555555577dd7 *)
or ymm4_0@uint64 ymm4_0 ymm0_0;
or ymm4_1@uint64 ymm4_1 ymm0_1;
or ymm4_2@uint64 ymm4_2 ymm0_2;
or ymm4_3@uint64 ymm4_3 ymm0_3;
(* vpandn %ymm4,%ymm11,%ymm0                       #! PC = 0x555555577ddb *)
not ymm11_0n@uint64 ymm11_0;
and ymm0_0@uint64 ymm11_0n ymm4_0;
not ymm11_1n@uint64 ymm11_1;
and ymm0_1@uint64 ymm11_1n ymm4_1;
not ymm11_2n@uint64 ymm11_2;
and ymm0_2@uint64 ymm11_2n ymm4_2;
not ymm11_3n@uint64 ymm11_3;
and ymm0_3@uint64 ymm11_3n ymm4_3;
(* vpxor  %ymm9,%ymm0,%ymm9                        #! PC = 0x555555577ddf *)
xor ymm9_0@uint64 ymm0_0 ymm9_0;
xor ymm9_1@uint64 ymm0_1 ymm9_1;
xor ymm9_2@uint64 ymm0_2 ymm9_2;
xor ymm9_3@uint64 ymm0_3 ymm9_3;
(* vpandn %ymm2,%ymm4,%ymm0                        #! PC = 0x555555577de4 *)
not ymm4_0n@uint64 ymm4_0;
and ymm0_0@uint64 ymm4_0n ymm2_0;
not ymm4_1n@uint64 ymm4_1;
and ymm0_1@uint64 ymm4_1n ymm2_1;
not ymm4_2n@uint64 ymm4_2;
and ymm0_2@uint64 ymm4_2n ymm2_2;
not ymm4_3n@uint64 ymm4_3;
and ymm0_3@uint64 ymm4_3n ymm2_3;
(* vpxor  %ymm11,%ymm0,%ymm11                      #! PC = 0x555555577de8 *)
xor ymm11_0@uint64 ymm0_0 ymm11_0;
xor ymm11_1@uint64 ymm0_1 ymm11_1;
xor ymm11_2@uint64 ymm0_2 ymm11_2;
xor ymm11_3@uint64 ymm0_3 ymm11_3;
(* vmovdqa %ymm9,-0x2d0(%rbp)                      #! EA = L0x7fffffffbc80; PC = 0x555555577ded *)
mov L0x7fffffffbc80 ymm9_0;
mov L0x7fffffffbc88 ymm9_1;
mov L0x7fffffffbc90 ymm9_2;
mov L0x7fffffffbc98 ymm9_3;
(* vmovdqa %ymm11,-0x130(%rbp)                     #! EA = L0x7fffffffbe20; PC = 0x555555577df5 *)
mov L0x7fffffffbe20 ymm11_0;
mov L0x7fffffffbe28 ymm11_1;
mov L0x7fffffffbe30 ymm11_2;
mov L0x7fffffffbe38 ymm11_3;
(* vpxor  %ymm4,%ymm10,%ymm11                      #! PC = 0x555555577dfd *)
xor ymm11_0@uint64 ymm10_0 ymm4_0;
xor ymm11_1@uint64 ymm10_1 ymm4_1;
xor ymm11_2@uint64 ymm10_2 ymm4_2;
xor ymm11_3@uint64 ymm10_3 ymm4_3;
(* vmovdqa %ymm11,-0x2b0(%rbp)                     #! EA = L0x7fffffffbca0; PC = 0x555555577e01 *)
mov L0x7fffffffbca0 ymm11_0;
mov L0x7fffffffbca8 ymm11_1;
mov L0x7fffffffbcb0 ymm11_2;
mov L0x7fffffffbcb8 ymm11_3;
(* vpxor  -0x110(%rbp),%ymm15,%ymm11               #! EA = L0x7fffffffbe40; Value = 0x0000000000000001; PC = 0x555555577e09 *)
xor ymm11_0@uint64 ymm15_0 L0x7fffffffbe40;
xor ymm11_1@uint64 ymm15_1 L0x7fffffffbe48;
xor ymm11_2@uint64 ymm15_2 L0x7fffffffbe50;
xor ymm11_3@uint64 ymm15_3 L0x7fffffffbe58;
(* vpsrlq $0x24,%ymm11,%ymm0                       #! PC = 0x555555577e11 *)
shr ymm0_0 ymm11_0 0x24@uint64;
shr ymm0_1 ymm11_1 0x24@uint64;
shr ymm0_2 ymm11_2 0x24@uint64;
shr ymm0_3 ymm11_3 0x24@uint64;
(* vpsllq $0x1c,%ymm11,%ymm2                       #! PC = 0x555555577e17 *)
shl ymm2_0 ymm11_0 0x1c@uint64;
shl ymm2_1 ymm11_1 0x1c@uint64;
shl ymm2_2 ymm11_2 0x1c@uint64;
shl ymm2_3 ymm11_3 0x1c@uint64;
(* vpor   %ymm0,%ymm2,%ymm2                        #! PC = 0x555555577e1d *)
or ymm2_0@uint64 ymm2_0 ymm0_0;
or ymm2_1@uint64 ymm2_1 ymm0_1;
or ymm2_2@uint64 ymm2_2 ymm0_2;
or ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpxor  -0xd0(%rbp),%ymm12,%ymm0                 #! EA = L0x7fffffffbe80; Value = 0x0000000000200000; PC = 0x555555577e21 *)
xor ymm0_0@uint64 ymm12_0 L0x7fffffffbe80;
xor ymm0_1@uint64 ymm12_1 L0x7fffffffbe88;
xor ymm0_2@uint64 ymm12_2 L0x7fffffffbe90;
xor ymm0_3@uint64 ymm12_3 L0x7fffffffbe98;
(* vpsrlq $0x2c,%ymm0,%ymm1                        #! PC = 0x555555577e29 *)
shr ymm1_0 ymm0_0 0x2c@uint64;
shr ymm1_1 ymm0_1 0x2c@uint64;
shr ymm1_2 ymm0_2 0x2c@uint64;
shr ymm1_3 ymm0_3 0x2c@uint64;
(* vpsllq $0x14,%ymm0,%ymm0                        #! PC = 0x555555577e2e *)
shl ymm0_0 ymm0_0 0x14@uint64;
shl ymm0_1 ymm0_1 0x14@uint64;
shl ymm0_2 ymm0_2 0x14@uint64;
shl ymm0_3 ymm0_3 0x14@uint64;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x555555577e33 *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x1d0(%rbp),%ymm5,%ymm1                 #! EA = L0x7fffffffbd80; Value = 0x0000000000000002; PC = 0x555555577e37 *)
xor ymm1_0@uint64 ymm5_0 L0x7fffffffbd80;
xor ymm1_1@uint64 ymm5_1 L0x7fffffffbd88;
xor ymm1_2@uint64 ymm5_2 L0x7fffffffbd90;
xor ymm1_3@uint64 ymm5_3 L0x7fffffffbd98;
(* vpsrlq $0x3d,%ymm1,%ymm4                        #! PC = 0x555555577e3f *)
shr ymm4_0 ymm1_0 0x3d@uint64;
shr ymm4_1 ymm1_1 0x3d@uint64;
shr ymm4_2 ymm1_2 0x3d@uint64;
shr ymm4_3 ymm1_3 0x3d@uint64;
(* vpsllq $0x3,%ymm1,%ymm1                         #! PC = 0x555555577e44 *)
shl ymm1_0 ymm1_0 0x3@uint64;
shl ymm1_1 ymm1_1 0x3@uint64;
shl ymm1_2 ymm1_2 0x3@uint64;
shl ymm1_3 ymm1_3 0x3@uint64;
(* vpor   %ymm4,%ymm1,%ymm1                        #! PC = 0x555555577e49 *)
or ymm1_0@uint64 ymm1_0 ymm4_0;
or ymm1_1@uint64 ymm1_1 ymm4_1;
or ymm1_2@uint64 ymm1_2 ymm4_2;
or ymm1_3@uint64 ymm1_3 ymm4_3;
(* vpandn %ymm1,%ymm0,%ymm4                        #! PC = 0x555555577e4d *)
not ymm0_0n@uint64 ymm0_0;
and ymm4_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm4_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm4_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm4_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm2,%ymm4,%ymm9                        #! PC = 0x555555577e51 *)
xor ymm9_0@uint64 ymm4_0 ymm2_0;
xor ymm9_1@uint64 ymm4_1 ymm2_1;
xor ymm9_2@uint64 ymm4_2 ymm2_2;
xor ymm9_3@uint64 ymm4_3 ymm2_3;
(* vpsrlq $0x13,%ymm7,%ymm4                        #! PC = 0x555555577e55 *)
shr ymm4_0 ymm7_0 0x13@uint64;
shr ymm4_1 ymm7_1 0x13@uint64;
shr ymm4_2 ymm7_2 0x13@uint64;
shr ymm4_3 ymm7_3 0x13@uint64;
(* vpsllq $0x2d,%ymm7,%ymm7                        #! PC = 0x555555577e5a *)
shl ymm7_0 ymm7_0 0x2d@uint64;
shl ymm7_1 ymm7_1 0x2d@uint64;
shl ymm7_2 ymm7_2 0x2d@uint64;
shl ymm7_3 ymm7_3 0x2d@uint64;
(* vmovdqa %ymm9,-0x290(%rbp)                      #! EA = L0x7fffffffbcc0; PC = 0x555555577e5f *)
mov L0x7fffffffbcc0 ymm9_0;
mov L0x7fffffffbcc8 ymm9_1;
mov L0x7fffffffbcd0 ymm9_2;
mov L0x7fffffffbcd8 ymm9_3;
(* vpor   %ymm4,%ymm7,%ymm13                       #! PC = 0x555555577e67 *)
or ymm13_0@uint64 ymm7_0 ymm4_0;
or ymm13_1@uint64 ymm7_1 ymm4_1;
or ymm13_2@uint64 ymm7_2 ymm4_2;
or ymm13_3@uint64 ymm7_3 ymm4_3;
(* vpsrlq $0x3,%ymm8,%ymm4                         #! PC = 0x555555577e6b *)
shr ymm4_0 ymm8_0 0x3@uint64;
shr ymm4_1 ymm8_1 0x3@uint64;
shr ymm4_2 ymm8_2 0x3@uint64;
shr ymm4_3 ymm8_3 0x3@uint64;
(* vpsllq $0x3d,%ymm8,%ymm8                        #! PC = 0x555555577e71 *)
shl ymm8_0 ymm8_0 0x3d@uint64;
shl ymm8_1 ymm8_1 0x3d@uint64;
shl ymm8_2 ymm8_2 0x3d@uint64;
shl ymm8_3 ymm8_3 0x3d@uint64;
(* vpandn %ymm13,%ymm1,%ymm9                       #! PC = 0x555555577e77 *)
not ymm1_0n@uint64 ymm1_0;
and ymm9_0@uint64 ymm1_0n ymm13_0;
not ymm1_1n@uint64 ymm1_1;
and ymm9_1@uint64 ymm1_1n ymm13_1;
not ymm1_2n@uint64 ymm1_2;
and ymm9_2@uint64 ymm1_2n ymm13_2;
not ymm1_3n@uint64 ymm1_3;
and ymm9_3@uint64 ymm1_3n ymm13_3;
(* vpor   %ymm4,%ymm8,%ymm8                        #! PC = 0x555555577e7c *)
or ymm8_0@uint64 ymm8_0 ymm4_0;
or ymm8_1@uint64 ymm8_1 ymm4_1;
or ymm8_2@uint64 ymm8_2 ymm4_2;
or ymm8_3@uint64 ymm8_3 ymm4_3;
(* vpxor  %ymm0,%ymm9,%ymm9                        #! PC = 0x555555577e80 *)
xor ymm9_0@uint64 ymm9_0 ymm0_0;
xor ymm9_1@uint64 ymm9_1 ymm0_1;
xor ymm9_2@uint64 ymm9_2 ymm0_2;
xor ymm9_3@uint64 ymm9_3 ymm0_3;
(* vpandn %ymm8,%ymm13,%ymm4                       #! PC = 0x555555577e84 *)
not ymm13_0n@uint64 ymm13_0;
and ymm4_0@uint64 ymm13_0n ymm8_0;
not ymm13_1n@uint64 ymm13_1;
and ymm4_1@uint64 ymm13_1n ymm8_1;
not ymm13_2n@uint64 ymm13_2;
and ymm4_2@uint64 ymm13_2n ymm8_2;
not ymm13_3n@uint64 ymm13_3;
and ymm4_3@uint64 ymm13_3n ymm8_3;
(* vpxor  %ymm1,%ymm4,%ymm10                       #! PC = 0x555555577e89 *)
xor ymm10_0@uint64 ymm4_0 ymm1_0;
xor ymm10_1@uint64 ymm4_1 ymm1_1;
xor ymm10_2@uint64 ymm4_2 ymm1_2;
xor ymm10_3@uint64 ymm4_3 ymm1_3;
(* vpandn %ymm2,%ymm8,%ymm1                        #! PC = 0x555555577e8d *)
not ymm8_0n@uint64 ymm8_0;
and ymm1_0@uint64 ymm8_0n ymm2_0;
not ymm8_1n@uint64 ymm8_1;
and ymm1_1@uint64 ymm8_1n ymm2_1;
not ymm8_2n@uint64 ymm8_2;
and ymm1_2@uint64 ymm8_2n ymm2_2;
not ymm8_3n@uint64 ymm8_3;
and ymm1_3@uint64 ymm8_3n ymm2_3;
(* vpandn %ymm0,%ymm2,%ymm2                        #! PC = 0x555555577e91 *)
not ymm2_0n@uint64 ymm2_0;
and ymm2_0@uint64 ymm2_0n ymm0_0;
not ymm2_1n@uint64 ymm2_1;
and ymm2_1@uint64 ymm2_1n ymm0_1;
not ymm2_2n@uint64 ymm2_2;
and ymm2_2@uint64 ymm2_2n ymm0_2;
not ymm2_3n@uint64 ymm2_3;
and ymm2_3@uint64 ymm2_3n ymm0_3;
(* vpxor  %ymm8,%ymm2,%ymm8                        #! PC = 0x555555577e95 *)
xor ymm8_0@uint64 ymm2_0 ymm8_0;
xor ymm8_1@uint64 ymm2_1 ymm8_1;
xor ymm8_2@uint64 ymm2_2 ymm8_2;
xor ymm8_3@uint64 ymm2_3 ymm8_3;
(* vpxor  -0x1b0(%rbp),%ymm3,%ymm2                 #! EA = L0x7fffffffbda0; Value = 0x0000100000000000; PC = 0x555555577e9a *)
xor ymm2_0@uint64 ymm3_0 L0x7fffffffbda0;
xor ymm2_1@uint64 ymm3_1 L0x7fffffffbda8;
xor ymm2_2@uint64 ymm3_2 L0x7fffffffbdb0;
xor ymm2_3@uint64 ymm3_3 L0x7fffffffbdb8;
(* vpxor  %ymm13,%ymm1,%ymm7                       #! PC = 0x555555577ea2 *)
xor ymm7_0@uint64 ymm1_0 ymm13_0;
xor ymm7_1@uint64 ymm1_1 ymm13_1;
xor ymm7_2@uint64 ymm1_2 ymm13_2;
xor ymm7_3@uint64 ymm1_3 ymm13_3;
(* vmovdqa %ymm10,-0x1f0(%rbp)                     #! EA = L0x7fffffffbd60; PC = 0x555555577ea7 *)
mov L0x7fffffffbd60 ymm10_0;
mov L0x7fffffffbd68 ymm10_1;
mov L0x7fffffffbd70 ymm10_2;
mov L0x7fffffffbd78 ymm10_3;
(* vmovdqa %ymm8,-0x110(%rbp)                      #! EA = L0x7fffffffbe40; PC = 0x555555577eaf *)
mov L0x7fffffffbe40 ymm8_0;
mov L0x7fffffffbe48 ymm8_1;
mov L0x7fffffffbe50 ymm8_2;
mov L0x7fffffffbe58 ymm8_3;
(* vpxor  -0xb0(%rbp),%ymm15,%ymm8                 #! EA = L0x7fffffffbea0; Value = 0x0000000000000202; PC = 0x555555577eb7 *)
xor ymm8_0@uint64 ymm15_0 L0x7fffffffbea0;
xor ymm8_1@uint64 ymm15_1 L0x7fffffffbea8;
xor ymm8_2@uint64 ymm15_2 L0x7fffffffbeb0;
xor ymm8_3@uint64 ymm15_3 L0x7fffffffbeb8;
(* vpsrlq $0x3f,%ymm2,%ymm1                        #! PC = 0x555555577ebf *)
shr ymm1_0 ymm2_0 0x3f@uint64;
shr ymm1_1 ymm2_1 0x3f@uint64;
shr ymm1_2 ymm2_2 0x3f@uint64;
shr ymm1_3 ymm2_3 0x3f@uint64;
(* vpsllq $0x1,%ymm2,%ymm2                         #! PC = 0x555555577ec4 *)
shl ymm2_0 ymm2_0 0x1@uint64;
shl ymm2_1 ymm2_1 0x1@uint64;
shl ymm2_2 ymm2_2 0x1@uint64;
shl ymm2_3 ymm2_3 0x1@uint64;
(* vmovdqa %ymm7,-0x1d0(%rbp)                      #! EA = L0x7fffffffbd80; PC = 0x555555577ec9 *)
mov L0x7fffffffbd80 ymm7_0;
mov L0x7fffffffbd88 ymm7_1;
mov L0x7fffffffbd90 ymm7_2;
mov L0x7fffffffbd98 ymm7_3;
(* vpor   %ymm1,%ymm2,%ymm0                        #! PC = 0x555555577ed1 *)
or ymm0_0@uint64 ymm2_0 ymm1_0;
or ymm0_1@uint64 ymm2_1 ymm1_1;
or ymm0_2@uint64 ymm2_2 ymm1_2;
or ymm0_3@uint64 ymm2_3 ymm1_3;
(* vpxor  -0xf0(%rbp),%ymm14,%ymm2                 #! EA = L0x7fffffffbe60; Value = 0x0000000000000000; PC = 0x555555577ed5 *)
xor ymm2_0@uint64 ymm14_0 L0x7fffffffbe60;
xor ymm2_1@uint64 ymm14_1 L0x7fffffffbe68;
xor ymm2_2@uint64 ymm14_2 L0x7fffffffbe70;
xor ymm2_3@uint64 ymm14_3 L0x7fffffffbe78;
(* vpsrlq $0x27,%ymm8,%ymm4                        #! PC = 0x555555577edd *)
shr ymm4_0 ymm8_0 0x27@uint64;
shr ymm4_1 ymm8_1 0x27@uint64;
shr ymm4_2 ymm8_2 0x27@uint64;
shr ymm4_3 ymm8_3 0x27@uint64;
(* vpsllq $0x19,%ymm8,%ymm8                        #! PC = 0x555555577ee3 *)
shl ymm8_0 ymm8_0 0x19@uint64;
shl ymm8_1 ymm8_1 0x19@uint64;
shl ymm8_2 ymm8_2 0x19@uint64;
shl ymm8_3 ymm8_3 0x19@uint64;
(* vpsrlq $0x3a,%ymm2,%ymm1                        #! PC = 0x555555577ee9 *)
shr ymm1_0 ymm2_0 0x3a@uint64;
shr ymm1_1 ymm2_1 0x3a@uint64;
shr ymm1_2 ymm2_2 0x3a@uint64;
shr ymm1_3 ymm2_3 0x3a@uint64;
(* vpsllq $0x6,%ymm2,%ymm2                         #! PC = 0x555555577eee *)
shl ymm2_0 ymm2_0 0x6@uint64;
shl ymm2_1 ymm2_1 0x6@uint64;
shl ymm2_2 ymm2_2 0x6@uint64;
shl ymm2_3 ymm2_3 0x6@uint64;
(* vpor   %ymm1,%ymm2,%ymm2                        #! PC = 0x555555577ef3 *)
or ymm2_0@uint64 ymm2_0 ymm1_0;
or ymm2_1@uint64 ymm2_1 ymm1_1;
or ymm2_2@uint64 ymm2_2 ymm1_2;
or ymm2_3@uint64 ymm2_3 ymm1_3;
(* vpor   %ymm4,%ymm8,%ymm1                        #! PC = 0x555555577ef7 *)
or ymm1_0@uint64 ymm8_0 ymm4_0;
or ymm1_1@uint64 ymm8_1 ymm4_1;
or ymm1_2@uint64 ymm8_2 ymm4_2;
or ymm1_3@uint64 ymm8_3 ymm4_3;
(* vpandn %ymm1,%ymm2,%ymm4                        #! PC = 0x555555577efb *)
not ymm2_0n@uint64 ymm2_0;
and ymm4_0@uint64 ymm2_0n ymm1_0;
not ymm2_1n@uint64 ymm2_1;
and ymm4_1@uint64 ymm2_1n ymm1_1;
not ymm2_2n@uint64 ymm2_2;
and ymm4_2@uint64 ymm2_2n ymm1_2;
not ymm2_3n@uint64 ymm2_3;
and ymm4_3@uint64 ymm2_3n ymm1_3;
(* vpxor  %ymm0,%ymm4,%ymm8                        #! PC = 0x555555577eff *)
xor ymm8_0@uint64 ymm4_0 ymm0_0;
xor ymm8_1@uint64 ymm4_1 ymm0_1;
xor ymm8_2@uint64 ymm4_2 ymm0_2;
xor ymm8_3@uint64 ymm4_3 ymm0_3;
(* vpxor  -0x50(%rbp),%ymm12,%ymm4                 #! EA = L0x7fffffffbf00; Value = 0x0000000000000000; PC = 0x555555577f03 *)
xor ymm4_0@uint64 ymm12_0 L0x7fffffffbf00;
xor ymm4_1@uint64 ymm12_1 L0x7fffffffbf08;
xor ymm4_2@uint64 ymm12_2 L0x7fffffffbf10;
xor ymm4_3@uint64 ymm12_3 L0x7fffffffbf18;
(* vmovdqa %ymm8,-0xd0(%rbp)                       #! EA = L0x7fffffffbe80; PC = 0x555555577f08 *)
mov L0x7fffffffbe80 ymm8_0;
mov L0x7fffffffbe88 ymm8_1;
mov L0x7fffffffbe90 ymm8_2;
mov L0x7fffffffbe98 ymm8_3;
(* vpshufb 0x55fe7(%rip),%ymm4,%ymm4        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x555555577f10 *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm4_0, ymm4_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm4_2, ymm4_3, tmp_2, tmp_3);
mov ymm4_0 tmp_0;
mov ymm4_1 tmp_1;
mov ymm4_2 tmp_2;
mov ymm4_3 tmp_3;
(* vpandn %ymm4,%ymm1,%ymm7                        #! PC = 0x555555577f19 *)
not ymm1_0n@uint64 ymm1_0;
and ymm7_0@uint64 ymm1_0n ymm4_0;
not ymm1_1n@uint64 ymm1_1;
and ymm7_1@uint64 ymm1_1n ymm4_1;
not ymm1_2n@uint64 ymm1_2;
and ymm7_2@uint64 ymm1_2n ymm4_2;
not ymm1_3n@uint64 ymm1_3;
and ymm7_3@uint64 ymm1_3n ymm4_3;
(* vpxor  %ymm2,%ymm7,%ymm13                       #! PC = 0x555555577f1d *)
xor ymm13_0@uint64 ymm7_0 ymm2_0;
xor ymm13_1@uint64 ymm7_1 ymm2_1;
xor ymm13_2@uint64 ymm7_2 ymm2_2;
xor ymm13_3@uint64 ymm7_3 ymm2_3;
(* vpsrlq $0x2e,%ymm6,%ymm7                        #! PC = 0x555555577f21 *)
shr ymm7_0 ymm6_0 0x2e@uint64;
shr ymm7_1 ymm6_1 0x2e@uint64;
shr ymm7_2 ymm6_2 0x2e@uint64;
shr ymm7_3 ymm6_3 0x2e@uint64;
(* vpsllq $0x12,%ymm6,%ymm6                        #! PC = 0x555555577f26 *)
shl ymm6_0 ymm6_0 0x12@uint64;
shl ymm6_1 ymm6_1 0x12@uint64;
shl ymm6_2 ymm6_2 0x12@uint64;
shl ymm6_3 ymm6_3 0x12@uint64;
(* vmovdqa %ymm13,-0x1b0(%rbp)                     #! EA = L0x7fffffffbda0; PC = 0x555555577f2b *)
mov L0x7fffffffbda0 ymm13_0;
mov L0x7fffffffbda8 ymm13_1;
mov L0x7fffffffbdb0 ymm13_2;
mov L0x7fffffffbdb8 ymm13_3;
(* vpor   %ymm7,%ymm6,%ymm11                       #! PC = 0x555555577f33 *)
or ymm11_0@uint64 ymm6_0 ymm7_0;
or ymm11_1@uint64 ymm6_1 ymm7_1;
or ymm11_2@uint64 ymm6_2 ymm7_2;
or ymm11_3@uint64 ymm6_3 ymm7_3;
(* vpandn %ymm11,%ymm4,%ymm8                       #! PC = 0x555555577f37 *)
not ymm4_0n@uint64 ymm4_0;
and ymm8_0@uint64 ymm4_0n ymm11_0;
not ymm4_1n@uint64 ymm4_1;
and ymm8_1@uint64 ymm4_1n ymm11_1;
not ymm4_2n@uint64 ymm4_2;
and ymm8_2@uint64 ymm4_2n ymm11_2;
not ymm4_3n@uint64 ymm4_3;
and ymm8_3@uint64 ymm4_3n ymm11_3;
(* vpxor  %ymm1,%ymm8,%ymm8                        #! PC = 0x555555577f3c *)
xor ymm8_0@uint64 ymm8_0 ymm1_0;
xor ymm8_1@uint64 ymm8_1 ymm1_1;
xor ymm8_2@uint64 ymm8_2 ymm1_2;
xor ymm8_3@uint64 ymm8_3 ymm1_3;
(* vpandn %ymm0,%ymm11,%ymm1                       #! PC = 0x555555577f40 *)
not ymm11_0n@uint64 ymm11_0;
and ymm1_0@uint64 ymm11_0n ymm0_0;
not ymm11_1n@uint64 ymm11_1;
and ymm1_1@uint64 ymm11_1n ymm0_1;
not ymm11_2n@uint64 ymm11_2;
and ymm1_2@uint64 ymm11_2n ymm0_2;
not ymm11_3n@uint64 ymm11_3;
and ymm1_3@uint64 ymm11_3n ymm0_3;
(* vpandn %ymm2,%ymm0,%ymm0                        #! PC = 0x555555577f44 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm2_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm2_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm2_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm2_3;
(* vpxor  %ymm11,%ymm0,%ymm2                       #! PC = 0x555555577f48 *)
xor ymm2_0@uint64 ymm0_0 ymm11_0;
xor ymm2_1@uint64 ymm0_1 ymm11_1;
xor ymm2_2@uint64 ymm0_2 ymm11_2;
xor ymm2_3@uint64 ymm0_3 ymm11_3;
(* vpxor  %ymm4,%ymm1,%ymm4                        #! PC = 0x555555577f4d *)
xor ymm4_0@uint64 ymm1_0 ymm4_0;
xor ymm4_1@uint64 ymm1_1 ymm4_1;
xor ymm4_2@uint64 ymm1_2 ymm4_2;
xor ymm4_3@uint64 ymm1_3 ymm4_3;
(* vmovdqa %ymm4,-0xb0(%rbp)                       #! EA = L0x7fffffffbea0; PC = 0x555555577f51 *)
mov L0x7fffffffbea0 ymm4_0;
mov L0x7fffffffbea8 ymm4_1;
mov L0x7fffffffbeb0 ymm4_2;
mov L0x7fffffffbeb8 ymm4_3;
(* vmovdqa %ymm2,-0xf0(%rbp)                       #! EA = L0x7fffffffbe60; PC = 0x555555577f59 *)
mov L0x7fffffffbe60 ymm2_0;
mov L0x7fffffffbe68 ymm2_1;
mov L0x7fffffffbe70 ymm2_2;
mov L0x7fffffffbe78 ymm2_3;
(* vpxor  -0x230(%rbp),%ymm12,%ymm10               #! EA = L0x7fffffffbd20; Value = 0x0000100000008000; PC = 0x555555577f61 *)
xor ymm10_0@uint64 ymm12_0 L0x7fffffffbd20;
xor ymm10_1@uint64 ymm12_1 L0x7fffffffbd28;
xor ymm10_2@uint64 ymm12_2 L0x7fffffffbd30;
xor ymm10_3@uint64 ymm12_3 L0x7fffffffbd38;
(* vpxor  -0x210(%rbp),%ymm5,%ymm0                 #! EA = L0x7fffffffbd40; Value = 0x0000000000000000; PC = 0x555555577f69 *)
xor ymm0_0@uint64 ymm5_0 L0x7fffffffbd40;
xor ymm0_1@uint64 ymm5_1 L0x7fffffffbd48;
xor ymm0_2@uint64 ymm5_2 L0x7fffffffbd50;
xor ymm0_3@uint64 ymm5_3 L0x7fffffffbd58;
(* vpxor  -0x170(%rbp),%ymm3,%ymm7                 #! EA = L0x7fffffffbde0; Value = 0x0000000000000200; PC = 0x555555577f71 *)
xor ymm7_0@uint64 ymm3_0 L0x7fffffffbde0;
xor ymm7_1@uint64 ymm3_1 L0x7fffffffbde8;
xor ymm7_2@uint64 ymm3_2 L0x7fffffffbdf0;
xor ymm7_3@uint64 ymm3_3 L0x7fffffffbdf8;
(* vpxor  -0x270(%rbp),%ymm15,%ymm6                #! EA = L0x7fffffffbce0; Value = 0x0000000000000000; PC = 0x555555577f79 *)
xor ymm6_0@uint64 ymm15_0 L0x7fffffffbce0;
xor ymm6_1@uint64 ymm15_1 L0x7fffffffbce8;
xor ymm6_2@uint64 ymm15_2 L0x7fffffffbcf0;
xor ymm6_3@uint64 ymm15_3 L0x7fffffffbcf8;
(* vpsrlq $0x25,%ymm10,%ymm2                       #! PC = 0x555555577f81 *)
shr ymm2_0 ymm10_0 0x25@uint64;
shr ymm2_1 ymm10_1 0x25@uint64;
shr ymm2_2 ymm10_2 0x25@uint64;
shr ymm2_3 ymm10_3 0x25@uint64;
(* vpsllq $0x1b,%ymm10,%ymm1                       #! PC = 0x555555577f87 *)
shl ymm1_0 ymm10_0 0x1b@uint64;
shl ymm1_1 ymm10_1 0x1b@uint64;
shl ymm1_2 ymm10_2 0x1b@uint64;
shl ymm1_3 ymm10_3 0x1b@uint64;
(* vpxor  -0x190(%rbp),%ymm15,%ymm15               #! EA = L0x7fffffffbdc0; Value = 0x0000200000000000; PC = 0x555555577f8d *)
xor ymm15_0@uint64 ymm15_0 L0x7fffffffbdc0;
xor ymm15_1@uint64 ymm15_1 L0x7fffffffbdc8;
xor ymm15_2@uint64 ymm15_2 L0x7fffffffbdd0;
xor ymm15_3@uint64 ymm15_3 L0x7fffffffbdd8;
(* vmovdqa -0xd0(%rbp),%ymm13                      #! EA = L0x7fffffffbe80; Value = 0x0000a50434031950; PC = 0x555555577f95 *)
mov ymm13_0 L0x7fffffffbe80;
mov ymm13_1 L0x7fffffffbe88;
mov ymm13_2 L0x7fffffffbe90;
mov ymm13_3 L0x7fffffffbe98;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x555555577f9d *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpsrlq $0x1c,%ymm0,%ymm2                        #! PC = 0x555555577fa1 *)
shr ymm2_0 ymm0_0 0x1c@uint64;
shr ymm2_1 ymm0_1 0x1c@uint64;
shr ymm2_2 ymm0_2 0x1c@uint64;
shr ymm2_3 ymm0_3 0x1c@uint64;
(* vpshufb 0x55f31(%rip),%ymm6,%ymm6        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x555555577fa6 *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm6_0, ymm6_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm6_2, ymm6_3, tmp_2, tmp_3);
mov ymm6_0 tmp_0;
mov ymm6_1 tmp_1;
mov ymm6_2 tmp_2;
mov ymm6_3 tmp_3;
(* vpsllq $0x24,%ymm0,%ymm0                        #! PC = 0x555555577faf *)
shl ymm0_0 ymm0_0 0x24@uint64;
shl ymm0_1 ymm0_1 0x24@uint64;
shl ymm0_2 ymm0_2 0x24@uint64;
shl ymm0_3 ymm0_3 0x24@uint64;
(* vpxor  -0x150(%rbp),%ymm12,%ymm12               #! EA = L0x7fffffffbe00; Value = 0x0000000000000000; PC = 0x555555577fb4 *)
xor ymm12_0@uint64 ymm12_0 L0x7fffffffbe00;
xor ymm12_1@uint64 ymm12_1 L0x7fffffffbe08;
xor ymm12_2@uint64 ymm12_2 L0x7fffffffbe10;
xor ymm12_3@uint64 ymm12_3 L0x7fffffffbe18;
(* vpxor  -0x350(%rbp),%ymm5,%ymm5                 #! EA = L0x7fffffffbc00; Value = 0x0000000010000400; PC = 0x555555577fbc *)
xor ymm5_0@uint64 ymm5_0 L0x7fffffffbc00;
xor ymm5_1@uint64 ymm5_1 L0x7fffffffbc08;
xor ymm5_2@uint64 ymm5_2 L0x7fffffffbc10;
xor ymm5_3@uint64 ymm5_3 L0x7fffffffbc18;
(* vpor   %ymm2,%ymm0,%ymm0                        #! PC = 0x555555577fc4 *)
or ymm0_0@uint64 ymm0_0 ymm2_0;
or ymm0_1@uint64 ymm0_1 ymm2_1;
or ymm0_2@uint64 ymm0_2 ymm2_2;
or ymm0_3@uint64 ymm0_3 ymm2_3;
(* vpsrlq $0x36,%ymm7,%ymm2                        #! PC = 0x555555577fc8 *)
shr ymm2_0 ymm7_0 0x36@uint64;
shr ymm2_1 ymm7_1 0x36@uint64;
shr ymm2_2 ymm7_2 0x36@uint64;
shr ymm2_3 ymm7_3 0x36@uint64;
(* vpxor  -0x310(%rbp),%ymm3,%ymm3                 #! EA = L0x7fffffffbc40; Value = 0x0000000000000000; PC = 0x555555577fcd *)
xor ymm3_0@uint64 ymm3_0 L0x7fffffffbc40;
xor ymm3_1@uint64 ymm3_1 L0x7fffffffbc48;
xor ymm3_2@uint64 ymm3_2 L0x7fffffffbc50;
xor ymm3_3@uint64 ymm3_3 L0x7fffffffbc58;
(* vpsllq $0xa,%ymm7,%ymm7                         #! PC = 0x555555577fd5 *)
shl ymm7_0 ymm7_0 0xa@uint64;
shl ymm7_1 ymm7_1 0xa@uint64;
shl ymm7_2 ymm7_2 0xa@uint64;
shl ymm7_3 ymm7_3 0xa@uint64;
(* vpor   %ymm2,%ymm7,%ymm4                        #! PC = 0x555555577fda *)
or ymm4_0@uint64 ymm7_0 ymm2_0;
or ymm4_1@uint64 ymm7_1 ymm2_1;
or ymm4_2@uint64 ymm7_2 ymm2_2;
or ymm4_3@uint64 ymm7_3 ymm2_3;
(* vpandn %ymm4,%ymm0,%ymm2                        #! PC = 0x555555577fde *)
not ymm0_0n@uint64 ymm0_0;
and ymm2_0@uint64 ymm0_0n ymm4_0;
not ymm0_1n@uint64 ymm0_1;
and ymm2_1@uint64 ymm0_1n ymm4_1;
not ymm0_2n@uint64 ymm0_2;
and ymm2_2@uint64 ymm0_2n ymm4_2;
not ymm0_3n@uint64 ymm0_3;
and ymm2_3@uint64 ymm0_3n ymm4_3;
(* vpxor  %ymm1,%ymm2,%ymm7                        #! PC = 0x555555577fe2 *)
xor ymm7_0@uint64 ymm2_0 ymm1_0;
xor ymm7_1@uint64 ymm2_1 ymm1_1;
xor ymm7_2@uint64 ymm2_2 ymm1_2;
xor ymm7_3@uint64 ymm2_3 ymm1_3;
(* vpxor  -0x70(%rbp),%ymm14,%ymm2                 #! EA = L0x7fffffffbee0; Value = 0x0000000000000400; PC = 0x555555577fe6 *)
xor ymm2_0@uint64 ymm14_0 L0x7fffffffbee0;
xor ymm2_1@uint64 ymm14_1 L0x7fffffffbee8;
xor ymm2_2@uint64 ymm14_2 L0x7fffffffbef0;
xor ymm2_3@uint64 ymm14_3 L0x7fffffffbef8;
(* vpxor  -0x250(%rbp),%ymm14,%ymm14               #! EA = L0x7fffffffbd00; Value = 0x0000000000008000; PC = 0x555555577feb *)
xor ymm14_0@uint64 ymm14_0 L0x7fffffffbd00;
xor ymm14_1@uint64 ymm14_1 L0x7fffffffbd08;
xor ymm14_2@uint64 ymm14_2 L0x7fffffffbd10;
xor ymm14_3@uint64 ymm14_3 L0x7fffffffbd18;
(* vmovdqa %ymm7,%ymm11                            #! PC = 0x555555577ff3 *)
mov ymm11_0 ymm7_0;
mov ymm11_1 ymm7_1;
mov ymm11_2 ymm7_2;
mov ymm11_3 ymm7_3;
(* vpsrlq $0x31,%ymm2,%ymm7                        #! PC = 0x555555577ff7 *)
shr ymm7_0 ymm2_0 0x31@uint64;
shr ymm7_1 ymm2_1 0x31@uint64;
shr ymm7_2 ymm2_2 0x31@uint64;
shr ymm7_3 ymm2_3 0x31@uint64;
(* vpsllq $0xf,%ymm2,%ymm2                         #! PC = 0x555555577ffc *)
shl ymm2_0 ymm2_0 0xf@uint64;
shl ymm2_1 ymm2_1 0xf@uint64;
shl ymm2_2 ymm2_2 0xf@uint64;
shl ymm2_3 ymm2_3 0xf@uint64;
(* vmovdqa %ymm11,-0x330(%rbp)                     #! EA = L0x7fffffffbc20; PC = 0x555555578001 *)
mov L0x7fffffffbc20 ymm11_0;
mov L0x7fffffffbc28 ymm11_1;
mov L0x7fffffffbc30 ymm11_2;
mov L0x7fffffffbc38 ymm11_3;
(* vpor   %ymm7,%ymm2,%ymm2                        #! PC = 0x555555578009 *)
or ymm2_0@uint64 ymm2_0 ymm7_0;
or ymm2_1@uint64 ymm2_1 ymm7_1;
or ymm2_2@uint64 ymm2_2 ymm7_2;
or ymm2_3@uint64 ymm2_3 ymm7_3;
(* vpandn %ymm6,%ymm2,%ymm10                       #! PC = 0x55555557800d *)
not ymm2_0n@uint64 ymm2_0;
and ymm10_0@uint64 ymm2_0n ymm6_0;
not ymm2_1n@uint64 ymm2_1;
and ymm10_1@uint64 ymm2_1n ymm6_1;
not ymm2_2n@uint64 ymm2_2;
and ymm10_2@uint64 ymm2_2n ymm6_2;
not ymm2_3n@uint64 ymm2_3;
and ymm10_3@uint64 ymm2_3n ymm6_3;
(* vpandn %ymm2,%ymm4,%ymm7                        #! PC = 0x555555578011 *)
not ymm4_0n@uint64 ymm4_0;
and ymm7_0@uint64 ymm4_0n ymm2_0;
not ymm4_1n@uint64 ymm4_1;
and ymm7_1@uint64 ymm4_1n ymm2_1;
not ymm4_2n@uint64 ymm4_2;
and ymm7_2@uint64 ymm4_2n ymm2_2;
not ymm4_3n@uint64 ymm4_3;
and ymm7_3@uint64 ymm4_3n ymm2_3;
(* vpxor  %ymm4,%ymm10,%ymm10                      #! PC = 0x555555578015 *)
xor ymm10_0@uint64 ymm10_0 ymm4_0;
xor ymm10_1@uint64 ymm10_1 ymm4_1;
xor ymm10_2@uint64 ymm10_2 ymm4_2;
xor ymm10_3@uint64 ymm10_3 ymm4_3;
(* vpxor  %ymm0,%ymm7,%ymm7                        #! PC = 0x555555578019 *)
xor ymm7_0@uint64 ymm7_0 ymm0_0;
xor ymm7_1@uint64 ymm7_1 ymm0_1;
xor ymm7_2@uint64 ymm7_2 ymm0_2;
xor ymm7_3@uint64 ymm7_3 ymm0_3;
(* vmovdqa %ymm10,-0x70(%rbp)                      #! EA = L0x7fffffffbee0; PC = 0x55555557801d *)
mov L0x7fffffffbee0 ymm10_0;
mov L0x7fffffffbee8 ymm10_1;
mov L0x7fffffffbef0 ymm10_2;
mov L0x7fffffffbef8 ymm10_3;
(* vpandn %ymm1,%ymm6,%ymm10                       #! PC = 0x555555578022 *)
not ymm6_0n@uint64 ymm6_0;
and ymm10_0@uint64 ymm6_0n ymm1_0;
not ymm6_1n@uint64 ymm6_1;
and ymm10_1@uint64 ymm6_1n ymm1_1;
not ymm6_2n@uint64 ymm6_2;
and ymm10_2@uint64 ymm6_2n ymm1_2;
not ymm6_3n@uint64 ymm6_3;
and ymm10_3@uint64 ymm6_3n ymm1_3;
(* vpandn %ymm0,%ymm1,%ymm1                        #! PC = 0x555555578026 *)
not ymm1_0n@uint64 ymm1_0;
and ymm1_0@uint64 ymm1_0n ymm0_0;
not ymm1_1n@uint64 ymm1_1;
and ymm1_1@uint64 ymm1_1n ymm0_1;
not ymm1_2n@uint64 ymm1_2;
and ymm1_2@uint64 ymm1_2n ymm0_2;
not ymm1_3n@uint64 ymm1_3;
and ymm1_3@uint64 ymm1_3n ymm0_3;
(* vpxor  %ymm2,%ymm10,%ymm10                      #! PC = 0x55555557802a *)
xor ymm10_0@uint64 ymm10_0 ymm2_0;
xor ymm10_1@uint64 ymm10_1 ymm2_1;
xor ymm10_2@uint64 ymm10_2 ymm2_2;
xor ymm10_3@uint64 ymm10_3 ymm2_3;
(* vpsrlq $0x9,%ymm15,%ymm0                        #! PC = 0x55555557802e *)
shr ymm0_0 ymm15_0 0x9@uint64;
shr ymm0_1 ymm15_1 0x9@uint64;
shr ymm0_2 ymm15_2 0x9@uint64;
shr ymm0_3 ymm15_3 0x9@uint64;
(* vpxor  %ymm6,%ymm1,%ymm2                        #! PC = 0x555555578034 *)
xor ymm2_0@uint64 ymm1_0 ymm6_0;
xor ymm2_1@uint64 ymm1_1 ymm6_1;
xor ymm2_2@uint64 ymm1_2 ymm6_2;
xor ymm2_3@uint64 ymm1_3 ymm6_3;
(* vpsllq $0x37,%ymm15,%ymm15                      #! PC = 0x555555578038 *)
shl ymm15_0 ymm15_0 0x37@uint64;
shl ymm15_1 ymm15_1 0x37@uint64;
shl ymm15_2 ymm15_2 0x37@uint64;
shl ymm15_3 ymm15_3 0x37@uint64;
(* vpsrlq $0x19,%ymm12,%ymm6                       #! PC = 0x55555557803e *)
shr ymm6_0 ymm12_0 0x19@uint64;
shr ymm6_1 ymm12_1 0x19@uint64;
shr ymm6_2 ymm12_2 0x19@uint64;
shr ymm6_3 ymm12_3 0x19@uint64;
(* vmovdqa %ymm2,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x555555578044 *)
mov L0x7fffffffbf00 ymm2_0;
mov L0x7fffffffbf08 ymm2_1;
mov L0x7fffffffbf10 ymm2_2;
mov L0x7fffffffbf18 ymm2_3;
(* vpsllq $0x27,%ymm12,%ymm12                      #! PC = 0x555555578049 *)
shl ymm12_0 ymm12_0 0x27@uint64;
shl ymm12_1 ymm12_1 0x27@uint64;
shl ymm12_2 ymm12_2 0x27@uint64;
shl ymm12_3 ymm12_3 0x27@uint64;
(* vpsrlq $0x2,%ymm14,%ymm1                        #! PC = 0x55555557804f *)
shr ymm1_0 ymm14_0 0x2@uint64;
shr ymm1_1 ymm14_1 0x2@uint64;
shr ymm1_2 ymm14_2 0x2@uint64;
shr ymm1_3 ymm14_3 0x2@uint64;
(* vpor   %ymm0,%ymm15,%ymm15                      #! PC = 0x555555578055 *)
or ymm15_0@uint64 ymm15_0 ymm0_0;
or ymm15_1@uint64 ymm15_1 ymm0_1;
or ymm15_2@uint64 ymm15_2 ymm0_2;
or ymm15_3@uint64 ymm15_3 ymm0_3;
(* vpor   %ymm6,%ymm12,%ymm12                      #! PC = 0x555555578059 *)
or ymm12_0@uint64 ymm12_0 ymm6_0;
or ymm12_1@uint64 ymm12_1 ymm6_1;
or ymm12_2@uint64 ymm12_2 ymm6_2;
or ymm12_3@uint64 ymm12_3 ymm6_3;
(* vpsllq $0x3e,%ymm14,%ymm14                      #! PC = 0x55555557805d *)
shl ymm14_0 ymm14_0 0x3e@uint64;
shl ymm14_1 ymm14_1 0x3e@uint64;
shl ymm14_2 ymm14_2 0x3e@uint64;
shl ymm14_3 ymm14_3 0x3e@uint64;
(* vpxor  -0x290(%rbp),%ymm13,%ymm0                #! EA = L0x7fffffffbcc0; Value = 0x00049840d3042220; PC = 0x555555578063 *)
xor ymm0_0@uint64 ymm13_0 L0x7fffffffbcc0;
xor ymm0_1@uint64 ymm13_1 L0x7fffffffbcc8;
xor ymm0_2@uint64 ymm13_2 L0x7fffffffbcd0;
xor ymm0_3@uint64 ymm13_3 L0x7fffffffbcd8;
(* vpor   %ymm1,%ymm14,%ymm4                       #! PC = 0x55555557806b *)
or ymm4_0@uint64 ymm14_0 ymm1_0;
or ymm4_1@uint64 ymm14_1 ymm1_1;
or ymm4_2@uint64 ymm14_2 ymm1_2;
or ymm4_3@uint64 ymm14_3 ymm1_3;
(* vpandn %ymm12,%ymm15,%ymm6                      #! PC = 0x55555557806f *)
not ymm15_0n@uint64 ymm15_0;
and ymm6_0@uint64 ymm15_0n ymm12_0;
not ymm15_1n@uint64 ymm15_1;
and ymm6_1@uint64 ymm15_1n ymm12_1;
not ymm15_2n@uint64 ymm15_2;
and ymm6_2@uint64 ymm15_2n ymm12_2;
not ymm15_3n@uint64 ymm15_3;
and ymm6_3@uint64 ymm15_3n ymm12_3;
(* vpxor  -0x70(%rbp),%ymm8,%ymm13                 #! EA = L0x7fffffffbee0; Value = 0x040c0061063a6584; PC = 0x555555578074 *)
xor ymm13_0@uint64 ymm8_0 L0x7fffffffbee0;
xor ymm13_1@uint64 ymm8_1 L0x7fffffffbee8;
xor ymm13_2@uint64 ymm8_2 L0x7fffffffbef0;
xor ymm13_3@uint64 ymm8_3 L0x7fffffffbef8;
(* vpxor  %ymm4,%ymm6,%ymm6                        #! PC = 0x555555578079 *)
xor ymm6_0@uint64 ymm6_0 ymm4_0;
xor ymm6_1@uint64 ymm6_1 ymm4_1;
xor ymm6_2@uint64 ymm6_2 ymm4_2;
xor ymm6_3@uint64 ymm6_3 ymm4_3;
(* vpxor  -0x1b0(%rbp),%ymm7,%ymm14                #! EA = L0x7fffffffbda0; Value = 0x003e0038090a8080; PC = 0x55555557807d *)
xor ymm14_0@uint64 ymm7_0 L0x7fffffffbda0;
xor ymm14_1@uint64 ymm7_1 L0x7fffffffbda8;
xor ymm14_2@uint64 ymm7_2 L0x7fffffffbdb0;
xor ymm14_3@uint64 ymm7_3 L0x7fffffffbdb8;
(* vpxor  %ymm11,%ymm6,%ymm2                       #! PC = 0x555555578085 *)
xor ymm2_0@uint64 ymm6_0 ymm11_0;
xor ymm2_1@uint64 ymm6_1 ymm11_1;
xor ymm2_2@uint64 ymm6_2 ymm11_2;
xor ymm2_3@uint64 ymm6_3 ymm11_3;
(* vpxor  %ymm0,%ymm2,%ymm2                        #! PC = 0x55555557808a *)
xor ymm2_0@uint64 ymm2_0 ymm0_0;
xor ymm2_1@uint64 ymm2_1 ymm0_1;
xor ymm2_2@uint64 ymm2_2 ymm0_2;
xor ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x17,%ymm5,%ymm0                        #! PC = 0x55555557808e *)
shr ymm0_0 ymm5_0 0x17@uint64;
shr ymm0_1 ymm5_1 0x17@uint64;
shr ymm0_2 ymm5_2 0x17@uint64;
shr ymm0_3 ymm5_3 0x17@uint64;
(* vpxor  -0x90(%rbp),%ymm2,%ymm2                  #! EA = L0x7fffffffbec0; Value = 0x8030500001e0840c; PC = 0x555555578093 *)
xor ymm2_0@uint64 ymm2_0 L0x7fffffffbec0;
xor ymm2_1@uint64 ymm2_1 L0x7fffffffbec8;
xor ymm2_2@uint64 ymm2_2 L0x7fffffffbed0;
xor ymm2_3@uint64 ymm2_3 L0x7fffffffbed8;
(* vpsllq $0x29,%ymm5,%ymm5                        #! PC = 0x55555557809b *)
shl ymm5_0 ymm5_0 0x29@uint64;
shl ymm5_1 ymm5_1 0x29@uint64;
shl ymm5_2 ymm5_2 0x29@uint64;
shl ymm5_3 ymm5_3 0x29@uint64;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x5555555780a0 *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpandn %ymm5,%ymm12,%ymm0                       #! PC = 0x5555555780a4 *)
not ymm12_0n@uint64 ymm12_0;
and ymm0_0@uint64 ymm12_0n ymm5_0;
not ymm12_1n@uint64 ymm12_1;
and ymm0_1@uint64 ymm12_1n ymm5_1;
not ymm12_2n@uint64 ymm12_2;
and ymm0_2@uint64 ymm12_2n ymm5_2;
not ymm12_3n@uint64 ymm12_3;
and ymm0_3@uint64 ymm12_3n ymm5_3;
(* vpxor  %ymm15,%ymm0,%ymm1                       #! PC = 0x5555555780a8 *)
xor ymm1_0@uint64 ymm0_0 ymm15_0;
xor ymm1_1@uint64 ymm0_1 ymm15_1;
xor ymm1_2@uint64 ymm0_2 ymm15_2;
xor ymm1_3@uint64 ymm0_3 ymm15_3;
(* vpxor  -0x2f0(%rbp),%ymm9,%ymm0                 #! EA = L0x7fffffffbc60; Value = 0x38ca983082300106; PC = 0x5555555780ad *)
xor ymm0_0@uint64 ymm9_0 L0x7fffffffbc60;
xor ymm0_1@uint64 ymm9_1 L0x7fffffffbc68;
xor ymm0_2@uint64 ymm9_2 L0x7fffffffbc70;
xor ymm0_3@uint64 ymm9_3 L0x7fffffffbc78;
(* vmovdqa %ymm1,-0x350(%rbp)                      #! EA = L0x7fffffffbc00; PC = 0x5555555780b5 *)
mov L0x7fffffffbc00 ymm1_0;
mov L0x7fffffffbc08 ymm1_1;
mov L0x7fffffffbc10 ymm1_2;
mov L0x7fffffffbc18 ymm1_3;
(* vpxor  %ymm0,%ymm14,%ymm14                      #! PC = 0x5555555780bd *)
xor ymm14_0@uint64 ymm14_0 ymm0_0;
xor ymm14_1@uint64 ymm14_1 ymm0_1;
xor ymm14_2@uint64 ymm14_2 ymm0_2;
xor ymm14_3@uint64 ymm14_3 ymm0_3;
(* vpxor  %ymm1,%ymm14,%ymm14                      #! PC = 0x5555555780c1 *)
xor ymm14_0@uint64 ymm14_0 ymm1_0;
xor ymm14_1@uint64 ymm14_1 ymm1_1;
xor ymm14_2@uint64 ymm14_2 ymm1_2;
xor ymm14_3@uint64 ymm14_3 ymm1_3;
(* vpsrlq $0x3e,%ymm3,%ymm1                        #! PC = 0x5555555780c5 *)
shr ymm1_0 ymm3_0 0x3e@uint64;
shr ymm1_1 ymm3_1 0x3e@uint64;
shr ymm1_2 ymm3_2 0x3e@uint64;
shr ymm1_3 ymm3_3 0x3e@uint64;
(* vpsllq $0x2,%ymm3,%ymm3                         #! PC = 0x5555555780ca *)
shl ymm3_0 ymm3_0 0x2@uint64;
shl ymm3_1 ymm3_1 0x2@uint64;
shl ymm3_2 ymm3_2 0x2@uint64;
shl ymm3_3 ymm3_3 0x2@uint64;
(* vpor   %ymm1,%ymm3,%ymm0                        #! PC = 0x5555555780cf *)
or ymm0_0@uint64 ymm3_0 ymm1_0;
or ymm0_1@uint64 ymm3_1 ymm1_1;
or ymm0_2@uint64 ymm3_2 ymm1_2;
or ymm0_3@uint64 ymm3_3 ymm1_3;
(* vmovdqa -0x1d0(%rbp),%ymm3                      #! EA = L0x7fffffffbd80; Value = 0x31953040d0600010; PC = 0x5555555780d3 *)
mov ymm3_0 L0x7fffffffbd80;
mov ymm3_1 L0x7fffffffbd88;
mov ymm3_2 L0x7fffffffbd90;
mov ymm3_3 L0x7fffffffbd98;
(* vpandn %ymm0,%ymm5,%ymm1                        #! PC = 0x5555555780db *)
not ymm5_0n@uint64 ymm5_0;
and ymm1_0@uint64 ymm5_0n ymm0_0;
not ymm5_1n@uint64 ymm5_1;
and ymm1_1@uint64 ymm5_1n ymm0_1;
not ymm5_2n@uint64 ymm5_2;
and ymm1_2@uint64 ymm5_2n ymm0_2;
not ymm5_3n@uint64 ymm5_3;
and ymm1_3@uint64 ymm5_3n ymm0_3;
(* vpxor  %ymm12,%ymm1,%ymm12                      #! PC = 0x5555555780df *)
xor ymm12_0@uint64 ymm1_0 ymm12_0;
xor ymm12_1@uint64 ymm1_1 ymm12_1;
xor ymm12_2@uint64 ymm1_2 ymm12_2;
xor ymm12_3@uint64 ymm1_3 ymm12_3;
(* vmovdqa -0x1f0(%rbp),%ymm1                      #! EA = L0x7fffffffbd60; Value = 0x40038e20070020f0; PC = 0x5555555780e4 *)
mov ymm1_0 L0x7fffffffbd60;
mov ymm1_1 L0x7fffffffbd68;
mov ymm1_2 L0x7fffffffbd70;
mov ymm1_3 L0x7fffffffbd78;
(* vpxor  -0x2d0(%rbp),%ymm1,%ymm1                 #! EA = L0x7fffffffbc80; Value = 0x08b0340041c14101; PC = 0x5555555780ec *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbc80;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbc88;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbc90;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbc98;
(* vpxor  %ymm1,%ymm13,%ymm13                      #! PC = 0x5555555780f4 *)
xor ymm13_0@uint64 ymm13_0 ymm1_0;
xor ymm13_1@uint64 ymm13_1 ymm1_1;
xor ymm13_2@uint64 ymm13_2 ymm1_2;
xor ymm13_3@uint64 ymm13_3 ymm1_3;
(* vpandn %ymm4,%ymm0,%ymm1                        #! PC = 0x5555555780f8 *)
not ymm0_0n@uint64 ymm0_0;
and ymm1_0@uint64 ymm0_0n ymm4_0;
not ymm0_1n@uint64 ymm0_1;
and ymm1_1@uint64 ymm0_1n ymm4_1;
not ymm0_2n@uint64 ymm0_2;
and ymm1_2@uint64 ymm0_2n ymm4_2;
not ymm0_3n@uint64 ymm0_3;
and ymm1_3@uint64 ymm0_3n ymm4_3;
(* vpxor  %ymm5,%ymm1,%ymm5                        #! PC = 0x5555555780fc *)
xor ymm5_0@uint64 ymm1_0 ymm5_0;
xor ymm5_1@uint64 ymm1_1 ymm5_1;
xor ymm5_2@uint64 ymm1_2 ymm5_2;
xor ymm5_3@uint64 ymm1_3 ymm5_3;
(* vpxor  -0x130(%rbp),%ymm3,%ymm1                 #! EA = L0x7fffffffbe20; Value = 0x2002783081a00483; PC = 0x555555578100 *)
xor ymm1_0@uint64 ymm3_0 L0x7fffffffbe20;
xor ymm1_1@uint64 ymm3_1 L0x7fffffffbe28;
xor ymm1_2@uint64 ymm3_2 L0x7fffffffbe30;
xor ymm1_3@uint64 ymm3_3 L0x7fffffffbe38;
(* vpandn %ymm15,%ymm4,%ymm3                       #! PC = 0x555555578108 *)
not ymm4_0n@uint64 ymm4_0;
and ymm3_0@uint64 ymm4_0n ymm15_0;
not ymm4_1n@uint64 ymm4_1;
and ymm3_1@uint64 ymm4_1n ymm15_1;
not ymm4_2n@uint64 ymm4_2;
and ymm3_2@uint64 ymm4_2n ymm15_2;
not ymm4_3n@uint64 ymm4_3;
and ymm3_3@uint64 ymm4_3n ymm15_3;
(* vmovdqa -0x110(%rbp),%ymm15                     #! EA = L0x7fffffffbe40; Value = 0x60030c30241400c2; PC = 0x55555557810d *)
mov ymm15_0 L0x7fffffffbe40;
mov ymm15_1 L0x7fffffffbe48;
mov ymm15_2 L0x7fffffffbe50;
mov ymm15_3 L0x7fffffffbe58;
(* vpxor  %ymm5,%ymm10,%ymm11                      #! PC = 0x555555578115 *)
xor ymm11_0@uint64 ymm10_0 ymm5_0;
xor ymm11_1@uint64 ymm10_1 ymm5_1;
xor ymm11_2@uint64 ymm10_2 ymm5_2;
xor ymm11_3@uint64 ymm10_3 ymm5_3;
(* vpxor  %ymm0,%ymm3,%ymm3                        #! PC = 0x555555578119 *)
xor ymm3_0@uint64 ymm3_0 ymm0_0;
xor ymm3_1@uint64 ymm3_1 ymm0_1;
xor ymm3_2@uint64 ymm3_2 ymm0_2;
xor ymm3_3@uint64 ymm3_3 ymm0_3;
(* vpxor  -0x2b0(%rbp),%ymm15,%ymm0                #! EA = L0x7fffffffbca0; Value = 0x10488c0040d14100; PC = 0x55555557811d *)
xor ymm0_0@uint64 ymm15_0 L0x7fffffffbca0;
xor ymm0_1@uint64 ymm15_1 L0x7fffffffbca8;
xor ymm0_2@uint64 ymm15_2 L0x7fffffffbcb0;
xor ymm0_3@uint64 ymm15_3 L0x7fffffffbcb8;
(* vmovdqa %ymm5,-0x310(%rbp)                      #! EA = L0x7fffffffbc40; PC = 0x555555578125 *)
mov L0x7fffffffbc40 ymm5_0;
mov L0x7fffffffbc48 ymm5_1;
mov L0x7fffffffbc50 ymm5_2;
mov L0x7fffffffbc58 ymm5_3;
(* vpxor  %ymm1,%ymm11,%ymm11                      #! PC = 0x55555557812d *)
xor ymm11_0@uint64 ymm11_0 ymm1_0;
xor ymm11_1@uint64 ymm11_1 ymm1_1;
xor ymm11_2@uint64 ymm11_2 ymm1_2;
xor ymm11_3@uint64 ymm11_3 ymm1_3;
(* vpxor  -0xf0(%rbp),%ymm3,%ymm1                  #! EA = L0x7fffffffbe60; Value = 0xc41c018a18108081; PC = 0x555555578131 *)
xor ymm1_0@uint64 ymm3_0 L0x7fffffffbe60;
xor ymm1_1@uint64 ymm3_1 L0x7fffffffbe68;
xor ymm1_2@uint64 ymm3_2 L0x7fffffffbe70;
xor ymm1_3@uint64 ymm3_3 L0x7fffffffbe78;
(* vpxor  %ymm12,%ymm13,%ymm13                     #! PC = 0x555555578139 *)
xor ymm13_0@uint64 ymm13_0 ymm12_0;
xor ymm13_1@uint64 ymm13_1 ymm12_1;
xor ymm13_2@uint64 ymm13_2 ymm12_2;
xor ymm13_3@uint64 ymm13_3 ymm12_3;
(* vpxor  -0xb0(%rbp),%ymm11,%ymm11                #! EA = L0x7fffffffbea0; Value = 0x0022263021081812; PC = 0x55555557813e *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbea0;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbea8;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbeb0;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbeb8;
(* vpsllq $0x1,%ymm14,%ymm5                        #! PC = 0x555555578146 *)
shl ymm5_0 ymm14_0 0x1@uint64;
shl ymm5_1 ymm14_1 0x1@uint64;
shl ymm5_2 ymm14_2 0x1@uint64;
shl ymm5_3 ymm14_3 0x1@uint64;
(* vpxor  %ymm0,%ymm1,%ymm1                        #! PC = 0x55555557814c *)
xor ymm1_0@uint64 ymm1_0 ymm0_0;
xor ymm1_1@uint64 ymm1_1 ymm0_1;
xor ymm1_2@uint64 ymm1_2 ymm0_2;
xor ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3f,%ymm14,%ymm0                       #! PC = 0x555555578150 *)
shr ymm0_0 ymm14_0 0x3f@uint64;
shr ymm0_1 ymm14_1 0x3f@uint64;
shr ymm0_2 ymm14_2 0x3f@uint64;
shr ymm0_3 ymm14_3 0x3f@uint64;
(* vpxor  -0x50(%rbp),%ymm1,%ymm1                  #! EA = L0x7fffffffbf00; Value = 0x0a08402100074184; PC = 0x555555578156 *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbf00;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbf08;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbf10;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbf18;
(* vpsllq $0x1,%ymm13,%ymm4                        #! PC = 0x55555557815b *)
shl ymm4_0 ymm13_0 0x1@uint64;
shl ymm4_1 ymm13_1 0x1@uint64;
shl ymm4_2 ymm13_2 0x1@uint64;
shl ymm4_3 ymm13_3 0x1@uint64;
(* vpsrlq $0x3f,%ymm11,%ymm15                      #! PC = 0x555555578161 *)
shr ymm15_0 ymm11_0 0x3f@uint64;
shr ymm15_1 ymm11_1 0x3f@uint64;
shr ymm15_2 ymm11_2 0x3f@uint64;
shr ymm15_3 ymm11_3 0x3f@uint64;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x555555578167 *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpsrlq $0x3f,%ymm13,%ymm0                       #! PC = 0x55555557816b *)
shr ymm0_0 ymm13_0 0x3f@uint64;
shr ymm0_1 ymm13_1 0x3f@uint64;
shr ymm0_2 ymm13_2 0x3f@uint64;
shr ymm0_3 ymm13_3 0x3f@uint64;
(* vpxor  %ymm1,%ymm5,%ymm5                        #! PC = 0x555555578171 *)
xor ymm5_0@uint64 ymm5_0 ymm1_0;
xor ymm5_1@uint64 ymm5_1 ymm1_1;
xor ymm5_2@uint64 ymm5_2 ymm1_2;
xor ymm5_3@uint64 ymm5_3 ymm1_3;
(* vpor   %ymm0,%ymm4,%ymm4                        #! PC = 0x555555578175 *)
or ymm4_0@uint64 ymm4_0 ymm0_0;
or ymm4_1@uint64 ymm4_1 ymm0_1;
or ymm4_2@uint64 ymm4_2 ymm0_2;
or ymm4_3@uint64 ymm4_3 ymm0_3;
(* vpsllq $0x1,%ymm11,%ymm0                        #! PC = 0x555555578179 *)
shl ymm0_0 ymm11_0 0x1@uint64;
shl ymm0_1 ymm11_1 0x1@uint64;
shl ymm0_2 ymm11_2 0x1@uint64;
shl ymm0_3 ymm11_3 0x1@uint64;
(* vpxor  %ymm6,%ymm5,%ymm6                        #! PC = 0x55555557817f *)
xor ymm6_0@uint64 ymm5_0 ymm6_0;
xor ymm6_1@uint64 ymm5_1 ymm6_1;
xor ymm6_2@uint64 ymm5_2 ymm6_2;
xor ymm6_3@uint64 ymm5_3 ymm6_3;
(* vpor   %ymm15,%ymm0,%ymm0                       #! PC = 0x555555578183 *)
or ymm0_0@uint64 ymm0_0 ymm15_0;
or ymm0_1@uint64 ymm0_1 ymm15_1;
or ymm0_2@uint64 ymm0_2 ymm15_2;
or ymm0_3@uint64 ymm0_3 ymm15_3;
(* vpxor  %ymm2,%ymm4,%ymm4                        #! PC = 0x555555578188 *)
xor ymm4_0@uint64 ymm4_0 ymm2_0;
xor ymm4_1@uint64 ymm4_1 ymm2_1;
xor ymm4_2@uint64 ymm4_2 ymm2_2;
xor ymm4_3@uint64 ymm4_3 ymm2_3;
(* vmovq  %rsi,%xmm15                              #! PC = 0x55555557818c *)
mov xmm15_0 rsi;
mov xmm15_1 0@uint64;
(* vpxor  %ymm14,%ymm0,%ymm14                      #! PC = 0x555555578191 *)
xor ymm14_0@uint64 ymm0_0 ymm14_0;
xor ymm14_1@uint64 ymm0_1 ymm14_1;
xor ymm14_2@uint64 ymm0_2 ymm14_2;
xor ymm14_3@uint64 ymm0_3 ymm14_3;
(* vpsrlq $0x3f,%ymm1,%ymm0                        #! PC = 0x555555578196 *)
shr ymm0_0 ymm1_0 0x3f@uint64;
shr ymm0_1 ymm1_1 0x3f@uint64;
shr ymm0_2 ymm1_2 0x3f@uint64;
shr ymm0_3 ymm1_3 0x3f@uint64;
(* vpxor  %ymm9,%ymm4,%ymm9                        #! PC = 0x55555557819b *)
xor ymm9_0@uint64 ymm4_0 ymm9_0;
xor ymm9_1@uint64 ymm4_1 ymm9_1;
xor ymm9_2@uint64 ymm4_2 ymm9_2;
xor ymm9_3@uint64 ymm4_3 ymm9_3;
(* vpsllq $0x1,%ymm1,%ymm1                         #! PC = 0x5555555781a0 *)
shl ymm1_0 ymm1_0 0x1@uint64;
shl ymm1_1 ymm1_1 0x1@uint64;
shl ymm1_2 ymm1_2 0x1@uint64;
shl ymm1_3 ymm1_3 0x1@uint64;
(* vpxor  %ymm8,%ymm14,%ymm8                       #! PC = 0x5555555781a5 *)
xor ymm8_0@uint64 ymm14_0 ymm8_0;
xor ymm8_1@uint64 ymm14_1 ymm8_1;
xor ymm8_2@uint64 ymm14_2 ymm8_2;
xor ymm8_3@uint64 ymm14_3 ymm8_3;
(* vpxor  %ymm7,%ymm4,%ymm7                        #! PC = 0x5555555781aa *)
xor ymm7_0@uint64 ymm4_0 ymm7_0;
xor ymm7_1@uint64 ymm4_1 ymm7_1;
xor ymm7_2@uint64 ymm4_2 ymm7_2;
xor ymm7_3@uint64 ymm4_3 ymm7_3;
(* vpor   %ymm0,%ymm1,%ymm1                        #! PC = 0x5555555781ae *)
or ymm1_0@uint64 ymm1_0 ymm0_0;
or ymm1_1@uint64 ymm1_1 ymm0_1;
or ymm1_2@uint64 ymm1_2 ymm0_2;
or ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3f,%ymm2,%ymm0                        #! PC = 0x5555555781b2 *)
shr ymm0_0 ymm2_0 0x3f@uint64;
shr ymm0_1 ymm2_1 0x3f@uint64;
shr ymm0_2 ymm2_2 0x3f@uint64;
shr ymm0_3 ymm2_3 0x3f@uint64;
(* vpxor  %ymm12,%ymm14,%ymm12                     #! PC = 0x5555555781b7 *)
xor ymm12_0@uint64 ymm14_0 ymm12_0;
xor ymm12_1@uint64 ymm14_1 ymm12_1;
xor ymm12_2@uint64 ymm14_2 ymm12_2;
xor ymm12_3@uint64 ymm14_3 ymm12_3;
(* vpxor  %ymm13,%ymm1,%ymm13                      #! PC = 0x5555555781bc *)
xor ymm13_0@uint64 ymm1_0 ymm13_0;
xor ymm13_1@uint64 ymm1_1 ymm13_1;
xor ymm13_2@uint64 ymm1_2 ymm13_2;
xor ymm13_3@uint64 ymm1_3 ymm13_3;
(* vpsrlq $0x14,%ymm9,%ymm1                        #! PC = 0x5555555781c1 *)
shr ymm1_0 ymm9_0 0x14@uint64;
shr ymm1_1 ymm9_1 0x14@uint64;
shr ymm1_2 ymm9_2 0x14@uint64;
shr ymm1_3 ymm9_3 0x14@uint64;
(* vpsllq $0x2c,%ymm9,%ymm9                        #! PC = 0x5555555781c7 *)
shl ymm9_0 ymm9_0 0x2c@uint64;
shl ymm9_1 ymm9_1 0x2c@uint64;
shl ymm9_2 ymm9_2 0x2c@uint64;
shl ymm9_3 ymm9_3 0x2c@uint64;
(* vpsllq $0x1,%ymm2,%ymm2                         #! PC = 0x5555555781cd *)
shl ymm2_0 ymm2_0 0x1@uint64;
shl ymm2_1 ymm2_1 0x1@uint64;
shl ymm2_2 ymm2_2 0x1@uint64;
shl ymm2_3 ymm2_3 0x1@uint64;
(* vpxor  %ymm10,%ymm13,%ymm10                     #! PC = 0x5555555781d2 *)
xor ymm10_0@uint64 ymm13_0 ymm10_0;
xor ymm10_1@uint64 ymm13_1 ymm10_1;
xor ymm10_2@uint64 ymm13_2 ymm10_2;
xor ymm10_3@uint64 ymm13_3 ymm10_3;
(* vpor   %ymm1,%ymm9,%ymm9                        #! PC = 0x5555555781d7 *)
or ymm9_0@uint64 ymm9_0 ymm1_0;
or ymm9_1@uint64 ymm9_1 ymm1_1;
or ymm9_2@uint64 ymm9_2 ymm1_2;
or ymm9_3@uint64 ymm9_3 ymm1_3;
(* vpsrlq $0x15,%ymm8,%ymm1                        #! PC = 0x5555555781db *)
shr ymm1_0 ymm8_0 0x15@uint64;
shr ymm1_1 ymm8_1 0x15@uint64;
shr ymm1_2 ymm8_2 0x15@uint64;
shr ymm1_3 ymm8_3 0x15@uint64;
(* vpor   %ymm0,%ymm2,%ymm2                        #! PC = 0x5555555781e1 *)
or ymm2_0@uint64 ymm2_0 ymm0_0;
or ymm2_1@uint64 ymm2_1 ymm0_1;
or ymm2_2@uint64 ymm2_2 ymm0_2;
or ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsllq $0x2b,%ymm8,%ymm8                        #! PC = 0x5555555781e5 *)
shl ymm8_0 ymm8_0 0x2b@uint64;
shl ymm8_1 ymm8_1 0x2b@uint64;
shl ymm8_2 ymm8_2 0x2b@uint64;
shl ymm8_3 ymm8_3 0x2b@uint64;
(* vpxor  %ymm11,%ymm2,%ymm11                      #! PC = 0x5555555781eb *)
xor ymm11_0@uint64 ymm2_0 ymm11_0;
xor ymm11_1@uint64 ymm2_1 ymm11_1;
xor ymm11_2@uint64 ymm2_2 ymm11_2;
xor ymm11_3@uint64 ymm2_3 ymm11_3;
(* vpxor  -0x90(%rbp),%ymm5,%ymm0                  #! EA = L0x7fffffffbec0; Value = 0x8030500001e0840c; PC = 0x5555555781f0 *)
xor ymm0_0@uint64 ymm5_0 L0x7fffffffbec0;
xor ymm0_1@uint64 ymm5_1 L0x7fffffffbec8;
xor ymm0_2@uint64 ymm5_2 L0x7fffffffbed0;
xor ymm0_3@uint64 ymm5_3 L0x7fffffffbed8;
(* vpor   %ymm1,%ymm8,%ymm8                        #! PC = 0x5555555781f8 *)
or ymm8_0@uint64 ymm8_0 ymm1_0;
or ymm8_1@uint64 ymm8_1 ymm1_1;
or ymm8_2@uint64 ymm8_2 ymm1_2;
or ymm8_3@uint64 ymm8_3 ymm1_3;
(* vpbroadcastq %xmm15,%ymm1                       #! PC = 0x5555555781fc *)
mov ymm1_0 xmm15_0;
mov ymm1_1 xmm15_0;
mov ymm1_2 xmm15_0;
mov ymm1_3 xmm15_0;
(* vpxor  %ymm3,%ymm11,%ymm3                       #! PC = 0x555555578201 *)
xor ymm3_0@uint64 ymm11_0 ymm3_0;
xor ymm3_1@uint64 ymm11_1 ymm3_1;
xor ymm3_2@uint64 ymm11_2 ymm3_2;
xor ymm3_3@uint64 ymm11_3 ymm3_3;
(* vpandn %ymm8,%ymm9,%ymm2                        #! PC = 0x555555578205 *)
not ymm9_0n@uint64 ymm9_0;
and ymm2_0@uint64 ymm9_0n ymm8_0;
not ymm9_1n@uint64 ymm9_1;
and ymm2_1@uint64 ymm9_1n ymm8_1;
not ymm9_2n@uint64 ymm9_2;
and ymm2_2@uint64 ymm9_2n ymm8_2;
not ymm9_3n@uint64 ymm9_3;
and ymm2_3@uint64 ymm9_3n ymm8_3;
(* vpxor  %ymm2,%ymm1,%ymm1                        #! PC = 0x55555557820a *)
xor ymm1_0@uint64 ymm1_0 ymm2_0;
xor ymm1_1@uint64 ymm1_1 ymm2_1;
xor ymm1_2@uint64 ymm1_2 ymm2_2;
xor ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpxor  %ymm0,%ymm1,%ymm2                        #! PC = 0x55555557820e *)
xor ymm2_0@uint64 ymm1_0 ymm0_0;
xor ymm2_1@uint64 ymm1_1 ymm0_1;
xor ymm2_2@uint64 ymm1_2 ymm0_2;
xor ymm2_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x2b,%ymm10,%ymm1                       #! PC = 0x555555578212 *)
shr ymm1_0 ymm10_0 0x2b@uint64;
shr ymm1_1 ymm10_1 0x2b@uint64;
shr ymm1_2 ymm10_2 0x2b@uint64;
shr ymm1_3 ymm10_3 0x2b@uint64;
(* vpsllq $0x15,%ymm10,%ymm10                      #! PC = 0x555555578218 *)
shl ymm10_0 ymm10_0 0x15@uint64;
shl ymm10_1 ymm10_1 0x15@uint64;
shl ymm10_2 ymm10_2 0x15@uint64;
shl ymm10_3 ymm10_3 0x15@uint64;
(* vmovdqa %ymm2,-0x90(%rbp)                       #! EA = L0x7fffffffbec0; PC = 0x55555557821e *)
mov L0x7fffffffbec0 ymm2_0;
mov L0x7fffffffbec8 ymm2_1;
mov L0x7fffffffbed0 ymm2_2;
mov L0x7fffffffbed8 ymm2_3;
(* vpor   %ymm1,%ymm10,%ymm10                      #! PC = 0x555555578226 *)
or ymm10_0@uint64 ymm10_0 ymm1_0;
or ymm10_1@uint64 ymm10_1 ymm1_1;
or ymm10_2@uint64 ymm10_2 ymm1_2;
or ymm10_3@uint64 ymm10_3 ymm1_3;
(* vpandn %ymm10,%ymm8,%ymm1                       #! PC = 0x55555557822a *)
not ymm8_0n@uint64 ymm8_0;
and ymm1_0@uint64 ymm8_0n ymm10_0;
not ymm8_1n@uint64 ymm8_1;
and ymm1_1@uint64 ymm8_1n ymm10_1;
not ymm8_2n@uint64 ymm8_2;
and ymm1_2@uint64 ymm8_2n ymm10_2;
not ymm8_3n@uint64 ymm8_3;
and ymm1_3@uint64 ymm8_3n ymm10_3;
(* vpxor  %ymm9,%ymm1,%ymm15                       #! PC = 0x55555557822f *)
xor ymm15_0@uint64 ymm1_0 ymm9_0;
xor ymm15_1@uint64 ymm1_1 ymm9_1;
xor ymm15_2@uint64 ymm1_2 ymm9_2;
xor ymm15_3@uint64 ymm1_3 ymm9_3;
(* vpsrlq $0x32,%ymm3,%ymm1                        #! PC = 0x555555578234 *)
shr ymm1_0 ymm3_0 0x32@uint64;
shr ymm1_1 ymm3_1 0x32@uint64;
shr ymm1_2 ymm3_2 0x32@uint64;
shr ymm1_3 ymm3_3 0x32@uint64;
(* vpandn %ymm9,%ymm0,%ymm9                        #! PC = 0x555555578239 *)
not ymm0_0n@uint64 ymm0_0;
and ymm9_0@uint64 ymm0_0n ymm9_0;
not ymm0_1n@uint64 ymm0_1;
and ymm9_1@uint64 ymm0_1n ymm9_1;
not ymm0_2n@uint64 ymm0_2;
and ymm9_2@uint64 ymm0_2n ymm9_2;
not ymm0_3n@uint64 ymm0_3;
and ymm9_3@uint64 ymm0_3n ymm9_3;
(* vpsllq $0xe,%ymm3,%ymm3                         #! PC = 0x55555557823e *)
shl ymm3_0 ymm3_0 0xe@uint64;
shl ymm3_1 ymm3_1 0xe@uint64;
shl ymm3_2 ymm3_2 0xe@uint64;
shl ymm3_3 ymm3_3 0xe@uint64;
(* vmovdqa %ymm15,-0x190(%rbp)                     #! EA = L0x7fffffffbdc0; PC = 0x555555578243 *)
mov L0x7fffffffbdc0 ymm15_0;
mov L0x7fffffffbdc8 ymm15_1;
mov L0x7fffffffbdd0 ymm15_2;
mov L0x7fffffffbdd8 ymm15_3;
(* vpor   %ymm1,%ymm3,%ymm3                        #! PC = 0x55555557824b *)
or ymm3_0@uint64 ymm3_0 ymm1_0;
or ymm3_1@uint64 ymm3_1 ymm1_1;
or ymm3_2@uint64 ymm3_2 ymm1_2;
or ymm3_3@uint64 ymm3_3 ymm1_3;
(* vpandn %ymm3,%ymm10,%ymm1                       #! PC = 0x55555557824f *)
not ymm10_0n@uint64 ymm10_0;
and ymm1_0@uint64 ymm10_0n ymm3_0;
not ymm10_1n@uint64 ymm10_1;
and ymm1_1@uint64 ymm10_1n ymm3_1;
not ymm10_2n@uint64 ymm10_2;
and ymm1_2@uint64 ymm10_2n ymm3_2;
not ymm10_3n@uint64 ymm10_3;
and ymm1_3@uint64 ymm10_3n ymm3_3;
(* vpxor  %ymm3,%ymm9,%ymm9                        #! PC = 0x555555578253 *)
xor ymm9_0@uint64 ymm9_0 ymm3_0;
xor ymm9_1@uint64 ymm9_1 ymm3_1;
xor ymm9_2@uint64 ymm9_2 ymm3_2;
xor ymm9_3@uint64 ymm9_3 ymm3_3;
(* vpxor  %ymm8,%ymm1,%ymm8                        #! PC = 0x555555578257 *)
xor ymm8_0@uint64 ymm1_0 ymm8_0;
xor ymm8_1@uint64 ymm1_1 ymm8_1;
xor ymm8_2@uint64 ymm1_2 ymm8_2;
xor ymm8_3@uint64 ymm1_3 ymm8_3;
(* vpandn %ymm0,%ymm3,%ymm1                        #! PC = 0x55555557825c *)
not ymm3_0n@uint64 ymm3_0;
and ymm1_0@uint64 ymm3_0n ymm0_0;
not ymm3_1n@uint64 ymm3_1;
and ymm1_1@uint64 ymm3_1n ymm0_1;
not ymm3_2n@uint64 ymm3_2;
and ymm1_2@uint64 ymm3_2n ymm0_2;
not ymm3_3n@uint64 ymm3_3;
and ymm1_3@uint64 ymm3_3n ymm0_3;
(* vpxor  -0x130(%rbp),%ymm13,%ymm0                #! EA = L0x7fffffffbe20; Value = 0x2002783081a00483; PC = 0x555555578260 *)
xor ymm0_0@uint64 ymm13_0 L0x7fffffffbe20;
xor ymm0_1@uint64 ymm13_1 L0x7fffffffbe28;
xor ymm0_2@uint64 ymm13_2 L0x7fffffffbe30;
xor ymm0_3@uint64 ymm13_3 L0x7fffffffbe38;
(* vmovdqa %ymm9,-0x250(%rbp)                      #! EA = L0x7fffffffbd00; PC = 0x555555578268 *)
mov L0x7fffffffbd00 ymm9_0;
mov L0x7fffffffbd08 ymm9_1;
mov L0x7fffffffbd10 ymm9_2;
mov L0x7fffffffbd18 ymm9_3;
(* vpxor  %ymm10,%ymm1,%ymm10                      #! PC = 0x555555578270 *)
xor ymm10_0@uint64 ymm1_0 ymm10_0;
xor ymm10_1@uint64 ymm1_1 ymm10_1;
xor ymm10_2@uint64 ymm1_2 ymm10_2;
xor ymm10_3@uint64 ymm1_3 ymm10_3;
(* vmovdqa %ymm8,-0x270(%rbp)                      #! EA = L0x7fffffffbce0; PC = 0x555555578275 *)
mov L0x7fffffffbce0 ymm8_0;
mov L0x7fffffffbce8 ymm8_1;
mov L0x7fffffffbcf0 ymm8_2;
mov L0x7fffffffbcf8 ymm8_3;
(* vpsrlq $0x24,%ymm0,%ymm1                        #! PC = 0x55555557827d *)
shr ymm1_0 ymm0_0 0x24@uint64;
shr ymm1_1 ymm0_1 0x24@uint64;
shr ymm1_2 ymm0_2 0x24@uint64;
shr ymm1_3 ymm0_3 0x24@uint64;
(* vpsllq $0x1c,%ymm0,%ymm0                        #! PC = 0x555555578282 *)
shl ymm0_0 ymm0_0 0x1c@uint64;
shl ymm0_1 ymm0_1 0x1c@uint64;
shl ymm0_2 ymm0_2 0x1c@uint64;
shl ymm0_3 ymm0_3 0x1c@uint64;
(* vmovdqa %ymm10,-0x170(%rbp)                     #! EA = L0x7fffffffbde0; PC = 0x555555578287 *)
mov L0x7fffffffbde0 ymm10_0;
mov L0x7fffffffbde8 ymm10_1;
mov L0x7fffffffbdf0 ymm10_2;
mov L0x7fffffffbdf8 ymm10_3;
(* vpxor  -0x2b0(%rbp),%ymm11,%ymm10               #! EA = L0x7fffffffbca0; Value = 0x10488c0040d14100; PC = 0x55555557828f *)
xor ymm10_0@uint64 ymm11_0 L0x7fffffffbca0;
xor ymm10_1@uint64 ymm11_1 L0x7fffffffbca8;
xor ymm10_2@uint64 ymm11_2 L0x7fffffffbcb0;
xor ymm10_3@uint64 ymm11_3 L0x7fffffffbcb8;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x555555578297 *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x110(%rbp),%ymm11,%ymm1                #! EA = L0x7fffffffbe40; Value = 0x60030c30241400c2; PC = 0x55555557829b *)
xor ymm1_0@uint64 ymm11_0 L0x7fffffffbe40;
xor ymm1_1@uint64 ymm11_1 L0x7fffffffbe48;
xor ymm1_2@uint64 ymm11_2 L0x7fffffffbe50;
xor ymm1_3@uint64 ymm11_3 L0x7fffffffbe58;
(* vpsrlq $0x2c,%ymm1,%ymm2                        #! PC = 0x5555555782a3 *)
shr ymm2_0 ymm1_0 0x2c@uint64;
shr ymm2_1 ymm1_1 0x2c@uint64;
shr ymm2_2 ymm1_2 0x2c@uint64;
shr ymm2_3 ymm1_3 0x2c@uint64;
(* vpsllq $0x14,%ymm1,%ymm1                        #! PC = 0x5555555782a8 *)
shl ymm1_0 ymm1_0 0x14@uint64;
shl ymm1_1 ymm1_1 0x14@uint64;
shl ymm1_2 ymm1_2 0x14@uint64;
shl ymm1_3 ymm1_3 0x14@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x5555555782ad *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpxor  -0xd0(%rbp),%ymm5,%ymm2                  #! EA = L0x7fffffffbe80; Value = 0x0000a50434031950; PC = 0x5555555782b1 *)
xor ymm2_0@uint64 ymm5_0 L0x7fffffffbe80;
xor ymm2_1@uint64 ymm5_1 L0x7fffffffbe88;
xor ymm2_2@uint64 ymm5_2 L0x7fffffffbe90;
xor ymm2_3@uint64 ymm5_3 L0x7fffffffbe98;
(* vpsrlq $0x3d,%ymm2,%ymm3                        #! PC = 0x5555555782b9 *)
shr ymm3_0 ymm2_0 0x3d@uint64;
shr ymm3_1 ymm2_1 0x3d@uint64;
shr ymm3_2 ymm2_2 0x3d@uint64;
shr ymm3_3 ymm2_3 0x3d@uint64;
(* vpsllq $0x3,%ymm2,%ymm2                         #! PC = 0x5555555782be *)
shl ymm2_0 ymm2_0 0x3@uint64;
shl ymm2_1 ymm2_1 0x3@uint64;
shl ymm2_2 ymm2_2 0x3@uint64;
shl ymm2_3 ymm2_3 0x3@uint64;
(* vpor   %ymm3,%ymm2,%ymm2                        #! PC = 0x5555555782c3 *)
or ymm2_0@uint64 ymm2_0 ymm3_0;
or ymm2_1@uint64 ymm2_1 ymm3_1;
or ymm2_2@uint64 ymm2_2 ymm3_2;
or ymm2_3@uint64 ymm2_3 ymm3_3;
(* vpandn %ymm2,%ymm1,%ymm3                        #! PC = 0x5555555782c7 *)
not ymm1_0n@uint64 ymm1_0;
and ymm3_0@uint64 ymm1_0n ymm2_0;
not ymm1_1n@uint64 ymm1_1;
and ymm3_1@uint64 ymm1_1n ymm2_1;
not ymm1_2n@uint64 ymm1_2;
and ymm3_2@uint64 ymm1_2n ymm2_2;
not ymm1_3n@uint64 ymm1_3;
and ymm3_3@uint64 ymm1_3n ymm2_3;
(* vpxor  %ymm0,%ymm3,%ymm8                        #! PC = 0x5555555782cb *)
xor ymm8_0@uint64 ymm3_0 ymm0_0;
xor ymm8_1@uint64 ymm3_1 ymm0_1;
xor ymm8_2@uint64 ymm3_2 ymm0_2;
xor ymm8_3@uint64 ymm3_3 ymm0_3;
(* vpsrlq $0x13,%ymm7,%ymm3                        #! PC = 0x5555555782cf *)
shr ymm3_0 ymm7_0 0x13@uint64;
shr ymm3_1 ymm7_1 0x13@uint64;
shr ymm3_2 ymm7_2 0x13@uint64;
shr ymm3_3 ymm7_3 0x13@uint64;
(* vpsllq $0x2d,%ymm7,%ymm7                        #! PC = 0x5555555782d4 *)
shl ymm7_0 ymm7_0 0x2d@uint64;
shl ymm7_1 ymm7_1 0x2d@uint64;
shl ymm7_2 ymm7_2 0x2d@uint64;
shl ymm7_3 ymm7_3 0x2d@uint64;
(* vmovdqa %ymm8,-0x230(%rbp)                      #! EA = L0x7fffffffbd20; PC = 0x5555555782d9 *)
mov L0x7fffffffbd20 ymm8_0;
mov L0x7fffffffbd28 ymm8_1;
mov L0x7fffffffbd30 ymm8_2;
mov L0x7fffffffbd38 ymm8_3;
(* vpor   %ymm3,%ymm7,%ymm15                       #! PC = 0x5555555782e1 *)
or ymm15_0@uint64 ymm7_0 ymm3_0;
or ymm15_1@uint64 ymm7_1 ymm3_1;
or ymm15_2@uint64 ymm7_2 ymm3_2;
or ymm15_3@uint64 ymm7_3 ymm3_3;
(* vpsrlq $0x3,%ymm12,%ymm3                        #! PC = 0x5555555782e5 *)
shr ymm3_0 ymm12_0 0x3@uint64;
shr ymm3_1 ymm12_1 0x3@uint64;
shr ymm3_2 ymm12_2 0x3@uint64;
shr ymm3_3 ymm12_3 0x3@uint64;
(* vpsllq $0x3d,%ymm12,%ymm12                      #! PC = 0x5555555782eb *)
shl ymm12_0 ymm12_0 0x3d@uint64;
shl ymm12_1 ymm12_1 0x3d@uint64;
shl ymm12_2 ymm12_2 0x3d@uint64;
shl ymm12_3 ymm12_3 0x3d@uint64;
(* vpandn %ymm15,%ymm2,%ymm8                       #! PC = 0x5555555782f1 *)
not ymm2_0n@uint64 ymm2_0;
and ymm8_0@uint64 ymm2_0n ymm15_0;
not ymm2_1n@uint64 ymm2_1;
and ymm8_1@uint64 ymm2_1n ymm15_1;
not ymm2_2n@uint64 ymm2_2;
and ymm8_2@uint64 ymm2_2n ymm15_2;
not ymm2_3n@uint64 ymm2_3;
and ymm8_3@uint64 ymm2_3n ymm15_3;
(* vpor   %ymm3,%ymm12,%ymm12                      #! PC = 0x5555555782f6 *)
or ymm12_0@uint64 ymm12_0 ymm3_0;
or ymm12_1@uint64 ymm12_1 ymm3_1;
or ymm12_2@uint64 ymm12_2 ymm3_2;
or ymm12_3@uint64 ymm12_3 ymm3_3;
(* vpxor  %ymm1,%ymm8,%ymm8                        #! PC = 0x5555555782fa *)
xor ymm8_0@uint64 ymm8_0 ymm1_0;
xor ymm8_1@uint64 ymm8_1 ymm1_1;
xor ymm8_2@uint64 ymm8_2 ymm1_2;
xor ymm8_3@uint64 ymm8_3 ymm1_3;
(* vpandn %ymm12,%ymm15,%ymm3                      #! PC = 0x5555555782fe *)
not ymm15_0n@uint64 ymm15_0;
and ymm3_0@uint64 ymm15_0n ymm12_0;
not ymm15_1n@uint64 ymm15_1;
and ymm3_1@uint64 ymm15_1n ymm12_1;
not ymm15_2n@uint64 ymm15_2;
and ymm3_2@uint64 ymm15_2n ymm12_2;
not ymm15_3n@uint64 ymm15_3;
and ymm3_3@uint64 ymm15_3n ymm12_3;
(* vpxor  %ymm2,%ymm3,%ymm7                        #! PC = 0x555555578303 *)
xor ymm7_0@uint64 ymm3_0 ymm2_0;
xor ymm7_1@uint64 ymm3_1 ymm2_1;
xor ymm7_2@uint64 ymm3_2 ymm2_2;
xor ymm7_3@uint64 ymm3_3 ymm2_3;
(* vpandn %ymm0,%ymm12,%ymm2                       #! PC = 0x555555578307 *)
not ymm12_0n@uint64 ymm12_0;
and ymm2_0@uint64 ymm12_0n ymm0_0;
not ymm12_1n@uint64 ymm12_1;
and ymm2_1@uint64 ymm12_1n ymm0_1;
not ymm12_2n@uint64 ymm12_2;
and ymm2_2@uint64 ymm12_2n ymm0_2;
not ymm12_3n@uint64 ymm12_3;
and ymm2_3@uint64 ymm12_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x55555557830b *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm12,%ymm0,%ymm12                      #! PC = 0x55555557830f *)
xor ymm12_0@uint64 ymm0_0 ymm12_0;
xor ymm12_1@uint64 ymm0_1 ymm12_1;
xor ymm12_2@uint64 ymm0_2 ymm12_2;
xor ymm12_3@uint64 ymm0_3 ymm12_3;
(* vpxor  -0x2f0(%rbp),%ymm4,%ymm0                 #! EA = L0x7fffffffbc60; Value = 0x38ca983082300106; PC = 0x555555578314 *)
xor ymm0_0@uint64 ymm4_0 L0x7fffffffbc60;
xor ymm0_1@uint64 ymm4_1 L0x7fffffffbc68;
xor ymm0_2@uint64 ymm4_2 L0x7fffffffbc70;
xor ymm0_3@uint64 ymm4_3 L0x7fffffffbc78;
(* vpxor  %ymm15,%ymm2,%ymm9                       #! PC = 0x55555557831c *)
xor ymm9_0@uint64 ymm2_0 ymm15_0;
xor ymm9_1@uint64 ymm2_1 ymm15_1;
xor ymm9_2@uint64 ymm2_2 ymm15_2;
xor ymm9_3@uint64 ymm2_3 ymm15_3;
(* vmovdqa %ymm7,-0x150(%rbp)                      #! EA = L0x7fffffffbe00; PC = 0x555555578321 *)
mov L0x7fffffffbe00 ymm7_0;
mov L0x7fffffffbe08 ymm7_1;
mov L0x7fffffffbe10 ymm7_2;
mov L0x7fffffffbe18 ymm7_3;
(* vmovdqa %ymm9,-0x210(%rbp)                      #! EA = L0x7fffffffbd40; PC = 0x555555578329 *)
mov L0x7fffffffbd40 ymm9_0;
mov L0x7fffffffbd48 ymm9_1;
mov L0x7fffffffbd50 ymm9_2;
mov L0x7fffffffbd58 ymm9_3;
(* vpxor  -0xb0(%rbp),%ymm13,%ymm9                 #! EA = L0x7fffffffbea0; Value = 0x0022263021081812; PC = 0x555555578331 *)
xor ymm9_0@uint64 ymm13_0 L0x7fffffffbea0;
xor ymm9_1@uint64 ymm13_1 L0x7fffffffbea8;
xor ymm9_2@uint64 ymm13_2 L0x7fffffffbeb0;
xor ymm9_3@uint64 ymm13_3 L0x7fffffffbeb8;
(* vpsrlq $0x3f,%ymm0,%ymm1                        #! PC = 0x555555578339 *)
shr ymm1_0 ymm0_0 0x3f@uint64;
shr ymm1_1 ymm0_1 0x3f@uint64;
shr ymm1_2 ymm0_2 0x3f@uint64;
shr ymm1_3 ymm0_3 0x3f@uint64;
(* vpsllq $0x1,%ymm0,%ymm0                         #! PC = 0x55555557833e *)
shl ymm0_0 ymm0_0 0x1@uint64;
shl ymm0_1 ymm0_1 0x1@uint64;
shl ymm0_2 ymm0_2 0x1@uint64;
shl ymm0_3 ymm0_3 0x1@uint64;
(* vmovdqa %ymm12,-0x130(%rbp)                     #! EA = L0x7fffffffbe20; PC = 0x555555578343 *)
mov L0x7fffffffbe20 ymm12_0;
mov L0x7fffffffbe28 ymm12_1;
mov L0x7fffffffbe30 ymm12_2;
mov L0x7fffffffbe38 ymm12_3;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x55555557834b *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x1f0(%rbp),%ymm14,%ymm1                #! EA = L0x7fffffffbd60; Value = 0x40038e20070020f0; PC = 0x55555557834f *)
xor ymm1_0@uint64 ymm14_0 L0x7fffffffbd60;
xor ymm1_1@uint64 ymm14_1 L0x7fffffffbd68;
xor ymm1_2@uint64 ymm14_2 L0x7fffffffbd70;
xor ymm1_3@uint64 ymm14_3 L0x7fffffffbd78;
(* vpsrlq $0x27,%ymm9,%ymm3                        #! PC = 0x555555578357 *)
shr ymm3_0 ymm9_0 0x27@uint64;
shr ymm3_1 ymm9_1 0x27@uint64;
shr ymm3_2 ymm9_2 0x27@uint64;
shr ymm3_3 ymm9_3 0x27@uint64;
(* vpsllq $0x19,%ymm9,%ymm9                        #! PC = 0x55555557835d *)
shl ymm9_0 ymm9_0 0x19@uint64;
shl ymm9_1 ymm9_1 0x19@uint64;
shl ymm9_2 ymm9_2 0x19@uint64;
shl ymm9_3 ymm9_3 0x19@uint64;
(* vpsrlq $0x3a,%ymm1,%ymm2                        #! PC = 0x555555578363 *)
shr ymm2_0 ymm1_0 0x3a@uint64;
shr ymm2_1 ymm1_1 0x3a@uint64;
shr ymm2_2 ymm1_2 0x3a@uint64;
shr ymm2_3 ymm1_3 0x3a@uint64;
(* vpsllq $0x6,%ymm1,%ymm1                         #! PC = 0x555555578368 *)
shl ymm1_0 ymm1_0 0x6@uint64;
shl ymm1_1 ymm1_1 0x6@uint64;
shl ymm1_2 ymm1_2 0x6@uint64;
shl ymm1_3 ymm1_3 0x6@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x55555557836d *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpor   %ymm3,%ymm9,%ymm2                        #! PC = 0x555555578371 *)
or ymm2_0@uint64 ymm9_0 ymm3_0;
or ymm2_1@uint64 ymm9_1 ymm3_1;
or ymm2_2@uint64 ymm9_2 ymm3_2;
or ymm2_3@uint64 ymm9_3 ymm3_3;
(* vpandn %ymm2,%ymm1,%ymm3                        #! PC = 0x555555578375 *)
not ymm1_0n@uint64 ymm1_0;
and ymm3_0@uint64 ymm1_0n ymm2_0;
not ymm1_1n@uint64 ymm1_1;
and ymm3_1@uint64 ymm1_1n ymm2_1;
not ymm1_2n@uint64 ymm1_2;
and ymm3_2@uint64 ymm1_2n ymm2_2;
not ymm1_3n@uint64 ymm1_3;
and ymm3_3@uint64 ymm1_3n ymm2_3;
(* vpxor  %ymm0,%ymm3,%ymm12                       #! PC = 0x555555578379 *)
xor ymm12_0@uint64 ymm3_0 ymm0_0;
xor ymm12_1@uint64 ymm3_1 ymm0_1;
xor ymm12_2@uint64 ymm3_2 ymm0_2;
xor ymm12_3@uint64 ymm3_3 ymm0_3;
(* vpxor  -0x50(%rbp),%ymm11,%ymm3                 #! EA = L0x7fffffffbf00; Value = 0x0a08402100074184; PC = 0x55555557837d *)
xor ymm3_0@uint64 ymm11_0 L0x7fffffffbf00;
xor ymm3_1@uint64 ymm11_1 L0x7fffffffbf08;
xor ymm3_2@uint64 ymm11_2 L0x7fffffffbf10;
xor ymm3_3@uint64 ymm11_3 L0x7fffffffbf18;
(* vpxor  -0xf0(%rbp),%ymm11,%ymm11                #! EA = L0x7fffffffbe60; Value = 0xc41c018a18108081; PC = 0x555555578382 *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbe60;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbe68;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbe70;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbe78;
(* vmovdqa %ymm12,-0xd0(%rbp)                      #! EA = L0x7fffffffbe80; PC = 0x55555557838a *)
mov L0x7fffffffbe80 ymm12_0;
mov L0x7fffffffbe88 ymm12_1;
mov L0x7fffffffbe90 ymm12_2;
mov L0x7fffffffbe98 ymm12_3;
(* vpshufb 0x55b65(%rip),%ymm3,%ymm3        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x555555578392 *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm3_0, ymm3_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm3_2, ymm3_3, tmp_2, tmp_3);
mov ymm3_0 tmp_0;
mov ymm3_1 tmp_1;
mov ymm3_2 tmp_2;
mov ymm3_3 tmp_3;
(* vpandn %ymm3,%ymm2,%ymm7                        #! PC = 0x55555557839b *)
not ymm2_0n@uint64 ymm2_0;
and ymm7_0@uint64 ymm2_0n ymm3_0;
not ymm2_1n@uint64 ymm2_1;
and ymm7_1@uint64 ymm2_1n ymm3_1;
not ymm2_2n@uint64 ymm2_2;
and ymm7_2@uint64 ymm2_2n ymm3_2;
not ymm2_3n@uint64 ymm2_3;
and ymm7_3@uint64 ymm2_3n ymm3_3;
(* vpxor  %ymm1,%ymm7,%ymm15                       #! PC = 0x55555557839f *)
xor ymm15_0@uint64 ymm7_0 ymm1_0;
xor ymm15_1@uint64 ymm7_1 ymm1_1;
xor ymm15_2@uint64 ymm7_2 ymm1_2;
xor ymm15_3@uint64 ymm7_3 ymm1_3;
(* vpsrlq $0x2e,%ymm6,%ymm7                        #! PC = 0x5555555783a3 *)
shr ymm7_0 ymm6_0 0x2e@uint64;
shr ymm7_1 ymm6_1 0x2e@uint64;
shr ymm7_2 ymm6_2 0x2e@uint64;
shr ymm7_3 ymm6_3 0x2e@uint64;
(* vpsllq $0x12,%ymm6,%ymm6                        #! PC = 0x5555555783a8 *)
shl ymm6_0 ymm6_0 0x12@uint64;
shl ymm6_1 ymm6_1 0x12@uint64;
shl ymm6_2 ymm6_2 0x12@uint64;
shl ymm6_3 ymm6_3 0x12@uint64;
(* vpor   %ymm7,%ymm6,%ymm12                       #! PC = 0x5555555783ad *)
or ymm12_0@uint64 ymm6_0 ymm7_0;
or ymm12_1@uint64 ymm6_1 ymm7_1;
or ymm12_2@uint64 ymm6_2 ymm7_2;
or ymm12_3@uint64 ymm6_3 ymm7_3;
(* vpxor  -0x1b0(%rbp),%ymm4,%ymm7                 #! EA = L0x7fffffffbda0; Value = 0x003e0038090a8080; PC = 0x5555555783b1 *)
xor ymm7_0@uint64 ymm4_0 L0x7fffffffbda0;
xor ymm7_1@uint64 ymm4_1 L0x7fffffffbda8;
xor ymm7_2@uint64 ymm4_2 L0x7fffffffbdb0;
xor ymm7_3@uint64 ymm4_3 L0x7fffffffbdb8;
(* vpandn %ymm12,%ymm3,%ymm9                       #! PC = 0x5555555783b9 *)
not ymm3_0n@uint64 ymm3_0;
and ymm9_0@uint64 ymm3_0n ymm12_0;
not ymm3_1n@uint64 ymm3_1;
and ymm9_1@uint64 ymm3_1n ymm12_1;
not ymm3_2n@uint64 ymm3_2;
and ymm9_2@uint64 ymm3_2n ymm12_2;
not ymm3_3n@uint64 ymm3_3;
and ymm9_3@uint64 ymm3_3n ymm12_3;
(* vpxor  %ymm2,%ymm9,%ymm9                        #! PC = 0x5555555783be *)
xor ymm9_0@uint64 ymm9_0 ymm2_0;
xor ymm9_1@uint64 ymm9_1 ymm2_1;
xor ymm9_2@uint64 ymm9_2 ymm2_2;
xor ymm9_3@uint64 ymm9_3 ymm2_3;
(* vpandn %ymm0,%ymm12,%ymm2                       #! PC = 0x5555555783c2 *)
not ymm12_0n@uint64 ymm12_0;
and ymm2_0@uint64 ymm12_0n ymm0_0;
not ymm12_1n@uint64 ymm12_1;
and ymm2_1@uint64 ymm12_1n ymm0_1;
not ymm12_2n@uint64 ymm12_2;
and ymm2_2@uint64 ymm12_2n ymm0_2;
not ymm12_3n@uint64 ymm12_3;
and ymm2_3@uint64 ymm12_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x5555555783c6 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm12,%ymm0,%ymm0                       #! PC = 0x5555555783ca *)
xor ymm0_0@uint64 ymm0_0 ymm12_0;
xor ymm0_1@uint64 ymm0_1 ymm12_1;
xor ymm0_2@uint64 ymm0_2 ymm12_2;
xor ymm0_3@uint64 ymm0_3 ymm12_3;
(* vpsrlq $0x25,%ymm10,%ymm1                       #! PC = 0x5555555783cf *)
shr ymm1_0 ymm10_0 0x25@uint64;
shr ymm1_1 ymm10_1 0x25@uint64;
shr ymm1_2 ymm10_2 0x25@uint64;
shr ymm1_3 ymm10_3 0x25@uint64;
(* vpxor  %ymm3,%ymm2,%ymm2                        #! PC = 0x5555555783d5 *)
xor ymm2_0@uint64 ymm2_0 ymm3_0;
xor ymm2_1@uint64 ymm2_1 ymm3_1;
xor ymm2_2@uint64 ymm2_2 ymm3_2;
xor ymm2_3@uint64 ymm2_3 ymm3_3;
(* vpsllq $0x1b,%ymm10,%ymm10                      #! PC = 0x5555555783d9 *)
shl ymm10_0 ymm10_0 0x1b@uint64;
shl ymm10_1 ymm10_1 0x1b@uint64;
shl ymm10_2 ymm10_2 0x1b@uint64;
shl ymm10_3 ymm10_3 0x1b@uint64;
(* vmovdqa %ymm0,-0x110(%rbp)                      #! EA = L0x7fffffffbe40; PC = 0x5555555783df *)
mov L0x7fffffffbe40 ymm0_0;
mov L0x7fffffffbe48 ymm0_1;
mov L0x7fffffffbe50 ymm0_2;
mov L0x7fffffffbe58 ymm0_3;
(* vpsrlq $0x36,%ymm7,%ymm3                        #! PC = 0x5555555783e7 *)
shr ymm3_0 ymm7_0 0x36@uint64;
shr ymm3_1 ymm7_1 0x36@uint64;
shr ymm3_2 ymm7_2 0x36@uint64;
shr ymm3_3 ymm7_3 0x36@uint64;
(* vpor   %ymm1,%ymm10,%ymm0                       #! PC = 0x5555555783ec *)
or ymm0_0@uint64 ymm10_0 ymm1_0;
or ymm0_1@uint64 ymm10_1 ymm1_1;
or ymm0_2@uint64 ymm10_2 ymm1_2;
or ymm0_3@uint64 ymm10_3 ymm1_3;
(* vpsllq $0xa,%ymm7,%ymm7                         #! PC = 0x5555555783f0 *)
shl ymm7_0 ymm7_0 0xa@uint64;
shl ymm7_1 ymm7_1 0xa@uint64;
shl ymm7_2 ymm7_2 0xa@uint64;
shl ymm7_3 ymm7_3 0xa@uint64;
(* vpxor  -0x70(%rbp),%ymm14,%ymm10                #! EA = L0x7fffffffbee0; Value = 0x040c0061063a6584; PC = 0x5555555783f5 *)
xor ymm10_0@uint64 ymm14_0 L0x7fffffffbee0;
xor ymm10_1@uint64 ymm14_1 L0x7fffffffbee8;
xor ymm10_2@uint64 ymm14_2 L0x7fffffffbef0;
xor ymm10_3@uint64 ymm14_3 L0x7fffffffbef8;
(* vmovdqa %ymm2,-0xb0(%rbp)                       #! EA = L0x7fffffffbea0; PC = 0x5555555783fa *)
mov L0x7fffffffbea0 ymm2_0;
mov L0x7fffffffbea8 ymm2_1;
mov L0x7fffffffbeb0 ymm2_2;
mov L0x7fffffffbeb8 ymm2_3;
(* vpxor  -0x290(%rbp),%ymm5,%ymm1                 #! EA = L0x7fffffffbcc0; Value = 0x00049840d3042220; PC = 0x555555578402 *)
xor ymm1_0@uint64 ymm5_0 L0x7fffffffbcc0;
xor ymm1_1@uint64 ymm5_1 L0x7fffffffbcc8;
xor ymm1_2@uint64 ymm5_2 L0x7fffffffbcd0;
xor ymm1_3@uint64 ymm5_3 L0x7fffffffbcd8;
(* vpxor  -0x2d0(%rbp),%ymm14,%ymm14               #! EA = L0x7fffffffbc80; Value = 0x08b0340041c14101; PC = 0x55555557840a *)
xor ymm14_0@uint64 ymm14_0 L0x7fffffffbc80;
xor ymm14_1@uint64 ymm14_1 L0x7fffffffbc88;
xor ymm14_2@uint64 ymm14_2 L0x7fffffffbc90;
xor ymm14_3@uint64 ymm14_3 L0x7fffffffbc98;
(* vpsrlq $0x1c,%ymm1,%ymm2                        #! PC = 0x555555578412 *)
shr ymm2_0 ymm1_0 0x1c@uint64;
shr ymm2_1 ymm1_1 0x1c@uint64;
shr ymm2_2 ymm1_2 0x1c@uint64;
shr ymm2_3 ymm1_3 0x1c@uint64;
(* vpsllq $0x24,%ymm1,%ymm1                        #! PC = 0x555555578417 *)
shl ymm1_0 ymm1_0 0x24@uint64;
shl ymm1_1 ymm1_1 0x24@uint64;
shl ymm1_2 ymm1_2 0x24@uint64;
shl ymm1_3 ymm1_3 0x24@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x55555557841c *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpor   %ymm3,%ymm7,%ymm2                        #! PC = 0x555555578420 *)
or ymm2_0@uint64 ymm7_0 ymm3_0;
or ymm2_1@uint64 ymm7_1 ymm3_1;
or ymm2_2@uint64 ymm7_2 ymm3_2;
or ymm2_3@uint64 ymm7_3 ymm3_3;
(* vpandn %ymm2,%ymm1,%ymm3                        #! PC = 0x555555578424 *)
not ymm1_0n@uint64 ymm1_0;
and ymm3_0@uint64 ymm1_0n ymm2_0;
not ymm1_1n@uint64 ymm1_1;
and ymm3_1@uint64 ymm1_1n ymm2_1;
not ymm1_2n@uint64 ymm1_2;
and ymm3_2@uint64 ymm1_2n ymm2_2;
not ymm1_3n@uint64 ymm1_3;
and ymm3_3@uint64 ymm1_3n ymm2_3;
(* vpxor  %ymm0,%ymm3,%ymm6                        #! PC = 0x555555578428 *)
xor ymm6_0@uint64 ymm3_0 ymm0_0;
xor ymm6_1@uint64 ymm3_1 ymm0_1;
xor ymm6_2@uint64 ymm3_2 ymm0_2;
xor ymm6_3@uint64 ymm3_3 ymm0_3;
(* vpsrlq $0x31,%ymm10,%ymm3                       #! PC = 0x55555557842c *)
shr ymm3_0 ymm10_0 0x31@uint64;
shr ymm3_1 ymm10_1 0x31@uint64;
shr ymm3_2 ymm10_2 0x31@uint64;
shr ymm3_3 ymm10_3 0x31@uint64;
(* vpsllq $0xf,%ymm10,%ymm10                       #! PC = 0x555555578432 *)
shl ymm10_0 ymm10_0 0xf@uint64;
shl ymm10_1 ymm10_1 0xf@uint64;
shl ymm10_2 ymm10_2 0xf@uint64;
shl ymm10_3 ymm10_3 0xf@uint64;
(* vmovdqa %ymm6,%ymm12                            #! PC = 0x555555578438 *)
mov ymm12_0 ymm6_0;
mov ymm12_1 ymm6_1;
mov ymm12_2 ymm6_2;
mov ymm12_3 ymm6_3;
(* vpor   %ymm3,%ymm10,%ymm10                      #! PC = 0x55555557843c *)
or ymm10_0@uint64 ymm10_0 ymm3_0;
or ymm10_1@uint64 ymm10_1 ymm3_1;
or ymm10_2@uint64 ymm10_2 ymm3_2;
or ymm10_3@uint64 ymm10_3 ymm3_3;
(* vpxor  -0x310(%rbp),%ymm13,%ymm3                #! EA = L0x7fffffffbc40; Value = 0x4100180008e801a1; PC = 0x555555578440 *)
xor ymm3_0@uint64 ymm13_0 L0x7fffffffbc40;
xor ymm3_1@uint64 ymm13_1 L0x7fffffffbc48;
xor ymm3_2@uint64 ymm13_2 L0x7fffffffbc50;
xor ymm3_3@uint64 ymm13_3 L0x7fffffffbc58;
(* vpxor  -0x1d0(%rbp),%ymm13,%ymm13               #! EA = L0x7fffffffbd80; Value = 0x31953040d0600010; PC = 0x555555578448 *)
xor ymm13_0@uint64 ymm13_0 L0x7fffffffbd80;
xor ymm13_1@uint64 ymm13_1 L0x7fffffffbd88;
xor ymm13_2@uint64 ymm13_2 L0x7fffffffbd90;
xor ymm13_3@uint64 ymm13_3 L0x7fffffffbd98;
(* vmovdqa %ymm12,-0x310(%rbp)                     #! EA = L0x7fffffffbc40; PC = 0x555555578450 *)
mov L0x7fffffffbc40 ymm12_0;
mov L0x7fffffffbc48 ymm12_1;
mov L0x7fffffffbc50 ymm12_2;
mov L0x7fffffffbc58 ymm12_3;
(* vpandn %ymm10,%ymm2,%ymm7                       #! PC = 0x555555578458 *)
not ymm2_0n@uint64 ymm2_0;
and ymm7_0@uint64 ymm2_0n ymm10_0;
not ymm2_1n@uint64 ymm2_1;
and ymm7_1@uint64 ymm2_1n ymm10_1;
not ymm2_2n@uint64 ymm2_2;
and ymm7_2@uint64 ymm2_2n ymm10_2;
not ymm2_3n@uint64 ymm2_3;
and ymm7_3@uint64 ymm2_3n ymm10_3;
(* vpshufb 0x55a7a(%rip),%ymm3,%ymm3        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x55555557845d *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm3_0, ymm3_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm3_2, ymm3_3, tmp_2, tmp_3);
mov ymm3_0 tmp_0;
mov ymm3_1 tmp_1;
mov ymm3_2 tmp_2;
mov ymm3_3 tmp_3;
(* vpxor  %ymm1,%ymm7,%ymm7                        #! PC = 0x555555578466 *)
xor ymm7_0@uint64 ymm7_0 ymm1_0;
xor ymm7_1@uint64 ymm7_1 ymm1_1;
xor ymm7_2@uint64 ymm7_2 ymm1_2;
xor ymm7_3@uint64 ymm7_3 ymm1_3;
(* vpandn %ymm3,%ymm10,%ymm6                       #! PC = 0x55555557846a *)
not ymm10_0n@uint64 ymm10_0;
and ymm6_0@uint64 ymm10_0n ymm3_0;
not ymm10_1n@uint64 ymm10_1;
and ymm6_1@uint64 ymm10_1n ymm3_1;
not ymm10_2n@uint64 ymm10_2;
and ymm6_2@uint64 ymm10_2n ymm3_2;
not ymm10_3n@uint64 ymm10_3;
and ymm6_3@uint64 ymm10_3n ymm3_3;
(* vpxor  %ymm2,%ymm6,%ymm6                        #! PC = 0x55555557846e *)
xor ymm6_0@uint64 ymm6_0 ymm2_0;
xor ymm6_1@uint64 ymm6_1 ymm2_1;
xor ymm6_2@uint64 ymm6_2 ymm2_2;
xor ymm6_3@uint64 ymm6_3 ymm2_3;
(* vpandn %ymm0,%ymm3,%ymm2                        #! PC = 0x555555578472 *)
not ymm3_0n@uint64 ymm3_0;
and ymm2_0@uint64 ymm3_0n ymm0_0;
not ymm3_1n@uint64 ymm3_1;
and ymm2_1@uint64 ymm3_1n ymm0_1;
not ymm3_2n@uint64 ymm3_2;
and ymm2_2@uint64 ymm3_2n ymm0_2;
not ymm3_3n@uint64 ymm3_3;
and ymm2_3@uint64 ymm3_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x555555578476 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpsrlq $0x2,%ymm14,%ymm1                        #! PC = 0x55555557847a *)
shr ymm1_0 ymm14_0 0x2@uint64;
shr ymm1_1 ymm14_1 0x2@uint64;
shr ymm1_2 ymm14_2 0x2@uint64;
shr ymm1_3 ymm14_3 0x2@uint64;
(* vpsllq $0x3e,%ymm14,%ymm14                      #! PC = 0x555555578480 *)
shl ymm14_0 ymm14_0 0x3e@uint64;
shl ymm14_1 ymm14_1 0x3e@uint64;
shl ymm14_2 ymm14_2 0x3e@uint64;
shl ymm14_3 ymm14_3 0x3e@uint64;
(* vmovdqa %ymm6,-0x70(%rbp)                       #! EA = L0x7fffffffbee0; PC = 0x555555578486 *)
mov L0x7fffffffbee0 ymm6_0;
mov L0x7fffffffbee8 ymm6_1;
mov L0x7fffffffbef0 ymm6_2;
mov L0x7fffffffbef8 ymm6_3;
(* vpxor  %ymm3,%ymm0,%ymm6                        #! PC = 0x55555557848b *)
xor ymm6_0@uint64 ymm0_0 ymm3_0;
xor ymm6_1@uint64 ymm0_1 ymm3_1;
xor ymm6_2@uint64 ymm0_2 ymm3_2;
xor ymm6_3@uint64 ymm0_3 ymm3_3;
(* vpor   %ymm1,%ymm14,%ymm3                       #! PC = 0x55555557848f *)
or ymm3_0@uint64 ymm14_0 ymm1_0;
or ymm3_1@uint64 ymm14_1 ymm1_1;
or ymm3_2@uint64 ymm14_2 ymm1_2;
or ymm3_3@uint64 ymm14_3 ymm1_3;
(* vpsrlq $0x9,%ymm13,%ymm0                        #! PC = 0x555555578493 *)
shr ymm0_0 ymm13_0 0x9@uint64;
shr ymm0_1 ymm13_1 0x9@uint64;
shr ymm0_2 ymm13_2 0x9@uint64;
shr ymm0_3 ymm13_3 0x9@uint64;
(* vmovdqa %ymm6,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x555555578499 *)
mov L0x7fffffffbf00 ymm6_0;
mov L0x7fffffffbf08 ymm6_1;
mov L0x7fffffffbf10 ymm6_2;
mov L0x7fffffffbf18 ymm6_3;
(* vpxor  %ymm10,%ymm2,%ymm10                      #! PC = 0x55555557849e *)
xor ymm10_0@uint64 ymm2_0 ymm10_0;
xor ymm10_1@uint64 ymm2_1 ymm10_1;
xor ymm10_2@uint64 ymm2_2 ymm10_2;
xor ymm10_3@uint64 ymm2_3 ymm10_3;
(* vpsrlq $0x19,%ymm11,%ymm1                       #! PC = 0x5555555784a3 *)
shr ymm1_0 ymm11_0 0x19@uint64;
shr ymm1_1 ymm11_1 0x19@uint64;
shr ymm1_2 ymm11_2 0x19@uint64;
shr ymm1_3 ymm11_3 0x19@uint64;
(* vpsllq $0x37,%ymm13,%ymm13                      #! PC = 0x5555555784a9 *)
shl ymm13_0 ymm13_0 0x37@uint64;
shl ymm13_1 ymm13_1 0x37@uint64;
shl ymm13_2 ymm13_2 0x37@uint64;
shl ymm13_3 ymm13_3 0x37@uint64;
(* vpsllq $0x27,%ymm11,%ymm11                      #! PC = 0x5555555784af *)
shl ymm11_0 ymm11_0 0x27@uint64;
shl ymm11_1 ymm11_1 0x27@uint64;
shl ymm11_2 ymm11_2 0x27@uint64;
shl ymm11_3 ymm11_3 0x27@uint64;
(* vpor   %ymm0,%ymm13,%ymm13                      #! PC = 0x5555555784b5 *)
or ymm13_0@uint64 ymm13_0 ymm0_0;
or ymm13_1@uint64 ymm13_1 ymm0_1;
or ymm13_2@uint64 ymm13_2 ymm0_2;
or ymm13_3@uint64 ymm13_3 ymm0_3;
(* vmovdqa -0xd0(%rbp),%ymm0                       #! EA = L0x7fffffffbe80; Value = 0x43149ae4f34cae4e; PC = 0x5555555784b9 *)
mov ymm0_0 L0x7fffffffbe80;
mov ymm0_1 L0x7fffffffbe88;
mov ymm0_2 L0x7fffffffbe90;
mov ymm0_3 L0x7fffffffbe98;
(* vpxor  -0x230(%rbp),%ymm0,%ymm0                 #! EA = L0x7fffffffbd20; Value = 0x8b4b752fe1ff3dd4; PC = 0x5555555784c1 *)
xor ymm0_0@uint64 ymm0_0 L0x7fffffffbd20;
xor ymm0_1@uint64 ymm0_1 L0x7fffffffbd28;
xor ymm0_2@uint64 ymm0_2 L0x7fffffffbd30;
xor ymm0_3@uint64 ymm0_3 L0x7fffffffbd38;
(* vpor   %ymm1,%ymm11,%ymm11                      #! PC = 0x5555555784c9 *)
or ymm11_0@uint64 ymm11_0 ymm1_0;
or ymm11_1@uint64 ymm11_1 ymm1_1;
or ymm11_2@uint64 ymm11_2 ymm1_2;
or ymm11_3@uint64 ymm11_3 ymm1_3;
(* vpxor  -0x330(%rbp),%ymm5,%ymm5                 #! EA = L0x7fffffffbc20; Value = 0x01840c580e382590; PC = 0x5555555784cd *)
xor ymm5_0@uint64 ymm5_0 L0x7fffffffbc20;
xor ymm5_1@uint64 ymm5_1 L0x7fffffffbc28;
xor ymm5_2@uint64 ymm5_2 L0x7fffffffbc30;
xor ymm5_3@uint64 ymm5_3 L0x7fffffffbc38;
(* vpxor  -0x350(%rbp),%ymm4,%ymm4                 #! EA = L0x7fffffffbc00; Value = 0xc700080080e020e2; PC = 0x5555555784d5 *)
xor ymm4_0@uint64 ymm4_0 L0x7fffffffbc00;
xor ymm4_1@uint64 ymm4_1 L0x7fffffffbc08;
xor ymm4_2@uint64 ymm4_2 L0x7fffffffbc10;
xor ymm4_3@uint64 ymm4_3 L0x7fffffffbc18;
(* vmovdqa %ymm15,-0x330(%rbp)                     #! EA = L0x7fffffffbc20; PC = 0x5555555784dd *)
mov L0x7fffffffbc20 ymm15_0;
mov L0x7fffffffbc28 ymm15_1;
mov L0x7fffffffbc30 ymm15_2;
mov L0x7fffffffbc38 ymm15_3;
(* vpandn %ymm11,%ymm13,%ymm6                      #! PC = 0x5555555784e5 *)
not ymm13_0n@uint64 ymm13_0;
and ymm6_0@uint64 ymm13_0n ymm11_0;
not ymm13_1n@uint64 ymm13_1;
and ymm6_1@uint64 ymm13_1n ymm11_1;
not ymm13_2n@uint64 ymm13_2;
and ymm6_2@uint64 ymm13_2n ymm11_2;
not ymm13_3n@uint64 ymm13_3;
and ymm6_3@uint64 ymm13_3n ymm11_3;
(* vpxor  %ymm3,%ymm6,%ymm6                        #! PC = 0x5555555784ea *)
xor ymm6_0@uint64 ymm6_0 ymm3_0;
xor ymm6_1@uint64 ymm6_1 ymm3_1;
xor ymm6_2@uint64 ymm6_2 ymm3_2;
xor ymm6_3@uint64 ymm6_3 ymm3_3;
(* vpsrlq $0x3e,%ymm4,%ymm1                        #! PC = 0x5555555784ee *)
shr ymm1_0 ymm4_0 0x3e@uint64;
shr ymm1_1 ymm4_1 0x3e@uint64;
shr ymm1_2 ymm4_2 0x3e@uint64;
shr ymm1_3 ymm4_3 0x3e@uint64;
(* vpxor  %ymm12,%ymm6,%ymm2                       #! PC = 0x5555555784f3 *)
xor ymm2_0@uint64 ymm6_0 ymm12_0;
xor ymm2_1@uint64 ymm6_1 ymm12_1;
xor ymm2_2@uint64 ymm6_2 ymm12_2;
xor ymm2_3@uint64 ymm6_3 ymm12_3;
(* vpsllq $0x2,%ymm4,%ymm4                         #! PC = 0x5555555784f8 *)
shl ymm4_0 ymm4_0 0x2@uint64;
shl ymm4_1 ymm4_1 0x2@uint64;
shl ymm4_2 ymm4_2 0x2@uint64;
shl ymm4_3 ymm4_3 0x2@uint64;
(* vpxor  %ymm0,%ymm2,%ymm2                        #! PC = 0x5555555784fd *)
xor ymm2_0@uint64 ymm2_0 ymm0_0;
xor ymm2_1@uint64 ymm2_1 ymm0_1;
xor ymm2_2@uint64 ymm2_2 ymm0_2;
xor ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x17,%ymm5,%ymm0                        #! PC = 0x555555578501 *)
shr ymm0_0 ymm5_0 0x17@uint64;
shr ymm0_1 ymm5_1 0x17@uint64;
shr ymm0_2 ymm5_2 0x17@uint64;
shr ymm0_3 ymm5_3 0x17@uint64;
(* vpxor  -0x90(%rbp),%ymm2,%ymm2                  #! EA = L0x7fffffffbec0; Value = 0x0838573a4deb6243; PC = 0x555555578506 *)
xor ymm2_0@uint64 ymm2_0 L0x7fffffffbec0;
xor ymm2_1@uint64 ymm2_1 L0x7fffffffbec8;
xor ymm2_2@uint64 ymm2_2 L0x7fffffffbed0;
xor ymm2_3@uint64 ymm2_3 L0x7fffffffbed8;
(* vpsllq $0x29,%ymm5,%ymm5                        #! PC = 0x55555557850e *)
shl ymm5_0 ymm5_0 0x29@uint64;
shl ymm5_1 ymm5_1 0x29@uint64;
shl ymm5_2 ymm5_2 0x29@uint64;
shl ymm5_3 ymm5_3 0x29@uint64;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x555555578513 *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpandn %ymm5,%ymm11,%ymm0                       #! PC = 0x555555578517 *)
not ymm11_0n@uint64 ymm11_0;
and ymm0_0@uint64 ymm11_0n ymm5_0;
not ymm11_1n@uint64 ymm11_1;
and ymm0_1@uint64 ymm11_1n ymm5_1;
not ymm11_2n@uint64 ymm11_2;
and ymm0_2@uint64 ymm11_2n ymm5_2;
not ymm11_3n@uint64 ymm11_3;
and ymm0_3@uint64 ymm11_3n ymm5_3;
(* vpxor  %ymm13,%ymm0,%ymm14                      #! PC = 0x55555557851b *)
xor ymm14_0@uint64 ymm0_0 ymm13_0;
xor ymm14_1@uint64 ymm0_1 ymm13_1;
xor ymm14_2@uint64 ymm0_2 ymm13_2;
xor ymm14_3@uint64 ymm0_3 ymm13_3;
(* vpxor  -0x190(%rbp),%ymm8,%ymm0                 #! EA = L0x7fffffffbdc0; Value = 0xcf57a0c778692494; PC = 0x555555578520 *)
xor ymm0_0@uint64 ymm8_0 L0x7fffffffbdc0;
xor ymm0_1@uint64 ymm8_1 L0x7fffffffbdc8;
xor ymm0_2@uint64 ymm8_2 L0x7fffffffbdd0;
xor ymm0_3@uint64 ymm8_3 L0x7fffffffbdd8;
(* vmovdqa %ymm14,%ymm12                           #! PC = 0x555555578528 *)
mov ymm12_0 ymm14_0;
mov ymm12_1 ymm14_1;
mov ymm12_2 ymm14_2;
mov ymm12_3 ymm14_3;
(* vpxor  %ymm15,%ymm7,%ymm14                      #! PC = 0x55555557852d *)
xor ymm14_0@uint64 ymm7_0 ymm15_0;
xor ymm14_1@uint64 ymm7_1 ymm15_1;
xor ymm14_2@uint64 ymm7_2 ymm15_2;
xor ymm14_3@uint64 ymm7_3 ymm15_3;
(* vpxor  -0x70(%rbp),%ymm9,%ymm15                 #! EA = L0x7fffffffbee0; Value = 0xbab5ebd7d1623c17; PC = 0x555555578532 *)
xor ymm15_0@uint64 ymm9_0 L0x7fffffffbee0;
xor ymm15_1@uint64 ymm9_1 L0x7fffffffbee8;
xor ymm15_2@uint64 ymm9_2 L0x7fffffffbef0;
xor ymm15_3@uint64 ymm9_3 L0x7fffffffbef8;
(* vpxor  %ymm0,%ymm14,%ymm14                      #! PC = 0x555555578537 *)
xor ymm14_0@uint64 ymm14_0 ymm0_0;
xor ymm14_1@uint64 ymm14_1 ymm0_1;
xor ymm14_2@uint64 ymm14_2 ymm0_2;
xor ymm14_3@uint64 ymm14_3 ymm0_3;
(* vpor   %ymm1,%ymm4,%ymm0                        #! PC = 0x55555557853b *)
or ymm0_0@uint64 ymm4_0 ymm1_0;
or ymm0_1@uint64 ymm4_1 ymm1_1;
or ymm0_2@uint64 ymm4_2 ymm1_2;
or ymm0_3@uint64 ymm4_3 ymm1_3;
(* vmovdqa %ymm12,-0x2f0(%rbp)                     #! EA = L0x7fffffffbc60; PC = 0x55555557853f *)
mov L0x7fffffffbc60 ymm12_0;
mov L0x7fffffffbc68 ymm12_1;
mov L0x7fffffffbc70 ymm12_2;
mov L0x7fffffffbc78 ymm12_3;
(* vpandn %ymm13,%ymm3,%ymm4                       #! PC = 0x555555578547 *)
not ymm3_0n@uint64 ymm3_0;
and ymm4_0@uint64 ymm3_0n ymm13_0;
not ymm3_1n@uint64 ymm3_1;
and ymm4_1@uint64 ymm3_1n ymm13_1;
not ymm3_2n@uint64 ymm3_2;
and ymm4_2@uint64 ymm3_2n ymm13_2;
not ymm3_3n@uint64 ymm3_3;
and ymm4_3@uint64 ymm3_3n ymm13_3;
(* vpandn %ymm0,%ymm5,%ymm1                        #! PC = 0x55555557854c *)
not ymm5_0n@uint64 ymm5_0;
and ymm1_0@uint64 ymm5_0n ymm0_0;
not ymm5_1n@uint64 ymm5_1;
and ymm1_1@uint64 ymm5_1n ymm0_1;
not ymm5_2n@uint64 ymm5_2;
and ymm1_2@uint64 ymm5_2n ymm0_2;
not ymm5_3n@uint64 ymm5_3;
and ymm1_3@uint64 ymm5_3n ymm0_3;
(* vpxor  %ymm12,%ymm14,%ymm14                     #! PC = 0x555555578550 *)
xor ymm14_0@uint64 ymm14_0 ymm12_0;
xor ymm14_1@uint64 ymm14_1 ymm12_1;
xor ymm14_2@uint64 ymm14_2 ymm12_2;
xor ymm14_3@uint64 ymm14_3 ymm12_3;
(* vpxor  %ymm0,%ymm4,%ymm4                        #! PC = 0x555555578555 *)
xor ymm4_0@uint64 ymm4_0 ymm0_0;
xor ymm4_1@uint64 ymm4_1 ymm0_1;
xor ymm4_2@uint64 ymm4_2 ymm0_2;
xor ymm4_3@uint64 ymm4_3 ymm0_3;
(* vmovdqa -0x130(%rbp),%ymm13                     #! EA = L0x7fffffffbe20; Value = 0x10401cdb131f356a; PC = 0x555555578559 *)
mov ymm13_0 L0x7fffffffbe20;
mov ymm13_1 L0x7fffffffbe28;
mov ymm13_2 L0x7fffffffbe30;
mov ymm13_3 L0x7fffffffbe38;
(* vpxor  %ymm11,%ymm1,%ymm11                      #! PC = 0x555555578561 *)
xor ymm11_0@uint64 ymm1_0 ymm11_0;
xor ymm11_1@uint64 ymm1_1 ymm11_1;
xor ymm11_2@uint64 ymm1_2 ymm11_2;
xor ymm11_3@uint64 ymm1_3 ymm11_3;
(* vmovdqa -0x150(%rbp),%ymm1                      #! EA = L0x7fffffffbe00; Value = 0x84157265ed62c980; PC = 0x555555578566 *)
mov ymm1_0 L0x7fffffffbe00;
mov ymm1_1 L0x7fffffffbe08;
mov ymm1_2 L0x7fffffffbe10;
mov ymm1_3 L0x7fffffffbe18;
(* vpxor  -0x270(%rbp),%ymm1,%ymm1                 #! EA = L0x7fffffffbce0; Value = 0x558a7a103de1a9f7; PC = 0x55555557856e *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbce0;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbce8;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbcf0;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbcf8;
(* vpxor  %ymm1,%ymm15,%ymm15                      #! PC = 0x555555578576 *)
xor ymm15_0@uint64 ymm15_0 ymm1_0;
xor ymm15_1@uint64 ymm15_1 ymm1_1;
xor ymm15_2@uint64 ymm15_2 ymm1_2;
xor ymm15_3@uint64 ymm15_3 ymm1_3;
(* vpandn %ymm3,%ymm0,%ymm1                        #! PC = 0x55555557857a *)
not ymm0_0n@uint64 ymm0_0;
and ymm1_0@uint64 ymm0_0n ymm3_0;
not ymm0_1n@uint64 ymm0_1;
and ymm1_1@uint64 ymm0_1n ymm3_1;
not ymm0_2n@uint64 ymm0_2;
and ymm1_2@uint64 ymm0_2n ymm3_2;
not ymm0_3n@uint64 ymm0_3;
and ymm1_3@uint64 ymm0_3n ymm3_3;
(* vpxor  -0x250(%rbp),%ymm13,%ymm0                #! EA = L0x7fffffffbd00; Value = 0xc20e7c2f247f1fc3; PC = 0x55555557857e *)
xor ymm0_0@uint64 ymm13_0 L0x7fffffffbd00;
xor ymm0_1@uint64 ymm13_1 L0x7fffffffbd08;
xor ymm0_2@uint64 ymm13_2 L0x7fffffffbd10;
xor ymm0_3@uint64 ymm13_3 L0x7fffffffbd18;
(* vpxor  %ymm5,%ymm1,%ymm5                        #! PC = 0x555555578586 *)
xor ymm5_0@uint64 ymm1_0 ymm5_0;
xor ymm5_1@uint64 ymm1_1 ymm5_1;
xor ymm5_2@uint64 ymm1_2 ymm5_2;
xor ymm5_3@uint64 ymm1_3 ymm5_3;
(* vmovdqa -0x210(%rbp),%ymm1                      #! EA = L0x7fffffffbd40; Value = 0x95eb44cdb62a1f93; PC = 0x55555557858a *)
mov ymm1_0 L0x7fffffffbd40;
mov ymm1_1 L0x7fffffffbd48;
mov ymm1_2 L0x7fffffffbd50;
mov ymm1_3 L0x7fffffffbd58;
(* vpxor  %ymm11,%ymm15,%ymm15                     #! PC = 0x555555578592 *)
xor ymm15_0@uint64 ymm15_0 ymm11_0;
xor ymm15_1@uint64 ymm15_1 ymm11_1;
xor ymm15_2@uint64 ymm15_2 ymm11_2;
xor ymm15_3@uint64 ymm15_3 ymm11_3;
(* vpxor  -0x170(%rbp),%ymm1,%ymm1                 #! EA = L0x7fffffffbde0; Value = 0x7ebf8bcaa7384087; PC = 0x555555578597 *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbde0;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbde8;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbdf0;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbdf8;
(* vpxor  %ymm5,%ymm10,%ymm12                      #! PC = 0x55555557859f *)
xor ymm12_0@uint64 ymm10_0 ymm5_0;
xor ymm12_1@uint64 ymm10_1 ymm5_1;
xor ymm12_2@uint64 ymm10_2 ymm5_2;
xor ymm12_3@uint64 ymm10_3 ymm5_3;
(* vmovdqa %ymm5,-0x290(%rbp)                      #! EA = L0x7fffffffbcc0; PC = 0x5555555785a3 *)
mov L0x7fffffffbcc0 ymm5_0;
mov L0x7fffffffbcc8 ymm5_1;
mov L0x7fffffffbcd0 ymm5_2;
mov L0x7fffffffbcd8 ymm5_3;
(* vpsllq $0x1,%ymm14,%ymm5                        #! PC = 0x5555555785ab *)
shl ymm5_0 ymm14_0 0x1@uint64;
shl ymm5_1 ymm14_1 0x1@uint64;
shl ymm5_2 ymm14_2 0x1@uint64;
shl ymm5_3 ymm14_3 0x1@uint64;
(* vpxor  %ymm1,%ymm12,%ymm12                      #! PC = 0x5555555785b1 *)
xor ymm12_0@uint64 ymm12_0 ymm1_0;
xor ymm12_1@uint64 ymm12_1 ymm1_1;
xor ymm12_2@uint64 ymm12_2 ymm1_2;
xor ymm12_3@uint64 ymm12_3 ymm1_3;
(* vpxor  -0x110(%rbp),%ymm4,%ymm1                 #! EA = L0x7fffffffbe40; Value = 0xd68102405f743186; PC = 0x5555555785b5 *)
xor ymm1_0@uint64 ymm4_0 L0x7fffffffbe40;
xor ymm1_1@uint64 ymm4_1 L0x7fffffffbe48;
xor ymm1_2@uint64 ymm4_2 L0x7fffffffbe50;
xor ymm1_3@uint64 ymm4_3 L0x7fffffffbe58;
(* vpxor  -0xb0(%rbp),%ymm12,%ymm12                #! EA = L0x7fffffffbea0; Value = 0x57c528940923d229; PC = 0x5555555785bd *)
xor ymm12_0@uint64 ymm12_0 L0x7fffffffbea0;
xor ymm12_1@uint64 ymm12_1 L0x7fffffffbea8;
xor ymm12_2@uint64 ymm12_2 L0x7fffffffbeb0;
xor ymm12_3@uint64 ymm12_3 L0x7fffffffbeb8;
(* vpsllq $0x1,%ymm15,%ymm3                        #! PC = 0x5555555785c5 *)
shl ymm3_0 ymm15_0 0x1@uint64;
shl ymm3_1 ymm15_1 0x1@uint64;
shl ymm3_2 ymm15_2 0x1@uint64;
shl ymm3_3 ymm15_3 0x1@uint64;
(* vpxor  %ymm0,%ymm1,%ymm1                        #! PC = 0x5555555785cb *)
xor ymm1_0@uint64 ymm1_0 ymm0_0;
xor ymm1_1@uint64 ymm1_1 ymm0_1;
xor ymm1_2@uint64 ymm1_2 ymm0_2;
xor ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3f,%ymm14,%ymm0                       #! PC = 0x5555555785cf *)
shr ymm0_0 ymm14_0 0x3f@uint64;
shr ymm0_1 ymm14_1 0x3f@uint64;
shr ymm0_2 ymm14_2 0x3f@uint64;
shr ymm0_3 ymm14_3 0x3f@uint64;
(* vpxor  -0x50(%rbp),%ymm1,%ymm1                  #! EA = L0x7fffffffbf00; Value = 0xe8a581be707cb952; PC = 0x5555555785d5 *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbf00;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbf08;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbf10;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbf18;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x5555555785da *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpsrlq $0x3f,%ymm15,%ymm0                       #! PC = 0x5555555785de *)
shr ymm0_0 ymm15_0 0x3f@uint64;
shr ymm0_1 ymm15_1 0x3f@uint64;
shr ymm0_2 ymm15_2 0x3f@uint64;
shr ymm0_3 ymm15_3 0x3f@uint64;
(* vpsrlq $0x3f,%ymm12,%ymm13                      #! PC = 0x5555555785e4 *)
shr ymm13_0 ymm12_0 0x3f@uint64;
shr ymm13_1 ymm12_1 0x3f@uint64;
shr ymm13_2 ymm12_2 0x3f@uint64;
shr ymm13_3 ymm12_3 0x3f@uint64;
(* vpor   %ymm0,%ymm3,%ymm3                        #! PC = 0x5555555785ea *)
or ymm3_0@uint64 ymm3_0 ymm0_0;
or ymm3_1@uint64 ymm3_1 ymm0_1;
or ymm3_2@uint64 ymm3_2 ymm0_2;
or ymm3_3@uint64 ymm3_3 ymm0_3;
(* vpxor  %ymm1,%ymm5,%ymm5                        #! PC = 0x5555555785ee *)
xor ymm5_0@uint64 ymm5_0 ymm1_0;
xor ymm5_1@uint64 ymm5_1 ymm1_1;
xor ymm5_2@uint64 ymm5_2 ymm1_2;
xor ymm5_3@uint64 ymm5_3 ymm1_3;
(* vpsllq $0x1,%ymm12,%ymm0                        #! PC = 0x5555555785f2 *)
shl ymm0_0 ymm12_0 0x1@uint64;
shl ymm0_1 ymm12_1 0x1@uint64;
shl ymm0_2 ymm12_2 0x1@uint64;
shl ymm0_3 ymm12_3 0x1@uint64;
(* vpxor  %ymm2,%ymm3,%ymm3                        #! PC = 0x5555555785f8 *)
xor ymm3_0@uint64 ymm3_0 ymm2_0;
xor ymm3_1@uint64 ymm3_1 ymm2_1;
xor ymm3_2@uint64 ymm3_2 ymm2_2;
xor ymm3_3@uint64 ymm3_3 ymm2_3;
(* vpxor  %ymm6,%ymm5,%ymm6                        #! PC = 0x5555555785fc *)
xor ymm6_0@uint64 ymm5_0 ymm6_0;
xor ymm6_1@uint64 ymm5_1 ymm6_1;
xor ymm6_2@uint64 ymm5_2 ymm6_2;
xor ymm6_3@uint64 ymm5_3 ymm6_3;
(* vpor   %ymm13,%ymm0,%ymm0                       #! PC = 0x555555578600 *)
or ymm0_0@uint64 ymm0_0 ymm13_0;
or ymm0_1@uint64 ymm0_1 ymm13_1;
or ymm0_2@uint64 ymm0_2 ymm13_2;
or ymm0_3@uint64 ymm0_3 ymm13_3;
(* vpxor  %ymm8,%ymm3,%ymm8                        #! PC = 0x555555578605 *)
xor ymm8_0@uint64 ymm3_0 ymm8_0;
xor ymm8_1@uint64 ymm3_1 ymm8_1;
xor ymm8_2@uint64 ymm3_2 ymm8_2;
xor ymm8_3@uint64 ymm3_3 ymm8_3;
(* vmovq  %r8,%xmm13                               #! PC = 0x55555557860a *)
mov xmm13_0 r8;
mov xmm13_1 0@uint64;
(* vpxor  %ymm14,%ymm0,%ymm14                      #! PC = 0x55555557860f *)
xor ymm14_0@uint64 ymm0_0 ymm14_0;
xor ymm14_1@uint64 ymm0_1 ymm14_1;
xor ymm14_2@uint64 ymm0_2 ymm14_2;
xor ymm14_3@uint64 ymm0_3 ymm14_3;
(* vpsrlq $0x3f,%ymm1,%ymm0                        #! PC = 0x555555578614 *)
shr ymm0_0 ymm1_0 0x3f@uint64;
shr ymm0_1 ymm1_1 0x3f@uint64;
shr ymm0_2 ymm1_2 0x3f@uint64;
shr ymm0_3 ymm1_3 0x3f@uint64;
(* vpxor  %ymm7,%ymm3,%ymm7                        #! PC = 0x555555578619 *)
xor ymm7_0@uint64 ymm3_0 ymm7_0;
xor ymm7_1@uint64 ymm3_1 ymm7_1;
xor ymm7_2@uint64 ymm3_2 ymm7_2;
xor ymm7_3@uint64 ymm3_3 ymm7_3;
(* vpsllq $0x1,%ymm1,%ymm1                         #! PC = 0x55555557861d *)
shl ymm1_0 ymm1_0 0x1@uint64;
shl ymm1_1 ymm1_1 0x1@uint64;
shl ymm1_2 ymm1_2 0x1@uint64;
shl ymm1_3 ymm1_3 0x1@uint64;
(* vpxor  %ymm9,%ymm14,%ymm9                       #! PC = 0x555555578622 *)
xor ymm9_0@uint64 ymm14_0 ymm9_0;
xor ymm9_1@uint64 ymm14_1 ymm9_1;
xor ymm9_2@uint64 ymm14_2 ymm9_2;
xor ymm9_3@uint64 ymm14_3 ymm9_3;
(* vpxor  %ymm11,%ymm14,%ymm11                     #! PC = 0x555555578627 *)
xor ymm11_0@uint64 ymm14_0 ymm11_0;
xor ymm11_1@uint64 ymm14_1 ymm11_1;
xor ymm11_2@uint64 ymm14_2 ymm11_2;
xor ymm11_3@uint64 ymm14_3 ymm11_3;
(* vpor   %ymm0,%ymm1,%ymm1                        #! PC = 0x55555557862c *)
or ymm1_0@uint64 ymm1_0 ymm0_0;
or ymm1_1@uint64 ymm1_1 ymm0_1;
or ymm1_2@uint64 ymm1_2 ymm0_2;
or ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3f,%ymm2,%ymm0                        #! PC = 0x555555578630 *)
shr ymm0_0 ymm2_0 0x3f@uint64;
shr ymm0_1 ymm2_1 0x3f@uint64;
shr ymm0_2 ymm2_2 0x3f@uint64;
shr ymm0_3 ymm2_3 0x3f@uint64;
(* vpsllq $0x1,%ymm2,%ymm2                         #! PC = 0x555555578635 *)
shl ymm2_0 ymm2_0 0x1@uint64;
shl ymm2_1 ymm2_1 0x1@uint64;
shl ymm2_2 ymm2_2 0x1@uint64;
shl ymm2_3 ymm2_3 0x1@uint64;
(* vpxor  %ymm15,%ymm1,%ymm15                      #! PC = 0x55555557863a *)
xor ymm15_0@uint64 ymm1_0 ymm15_0;
xor ymm15_1@uint64 ymm1_1 ymm15_1;
xor ymm15_2@uint64 ymm1_2 ymm15_2;
xor ymm15_3@uint64 ymm1_3 ymm15_3;
(* vpxor  -0x90(%rbp),%ymm5,%ymm1                  #! EA = L0x7fffffffbec0; Value = 0x0838573a4deb6243; PC = 0x55555557863f *)
xor ymm1_0@uint64 ymm5_0 L0x7fffffffbec0;
xor ymm1_1@uint64 ymm5_1 L0x7fffffffbec8;
xor ymm1_2@uint64 ymm5_2 L0x7fffffffbed0;
xor ymm1_3@uint64 ymm5_3 L0x7fffffffbed8;
(* vpor   %ymm0,%ymm2,%ymm2                        #! PC = 0x555555578647 *)
or ymm2_0@uint64 ymm2_0 ymm0_0;
or ymm2_1@uint64 ymm2_1 ymm0_1;
or ymm2_2@uint64 ymm2_2 ymm0_2;
or ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x14,%ymm8,%ymm0                        #! PC = 0x55555557864b *)
shr ymm0_0 ymm8_0 0x14@uint64;
shr ymm0_1 ymm8_1 0x14@uint64;
shr ymm0_2 ymm8_2 0x14@uint64;
shr ymm0_3 ymm8_3 0x14@uint64;
(* vpxor  %ymm10,%ymm15,%ymm10                     #! PC = 0x555555578651 *)
xor ymm10_0@uint64 ymm15_0 ymm10_0;
xor ymm10_1@uint64 ymm15_1 ymm10_1;
xor ymm10_2@uint64 ymm15_2 ymm10_2;
xor ymm10_3@uint64 ymm15_3 ymm10_3;
(* vpsllq $0x2c,%ymm8,%ymm8                        #! PC = 0x555555578656 *)
shl ymm8_0 ymm8_0 0x2c@uint64;
shl ymm8_1 ymm8_1 0x2c@uint64;
shl ymm8_2 ymm8_2 0x2c@uint64;
shl ymm8_3 ymm8_3 0x2c@uint64;
(* vpxor  %ymm12,%ymm2,%ymm12                      #! PC = 0x55555557865c *)
xor ymm12_0@uint64 ymm2_0 ymm12_0;
xor ymm12_1@uint64 ymm2_1 ymm12_1;
xor ymm12_2@uint64 ymm2_2 ymm12_2;
xor ymm12_3@uint64 ymm2_3 ymm12_3;
(* vpor   %ymm0,%ymm8,%ymm8                        #! PC = 0x555555578661 *)
or ymm8_0@uint64 ymm8_0 ymm0_0;
or ymm8_1@uint64 ymm8_1 ymm0_1;
or ymm8_2@uint64 ymm8_2 ymm0_2;
or ymm8_3@uint64 ymm8_3 ymm0_3;
(* vpsrlq $0x15,%ymm9,%ymm0                        #! PC = 0x555555578665 *)
shr ymm0_0 ymm9_0 0x15@uint64;
shr ymm0_1 ymm9_1 0x15@uint64;
shr ymm0_2 ymm9_2 0x15@uint64;
shr ymm0_3 ymm9_3 0x15@uint64;
(* vpxor  %ymm4,%ymm12,%ymm4                       #! PC = 0x55555557866b *)
xor ymm4_0@uint64 ymm12_0 ymm4_0;
xor ymm4_1@uint64 ymm12_1 ymm4_1;
xor ymm4_2@uint64 ymm12_2 ymm4_2;
xor ymm4_3@uint64 ymm12_3 ymm4_3;
(* vpsllq $0x2b,%ymm9,%ymm9                        #! PC = 0x55555557866f *)
shl ymm9_0 ymm9_0 0x2b@uint64;
shl ymm9_1 ymm9_1 0x2b@uint64;
shl ymm9_2 ymm9_2 0x2b@uint64;
shl ymm9_3 ymm9_3 0x2b@uint64;
(* vpor   %ymm0,%ymm9,%ymm9                        #! PC = 0x555555578675 *)
or ymm9_0@uint64 ymm9_0 ymm0_0;
or ymm9_1@uint64 ymm9_1 ymm0_1;
or ymm9_2@uint64 ymm9_2 ymm0_2;
or ymm9_3@uint64 ymm9_3 ymm0_3;
(* vpbroadcastq %xmm13,%ymm0                       #! PC = 0x555555578679 *)
mov ymm0_0 xmm13_0;
mov ymm0_1 xmm13_0;
mov ymm0_2 xmm13_0;
mov ymm0_3 xmm13_0;
(* vpandn %ymm9,%ymm8,%ymm2                        #! PC = 0x55555557867e *)
not ymm8_0n@uint64 ymm8_0;
and ymm2_0@uint64 ymm8_0n ymm9_0;
not ymm8_1n@uint64 ymm8_1;
and ymm2_1@uint64 ymm8_1n ymm9_1;
not ymm8_2n@uint64 ymm8_2;
and ymm2_2@uint64 ymm8_2n ymm9_2;
not ymm8_3n@uint64 ymm8_3;
and ymm2_3@uint64 ymm8_3n ymm9_3;
(* vpxor  %ymm2,%ymm0,%ymm0                        #! PC = 0x555555578683 *)
xor ymm0_0@uint64 ymm0_0 ymm2_0;
xor ymm0_1@uint64 ymm0_1 ymm2_1;
xor ymm0_2@uint64 ymm0_2 ymm2_2;
xor ymm0_3@uint64 ymm0_3 ymm2_3;
(* vpxor  %ymm1,%ymm0,%ymm2                        #! PC = 0x555555578687 *)
xor ymm2_0@uint64 ymm0_0 ymm1_0;
xor ymm2_1@uint64 ymm0_1 ymm1_1;
xor ymm2_2@uint64 ymm0_2 ymm1_2;
xor ymm2_3@uint64 ymm0_3 ymm1_3;
(* vpsrlq $0x2b,%ymm10,%ymm0                       #! PC = 0x55555557868b *)
shr ymm0_0 ymm10_0 0x2b@uint64;
shr ymm0_1 ymm10_1 0x2b@uint64;
shr ymm0_2 ymm10_2 0x2b@uint64;
shr ymm0_3 ymm10_3 0x2b@uint64;
(* vpsllq $0x15,%ymm10,%ymm10                      #! PC = 0x555555578691 *)
shl ymm10_0 ymm10_0 0x15@uint64;
shl ymm10_1 ymm10_1 0x15@uint64;
shl ymm10_2 ymm10_2 0x15@uint64;
shl ymm10_3 ymm10_3 0x15@uint64;
(* vmovdqa %ymm2,-0x90(%rbp)                       #! EA = L0x7fffffffbec0; PC = 0x555555578697 *)
mov L0x7fffffffbec0 ymm2_0;
mov L0x7fffffffbec8 ymm2_1;
mov L0x7fffffffbed0 ymm2_2;
mov L0x7fffffffbed8 ymm2_3;
(* vpor   %ymm0,%ymm10,%ymm10                      #! PC = 0x55555557869f *)
or ymm10_0@uint64 ymm10_0 ymm0_0;
or ymm10_1@uint64 ymm10_1 ymm0_1;
or ymm10_2@uint64 ymm10_2 ymm0_2;
or ymm10_3@uint64 ymm10_3 ymm0_3;
(* vpandn %ymm10,%ymm9,%ymm0                       #! PC = 0x5555555786a3 *)
not ymm9_0n@uint64 ymm9_0;
and ymm0_0@uint64 ymm9_0n ymm10_0;
not ymm9_1n@uint64 ymm9_1;
and ymm0_1@uint64 ymm9_1n ymm10_1;
not ymm9_2n@uint64 ymm9_2;
and ymm0_2@uint64 ymm9_2n ymm10_2;
not ymm9_3n@uint64 ymm9_3;
and ymm0_3@uint64 ymm9_3n ymm10_3;
(* vpxor  %ymm8,%ymm0,%ymm13                       #! PC = 0x5555555786a8 *)
xor ymm13_0@uint64 ymm0_0 ymm8_0;
xor ymm13_1@uint64 ymm0_1 ymm8_1;
xor ymm13_2@uint64 ymm0_2 ymm8_2;
xor ymm13_3@uint64 ymm0_3 ymm8_3;
(* vpsrlq $0x32,%ymm4,%ymm0                        #! PC = 0x5555555786ad *)
shr ymm0_0 ymm4_0 0x32@uint64;
shr ymm0_1 ymm4_1 0x32@uint64;
shr ymm0_2 ymm4_2 0x32@uint64;
shr ymm0_3 ymm4_3 0x32@uint64;
(* vpandn %ymm8,%ymm1,%ymm8                        #! PC = 0x5555555786b2 *)
not ymm1_0n@uint64 ymm1_0;
and ymm8_0@uint64 ymm1_0n ymm8_0;
not ymm1_1n@uint64 ymm1_1;
and ymm8_1@uint64 ymm1_1n ymm8_1;
not ymm1_2n@uint64 ymm1_2;
and ymm8_2@uint64 ymm1_2n ymm8_2;
not ymm1_3n@uint64 ymm1_3;
and ymm8_3@uint64 ymm1_3n ymm8_3;
(* vpsllq $0xe,%ymm4,%ymm4                         #! PC = 0x5555555786b7 *)
shl ymm4_0 ymm4_0 0xe@uint64;
shl ymm4_1 ymm4_1 0xe@uint64;
shl ymm4_2 ymm4_2 0xe@uint64;
shl ymm4_3 ymm4_3 0xe@uint64;
(* vmovdqa %ymm13,-0x1d0(%rbp)                     #! EA = L0x7fffffffbd80; PC = 0x5555555786bc *)
mov L0x7fffffffbd80 ymm13_0;
mov L0x7fffffffbd88 ymm13_1;
mov L0x7fffffffbd90 ymm13_2;
mov L0x7fffffffbd98 ymm13_3;
(* vpor   %ymm0,%ymm4,%ymm4                        #! PC = 0x5555555786c4 *)
or ymm4_0@uint64 ymm4_0 ymm0_0;
or ymm4_1@uint64 ymm4_1 ymm0_1;
or ymm4_2@uint64 ymm4_2 ymm0_2;
or ymm4_3@uint64 ymm4_3 ymm0_3;
(* vpandn %ymm4,%ymm10,%ymm0                       #! PC = 0x5555555786c8 *)
not ymm10_0n@uint64 ymm10_0;
and ymm0_0@uint64 ymm10_0n ymm4_0;
not ymm10_1n@uint64 ymm10_1;
and ymm0_1@uint64 ymm10_1n ymm4_1;
not ymm10_2n@uint64 ymm10_2;
and ymm0_2@uint64 ymm10_2n ymm4_2;
not ymm10_3n@uint64 ymm10_3;
and ymm0_3@uint64 ymm10_3n ymm4_3;
(* vpxor  %ymm4,%ymm8,%ymm8                        #! PC = 0x5555555786cc *)
xor ymm8_0@uint64 ymm8_0 ymm4_0;
xor ymm8_1@uint64 ymm8_1 ymm4_1;
xor ymm8_2@uint64 ymm8_2 ymm4_2;
xor ymm8_3@uint64 ymm8_3 ymm4_3;
(* vpxor  %ymm9,%ymm0,%ymm9                        #! PC = 0x5555555786d0 *)
xor ymm9_0@uint64 ymm0_0 ymm9_0;
xor ymm9_1@uint64 ymm0_1 ymm9_1;
xor ymm9_2@uint64 ymm0_2 ymm9_2;
xor ymm9_3@uint64 ymm0_3 ymm9_3;
(* vpandn %ymm1,%ymm4,%ymm0                        #! PC = 0x5555555786d5 *)
not ymm4_0n@uint64 ymm4_0;
and ymm0_0@uint64 ymm4_0n ymm1_0;
not ymm4_1n@uint64 ymm4_1;
and ymm0_1@uint64 ymm4_1n ymm1_1;
not ymm4_2n@uint64 ymm4_2;
and ymm0_2@uint64 ymm4_2n ymm1_2;
not ymm4_3n@uint64 ymm4_3;
and ymm0_3@uint64 ymm4_3n ymm1_3;
(* vmovdqa %ymm8,-0x2b0(%rbp)                      #! EA = L0x7fffffffbca0; PC = 0x5555555786d9 *)
mov L0x7fffffffbca0 ymm8_0;
mov L0x7fffffffbca8 ymm8_1;
mov L0x7fffffffbcb0 ymm8_2;
mov L0x7fffffffbcb8 ymm8_3;
(* vpxor  %ymm10,%ymm0,%ymm10                      #! PC = 0x5555555786e1 *)
xor ymm10_0@uint64 ymm0_0 ymm10_0;
xor ymm10_1@uint64 ymm0_1 ymm10_1;
xor ymm10_2@uint64 ymm0_2 ymm10_2;
xor ymm10_3@uint64 ymm0_3 ymm10_3;
(* vmovdqa %ymm9,-0x2d0(%rbp)                      #! EA = L0x7fffffffbc80; PC = 0x5555555786e6 *)
mov L0x7fffffffbc80 ymm9_0;
mov L0x7fffffffbc88 ymm9_1;
mov L0x7fffffffbc90 ymm9_2;
mov L0x7fffffffbc98 ymm9_3;
(* vmovdqa %ymm10,-0xf0(%rbp)                      #! EA = L0x7fffffffbe60; PC = 0x5555555786ee *)
mov L0x7fffffffbe60 ymm10_0;
mov L0x7fffffffbe68 ymm10_1;
mov L0x7fffffffbe70 ymm10_2;
mov L0x7fffffffbe78 ymm10_3;
(* vpxor  -0x170(%rbp),%ymm15,%ymm10               #! EA = L0x7fffffffbde0; Value = 0x7ebf8bcaa7384087; PC = 0x5555555786f6 *)
xor ymm10_0@uint64 ymm15_0 L0x7fffffffbde0;
xor ymm10_1@uint64 ymm15_1 L0x7fffffffbde8;
xor ymm10_2@uint64 ymm15_2 L0x7fffffffbdf0;
xor ymm10_3@uint64 ymm15_3 L0x7fffffffbdf8;
(* vpsrlq $0x24,%ymm10,%ymm0                       #! PC = 0x5555555786fe *)
shr ymm0_0 ymm10_0 0x24@uint64;
shr ymm0_1 ymm10_1 0x24@uint64;
shr ymm0_2 ymm10_2 0x24@uint64;
shr ymm0_3 ymm10_3 0x24@uint64;
(* vpsllq $0x1c,%ymm10,%ymm1                       #! PC = 0x555555578704 *)
shl ymm1_0 ymm10_0 0x1c@uint64;
shl ymm1_1 ymm10_1 0x1c@uint64;
shl ymm1_2 ymm10_2 0x1c@uint64;
shl ymm1_3 ymm10_3 0x1c@uint64;
(* vpor   %ymm0,%ymm1,%ymm1                        #! PC = 0x55555557870a *)
or ymm1_0@uint64 ymm1_0 ymm0_0;
or ymm1_1@uint64 ymm1_1 ymm0_1;
or ymm1_2@uint64 ymm1_2 ymm0_2;
or ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpxor  -0x130(%rbp),%ymm12,%ymm0                #! EA = L0x7fffffffbe20; Value = 0x10401cdb131f356a; PC = 0x55555557870e *)
xor ymm0_0@uint64 ymm12_0 L0x7fffffffbe20;
xor ymm0_1@uint64 ymm12_1 L0x7fffffffbe28;
xor ymm0_2@uint64 ymm12_2 L0x7fffffffbe30;
xor ymm0_3@uint64 ymm12_3 L0x7fffffffbe38;
(* vpsrlq $0x2c,%ymm0,%ymm2                        #! PC = 0x555555578716 *)
shr ymm2_0 ymm0_0 0x2c@uint64;
shr ymm2_1 ymm0_1 0x2c@uint64;
shr ymm2_2 ymm0_2 0x2c@uint64;
shr ymm2_3 ymm0_3 0x2c@uint64;
(* vpsllq $0x14,%ymm0,%ymm0                        #! PC = 0x55555557871b *)
shl ymm0_0 ymm0_0 0x14@uint64;
shl ymm0_1 ymm0_1 0x14@uint64;
shl ymm0_2 ymm0_2 0x14@uint64;
shl ymm0_3 ymm0_3 0x14@uint64;
(* vpor   %ymm2,%ymm0,%ymm0                        #! PC = 0x555555578720 *)
or ymm0_0@uint64 ymm0_0 ymm2_0;
or ymm0_1@uint64 ymm0_1 ymm2_1;
or ymm0_2@uint64 ymm0_2 ymm2_2;
or ymm0_3@uint64 ymm0_3 ymm2_3;
(* vpxor  -0xd0(%rbp),%ymm5,%ymm2                  #! EA = L0x7fffffffbe80; Value = 0x43149ae4f34cae4e; PC = 0x555555578724 *)
xor ymm2_0@uint64 ymm5_0 L0x7fffffffbe80;
xor ymm2_1@uint64 ymm5_1 L0x7fffffffbe88;
xor ymm2_2@uint64 ymm5_2 L0x7fffffffbe90;
xor ymm2_3@uint64 ymm5_3 L0x7fffffffbe98;
(* vpsrlq $0x3d,%ymm2,%ymm4                        #! PC = 0x55555557872c *)
shr ymm4_0 ymm2_0 0x3d@uint64;
shr ymm4_1 ymm2_1 0x3d@uint64;
shr ymm4_2 ymm2_2 0x3d@uint64;
shr ymm4_3 ymm2_3 0x3d@uint64;
(* vpsllq $0x3,%ymm2,%ymm2                         #! PC = 0x555555578731 *)
shl ymm2_0 ymm2_0 0x3@uint64;
shl ymm2_1 ymm2_1 0x3@uint64;
shl ymm2_2 ymm2_2 0x3@uint64;
shl ymm2_3 ymm2_3 0x3@uint64;
(* vpor   %ymm4,%ymm2,%ymm2                        #! PC = 0x555555578736 *)
or ymm2_0@uint64 ymm2_0 ymm4_0;
or ymm2_1@uint64 ymm2_1 ymm4_1;
or ymm2_2@uint64 ymm2_2 ymm4_2;
or ymm2_3@uint64 ymm2_3 ymm4_3;
(* vpandn %ymm2,%ymm0,%ymm4                        #! PC = 0x55555557873a *)
not ymm0_0n@uint64 ymm0_0;
and ymm4_0@uint64 ymm0_0n ymm2_0;
not ymm0_1n@uint64 ymm0_1;
and ymm4_1@uint64 ymm0_1n ymm2_1;
not ymm0_2n@uint64 ymm0_2;
and ymm4_2@uint64 ymm0_2n ymm2_2;
not ymm0_3n@uint64 ymm0_3;
and ymm4_3@uint64 ymm0_3n ymm2_3;
(* vpxor  %ymm1,%ymm4,%ymm13                       #! PC = 0x55555557873e *)
xor ymm13_0@uint64 ymm4_0 ymm1_0;
xor ymm13_1@uint64 ymm4_1 ymm1_1;
xor ymm13_2@uint64 ymm4_2 ymm1_2;
xor ymm13_3@uint64 ymm4_3 ymm1_3;
(* vpsrlq $0x13,%ymm7,%ymm4                        #! PC = 0x555555578742 *)
shr ymm4_0 ymm7_0 0x13@uint64;
shr ymm4_1 ymm7_1 0x13@uint64;
shr ymm4_2 ymm7_2 0x13@uint64;
shr ymm4_3 ymm7_3 0x13@uint64;
(* vpsllq $0x2d,%ymm7,%ymm7                        #! PC = 0x555555578747 *)
shl ymm7_0 ymm7_0 0x2d@uint64;
shl ymm7_1 ymm7_1 0x2d@uint64;
shl ymm7_2 ymm7_2 0x2d@uint64;
shl ymm7_3 ymm7_3 0x2d@uint64;
(* vmovdqa %ymm13,-0x1f0(%rbp)                     #! EA = L0x7fffffffbd60; PC = 0x55555557874c *)
mov L0x7fffffffbd60 ymm13_0;
mov L0x7fffffffbd68 ymm13_1;
mov L0x7fffffffbd70 ymm13_2;
mov L0x7fffffffbd78 ymm13_3;
(* vpor   %ymm4,%ymm7,%ymm13                       #! PC = 0x555555578754 *)
or ymm13_0@uint64 ymm7_0 ymm4_0;
or ymm13_1@uint64 ymm7_1 ymm4_1;
or ymm13_2@uint64 ymm7_2 ymm4_2;
or ymm13_3@uint64 ymm7_3 ymm4_3;
(* vpsrlq $0x3,%ymm11,%ymm4                        #! PC = 0x555555578758 *)
shr ymm4_0 ymm11_0 0x3@uint64;
shr ymm4_1 ymm11_1 0x3@uint64;
shr ymm4_2 ymm11_2 0x3@uint64;
shr ymm4_3 ymm11_3 0x3@uint64;
(* vpsllq $0x3d,%ymm11,%ymm11                      #! PC = 0x55555557875e *)
shl ymm11_0 ymm11_0 0x3d@uint64;
shl ymm11_1 ymm11_1 0x3d@uint64;
shl ymm11_2 ymm11_2 0x3d@uint64;
shl ymm11_3 ymm11_3 0x3d@uint64;
(* vpandn %ymm13,%ymm2,%ymm8                       #! PC = 0x555555578764 *)
not ymm2_0n@uint64 ymm2_0;
and ymm8_0@uint64 ymm2_0n ymm13_0;
not ymm2_1n@uint64 ymm2_1;
and ymm8_1@uint64 ymm2_1n ymm13_1;
not ymm2_2n@uint64 ymm2_2;
and ymm8_2@uint64 ymm2_2n ymm13_2;
not ymm2_3n@uint64 ymm2_3;
and ymm8_3@uint64 ymm2_3n ymm13_3;
(* vpor   %ymm4,%ymm11,%ymm11                      #! PC = 0x555555578769 *)
or ymm11_0@uint64 ymm11_0 ymm4_0;
or ymm11_1@uint64 ymm11_1 ymm4_1;
or ymm11_2@uint64 ymm11_2 ymm4_2;
or ymm11_3@uint64 ymm11_3 ymm4_3;
(* vpxor  %ymm0,%ymm8,%ymm8                        #! PC = 0x55555557876d *)
xor ymm8_0@uint64 ymm8_0 ymm0_0;
xor ymm8_1@uint64 ymm8_1 ymm0_1;
xor ymm8_2@uint64 ymm8_2 ymm0_2;
xor ymm8_3@uint64 ymm8_3 ymm0_3;
(* vpandn %ymm11,%ymm13,%ymm4                      #! PC = 0x555555578771 *)
not ymm13_0n@uint64 ymm13_0;
and ymm4_0@uint64 ymm13_0n ymm11_0;
not ymm13_1n@uint64 ymm13_1;
and ymm4_1@uint64 ymm13_1n ymm11_1;
not ymm13_2n@uint64 ymm13_2;
and ymm4_2@uint64 ymm13_2n ymm11_2;
not ymm13_3n@uint64 ymm13_3;
and ymm4_3@uint64 ymm13_3n ymm11_3;
(* vpxor  %ymm2,%ymm4,%ymm10                       #! PC = 0x555555578776 *)
xor ymm10_0@uint64 ymm4_0 ymm2_0;
xor ymm10_1@uint64 ymm4_1 ymm2_1;
xor ymm10_2@uint64 ymm4_2 ymm2_2;
xor ymm10_3@uint64 ymm4_3 ymm2_3;
(* vpandn %ymm1,%ymm11,%ymm2                       #! PC = 0x55555557877a *)
not ymm11_0n@uint64 ymm11_0;
and ymm2_0@uint64 ymm11_0n ymm1_0;
not ymm11_1n@uint64 ymm11_1;
and ymm2_1@uint64 ymm11_1n ymm1_1;
not ymm11_2n@uint64 ymm11_2;
and ymm2_2@uint64 ymm11_2n ymm1_2;
not ymm11_3n@uint64 ymm11_3;
and ymm2_3@uint64 ymm11_3n ymm1_3;
(* vpandn %ymm0,%ymm1,%ymm1                        #! PC = 0x55555557877e *)
not ymm1_0n@uint64 ymm1_0;
and ymm1_0@uint64 ymm1_0n ymm0_0;
not ymm1_1n@uint64 ymm1_1;
and ymm1_1@uint64 ymm1_1n ymm0_1;
not ymm1_2n@uint64 ymm1_2;
and ymm1_2@uint64 ymm1_2n ymm0_2;
not ymm1_3n@uint64 ymm1_3;
and ymm1_3@uint64 ymm1_3n ymm0_3;
(* vpxor  %ymm13,%ymm2,%ymm9                       #! PC = 0x555555578782 *)
xor ymm9_0@uint64 ymm2_0 ymm13_0;
xor ymm9_1@uint64 ymm2_1 ymm13_1;
xor ymm9_2@uint64 ymm2_2 ymm13_2;
xor ymm9_3@uint64 ymm2_3 ymm13_3;
(* vpxor  %ymm11,%ymm1,%ymm11                      #! PC = 0x555555578787 *)
xor ymm11_0@uint64 ymm1_0 ymm11_0;
xor ymm11_1@uint64 ymm1_1 ymm11_1;
xor ymm11_2@uint64 ymm1_2 ymm11_2;
xor ymm11_3@uint64 ymm1_3 ymm11_3;
(* vpxor  -0x190(%rbp),%ymm3,%ymm1                 #! EA = L0x7fffffffbdc0; Value = 0xcf57a0c778692494; PC = 0x55555557878c *)
xor ymm1_0@uint64 ymm3_0 L0x7fffffffbdc0;
xor ymm1_1@uint64 ymm3_1 L0x7fffffffbdc8;
xor ymm1_2@uint64 ymm3_2 L0x7fffffffbdd0;
xor ymm1_3@uint64 ymm3_3 L0x7fffffffbdd8;
(* vmovdqa %ymm10,-0x170(%rbp)                     #! EA = L0x7fffffffbde0; PC = 0x555555578794 *)
mov L0x7fffffffbde0 ymm10_0;
mov L0x7fffffffbde8 ymm10_1;
mov L0x7fffffffbdf0 ymm10_2;
mov L0x7fffffffbdf8 ymm10_3;
(* vmovdqa %ymm9,-0x1b0(%rbp)                      #! EA = L0x7fffffffbda0; PC = 0x55555557879c *)
mov L0x7fffffffbda0 ymm9_0;
mov L0x7fffffffbda8 ymm9_1;
mov L0x7fffffffbdb0 ymm9_2;
mov L0x7fffffffbdb8 ymm9_3;
(* vmovdqa %ymm11,-0xd0(%rbp)                      #! EA = L0x7fffffffbe80; PC = 0x5555555787a4 *)
mov L0x7fffffffbe80 ymm11_0;
mov L0x7fffffffbe88 ymm11_1;
mov L0x7fffffffbe90 ymm11_2;
mov L0x7fffffffbe98 ymm11_3;
(* vpxor  -0x150(%rbp),%ymm14,%ymm2                #! EA = L0x7fffffffbe00; Value = 0x84157265ed62c980; PC = 0x5555555787ac *)
xor ymm2_0@uint64 ymm14_0 L0x7fffffffbe00;
xor ymm2_1@uint64 ymm14_1 L0x7fffffffbe08;
xor ymm2_2@uint64 ymm14_2 L0x7fffffffbe10;
xor ymm2_3@uint64 ymm14_3 L0x7fffffffbe18;
(* vpxor  -0xb0(%rbp),%ymm15,%ymm9                 #! EA = L0x7fffffffbea0; Value = 0x57c528940923d229; PC = 0x5555555787b4 *)
xor ymm9_0@uint64 ymm15_0 L0x7fffffffbea0;
xor ymm9_1@uint64 ymm15_1 L0x7fffffffbea8;
xor ymm9_2@uint64 ymm15_2 L0x7fffffffbeb0;
xor ymm9_3@uint64 ymm15_3 L0x7fffffffbeb8;
(* vpsrlq $0x3f,%ymm1,%ymm0                        #! PC = 0x5555555787bc *)
shr ymm0_0 ymm1_0 0x3f@uint64;
shr ymm0_1 ymm1_1 0x3f@uint64;
shr ymm0_2 ymm1_2 0x3f@uint64;
shr ymm0_3 ymm1_3 0x3f@uint64;
(* vpsllq $0x1,%ymm1,%ymm1                         #! PC = 0x5555555787c1 *)
shl ymm1_0 ymm1_0 0x1@uint64;
shl ymm1_1 ymm1_1 0x1@uint64;
shl ymm1_2 ymm1_2 0x1@uint64;
shl ymm1_3 ymm1_3 0x1@uint64;
(* vpor   %ymm0,%ymm1,%ymm1                        #! PC = 0x5555555787c6 *)
or ymm1_0@uint64 ymm1_0 ymm0_0;
or ymm1_1@uint64 ymm1_1 ymm0_1;
or ymm1_2@uint64 ymm1_2 ymm0_2;
or ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x27,%ymm9,%ymm4                        #! PC = 0x5555555787ca *)
shr ymm4_0 ymm9_0 0x27@uint64;
shr ymm4_1 ymm9_1 0x27@uint64;
shr ymm4_2 ymm9_2 0x27@uint64;
shr ymm4_3 ymm9_3 0x27@uint64;
(* vpsrlq $0x3a,%ymm2,%ymm0                        #! PC = 0x5555555787d0 *)
shr ymm0_0 ymm2_0 0x3a@uint64;
shr ymm0_1 ymm2_1 0x3a@uint64;
shr ymm0_2 ymm2_2 0x3a@uint64;
shr ymm0_3 ymm2_3 0x3a@uint64;
(* vpsllq $0x19,%ymm9,%ymm9                        #! PC = 0x5555555787d5 *)
shl ymm9_0 ymm9_0 0x19@uint64;
shl ymm9_1 ymm9_1 0x19@uint64;
shl ymm9_2 ymm9_2 0x19@uint64;
shl ymm9_3 ymm9_3 0x19@uint64;
(* vpsllq $0x6,%ymm2,%ymm2                         #! PC = 0x5555555787db *)
shl ymm2_0 ymm2_0 0x6@uint64;
shl ymm2_1 ymm2_1 0x6@uint64;
shl ymm2_2 ymm2_2 0x6@uint64;
shl ymm2_3 ymm2_3 0x6@uint64;
(* vpor   %ymm0,%ymm2,%ymm2                        #! PC = 0x5555555787e0 *)
or ymm2_0@uint64 ymm2_0 ymm0_0;
or ymm2_1@uint64 ymm2_1 ymm0_1;
or ymm2_2@uint64 ymm2_2 ymm0_2;
or ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpor   %ymm4,%ymm9,%ymm0                        #! PC = 0x5555555787e4 *)
or ymm0_0@uint64 ymm9_0 ymm4_0;
or ymm0_1@uint64 ymm9_1 ymm4_1;
or ymm0_2@uint64 ymm9_2 ymm4_2;
or ymm0_3@uint64 ymm9_3 ymm4_3;
(* vpandn %ymm0,%ymm2,%ymm4                        #! PC = 0x5555555787e8 *)
not ymm2_0n@uint64 ymm2_0;
and ymm4_0@uint64 ymm2_0n ymm0_0;
not ymm2_1n@uint64 ymm2_1;
and ymm4_1@uint64 ymm2_1n ymm0_1;
not ymm2_2n@uint64 ymm2_2;
and ymm4_2@uint64 ymm2_2n ymm0_2;
not ymm2_3n@uint64 ymm2_3;
and ymm4_3@uint64 ymm2_3n ymm0_3;
(* vpxor  %ymm1,%ymm4,%ymm7                        #! PC = 0x5555555787ec *)
xor ymm7_0@uint64 ymm4_0 ymm1_0;
xor ymm7_1@uint64 ymm4_1 ymm1_1;
xor ymm7_2@uint64 ymm4_2 ymm1_2;
xor ymm7_3@uint64 ymm4_3 ymm1_3;
(* vpxor  -0x50(%rbp),%ymm12,%ymm4                 #! EA = L0x7fffffffbf00; Value = 0xe8a581be707cb952; PC = 0x5555555787f0 *)
xor ymm4_0@uint64 ymm12_0 L0x7fffffffbf00;
xor ymm4_1@uint64 ymm12_1 L0x7fffffffbf08;
xor ymm4_2@uint64 ymm12_2 L0x7fffffffbf10;
xor ymm4_3@uint64 ymm12_3 L0x7fffffffbf18;
(* vmovdqa %ymm7,%ymm13                            #! PC = 0x5555555787f5 *)
mov ymm13_0 ymm7_0;
mov ymm13_1 ymm7_1;
mov ymm13_2 ymm7_2;
mov ymm13_3 ymm7_3;
(* vpshufb 0x556fe(%rip),%ymm4,%ymm4        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x5555555787f9 *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm4_0, ymm4_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm4_2, ymm4_3, tmp_2, tmp_3);
mov ymm4_0 tmp_0;
mov ymm4_1 tmp_1;
mov ymm4_2 tmp_2;
mov ymm4_3 tmp_3;
(* vpandn %ymm4,%ymm0,%ymm7                        #! PC = 0x555555578802 *)
not ymm0_0n@uint64 ymm0_0;
and ymm7_0@uint64 ymm0_0n ymm4_0;
not ymm0_1n@uint64 ymm0_1;
and ymm7_1@uint64 ymm0_1n ymm4_1;
not ymm0_2n@uint64 ymm0_2;
and ymm7_2@uint64 ymm0_2n ymm4_2;
not ymm0_3n@uint64 ymm0_3;
and ymm7_3@uint64 ymm0_3n ymm4_3;
(* vpxor  %ymm2,%ymm7,%ymm7                        #! PC = 0x555555578806 *)
xor ymm7_0@uint64 ymm7_0 ymm2_0;
xor ymm7_1@uint64 ymm7_1 ymm2_1;
xor ymm7_2@uint64 ymm7_2 ymm2_2;
xor ymm7_3@uint64 ymm7_3 ymm2_3;
(* vmovdqa %ymm7,-0x190(%rbp)                      #! EA = L0x7fffffffbdc0; PC = 0x55555557880a *)
mov L0x7fffffffbdc0 ymm7_0;
mov L0x7fffffffbdc8 ymm7_1;
mov L0x7fffffffbdd0 ymm7_2;
mov L0x7fffffffbdd8 ymm7_3;
(* vpsrlq $0x2e,%ymm6,%ymm7                        #! PC = 0x555555578812 *)
shr ymm7_0 ymm6_0 0x2e@uint64;
shr ymm7_1 ymm6_1 0x2e@uint64;
shr ymm7_2 ymm6_2 0x2e@uint64;
shr ymm7_3 ymm6_3 0x2e@uint64;
(* vpsllq $0x12,%ymm6,%ymm6                        #! PC = 0x555555578817 *)
shl ymm6_0 ymm6_0 0x12@uint64;
shl ymm6_1 ymm6_1 0x12@uint64;
shl ymm6_2 ymm6_2 0x12@uint64;
shl ymm6_3 ymm6_3 0x12@uint64;
(* vpor   %ymm7,%ymm6,%ymm11                       #! PC = 0x55555557881c *)
or ymm11_0@uint64 ymm6_0 ymm7_0;
or ymm11_1@uint64 ymm6_1 ymm7_1;
or ymm11_2@uint64 ymm6_2 ymm7_2;
or ymm11_3@uint64 ymm6_3 ymm7_3;
(* vpxor  -0x290(%rbp),%ymm15,%ymm6                #! EA = L0x7fffffffbcc0; Value = 0xe90eb8a4080a858f; PC = 0x555555578820 *)
xor ymm6_0@uint64 ymm15_0 L0x7fffffffbcc0;
xor ymm6_1@uint64 ymm15_1 L0x7fffffffbcc8;
xor ymm6_2@uint64 ymm15_2 L0x7fffffffbcd0;
xor ymm6_3@uint64 ymm15_3 L0x7fffffffbcd8;
(* vpxor  -0x210(%rbp),%ymm15,%ymm15               #! EA = L0x7fffffffbd40; Value = 0x95eb44cdb62a1f93; PC = 0x555555578828 *)
xor ymm15_0@uint64 ymm15_0 L0x7fffffffbd40;
xor ymm15_1@uint64 ymm15_1 L0x7fffffffbd48;
xor ymm15_2@uint64 ymm15_2 L0x7fffffffbd50;
xor ymm15_3@uint64 ymm15_3 L0x7fffffffbd58;
(* vpandn %ymm11,%ymm4,%ymm9                       #! PC = 0x555555578830 *)
not ymm4_0n@uint64 ymm4_0;
and ymm9_0@uint64 ymm4_0n ymm11_0;
not ymm4_1n@uint64 ymm4_1;
and ymm9_1@uint64 ymm4_1n ymm11_1;
not ymm4_2n@uint64 ymm4_2;
and ymm9_2@uint64 ymm4_2n ymm11_2;
not ymm4_3n@uint64 ymm4_3;
and ymm9_3@uint64 ymm4_3n ymm11_3;
(* vpshufb 0x556a2(%rip),%ymm6,%ymm6        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x555555578835 *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm6_0, ymm6_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm6_2, ymm6_3, tmp_2, tmp_3);
mov ymm6_0 tmp_0;
mov ymm6_1 tmp_1;
mov ymm6_2 tmp_2;
mov ymm6_3 tmp_3;
(* vpxor  %ymm0,%ymm9,%ymm9                        #! PC = 0x55555557883e *)
xor ymm9_0@uint64 ymm9_0 ymm0_0;
xor ymm9_1@uint64 ymm9_1 ymm0_1;
xor ymm9_2@uint64 ymm9_2 ymm0_2;
xor ymm9_3@uint64 ymm9_3 ymm0_3;
(* vpandn %ymm1,%ymm11,%ymm0                       #! PC = 0x555555578842 *)
not ymm11_0n@uint64 ymm11_0;
and ymm0_0@uint64 ymm11_0n ymm1_0;
not ymm11_1n@uint64 ymm11_1;
and ymm0_1@uint64 ymm11_1n ymm1_1;
not ymm11_2n@uint64 ymm11_2;
and ymm0_2@uint64 ymm11_2n ymm1_2;
not ymm11_3n@uint64 ymm11_3;
and ymm0_3@uint64 ymm11_3n ymm1_3;
(* vpxor  %ymm4,%ymm0,%ymm7                        #! PC = 0x555555578846 *)
xor ymm7_0@uint64 ymm0_0 ymm4_0;
xor ymm7_1@uint64 ymm0_1 ymm4_1;
xor ymm7_2@uint64 ymm0_2 ymm4_2;
xor ymm7_3@uint64 ymm0_3 ymm4_3;
(* vpxor  -0x250(%rbp),%ymm12,%ymm0                #! EA = L0x7fffffffbd00; Value = 0xc20e7c2f247f1fc3; PC = 0x55555557884a *)
xor ymm0_0@uint64 ymm12_0 L0x7fffffffbd00;
xor ymm0_1@uint64 ymm12_1 L0x7fffffffbd08;
xor ymm0_2@uint64 ymm12_2 L0x7fffffffbd10;
xor ymm0_3@uint64 ymm12_3 L0x7fffffffbd18;
(* vpandn %ymm2,%ymm1,%ymm1                        #! PC = 0x555555578852 *)
not ymm1_0n@uint64 ymm1_0;
and ymm1_0@uint64 ymm1_0n ymm2_0;
not ymm1_1n@uint64 ymm1_1;
and ymm1_1@uint64 ymm1_1n ymm2_1;
not ymm1_2n@uint64 ymm1_2;
and ymm1_2@uint64 ymm1_2n ymm2_2;
not ymm1_3n@uint64 ymm1_3;
and ymm1_3@uint64 ymm1_3n ymm2_3;
(* vpxor  %ymm11,%ymm1,%ymm11                      #! PC = 0x555555578856 *)
xor ymm11_0@uint64 ymm1_0 ymm11_0;
xor ymm11_1@uint64 ymm1_1 ymm11_1;
xor ymm11_2@uint64 ymm1_2 ymm11_2;
xor ymm11_3@uint64 ymm1_3 ymm11_3;
(* vmovdqa %ymm7,-0xb0(%rbp)                       #! EA = L0x7fffffffbea0; PC = 0x55555557885b *)
mov L0x7fffffffbea0 ymm7_0;
mov L0x7fffffffbea8 ymm7_1;
mov L0x7fffffffbeb0 ymm7_2;
mov L0x7fffffffbeb8 ymm7_3;
(* vpxor  -0x330(%rbp),%ymm3,%ymm7                 #! EA = L0x7fffffffbc20; Value = 0xc7ff1e37f2a46df1; PC = 0x555555578863 *)
xor ymm7_0@uint64 ymm3_0 L0x7fffffffbc20;
xor ymm7_1@uint64 ymm3_1 L0x7fffffffbc28;
xor ymm7_2@uint64 ymm3_2 L0x7fffffffbc30;
xor ymm7_3@uint64 ymm3_3 L0x7fffffffbc38;
(* vpxor  -0x110(%rbp),%ymm12,%ymm12               #! EA = L0x7fffffffbe40; Value = 0xd68102405f743186; PC = 0x55555557886b *)
xor ymm12_0@uint64 ymm12_0 L0x7fffffffbe40;
xor ymm12_1@uint64 ymm12_1 L0x7fffffffbe48;
xor ymm12_2@uint64 ymm12_2 L0x7fffffffbe50;
xor ymm12_3@uint64 ymm12_3 L0x7fffffffbe58;
(* vpsrlq $0x25,%ymm0,%ymm1                        #! PC = 0x555555578873 *)
shr ymm1_0 ymm0_0 0x25@uint64;
shr ymm1_1 ymm0_1 0x25@uint64;
shr ymm1_2 ymm0_2 0x25@uint64;
shr ymm1_3 ymm0_3 0x25@uint64;
(* vpsllq $0x1b,%ymm0,%ymm0                        #! PC = 0x555555578878 *)
shl ymm0_0 ymm0_0 0x1b@uint64;
shl ymm0_1 ymm0_1 0x1b@uint64;
shl ymm0_2 ymm0_2 0x1b@uint64;
shl ymm0_3 ymm0_3 0x1b@uint64;
(* vmovdqa %ymm11,-0x150(%rbp)                     #! EA = L0x7fffffffbe00; PC = 0x55555557887d *)
mov L0x7fffffffbe00 ymm11_0;
mov L0x7fffffffbe08 ymm11_1;
mov L0x7fffffffbe10 ymm11_2;
mov L0x7fffffffbe18 ymm11_3;
(* vpxor  -0x2f0(%rbp),%ymm3,%ymm3                 #! EA = L0x7fffffffbc60; Value = 0x9c66d82b94503aa2; PC = 0x555555578885 *)
xor ymm3_0@uint64 ymm3_0 L0x7fffffffbc60;
xor ymm3_1@uint64 ymm3_1 L0x7fffffffbc68;
xor ymm3_2@uint64 ymm3_2 L0x7fffffffbc70;
xor ymm3_3@uint64 ymm3_3 L0x7fffffffbc78;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x55555557888d *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x230(%rbp),%ymm5,%ymm1                 #! EA = L0x7fffffffbd20; Value = 0x8b4b752fe1ff3dd4; PC = 0x555555578891 *)
xor ymm1_0@uint64 ymm5_0 L0x7fffffffbd20;
xor ymm1_1@uint64 ymm5_1 L0x7fffffffbd28;
xor ymm1_2@uint64 ymm5_2 L0x7fffffffbd30;
xor ymm1_3@uint64 ymm5_3 L0x7fffffffbd38;
(* vpxor  -0x310(%rbp),%ymm5,%ymm5                 #! EA = L0x7fffffffbc40; Value = 0xdd83725682ce30ad; PC = 0x555555578899 *)
xor ymm5_0@uint64 ymm5_0 L0x7fffffffbc40;
xor ymm5_1@uint64 ymm5_1 L0x7fffffffbc48;
xor ymm5_2@uint64 ymm5_2 L0x7fffffffbc50;
xor ymm5_3@uint64 ymm5_3 L0x7fffffffbc58;
(* vmovdqa %ymm13,-0x110(%rbp)                     #! EA = L0x7fffffffbe40; PC = 0x5555555788a1 *)
mov L0x7fffffffbe40 ymm13_0;
mov L0x7fffffffbe48 ymm13_1;
mov L0x7fffffffbe50 ymm13_2;
mov L0x7fffffffbe58 ymm13_3;
(* vpsrlq $0x1c,%ymm1,%ymm2                        #! PC = 0x5555555788a9 *)
shr ymm2_0 ymm1_0 0x1c@uint64;
shr ymm2_1 ymm1_1 0x1c@uint64;
shr ymm2_2 ymm1_2 0x1c@uint64;
shr ymm2_3 ymm1_3 0x1c@uint64;
(* vpsllq $0x24,%ymm1,%ymm1                        #! PC = 0x5555555788ae *)
shl ymm1_0 ymm1_0 0x24@uint64;
shl ymm1_1 ymm1_1 0x24@uint64;
shl ymm1_2 ymm1_2 0x24@uint64;
shl ymm1_3 ymm1_3 0x24@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x5555555788b3 *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpsrlq $0x36,%ymm7,%ymm2                        #! PC = 0x5555555788b7 *)
shr ymm2_0 ymm7_0 0x36@uint64;
shr ymm2_1 ymm7_1 0x36@uint64;
shr ymm2_2 ymm7_2 0x36@uint64;
shr ymm2_3 ymm7_3 0x36@uint64;
(* vpsllq $0xa,%ymm7,%ymm7                         #! PC = 0x5555555788bc *)
shl ymm7_0 ymm7_0 0xa@uint64;
shl ymm7_1 ymm7_1 0xa@uint64;
shl ymm7_2 ymm7_2 0xa@uint64;
shl ymm7_3 ymm7_3 0xa@uint64;
(* vpor   %ymm2,%ymm7,%ymm4                        #! PC = 0x5555555788c1 *)
or ymm4_0@uint64 ymm7_0 ymm2_0;
or ymm4_1@uint64 ymm7_1 ymm2_1;
or ymm4_2@uint64 ymm7_2 ymm2_2;
or ymm4_3@uint64 ymm7_3 ymm2_3;
(* vpandn %ymm4,%ymm1,%ymm2                        #! PC = 0x5555555788c5 *)
not ymm1_0n@uint64 ymm1_0;
and ymm2_0@uint64 ymm1_0n ymm4_0;
not ymm1_1n@uint64 ymm1_1;
and ymm2_1@uint64 ymm1_1n ymm4_1;
not ymm1_2n@uint64 ymm1_2;
and ymm2_2@uint64 ymm1_2n ymm4_2;
not ymm1_3n@uint64 ymm1_3;
and ymm2_3@uint64 ymm1_3n ymm4_3;
(* vpxor  %ymm0,%ymm2,%ymm11                       #! PC = 0x5555555788c9 *)
xor ymm11_0@uint64 ymm2_0 ymm0_0;
xor ymm11_1@uint64 ymm2_1 ymm0_1;
xor ymm11_2@uint64 ymm2_2 ymm0_2;
xor ymm11_3@uint64 ymm2_3 ymm0_3;
(* vpxor  -0x70(%rbp),%ymm14,%ymm2                 #! EA = L0x7fffffffbee0; Value = 0xbab5ebd7d1623c17; PC = 0x5555555788cd *)
xor ymm2_0@uint64 ymm14_0 L0x7fffffffbee0;
xor ymm2_1@uint64 ymm14_1 L0x7fffffffbee8;
xor ymm2_2@uint64 ymm14_2 L0x7fffffffbef0;
xor ymm2_3@uint64 ymm14_3 L0x7fffffffbef8;
(* vpxor  -0x270(%rbp),%ymm14,%ymm14               #! EA = L0x7fffffffbce0; Value = 0x558a7a103de1a9f7; PC = 0x5555555788d2 *)
xor ymm14_0@uint64 ymm14_0 L0x7fffffffbce0;
xor ymm14_1@uint64 ymm14_1 L0x7fffffffbce8;
xor ymm14_2@uint64 ymm14_2 L0x7fffffffbcf0;
xor ymm14_3@uint64 ymm14_3 L0x7fffffffbcf8;
(* vmovdqa %ymm11,-0x330(%rbp)                     #! EA = L0x7fffffffbc20; PC = 0x5555555788da *)
mov L0x7fffffffbc20 ymm11_0;
mov L0x7fffffffbc28 ymm11_1;
mov L0x7fffffffbc30 ymm11_2;
mov L0x7fffffffbc38 ymm11_3;
(* vpsrlq $0x31,%ymm2,%ymm7                        #! PC = 0x5555555788e2 *)
shr ymm7_0 ymm2_0 0x31@uint64;
shr ymm7_1 ymm2_1 0x31@uint64;
shr ymm7_2 ymm2_2 0x31@uint64;
shr ymm7_3 ymm2_3 0x31@uint64;
(* vpsllq $0xf,%ymm2,%ymm2                         #! PC = 0x5555555788e7 *)
shl ymm2_0 ymm2_0 0xf@uint64;
shl ymm2_1 ymm2_1 0xf@uint64;
shl ymm2_2 ymm2_2 0xf@uint64;
shl ymm2_3 ymm2_3 0xf@uint64;
(* vpor   %ymm7,%ymm2,%ymm2                        #! PC = 0x5555555788ec *)
or ymm2_0@uint64 ymm2_0 ymm7_0;
or ymm2_1@uint64 ymm2_1 ymm7_1;
or ymm2_2@uint64 ymm2_2 ymm7_2;
or ymm2_3@uint64 ymm2_3 ymm7_3;
(* vpandn %ymm6,%ymm2,%ymm10                       #! PC = 0x5555555788f0 *)
not ymm2_0n@uint64 ymm2_0;
and ymm10_0@uint64 ymm2_0n ymm6_0;
not ymm2_1n@uint64 ymm2_1;
and ymm10_1@uint64 ymm2_1n ymm6_1;
not ymm2_2n@uint64 ymm2_2;
and ymm10_2@uint64 ymm2_2n ymm6_2;
not ymm2_3n@uint64 ymm2_3;
and ymm10_3@uint64 ymm2_3n ymm6_3;
(* vpandn %ymm2,%ymm4,%ymm7                        #! PC = 0x5555555788f4 *)
not ymm4_0n@uint64 ymm4_0;
and ymm7_0@uint64 ymm4_0n ymm2_0;
not ymm4_1n@uint64 ymm4_1;
and ymm7_1@uint64 ymm4_1n ymm2_1;
not ymm4_2n@uint64 ymm4_2;
and ymm7_2@uint64 ymm4_2n ymm2_2;
not ymm4_3n@uint64 ymm4_3;
and ymm7_3@uint64 ymm4_3n ymm2_3;
(* vpxor  %ymm4,%ymm10,%ymm4                       #! PC = 0x5555555788f8 *)
xor ymm4_0@uint64 ymm10_0 ymm4_0;
xor ymm4_1@uint64 ymm10_1 ymm4_1;
xor ymm4_2@uint64 ymm10_2 ymm4_2;
xor ymm4_3@uint64 ymm10_3 ymm4_3;
(* vpandn %ymm0,%ymm6,%ymm10                       #! PC = 0x5555555788fc *)
not ymm6_0n@uint64 ymm6_0;
and ymm10_0@uint64 ymm6_0n ymm0_0;
not ymm6_1n@uint64 ymm6_1;
and ymm10_1@uint64 ymm6_1n ymm0_1;
not ymm6_2n@uint64 ymm6_2;
and ymm10_2@uint64 ymm6_2n ymm0_2;
not ymm6_3n@uint64 ymm6_3;
and ymm10_3@uint64 ymm6_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x555555578900 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm6,%ymm0,%ymm6                        #! PC = 0x555555578904 *)
xor ymm6_0@uint64 ymm0_0 ymm6_0;
xor ymm6_1@uint64 ymm0_1 ymm6_1;
xor ymm6_2@uint64 ymm0_2 ymm6_2;
xor ymm6_3@uint64 ymm0_3 ymm6_3;
(* vpsrlq $0x9,%ymm15,%ymm0                        #! PC = 0x555555578908 *)
shr ymm0_0 ymm15_0 0x9@uint64;
shr ymm0_1 ymm15_1 0x9@uint64;
shr ymm0_2 ymm15_2 0x9@uint64;
shr ymm0_3 ymm15_3 0x9@uint64;
(* vpxor  %ymm2,%ymm10,%ymm10                      #! PC = 0x55555557890e *)
xor ymm10_0@uint64 ymm10_0 ymm2_0;
xor ymm10_1@uint64 ymm10_1 ymm2_1;
xor ymm10_2@uint64 ymm10_2 ymm2_2;
xor ymm10_3@uint64 ymm10_3 ymm2_3;
(* vmovdqa %ymm4,-0x70(%rbp)                       #! EA = L0x7fffffffbee0; PC = 0x555555578912 *)
mov L0x7fffffffbee0 ymm4_0;
mov L0x7fffffffbee8 ymm4_1;
mov L0x7fffffffbef0 ymm4_2;
mov L0x7fffffffbef8 ymm4_3;
(* vmovdqa %ymm6,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x555555578917 *)
mov L0x7fffffffbf00 ymm6_0;
mov L0x7fffffffbf08 ymm6_1;
mov L0x7fffffffbf10 ymm6_2;
mov L0x7fffffffbf18 ymm6_3;
(* vpsllq $0x37,%ymm15,%ymm15                      #! PC = 0x55555557891c *)
shl ymm15_0 ymm15_0 0x37@uint64;
shl ymm15_1 ymm15_1 0x37@uint64;
shl ymm15_2 ymm15_2 0x37@uint64;
shl ymm15_3 ymm15_3 0x37@uint64;
(* vpsrlq $0x19,%ymm12,%ymm6                       #! PC = 0x555555578922 *)
shr ymm6_0 ymm12_0 0x19@uint64;
shr ymm6_1 ymm12_1 0x19@uint64;
shr ymm6_2 ymm12_2 0x19@uint64;
shr ymm6_3 ymm12_3 0x19@uint64;
(* vpxor  %ymm1,%ymm7,%ymm7                        #! PC = 0x555555578928 *)
xor ymm7_0@uint64 ymm7_0 ymm1_0;
xor ymm7_1@uint64 ymm7_1 ymm1_1;
xor ymm7_2@uint64 ymm7_2 ymm1_2;
xor ymm7_3@uint64 ymm7_3 ymm1_3;
(* vpsllq $0x27,%ymm12,%ymm12                      #! PC = 0x55555557892c *)
shl ymm12_0 ymm12_0 0x27@uint64;
shl ymm12_1 ymm12_1 0x27@uint64;
shl ymm12_2 ymm12_2 0x27@uint64;
shl ymm12_3 ymm12_3 0x27@uint64;
(* vpsrlq $0x2,%ymm14,%ymm2                        #! PC = 0x555555578932 *)
shr ymm2_0 ymm14_0 0x2@uint64;
shr ymm2_1 ymm14_1 0x2@uint64;
shr ymm2_2 ymm14_2 0x2@uint64;
shr ymm2_3 ymm14_3 0x2@uint64;
(* vpor   %ymm0,%ymm15,%ymm15                      #! PC = 0x555555578938 *)
or ymm15_0@uint64 ymm15_0 ymm0_0;
or ymm15_1@uint64 ymm15_1 ymm0_1;
or ymm15_2@uint64 ymm15_2 ymm0_2;
or ymm15_3@uint64 ymm15_3 ymm0_3;
(* vpor   %ymm6,%ymm12,%ymm12                      #! PC = 0x55555557893c *)
or ymm12_0@uint64 ymm12_0 ymm6_0;
or ymm12_1@uint64 ymm12_1 ymm6_1;
or ymm12_2@uint64 ymm12_2 ymm6_2;
or ymm12_3@uint64 ymm12_3 ymm6_3;
(* vpsllq $0x3e,%ymm14,%ymm14                      #! PC = 0x555555578940 *)
shl ymm14_0 ymm14_0 0x3e@uint64;
shl ymm14_1 ymm14_1 0x3e@uint64;
shl ymm14_2 ymm14_2 0x3e@uint64;
shl ymm14_3 ymm14_3 0x3e@uint64;
(* vpxor  -0x1f0(%rbp),%ymm13,%ymm0                #! EA = L0x7fffffffbd60; Value = 0x73fe33c9b1038c36; PC = 0x555555578946 *)
xor ymm0_0@uint64 ymm13_0 L0x7fffffffbd60;
xor ymm0_1@uint64 ymm13_1 L0x7fffffffbd68;
xor ymm0_2@uint64 ymm13_2 L0x7fffffffbd70;
xor ymm0_3@uint64 ymm13_3 L0x7fffffffbd78;
(* vpor   %ymm2,%ymm14,%ymm4                       #! PC = 0x55555557894e *)
or ymm4_0@uint64 ymm14_0 ymm2_0;
or ymm4_1@uint64 ymm14_1 ymm2_1;
or ymm4_2@uint64 ymm14_2 ymm2_2;
or ymm4_3@uint64 ymm14_3 ymm2_3;
(* vpandn %ymm12,%ymm15,%ymm6                      #! PC = 0x555555578952 *)
not ymm15_0n@uint64 ymm15_0;
and ymm6_0@uint64 ymm15_0n ymm12_0;
not ymm15_1n@uint64 ymm15_1;
and ymm6_1@uint64 ymm15_1n ymm12_1;
not ymm15_2n@uint64 ymm15_2;
and ymm6_2@uint64 ymm15_2n ymm12_2;
not ymm15_3n@uint64 ymm15_3;
and ymm6_3@uint64 ymm15_3n ymm12_3;
(* vpxor  -0x70(%rbp),%ymm9,%ymm13                 #! EA = L0x7fffffffbee0; Value = 0xf377844620f2d499; PC = 0x555555578957 *)
xor ymm13_0@uint64 ymm9_0 L0x7fffffffbee0;
xor ymm13_1@uint64 ymm9_1 L0x7fffffffbee8;
xor ymm13_2@uint64 ymm9_2 L0x7fffffffbef0;
xor ymm13_3@uint64 ymm9_3 L0x7fffffffbef8;
(* vpxor  %ymm4,%ymm6,%ymm6                        #! PC = 0x55555557895c *)
xor ymm6_0@uint64 ymm6_0 ymm4_0;
xor ymm6_1@uint64 ymm6_1 ymm4_1;
xor ymm6_2@uint64 ymm6_2 ymm4_2;
xor ymm6_3@uint64 ymm6_3 ymm4_3;
(* vpxor  -0x190(%rbp),%ymm7,%ymm14                #! EA = L0x7fffffffbdc0; Value = 0xa81edbeb54d20fe1; PC = 0x555555578960 *)
xor ymm14_0@uint64 ymm7_0 L0x7fffffffbdc0;
xor ymm14_1@uint64 ymm7_1 L0x7fffffffbdc8;
xor ymm14_2@uint64 ymm7_2 L0x7fffffffbdd0;
xor ymm14_3@uint64 ymm7_3 L0x7fffffffbdd8;
(* vpxor  %ymm11,%ymm6,%ymm2                       #! PC = 0x555555578968 *)
xor ymm2_0@uint64 ymm6_0 ymm11_0;
xor ymm2_1@uint64 ymm6_1 ymm11_1;
xor ymm2_2@uint64 ymm6_2 ymm11_2;
xor ymm2_3@uint64 ymm6_3 ymm11_3;
(* vmovdqa -0x170(%rbp),%ymm11                     #! EA = L0x7fffffffbde0; Value = 0x0fe03a842f22afcb; PC = 0x55555557896d *)
mov ymm11_0 L0x7fffffffbde0;
mov ymm11_1 L0x7fffffffbde8;
mov ymm11_2 L0x7fffffffbdf0;
mov ymm11_3 L0x7fffffffbdf8;
(* vpxor  %ymm0,%ymm2,%ymm2                        #! PC = 0x555555578975 *)
xor ymm2_0@uint64 ymm2_0 ymm0_0;
xor ymm2_1@uint64 ymm2_1 ymm0_1;
xor ymm2_2@uint64 ymm2_2 ymm0_2;
xor ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x17,%ymm5,%ymm0                        #! PC = 0x555555578979 *)
shr ymm0_0 ymm5_0 0x17@uint64;
shr ymm0_1 ymm5_1 0x17@uint64;
shr ymm0_2 ymm5_2 0x17@uint64;
shr ymm0_3 ymm5_3 0x17@uint64;
(* vpxor  -0x90(%rbp),%ymm2,%ymm2                  #! EA = L0x7fffffffbec0; Value = 0x6a00840802752a6f; PC = 0x55555557897e *)
xor ymm2_0@uint64 ymm2_0 L0x7fffffffbec0;
xor ymm2_1@uint64 ymm2_1 L0x7fffffffbec8;
xor ymm2_2@uint64 ymm2_2 L0x7fffffffbed0;
xor ymm2_3@uint64 ymm2_3 L0x7fffffffbed8;
(* vpsllq $0x29,%ymm5,%ymm5                        #! PC = 0x555555578986 *)
shl ymm5_0 ymm5_0 0x29@uint64;
shl ymm5_1 ymm5_1 0x29@uint64;
shl ymm5_2 ymm5_2 0x29@uint64;
shl ymm5_3 ymm5_3 0x29@uint64;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x55555557898b *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpandn %ymm5,%ymm12,%ymm0                       #! PC = 0x55555557898f *)
not ymm12_0n@uint64 ymm12_0;
and ymm0_0@uint64 ymm12_0n ymm5_0;
not ymm12_1n@uint64 ymm12_1;
and ymm0_1@uint64 ymm12_1n ymm5_1;
not ymm12_2n@uint64 ymm12_2;
and ymm0_2@uint64 ymm12_2n ymm5_2;
not ymm12_3n@uint64 ymm12_3;
and ymm0_3@uint64 ymm12_3n ymm5_3;
(* vpxor  %ymm15,%ymm0,%ymm1                       #! PC = 0x555555578993 *)
xor ymm1_0@uint64 ymm0_0 ymm15_0;
xor ymm1_1@uint64 ymm0_1 ymm15_1;
xor ymm1_2@uint64 ymm0_2 ymm15_2;
xor ymm1_3@uint64 ymm0_3 ymm15_3;
(* vpxor  -0x1d0(%rbp),%ymm8,%ymm0                 #! EA = L0x7fffffffbd80; Value = 0xf9a9c3ab00c9c931; PC = 0x555555578998 *)
xor ymm0_0@uint64 ymm8_0 L0x7fffffffbd80;
xor ymm0_1@uint64 ymm8_1 L0x7fffffffbd88;
xor ymm0_2@uint64 ymm8_2 L0x7fffffffbd90;
xor ymm0_3@uint64 ymm8_3 L0x7fffffffbd98;
(* vmovdqa %ymm1,-0x310(%rbp)                      #! EA = L0x7fffffffbc40; PC = 0x5555555789a0 *)
mov L0x7fffffffbc40 ymm1_0;
mov L0x7fffffffbc48 ymm1_1;
mov L0x7fffffffbc50 ymm1_2;
mov L0x7fffffffbc58 ymm1_3;
(* vpxor  %ymm0,%ymm14,%ymm14                      #! PC = 0x5555555789a8 *)
xor ymm14_0@uint64 ymm14_0 ymm0_0;
xor ymm14_1@uint64 ymm14_1 ymm0_1;
xor ymm14_2@uint64 ymm14_2 ymm0_2;
xor ymm14_3@uint64 ymm14_3 ymm0_3;
(* vpxor  %ymm1,%ymm14,%ymm14                      #! PC = 0x5555555789ac *)
xor ymm14_0@uint64 ymm14_0 ymm1_0;
xor ymm14_1@uint64 ymm14_1 ymm1_1;
xor ymm14_2@uint64 ymm14_2 ymm1_2;
xor ymm14_3@uint64 ymm14_3 ymm1_3;
(* vpsrlq $0x3e,%ymm3,%ymm1                        #! PC = 0x5555555789b0 *)
shr ymm1_0 ymm3_0 0x3e@uint64;
shr ymm1_1 ymm3_1 0x3e@uint64;
shr ymm1_2 ymm3_2 0x3e@uint64;
shr ymm1_3 ymm3_3 0x3e@uint64;
(* vpsllq $0x2,%ymm3,%ymm3                         #! PC = 0x5555555789b5 *)
shl ymm3_0 ymm3_0 0x2@uint64;
shl ymm3_1 ymm3_1 0x2@uint64;
shl ymm3_2 ymm3_2 0x2@uint64;
shl ymm3_3 ymm3_3 0x2@uint64;
(* vpor   %ymm1,%ymm3,%ymm0                        #! PC = 0x5555555789ba *)
or ymm0_0@uint64 ymm3_0 ymm1_0;
or ymm0_1@uint64 ymm3_1 ymm1_1;
or ymm0_2@uint64 ymm3_2 ymm1_2;
or ymm0_3@uint64 ymm3_3 ymm1_3;
(* vmovdqa -0x1b0(%rbp),%ymm3                      #! EA = L0x7fffffffbda0; Value = 0xd95c0a4ec94ad619; PC = 0x5555555789be *)
mov ymm3_0 L0x7fffffffbda0;
mov ymm3_1 L0x7fffffffbda8;
mov ymm3_2 L0x7fffffffbdb0;
mov ymm3_3 L0x7fffffffbdb8;
(* vpandn %ymm0,%ymm5,%ymm1                        #! PC = 0x5555555789c6 *)
not ymm5_0n@uint64 ymm5_0;
and ymm1_0@uint64 ymm5_0n ymm0_0;
not ymm5_1n@uint64 ymm5_1;
and ymm1_1@uint64 ymm5_1n ymm0_1;
not ymm5_2n@uint64 ymm5_2;
and ymm1_2@uint64 ymm5_2n ymm0_2;
not ymm5_3n@uint64 ymm5_3;
and ymm1_3@uint64 ymm5_3n ymm0_3;
(* vpxor  %ymm12,%ymm1,%ymm12                      #! PC = 0x5555555789ca *)
xor ymm12_0@uint64 ymm1_0 ymm12_0;
xor ymm12_1@uint64 ymm1_1 ymm12_1;
xor ymm12_2@uint64 ymm1_2 ymm12_2;
xor ymm12_3@uint64 ymm1_3 ymm12_3;
(* vpxor  -0x2d0(%rbp),%ymm11,%ymm1                #! EA = L0x7fffffffbc80; Value = 0x6db98725571f1604; PC = 0x5555555789cf *)
xor ymm1_0@uint64 ymm11_0 L0x7fffffffbc80;
xor ymm1_1@uint64 ymm11_1 L0x7fffffffbc88;
xor ymm1_2@uint64 ymm11_2 L0x7fffffffbc90;
xor ymm1_3@uint64 ymm11_3 L0x7fffffffbc98;
(* vpxor  %ymm1,%ymm13,%ymm13                      #! PC = 0x5555555789d7 *)
xor ymm13_0@uint64 ymm13_0 ymm1_0;
xor ymm13_1@uint64 ymm13_1 ymm1_1;
xor ymm13_2@uint64 ymm13_2 ymm1_2;
xor ymm13_3@uint64 ymm13_3 ymm1_3;
(* vpandn %ymm4,%ymm0,%ymm1                        #! PC = 0x5555555789db *)
not ymm0_0n@uint64 ymm0_0;
and ymm1_0@uint64 ymm0_0n ymm4_0;
not ymm0_1n@uint64 ymm0_1;
and ymm1_1@uint64 ymm0_1n ymm4_1;
not ymm0_2n@uint64 ymm0_2;
and ymm1_2@uint64 ymm0_2n ymm4_2;
not ymm0_3n@uint64 ymm0_3;
and ymm1_3@uint64 ymm0_3n ymm4_3;
(* vpxor  %ymm5,%ymm1,%ymm5                        #! PC = 0x5555555789df *)
xor ymm5_0@uint64 ymm1_0 ymm5_0;
xor ymm5_1@uint64 ymm1_1 ymm5_1;
xor ymm5_2@uint64 ymm1_2 ymm5_2;
xor ymm5_3@uint64 ymm1_3 ymm5_3;
(* vpxor  %ymm12,%ymm13,%ymm13                     #! PC = 0x5555555789e3 *)
xor ymm13_0@uint64 ymm13_0 ymm12_0;
xor ymm13_1@uint64 ymm13_1 ymm12_1;
xor ymm13_2@uint64 ymm13_2 ymm12_2;
xor ymm13_3@uint64 ymm13_3 ymm12_3;
(* vmovdqa %ymm5,-0x2f0(%rbp)                      #! EA = L0x7fffffffbc60; PC = 0x5555555789e8 *)
mov L0x7fffffffbc60 ymm5_0;
mov L0x7fffffffbc68 ymm5_1;
mov L0x7fffffffbc70 ymm5_2;
mov L0x7fffffffbc78 ymm5_3;
(* vpxor  -0xf0(%rbp),%ymm3,%ymm1                  #! EA = L0x7fffffffbe60; Value = 0x96ba275ba7474a93; PC = 0x5555555789f0 *)
xor ymm1_0@uint64 ymm3_0 L0x7fffffffbe60;
xor ymm1_1@uint64 ymm3_1 L0x7fffffffbe68;
xor ymm1_2@uint64 ymm3_2 L0x7fffffffbe70;
xor ymm1_3@uint64 ymm3_3 L0x7fffffffbe78;
(* vpxor  %ymm5,%ymm10,%ymm11                      #! PC = 0x5555555789f8 *)
xor ymm11_0@uint64 ymm10_0 ymm5_0;
xor ymm11_1@uint64 ymm10_1 ymm5_1;
xor ymm11_2@uint64 ymm10_2 ymm5_2;
xor ymm11_3@uint64 ymm10_3 ymm5_3;
(* vpandn %ymm15,%ymm4,%ymm3                       #! PC = 0x5555555789fc *)
not ymm4_0n@uint64 ymm4_0;
and ymm3_0@uint64 ymm4_0n ymm15_0;
not ymm4_1n@uint64 ymm4_1;
and ymm3_1@uint64 ymm4_1n ymm15_1;
not ymm4_2n@uint64 ymm4_2;
and ymm3_2@uint64 ymm4_2n ymm15_2;
not ymm4_3n@uint64 ymm4_3;
and ymm3_3@uint64 ymm4_3n ymm15_3;
(* vpsllq $0x1,%ymm14,%ymm5                        #! PC = 0x555555578a01 *)
shl ymm5_0 ymm14_0 0x1@uint64;
shl ymm5_1 ymm14_1 0x1@uint64;
shl ymm5_2 ymm14_2 0x1@uint64;
shl ymm5_3 ymm14_3 0x1@uint64;
(* vpsllq $0x1,%ymm13,%ymm4                        #! PC = 0x555555578a07 *)
shl ymm4_0 ymm13_0 0x1@uint64;
shl ymm4_1 ymm13_1 0x1@uint64;
shl ymm4_2 ymm13_2 0x1@uint64;
shl ymm4_3 ymm13_3 0x1@uint64;
(* vpxor  %ymm0,%ymm3,%ymm3                        #! PC = 0x555555578a0d *)
xor ymm3_0@uint64 ymm3_0 ymm0_0;
xor ymm3_1@uint64 ymm3_1 ymm0_1;
xor ymm3_2@uint64 ymm3_2 ymm0_2;
xor ymm3_3@uint64 ymm3_3 ymm0_3;
(* vmovdqa -0xd0(%rbp),%ymm15                      #! EA = L0x7fffffffbe80; Value = 0xe57a1a2bb2ae09c2; PC = 0x555555578a11 *)
mov ymm15_0 L0x7fffffffbe80;
mov ymm15_1 L0x7fffffffbe88;
mov ymm15_2 L0x7fffffffbe90;
mov ymm15_3 L0x7fffffffbe98;
(* vpxor  %ymm1,%ymm11,%ymm11                      #! PC = 0x555555578a19 *)
xor ymm11_0@uint64 ymm11_0 ymm1_0;
xor ymm11_1@uint64 ymm11_1 ymm1_1;
xor ymm11_2@uint64 ymm11_2 ymm1_2;
xor ymm11_3@uint64 ymm11_3 ymm1_3;
(* vpxor  -0x2b0(%rbp),%ymm15,%ymm0                #! EA = L0x7fffffffbca0; Value = 0x9b5e7a3feeb7e41e; PC = 0x555555578a1d *)
xor ymm0_0@uint64 ymm15_0 L0x7fffffffbca0;
xor ymm0_1@uint64 ymm15_1 L0x7fffffffbca8;
xor ymm0_2@uint64 ymm15_2 L0x7fffffffbcb0;
xor ymm0_3@uint64 ymm15_3 L0x7fffffffbcb8;
(* vpxor  -0x150(%rbp),%ymm3,%ymm1                 #! EA = L0x7fffffffbe00; Value = 0xa1c3cee067ab9f52; PC = 0x555555578a25 *)
xor ymm1_0@uint64 ymm3_0 L0x7fffffffbe00;
xor ymm1_1@uint64 ymm3_1 L0x7fffffffbe08;
xor ymm1_2@uint64 ymm3_2 L0x7fffffffbe10;
xor ymm1_3@uint64 ymm3_3 L0x7fffffffbe18;
(* vpxor  -0xb0(%rbp),%ymm11,%ymm11                #! EA = L0x7fffffffbea0; Value = 0xc17e6a6ab6526eb1; PC = 0x555555578a2d *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbea0;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbea8;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbeb0;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbeb8;
(* vpxor  %ymm0,%ymm1,%ymm1                        #! PC = 0x555555578a35 *)
xor ymm1_0@uint64 ymm1_0 ymm0_0;
xor ymm1_1@uint64 ymm1_1 ymm0_1;
xor ymm1_2@uint64 ymm1_2 ymm0_2;
xor ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3f,%ymm14,%ymm0                       #! PC = 0x555555578a39 *)
shr ymm0_0 ymm14_0 0x3f@uint64;
shr ymm0_1 ymm14_1 0x3f@uint64;
shr ymm0_2 ymm14_2 0x3f@uint64;
shr ymm0_3 ymm14_3 0x3f@uint64;
(* vpxor  -0x50(%rbp),%ymm1,%ymm1                  #! EA = L0x7fffffffbf00; Value = 0x965393ca1b42f1db; PC = 0x555555578a3f *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbf00;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbf08;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbf10;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbf18;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x555555578a44 *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpsrlq $0x3f,%ymm13,%ymm0                       #! PC = 0x555555578a48 *)
shr ymm0_0 ymm13_0 0x3f@uint64;
shr ymm0_1 ymm13_1 0x3f@uint64;
shr ymm0_2 ymm13_2 0x3f@uint64;
shr ymm0_3 ymm13_3 0x3f@uint64;
(* vpsrlq $0x3f,%ymm11,%ymm15                      #! PC = 0x555555578a4e *)
shr ymm15_0 ymm11_0 0x3f@uint64;
shr ymm15_1 ymm11_1 0x3f@uint64;
shr ymm15_2 ymm11_2 0x3f@uint64;
shr ymm15_3 ymm11_3 0x3f@uint64;
(* vpor   %ymm0,%ymm4,%ymm4                        #! PC = 0x555555578a54 *)
or ymm4_0@uint64 ymm4_0 ymm0_0;
or ymm4_1@uint64 ymm4_1 ymm0_1;
or ymm4_2@uint64 ymm4_2 ymm0_2;
or ymm4_3@uint64 ymm4_3 ymm0_3;
(* vpxor  %ymm1,%ymm5,%ymm5                        #! PC = 0x555555578a58 *)
xor ymm5_0@uint64 ymm5_0 ymm1_0;
xor ymm5_1@uint64 ymm5_1 ymm1_1;
xor ymm5_2@uint64 ymm5_2 ymm1_2;
xor ymm5_3@uint64 ymm5_3 ymm1_3;
(* vpsllq $0x1,%ymm11,%ymm0                        #! PC = 0x555555578a5c *)
shl ymm0_0 ymm11_0 0x1@uint64;
shl ymm0_1 ymm11_1 0x1@uint64;
shl ymm0_2 ymm11_2 0x1@uint64;
shl ymm0_3 ymm11_3 0x1@uint64;
(* vpxor  %ymm2,%ymm4,%ymm4                        #! PC = 0x555555578a62 *)
xor ymm4_0@uint64 ymm4_0 ymm2_0;
xor ymm4_1@uint64 ymm4_1 ymm2_1;
xor ymm4_2@uint64 ymm4_2 ymm2_2;
xor ymm4_3@uint64 ymm4_3 ymm2_3;
(* vpxor  %ymm6,%ymm5,%ymm6                        #! PC = 0x555555578a66 *)
xor ymm6_0@uint64 ymm5_0 ymm6_0;
xor ymm6_1@uint64 ymm5_1 ymm6_1;
xor ymm6_2@uint64 ymm5_2 ymm6_2;
xor ymm6_3@uint64 ymm5_3 ymm6_3;
(* vpor   %ymm15,%ymm0,%ymm0                       #! PC = 0x555555578a6a *)
or ymm0_0@uint64 ymm0_0 ymm15_0;
or ymm0_1@uint64 ymm0_1 ymm15_1;
or ymm0_2@uint64 ymm0_2 ymm15_2;
or ymm0_3@uint64 ymm0_3 ymm15_3;
(* vpxor  %ymm8,%ymm4,%ymm8                        #! PC = 0x555555578a6f *)
xor ymm8_0@uint64 ymm4_0 ymm8_0;
xor ymm8_1@uint64 ymm4_1 ymm8_1;
xor ymm8_2@uint64 ymm4_2 ymm8_2;
xor ymm8_3@uint64 ymm4_3 ymm8_3;
(* vpxor  %ymm7,%ymm4,%ymm7                        #! PC = 0x555555578a74 *)
xor ymm7_0@uint64 ymm4_0 ymm7_0;
xor ymm7_1@uint64 ymm4_1 ymm7_1;
xor ymm7_2@uint64 ymm4_2 ymm7_2;
xor ymm7_3@uint64 ymm4_3 ymm7_3;
(* vpxor  %ymm14,%ymm0,%ymm14                      #! PC = 0x555555578a78 *)
xor ymm14_0@uint64 ymm0_0 ymm14_0;
xor ymm14_1@uint64 ymm0_1 ymm14_1;
xor ymm14_2@uint64 ymm0_2 ymm14_2;
xor ymm14_3@uint64 ymm0_3 ymm14_3;
(* vpsrlq $0x3f,%ymm1,%ymm0                        #! PC = 0x555555578a7d *)
shr ymm0_0 ymm1_0 0x3f@uint64;
shr ymm0_1 ymm1_1 0x3f@uint64;
shr ymm0_2 ymm1_2 0x3f@uint64;
shr ymm0_3 ymm1_3 0x3f@uint64;
(* vpsllq $0x1,%ymm1,%ymm1                         #! PC = 0x555555578a82 *)
shl ymm1_0 ymm1_0 0x1@uint64;
shl ymm1_1 ymm1_1 0x1@uint64;
shl ymm1_2 ymm1_2 0x1@uint64;
shl ymm1_3 ymm1_3 0x1@uint64;
(* vpxor  %ymm9,%ymm14,%ymm9                       #! PC = 0x555555578a87 *)
xor ymm9_0@uint64 ymm14_0 ymm9_0;
xor ymm9_1@uint64 ymm14_1 ymm9_1;
xor ymm9_2@uint64 ymm14_2 ymm9_2;
xor ymm9_3@uint64 ymm14_3 ymm9_3;
(* vpxor  %ymm12,%ymm14,%ymm12                     #! PC = 0x555555578a8c *)
xor ymm12_0@uint64 ymm14_0 ymm12_0;
xor ymm12_1@uint64 ymm14_1 ymm12_1;
xor ymm12_2@uint64 ymm14_2 ymm12_2;
xor ymm12_3@uint64 ymm14_3 ymm12_3;
(* vpor   %ymm0,%ymm1,%ymm1                        #! PC = 0x555555578a91 *)
or ymm1_0@uint64 ymm1_0 ymm0_0;
or ymm1_1@uint64 ymm1_1 ymm0_1;
or ymm1_2@uint64 ymm1_2 ymm0_2;
or ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3f,%ymm2,%ymm0                        #! PC = 0x555555578a95 *)
shr ymm0_0 ymm2_0 0x3f@uint64;
shr ymm0_1 ymm2_1 0x3f@uint64;
shr ymm0_2 ymm2_2 0x3f@uint64;
shr ymm0_3 ymm2_3 0x3f@uint64;
(* vpxor  %ymm13,%ymm1,%ymm13                      #! PC = 0x555555578a9a *)
xor ymm13_0@uint64 ymm1_0 ymm13_0;
xor ymm13_1@uint64 ymm1_1 ymm13_1;
xor ymm13_2@uint64 ymm1_2 ymm13_2;
xor ymm13_3@uint64 ymm1_3 ymm13_3;
(* vpsrlq $0x14,%ymm8,%ymm1                        #! PC = 0x555555578a9f *)
shr ymm1_0 ymm8_0 0x14@uint64;
shr ymm1_1 ymm8_1 0x14@uint64;
shr ymm1_2 ymm8_2 0x14@uint64;
shr ymm1_3 ymm8_3 0x14@uint64;
(* vpsllq $0x2c,%ymm8,%ymm8                        #! PC = 0x555555578aa5 *)
shl ymm8_0 ymm8_0 0x2c@uint64;
shl ymm8_1 ymm8_1 0x2c@uint64;
shl ymm8_2 ymm8_2 0x2c@uint64;
shl ymm8_3 ymm8_3 0x2c@uint64;
(* vpsllq $0x1,%ymm2,%ymm2                         #! PC = 0x555555578aab *)
shl ymm2_0 ymm2_0 0x1@uint64;
shl ymm2_1 ymm2_1 0x1@uint64;
shl ymm2_2 ymm2_2 0x1@uint64;
shl ymm2_3 ymm2_3 0x1@uint64;
(* vpxor  %ymm10,%ymm13,%ymm10                     #! PC = 0x555555578ab0 *)
xor ymm10_0@uint64 ymm13_0 ymm10_0;
xor ymm10_1@uint64 ymm13_1 ymm10_1;
xor ymm10_2@uint64 ymm13_2 ymm10_2;
xor ymm10_3@uint64 ymm13_3 ymm10_3;
(* vpor   %ymm1,%ymm8,%ymm8                        #! PC = 0x555555578ab5 *)
or ymm8_0@uint64 ymm8_0 ymm1_0;
or ymm8_1@uint64 ymm8_1 ymm1_1;
or ymm8_2@uint64 ymm8_2 ymm1_2;
or ymm8_3@uint64 ymm8_3 ymm1_3;
(* vpsrlq $0x15,%ymm9,%ymm1                        #! PC = 0x555555578ab9 *)
shr ymm1_0 ymm9_0 0x15@uint64;
shr ymm1_1 ymm9_1 0x15@uint64;
shr ymm1_2 ymm9_2 0x15@uint64;
shr ymm1_3 ymm9_3 0x15@uint64;
(* vpor   %ymm0,%ymm2,%ymm2                        #! PC = 0x555555578abf *)
or ymm2_0@uint64 ymm2_0 ymm0_0;
or ymm2_1@uint64 ymm2_1 ymm0_1;
or ymm2_2@uint64 ymm2_2 ymm0_2;
or ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsllq $0x2b,%ymm9,%ymm9                        #! PC = 0x555555578ac3 *)
shl ymm9_0 ymm9_0 0x2b@uint64;
shl ymm9_1 ymm9_1 0x2b@uint64;
shl ymm9_2 ymm9_2 0x2b@uint64;
shl ymm9_3 ymm9_3 0x2b@uint64;
(* vpxor  -0x90(%rbp),%ymm5,%ymm0                  #! EA = L0x7fffffffbec0; Value = 0x6a00840802752a6f; PC = 0x555555578ac9 *)
xor ymm0_0@uint64 ymm5_0 L0x7fffffffbec0;
xor ymm0_1@uint64 ymm5_1 L0x7fffffffbec8;
xor ymm0_2@uint64 ymm5_2 L0x7fffffffbed0;
xor ymm0_3@uint64 ymm5_3 L0x7fffffffbed8;
(* vpxor  %ymm11,%ymm2,%ymm11                      #! PC = 0x555555578ad1 *)
xor ymm11_0@uint64 ymm2_0 ymm11_0;
xor ymm11_1@uint64 ymm2_1 ymm11_1;
xor ymm11_2@uint64 ymm2_2 ymm11_2;
xor ymm11_3@uint64 ymm2_3 ymm11_3;
(* vpor   %ymm1,%ymm9,%ymm9                        #! PC = 0x555555578ad6 *)
or ymm9_0@uint64 ymm9_0 ymm1_0;
or ymm9_1@uint64 ymm9_1 ymm1_1;
or ymm9_2@uint64 ymm9_2 ymm1_2;
or ymm9_3@uint64 ymm9_3 ymm1_3;
(* vmovq  %r9,%xmm1                                #! PC = 0x555555578ada *)
mov xmm1_0 r9;
mov xmm1_1 0@uint64;
(* vpxor  %ymm3,%ymm11,%ymm3                       #! PC = 0x555555578adf *)
xor ymm3_0@uint64 ymm11_0 ymm3_0;
xor ymm3_1@uint64 ymm11_1 ymm3_1;
xor ymm3_2@uint64 ymm11_2 ymm3_2;
xor ymm3_3@uint64 ymm11_3 ymm3_3;
(* vpandn %ymm9,%ymm8,%ymm2                        #! PC = 0x555555578ae3 *)
not ymm8_0n@uint64 ymm8_0;
and ymm2_0@uint64 ymm8_0n ymm9_0;
not ymm8_1n@uint64 ymm8_1;
and ymm2_1@uint64 ymm8_1n ymm9_1;
not ymm8_2n@uint64 ymm8_2;
and ymm2_2@uint64 ymm8_2n ymm9_2;
not ymm8_3n@uint64 ymm8_3;
and ymm2_3@uint64 ymm8_3n ymm9_3;
(* vpbroadcastq %xmm1,%ymm1                        #! PC = 0x555555578ae8 *)
mov ymm1_0 xmm1_0;
mov ymm1_1 xmm1_0;
mov ymm1_2 xmm1_0;
mov ymm1_3 xmm1_0;
(* vpxor  %ymm2,%ymm1,%ymm1                        #! PC = 0x555555578aed *)
xor ymm1_0@uint64 ymm1_0 ymm2_0;
xor ymm1_1@uint64 ymm1_1 ymm2_1;
xor ymm1_2@uint64 ymm1_2 ymm2_2;
xor ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpxor  %ymm0,%ymm1,%ymm2                        #! PC = 0x555555578af1 *)
xor ymm2_0@uint64 ymm1_0 ymm0_0;
xor ymm2_1@uint64 ymm1_1 ymm0_1;
xor ymm2_2@uint64 ymm1_2 ymm0_2;
xor ymm2_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x2b,%ymm10,%ymm1                       #! PC = 0x555555578af5 *)
shr ymm1_0 ymm10_0 0x2b@uint64;
shr ymm1_1 ymm10_1 0x2b@uint64;
shr ymm1_2 ymm10_2 0x2b@uint64;
shr ymm1_3 ymm10_3 0x2b@uint64;
(* vpsllq $0x15,%ymm10,%ymm10                      #! PC = 0x555555578afb *)
shl ymm10_0 ymm10_0 0x15@uint64;
shl ymm10_1 ymm10_1 0x15@uint64;
shl ymm10_2 ymm10_2 0x15@uint64;
shl ymm10_3 ymm10_3 0x15@uint64;
(* vmovdqa %ymm2,-0x90(%rbp)                       #! EA = L0x7fffffffbec0; PC = 0x555555578b01 *)
mov L0x7fffffffbec0 ymm2_0;
mov L0x7fffffffbec8 ymm2_1;
mov L0x7fffffffbed0 ymm2_2;
mov L0x7fffffffbed8 ymm2_3;
(* vpor   %ymm1,%ymm10,%ymm10                      #! PC = 0x555555578b09 *)
or ymm10_0@uint64 ymm10_0 ymm1_0;
or ymm10_1@uint64 ymm10_1 ymm1_1;
or ymm10_2@uint64 ymm10_2 ymm1_2;
or ymm10_3@uint64 ymm10_3 ymm1_3;
(* vpandn %ymm10,%ymm9,%ymm1                       #! PC = 0x555555578b0d *)
not ymm9_0n@uint64 ymm9_0;
and ymm1_0@uint64 ymm9_0n ymm10_0;
not ymm9_1n@uint64 ymm9_1;
and ymm1_1@uint64 ymm9_1n ymm10_1;
not ymm9_2n@uint64 ymm9_2;
and ymm1_2@uint64 ymm9_2n ymm10_2;
not ymm9_3n@uint64 ymm9_3;
and ymm1_3@uint64 ymm9_3n ymm10_3;
(* vpxor  %ymm8,%ymm1,%ymm2                        #! PC = 0x555555578b12 *)
xor ymm2_0@uint64 ymm1_0 ymm8_0;
xor ymm2_1@uint64 ymm1_1 ymm8_1;
xor ymm2_2@uint64 ymm1_2 ymm8_2;
xor ymm2_3@uint64 ymm1_3 ymm8_3;
(* vpsrlq $0x32,%ymm3,%ymm1                        #! PC = 0x555555578b17 *)
shr ymm1_0 ymm3_0 0x32@uint64;
shr ymm1_1 ymm3_1 0x32@uint64;
shr ymm1_2 ymm3_2 0x32@uint64;
shr ymm1_3 ymm3_3 0x32@uint64;
(* vpandn %ymm8,%ymm0,%ymm8                        #! PC = 0x555555578b1c *)
not ymm0_0n@uint64 ymm0_0;
and ymm8_0@uint64 ymm0_0n ymm8_0;
not ymm0_1n@uint64 ymm0_1;
and ymm8_1@uint64 ymm0_1n ymm8_1;
not ymm0_2n@uint64 ymm0_2;
and ymm8_2@uint64 ymm0_2n ymm8_2;
not ymm0_3n@uint64 ymm0_3;
and ymm8_3@uint64 ymm0_3n ymm8_3;
(* vpsllq $0xe,%ymm3,%ymm3                         #! PC = 0x555555578b21 *)
shl ymm3_0 ymm3_0 0xe@uint64;
shl ymm3_1 ymm3_1 0xe@uint64;
shl ymm3_2 ymm3_2 0xe@uint64;
shl ymm3_3 ymm3_3 0xe@uint64;
(* vmovdqa %ymm2,-0x290(%rbp)                      #! EA = L0x7fffffffbcc0; PC = 0x555555578b26 *)
mov L0x7fffffffbcc0 ymm2_0;
mov L0x7fffffffbcc8 ymm2_1;
mov L0x7fffffffbcd0 ymm2_2;
mov L0x7fffffffbcd8 ymm2_3;
(* vpor   %ymm1,%ymm3,%ymm3                        #! PC = 0x555555578b2e *)
or ymm3_0@uint64 ymm3_0 ymm1_0;
or ymm3_1@uint64 ymm3_1 ymm1_1;
or ymm3_2@uint64 ymm3_2 ymm1_2;
or ymm3_3@uint64 ymm3_3 ymm1_3;
(* vpandn %ymm3,%ymm10,%ymm1                       #! PC = 0x555555578b32 *)
not ymm10_0n@uint64 ymm10_0;
and ymm1_0@uint64 ymm10_0n ymm3_0;
not ymm10_1n@uint64 ymm10_1;
and ymm1_1@uint64 ymm10_1n ymm3_1;
not ymm10_2n@uint64 ymm10_2;
and ymm1_2@uint64 ymm10_2n ymm3_2;
not ymm10_3n@uint64 ymm10_3;
and ymm1_3@uint64 ymm10_3n ymm3_3;
(* vpxor  %ymm3,%ymm8,%ymm8                        #! PC = 0x555555578b36 *)
xor ymm8_0@uint64 ymm8_0 ymm3_0;
xor ymm8_1@uint64 ymm8_1 ymm3_1;
xor ymm8_2@uint64 ymm8_2 ymm3_2;
xor ymm8_3@uint64 ymm8_3 ymm3_3;
(* vpxor  %ymm9,%ymm1,%ymm9                        #! PC = 0x555555578b3a *)
xor ymm9_0@uint64 ymm1_0 ymm9_0;
xor ymm9_1@uint64 ymm1_1 ymm9_1;
xor ymm9_2@uint64 ymm1_2 ymm9_2;
xor ymm9_3@uint64 ymm1_3 ymm9_3;
(* vpandn %ymm0,%ymm3,%ymm1                        #! PC = 0x555555578b3f *)
not ymm3_0n@uint64 ymm3_0;
and ymm1_0@uint64 ymm3_0n ymm0_0;
not ymm3_1n@uint64 ymm3_1;
and ymm1_1@uint64 ymm3_1n ymm0_1;
not ymm3_2n@uint64 ymm3_2;
and ymm1_2@uint64 ymm3_2n ymm0_2;
not ymm3_3n@uint64 ymm3_3;
and ymm1_3@uint64 ymm3_3n ymm0_3;
(* vmovdqa %ymm8,-0x250(%rbp)                      #! EA = L0x7fffffffbd00; PC = 0x555555578b43 *)
mov L0x7fffffffbd00 ymm8_0;
mov L0x7fffffffbd08 ymm8_1;
mov L0x7fffffffbd10 ymm8_2;
mov L0x7fffffffbd18 ymm8_3;
(* vmovdqa %ymm9,-0x270(%rbp)                      #! EA = L0x7fffffffbce0; PC = 0x555555578b4b *)
mov L0x7fffffffbce0 ymm9_0;
mov L0x7fffffffbce8 ymm9_1;
mov L0x7fffffffbcf0 ymm9_2;
mov L0x7fffffffbcf8 ymm9_3;
(* vpxor  %ymm10,%ymm1,%ymm9                       #! PC = 0x555555578b53 *)
xor ymm9_0@uint64 ymm1_0 ymm10_0;
xor ymm9_1@uint64 ymm1_1 ymm10_1;
xor ymm9_2@uint64 ymm1_2 ymm10_2;
xor ymm9_3@uint64 ymm1_3 ymm10_3;
(* vpxor  -0xf0(%rbp),%ymm13,%ymm10                #! EA = L0x7fffffffbe60; Value = 0x96ba275ba7474a93; PC = 0x555555578b58 *)
xor ymm10_0@uint64 ymm13_0 L0x7fffffffbe60;
xor ymm10_1@uint64 ymm13_1 L0x7fffffffbe68;
xor ymm10_2@uint64 ymm13_2 L0x7fffffffbe70;
xor ymm10_3@uint64 ymm13_3 L0x7fffffffbe78;
(* vmovdqa %ymm9,-0x130(%rbp)                      #! EA = L0x7fffffffbe20; PC = 0x555555578b60 *)
mov L0x7fffffffbe20 ymm9_0;
mov L0x7fffffffbe28 ymm9_1;
mov L0x7fffffffbe30 ymm9_2;
mov L0x7fffffffbe38 ymm9_3;
(* vpxor  -0xb0(%rbp),%ymm13,%ymm9                 #! EA = L0x7fffffffbea0; Value = 0xc17e6a6ab6526eb1; PC = 0x555555578b68 *)
xor ymm9_0@uint64 ymm13_0 L0x7fffffffbea0;
xor ymm9_1@uint64 ymm13_1 L0x7fffffffbea8;
xor ymm9_2@uint64 ymm13_2 L0x7fffffffbeb0;
xor ymm9_3@uint64 ymm13_3 L0x7fffffffbeb8;
(* vpsrlq $0x24,%ymm10,%ymm1                       #! PC = 0x555555578b70 *)
shr ymm1_0 ymm10_0 0x24@uint64;
shr ymm1_1 ymm10_1 0x24@uint64;
shr ymm1_2 ymm10_2 0x24@uint64;
shr ymm1_3 ymm10_3 0x24@uint64;
(* vpsllq $0x1c,%ymm10,%ymm0                       #! PC = 0x555555578b76 *)
shl ymm0_0 ymm10_0 0x1c@uint64;
shl ymm0_1 ymm10_1 0x1c@uint64;
shl ymm0_2 ymm10_2 0x1c@uint64;
shl ymm0_3 ymm10_3 0x1c@uint64;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x555555578b7c *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0xd0(%rbp),%ymm11,%ymm1                 #! EA = L0x7fffffffbe80; Value = 0xe57a1a2bb2ae09c2; PC = 0x555555578b80 *)
xor ymm1_0@uint64 ymm11_0 L0x7fffffffbe80;
xor ymm1_1@uint64 ymm11_1 L0x7fffffffbe88;
xor ymm1_2@uint64 ymm11_2 L0x7fffffffbe90;
xor ymm1_3@uint64 ymm11_3 L0x7fffffffbe98;
(* vpsrlq $0x2c,%ymm1,%ymm2                        #! PC = 0x555555578b88 *)
shr ymm2_0 ymm1_0 0x2c@uint64;
shr ymm2_1 ymm1_1 0x2c@uint64;
shr ymm2_2 ymm1_2 0x2c@uint64;
shr ymm2_3 ymm1_3 0x2c@uint64;
(* vpsllq $0x14,%ymm1,%ymm1                        #! PC = 0x555555578b8d *)
shl ymm1_0 ymm1_0 0x14@uint64;
shl ymm1_1 ymm1_1 0x14@uint64;
shl ymm1_2 ymm1_2 0x14@uint64;
shl ymm1_3 ymm1_3 0x14@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x555555578b92 *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpxor  -0x110(%rbp),%ymm5,%ymm2                 #! EA = L0x7fffffffbe40; Value = 0x2728e4f2b7aeebe8; PC = 0x555555578b96 *)
xor ymm2_0@uint64 ymm5_0 L0x7fffffffbe40;
xor ymm2_1@uint64 ymm5_1 L0x7fffffffbe48;
xor ymm2_2@uint64 ymm5_2 L0x7fffffffbe50;
xor ymm2_3@uint64 ymm5_3 L0x7fffffffbe58;
(* vpsrlq $0x3d,%ymm2,%ymm3                        #! PC = 0x555555578b9e *)
shr ymm3_0 ymm2_0 0x3d@uint64;
shr ymm3_1 ymm2_1 0x3d@uint64;
shr ymm3_2 ymm2_2 0x3d@uint64;
shr ymm3_3 ymm2_3 0x3d@uint64;
(* vpsllq $0x3,%ymm2,%ymm2                         #! PC = 0x555555578ba3 *)
shl ymm2_0 ymm2_0 0x3@uint64;
shl ymm2_1 ymm2_1 0x3@uint64;
shl ymm2_2 ymm2_2 0x3@uint64;
shl ymm2_3 ymm2_3 0x3@uint64;
(* vpor   %ymm3,%ymm2,%ymm2                        #! PC = 0x555555578ba8 *)
or ymm2_0@uint64 ymm2_0 ymm3_0;
or ymm2_1@uint64 ymm2_1 ymm3_1;
or ymm2_2@uint64 ymm2_2 ymm3_2;
or ymm2_3@uint64 ymm2_3 ymm3_3;
(* vpandn %ymm2,%ymm1,%ymm3                        #! PC = 0x555555578bac *)
not ymm1_0n@uint64 ymm1_0;
and ymm3_0@uint64 ymm1_0n ymm2_0;
not ymm1_1n@uint64 ymm1_1;
and ymm3_1@uint64 ymm1_1n ymm2_1;
not ymm1_2n@uint64 ymm1_2;
and ymm3_2@uint64 ymm1_2n ymm2_2;
not ymm1_3n@uint64 ymm1_3;
and ymm3_3@uint64 ymm1_3n ymm2_3;
(* vpxor  %ymm0,%ymm3,%ymm8                        #! PC = 0x555555578bb0 *)
xor ymm8_0@uint64 ymm3_0 ymm0_0;
xor ymm8_1@uint64 ymm3_1 ymm0_1;
xor ymm8_2@uint64 ymm3_2 ymm0_2;
xor ymm8_3@uint64 ymm3_3 ymm0_3;
(* vpsrlq $0x13,%ymm7,%ymm3                        #! PC = 0x555555578bb4 *)
shr ymm3_0 ymm7_0 0x13@uint64;
shr ymm3_1 ymm7_1 0x13@uint64;
shr ymm3_2 ymm7_2 0x13@uint64;
shr ymm3_3 ymm7_3 0x13@uint64;
(* vpsllq $0x2d,%ymm7,%ymm7                        #! PC = 0x555555578bb9 *)
shl ymm7_0 ymm7_0 0x2d@uint64;
shl ymm7_1 ymm7_1 0x2d@uint64;
shl ymm7_2 ymm7_2 0x2d@uint64;
shl ymm7_3 ymm7_3 0x2d@uint64;
(* vmovdqa %ymm8,-0x230(%rbp)                      #! EA = L0x7fffffffbd20; PC = 0x555555578bbe *)
mov L0x7fffffffbd20 ymm8_0;
mov L0x7fffffffbd28 ymm8_1;
mov L0x7fffffffbd30 ymm8_2;
mov L0x7fffffffbd38 ymm8_3;
(* vpor   %ymm3,%ymm7,%ymm15                       #! PC = 0x555555578bc6 *)
or ymm15_0@uint64 ymm7_0 ymm3_0;
or ymm15_1@uint64 ymm7_1 ymm3_1;
or ymm15_2@uint64 ymm7_2 ymm3_2;
or ymm15_3@uint64 ymm7_3 ymm3_3;
(* vpsrlq $0x3,%ymm12,%ymm3                        #! PC = 0x555555578bca *)
shr ymm3_0 ymm12_0 0x3@uint64;
shr ymm3_1 ymm12_1 0x3@uint64;
shr ymm3_2 ymm12_2 0x3@uint64;
shr ymm3_3 ymm12_3 0x3@uint64;
(* vpsllq $0x3d,%ymm12,%ymm12                      #! PC = 0x555555578bd0 *)
shl ymm12_0 ymm12_0 0x3d@uint64;
shl ymm12_1 ymm12_1 0x3d@uint64;
shl ymm12_2 ymm12_2 0x3d@uint64;
shl ymm12_3 ymm12_3 0x3d@uint64;
(* vpandn %ymm15,%ymm2,%ymm8                       #! PC = 0x555555578bd6 *)
not ymm2_0n@uint64 ymm2_0;
and ymm8_0@uint64 ymm2_0n ymm15_0;
not ymm2_1n@uint64 ymm2_1;
and ymm8_1@uint64 ymm2_1n ymm15_1;
not ymm2_2n@uint64 ymm2_2;
and ymm8_2@uint64 ymm2_2n ymm15_2;
not ymm2_3n@uint64 ymm2_3;
and ymm8_3@uint64 ymm2_3n ymm15_3;
(* vpor   %ymm3,%ymm12,%ymm12                      #! PC = 0x555555578bdb *)
or ymm12_0@uint64 ymm12_0 ymm3_0;
or ymm12_1@uint64 ymm12_1 ymm3_1;
or ymm12_2@uint64 ymm12_2 ymm3_2;
or ymm12_3@uint64 ymm12_3 ymm3_3;
(* vpxor  %ymm1,%ymm8,%ymm8                        #! PC = 0x555555578bdf *)
xor ymm8_0@uint64 ymm8_0 ymm1_0;
xor ymm8_1@uint64 ymm8_1 ymm1_1;
xor ymm8_2@uint64 ymm8_2 ymm1_2;
xor ymm8_3@uint64 ymm8_3 ymm1_3;
(* vpandn %ymm12,%ymm15,%ymm3                      #! PC = 0x555555578be3 *)
not ymm15_0n@uint64 ymm15_0;
and ymm3_0@uint64 ymm15_0n ymm12_0;
not ymm15_1n@uint64 ymm15_1;
and ymm3_1@uint64 ymm15_1n ymm12_1;
not ymm15_2n@uint64 ymm15_2;
and ymm3_2@uint64 ymm15_2n ymm12_2;
not ymm15_3n@uint64 ymm15_3;
and ymm3_3@uint64 ymm15_3n ymm12_3;
(* vpxor  %ymm2,%ymm3,%ymm10                       #! PC = 0x555555578be8 *)
xor ymm10_0@uint64 ymm3_0 ymm2_0;
xor ymm10_1@uint64 ymm3_1 ymm2_1;
xor ymm10_2@uint64 ymm3_2 ymm2_2;
xor ymm10_3@uint64 ymm3_3 ymm2_3;
(* vpsrlq $0x27,%ymm9,%ymm3                        #! PC = 0x555555578bec *)
shr ymm3_0 ymm9_0 0x27@uint64;
shr ymm3_1 ymm9_1 0x27@uint64;
shr ymm3_2 ymm9_2 0x27@uint64;
shr ymm3_3 ymm9_3 0x27@uint64;
(* vpandn %ymm0,%ymm12,%ymm2                       #! PC = 0x555555578bf2 *)
not ymm12_0n@uint64 ymm12_0;
and ymm2_0@uint64 ymm12_0n ymm0_0;
not ymm12_1n@uint64 ymm12_1;
and ymm2_1@uint64 ymm12_1n ymm0_1;
not ymm12_2n@uint64 ymm12_2;
and ymm2_2@uint64 ymm12_2n ymm0_2;
not ymm12_3n@uint64 ymm12_3;
and ymm2_3@uint64 ymm12_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x555555578bf6 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm15,%ymm2,%ymm2                       #! PC = 0x555555578bfa *)
xor ymm2_0@uint64 ymm2_0 ymm15_0;
xor ymm2_1@uint64 ymm2_1 ymm15_1;
xor ymm2_2@uint64 ymm2_2 ymm15_2;
xor ymm2_3@uint64 ymm2_3 ymm15_3;
(* vmovdqa %ymm10,-0x110(%rbp)                     #! EA = L0x7fffffffbe40; PC = 0x555555578bff *)
mov L0x7fffffffbe40 ymm10_0;
mov L0x7fffffffbe48 ymm10_1;
mov L0x7fffffffbe50 ymm10_2;
mov L0x7fffffffbe58 ymm10_3;
(* vpxor  %ymm12,%ymm0,%ymm12                      #! PC = 0x555555578c07 *)
xor ymm12_0@uint64 ymm0_0 ymm12_0;
xor ymm12_1@uint64 ymm0_1 ymm12_1;
xor ymm12_2@uint64 ymm0_2 ymm12_2;
xor ymm12_3@uint64 ymm0_3 ymm12_3;
(* vpxor  -0x1d0(%rbp),%ymm4,%ymm0                 #! EA = L0x7fffffffbd80; Value = 0xf9a9c3ab00c9c931; PC = 0x555555578c0c *)
xor ymm0_0@uint64 ymm4_0 L0x7fffffffbd80;
xor ymm0_1@uint64 ymm4_1 L0x7fffffffbd88;
xor ymm0_2@uint64 ymm4_2 L0x7fffffffbd90;
xor ymm0_3@uint64 ymm4_3 L0x7fffffffbd98;
(* vmovdqa %ymm2,-0x210(%rbp)                      #! EA = L0x7fffffffbd40; PC = 0x555555578c14 *)
mov L0x7fffffffbd40 ymm2_0;
mov L0x7fffffffbd48 ymm2_1;
mov L0x7fffffffbd50 ymm2_2;
mov L0x7fffffffbd58 ymm2_3;
(* vpxor  -0x170(%rbp),%ymm14,%ymm2                #! EA = L0x7fffffffbde0; Value = 0x0fe03a842f22afcb; PC = 0x555555578c1c *)
xor ymm2_0@uint64 ymm14_0 L0x7fffffffbde0;
xor ymm2_1@uint64 ymm14_1 L0x7fffffffbde8;
xor ymm2_2@uint64 ymm14_2 L0x7fffffffbdf0;
xor ymm2_3@uint64 ymm14_3 L0x7fffffffbdf8;
(* vpsllq $0x19,%ymm9,%ymm9                        #! PC = 0x555555578c24 *)
shl ymm9_0 ymm9_0 0x19@uint64;
shl ymm9_1 ymm9_1 0x19@uint64;
shl ymm9_2 ymm9_2 0x19@uint64;
shl ymm9_3 ymm9_3 0x19@uint64;
(* vmovdqa %ymm12,-0xf0(%rbp)                      #! EA = L0x7fffffffbe60; PC = 0x555555578c2a *)
mov L0x7fffffffbe60 ymm12_0;
mov L0x7fffffffbe68 ymm12_1;
mov L0x7fffffffbe70 ymm12_2;
mov L0x7fffffffbe78 ymm12_3;
(* vpsrlq $0x3f,%ymm0,%ymm1                        #! PC = 0x555555578c32 *)
shr ymm1_0 ymm0_0 0x3f@uint64;
shr ymm1_1 ymm0_1 0x3f@uint64;
shr ymm1_2 ymm0_2 0x3f@uint64;
shr ymm1_3 ymm0_3 0x3f@uint64;
(* vpsllq $0x1,%ymm0,%ymm0                         #! PC = 0x555555578c37 *)
shl ymm0_0 ymm0_0 0x1@uint64;
shl ymm0_1 ymm0_1 0x1@uint64;
shl ymm0_2 ymm0_2 0x1@uint64;
shl ymm0_3 ymm0_3 0x1@uint64;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x555555578c3c *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpsrlq $0x3a,%ymm2,%ymm1                        #! PC = 0x555555578c40 *)
shr ymm1_0 ymm2_0 0x3a@uint64;
shr ymm1_1 ymm2_1 0x3a@uint64;
shr ymm1_2 ymm2_2 0x3a@uint64;
shr ymm1_3 ymm2_3 0x3a@uint64;
(* vpsllq $0x6,%ymm2,%ymm2                         #! PC = 0x555555578c45 *)
shl ymm2_0 ymm2_0 0x6@uint64;
shl ymm2_1 ymm2_1 0x6@uint64;
shl ymm2_2 ymm2_2 0x6@uint64;
shl ymm2_3 ymm2_3 0x6@uint64;
(* vpor   %ymm1,%ymm2,%ymm2                        #! PC = 0x555555578c4a *)
or ymm2_0@uint64 ymm2_0 ymm1_0;
or ymm2_1@uint64 ymm2_1 ymm1_1;
or ymm2_2@uint64 ymm2_2 ymm1_2;
or ymm2_3@uint64 ymm2_3 ymm1_3;
(* vpor   %ymm3,%ymm9,%ymm1                        #! PC = 0x555555578c4e *)
or ymm1_0@uint64 ymm9_0 ymm3_0;
or ymm1_1@uint64 ymm9_1 ymm3_1;
or ymm1_2@uint64 ymm9_2 ymm3_2;
or ymm1_3@uint64 ymm9_3 ymm3_3;
(* vpandn %ymm1,%ymm2,%ymm3                        #! PC = 0x555555578c52 *)
not ymm2_0n@uint64 ymm2_0;
and ymm3_0@uint64 ymm2_0n ymm1_0;
not ymm2_1n@uint64 ymm2_1;
and ymm3_1@uint64 ymm2_1n ymm1_1;
not ymm2_2n@uint64 ymm2_2;
and ymm3_2@uint64 ymm2_2n ymm1_2;
not ymm2_3n@uint64 ymm2_3;
and ymm3_3@uint64 ymm2_3n ymm1_3;
(* vpxor  %ymm0,%ymm3,%ymm12                       #! PC = 0x555555578c56 *)
xor ymm12_0@uint64 ymm3_0 ymm0_0;
xor ymm12_1@uint64 ymm3_1 ymm0_1;
xor ymm12_2@uint64 ymm3_2 ymm0_2;
xor ymm12_3@uint64 ymm3_3 ymm0_3;
(* vpxor  -0x50(%rbp),%ymm11,%ymm3                 #! EA = L0x7fffffffbf00; Value = 0x965393ca1b42f1db; PC = 0x555555578c5a *)
xor ymm3_0@uint64 ymm11_0 L0x7fffffffbf00;
xor ymm3_1@uint64 ymm11_1 L0x7fffffffbf08;
xor ymm3_2@uint64 ymm11_2 L0x7fffffffbf10;
xor ymm3_3@uint64 ymm11_3 L0x7fffffffbf18;
(* vmovdqa %ymm12,-0xd0(%rbp)                      #! EA = L0x7fffffffbe80; PC = 0x555555578c5f *)
mov L0x7fffffffbe80 ymm12_0;
mov L0x7fffffffbe88 ymm12_1;
mov L0x7fffffffbe90 ymm12_2;
mov L0x7fffffffbe98 ymm12_3;
(* vpshufb 0x55290(%rip),%ymm3,%ymm3        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x555555578c67 *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm3_0, ymm3_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm3_2, ymm3_3, tmp_2, tmp_3);
mov ymm3_0 tmp_0;
mov ymm3_1 tmp_1;
mov ymm3_2 tmp_2;
mov ymm3_3 tmp_3;
(* vpandn %ymm3,%ymm1,%ymm7                        #! PC = 0x555555578c70 *)
not ymm1_0n@uint64 ymm1_0;
and ymm7_0@uint64 ymm1_0n ymm3_0;
not ymm1_1n@uint64 ymm1_1;
and ymm7_1@uint64 ymm1_1n ymm3_1;
not ymm1_2n@uint64 ymm1_2;
and ymm7_2@uint64 ymm1_2n ymm3_2;
not ymm1_3n@uint64 ymm1_3;
and ymm7_3@uint64 ymm1_3n ymm3_3;
(* vpxor  %ymm2,%ymm7,%ymm10                       #! PC = 0x555555578c74 *)
xor ymm10_0@uint64 ymm7_0 ymm2_0;
xor ymm10_1@uint64 ymm7_1 ymm2_1;
xor ymm10_2@uint64 ymm7_2 ymm2_2;
xor ymm10_3@uint64 ymm7_3 ymm2_3;
(* vpsrlq $0x2e,%ymm6,%ymm7                        #! PC = 0x555555578c78 *)
shr ymm7_0 ymm6_0 0x2e@uint64;
shr ymm7_1 ymm6_1 0x2e@uint64;
shr ymm7_2 ymm6_2 0x2e@uint64;
shr ymm7_3 ymm6_3 0x2e@uint64;
(* vpsllq $0x12,%ymm6,%ymm6                        #! PC = 0x555555578c7d *)
shl ymm6_0 ymm6_0 0x12@uint64;
shl ymm6_1 ymm6_1 0x12@uint64;
shl ymm6_2 ymm6_2 0x12@uint64;
shl ymm6_3 ymm6_3 0x12@uint64;
(* vmovdqa %ymm10,-0x1d0(%rbp)                     #! EA = L0x7fffffffbd80; PC = 0x555555578c82 *)
mov L0x7fffffffbd80 ymm10_0;
mov L0x7fffffffbd88 ymm10_1;
mov L0x7fffffffbd90 ymm10_2;
mov L0x7fffffffbd98 ymm10_3;
(* vpxor  -0x70(%rbp),%ymm14,%ymm10                #! EA = L0x7fffffffbee0; Value = 0xf377844620f2d499; PC = 0x555555578c8a *)
xor ymm10_0@uint64 ymm14_0 L0x7fffffffbee0;
xor ymm10_1@uint64 ymm14_1 L0x7fffffffbee8;
xor ymm10_2@uint64 ymm14_2 L0x7fffffffbef0;
xor ymm10_3@uint64 ymm14_3 L0x7fffffffbef8;
(* vpor   %ymm7,%ymm6,%ymm12                       #! PC = 0x555555578c8f *)
or ymm12_0@uint64 ymm6_0 ymm7_0;
or ymm12_1@uint64 ymm6_1 ymm7_1;
or ymm12_2@uint64 ymm6_2 ymm7_2;
or ymm12_3@uint64 ymm6_3 ymm7_3;
(* vpxor  -0x190(%rbp),%ymm4,%ymm7                 #! EA = L0x7fffffffbdc0; Value = 0xa81edbeb54d20fe1; PC = 0x555555578c93 *)
xor ymm7_0@uint64 ymm4_0 L0x7fffffffbdc0;
xor ymm7_1@uint64 ymm4_1 L0x7fffffffbdc8;
xor ymm7_2@uint64 ymm4_2 L0x7fffffffbdd0;
xor ymm7_3@uint64 ymm4_3 L0x7fffffffbdd8;
(* vpandn %ymm12,%ymm3,%ymm9                       #! PC = 0x555555578c9b *)
not ymm3_0n@uint64 ymm3_0;
and ymm9_0@uint64 ymm3_0n ymm12_0;
not ymm3_1n@uint64 ymm3_1;
and ymm9_1@uint64 ymm3_1n ymm12_1;
not ymm3_2n@uint64 ymm3_2;
and ymm9_2@uint64 ymm3_2n ymm12_2;
not ymm3_3n@uint64 ymm3_3;
and ymm9_3@uint64 ymm3_3n ymm12_3;
(* vpxor  %ymm1,%ymm9,%ymm9                        #! PC = 0x555555578ca0 *)
xor ymm9_0@uint64 ymm9_0 ymm1_0;
xor ymm9_1@uint64 ymm9_1 ymm1_1;
xor ymm9_2@uint64 ymm9_2 ymm1_2;
xor ymm9_3@uint64 ymm9_3 ymm1_3;
(* vpandn %ymm0,%ymm12,%ymm1                       #! PC = 0x555555578ca4 *)
not ymm12_0n@uint64 ymm12_0;
and ymm1_0@uint64 ymm12_0n ymm0_0;
not ymm12_1n@uint64 ymm12_1;
and ymm1_1@uint64 ymm12_1n ymm0_1;
not ymm12_2n@uint64 ymm12_2;
and ymm1_2@uint64 ymm12_2n ymm0_2;
not ymm12_3n@uint64 ymm12_3;
and ymm1_3@uint64 ymm12_3n ymm0_3;
(* vpandn %ymm2,%ymm0,%ymm0                        #! PC = 0x555555578ca8 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm2_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm2_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm2_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm2_3;
(* vpxor  %ymm3,%ymm1,%ymm15                       #! PC = 0x555555578cac *)
xor ymm15_0@uint64 ymm1_0 ymm3_0;
xor ymm15_1@uint64 ymm1_1 ymm3_1;
xor ymm15_2@uint64 ymm1_2 ymm3_2;
xor ymm15_3@uint64 ymm1_3 ymm3_3;
(* vpxor  -0x2b0(%rbp),%ymm11,%ymm1                #! EA = L0x7fffffffbca0; Value = 0x9b5e7a3feeb7e41e; PC = 0x555555578cb0 *)
xor ymm1_0@uint64 ymm11_0 L0x7fffffffbca0;
xor ymm1_1@uint64 ymm11_1 L0x7fffffffbca8;
xor ymm1_2@uint64 ymm11_2 L0x7fffffffbcb0;
xor ymm1_3@uint64 ymm11_3 L0x7fffffffbcb8;
(* vpxor  %ymm12,%ymm0,%ymm0                       #! PC = 0x555555578cb8 *)
xor ymm0_0@uint64 ymm0_0 ymm12_0;
xor ymm0_1@uint64 ymm0_1 ymm12_1;
xor ymm0_2@uint64 ymm0_2 ymm12_2;
xor ymm0_3@uint64 ymm0_3 ymm12_3;
(* vmovdqa %ymm0,-0x170(%rbp)                      #! EA = L0x7fffffffbde0; PC = 0x555555578cbd *)
mov L0x7fffffffbde0 ymm0_0;
mov L0x7fffffffbde8 ymm0_1;
mov L0x7fffffffbdf0 ymm0_2;
mov L0x7fffffffbdf8 ymm0_3;
(* vpsrlq $0x36,%ymm7,%ymm3                        #! PC = 0x555555578cc5 *)
shr ymm3_0 ymm7_0 0x36@uint64;
shr ymm3_1 ymm7_1 0x36@uint64;
shr ymm3_2 ymm7_2 0x36@uint64;
shr ymm3_3 ymm7_3 0x36@uint64;
(* vpsllq $0xa,%ymm7,%ymm7                         #! PC = 0x555555578cca *)
shl ymm7_0 ymm7_0 0xa@uint64;
shl ymm7_1 ymm7_1 0xa@uint64;
shl ymm7_2 ymm7_2 0xa@uint64;
shl ymm7_3 ymm7_3 0xa@uint64;
(* vpsrlq $0x25,%ymm1,%ymm2                        #! PC = 0x555555578ccf *)
shr ymm2_0 ymm1_0 0x25@uint64;
shr ymm2_1 ymm1_1 0x25@uint64;
shr ymm2_2 ymm1_2 0x25@uint64;
shr ymm2_3 ymm1_3 0x25@uint64;
(* vpsllq $0x1b,%ymm1,%ymm0                        #! PC = 0x555555578cd4 *)
shl ymm0_0 ymm1_0 0x1b@uint64;
shl ymm0_1 ymm1_1 0x1b@uint64;
shl ymm0_2 ymm1_2 0x1b@uint64;
shl ymm0_3 ymm1_3 0x1b@uint64;
(* vpxor  -0x1f0(%rbp),%ymm5,%ymm1                 #! EA = L0x7fffffffbd60; Value = 0x73fe33c9b1038c36; PC = 0x555555578cd9 *)
xor ymm1_0@uint64 ymm5_0 L0x7fffffffbd60;
xor ymm1_1@uint64 ymm5_1 L0x7fffffffbd68;
xor ymm1_2@uint64 ymm5_2 L0x7fffffffbd70;
xor ymm1_3@uint64 ymm5_3 L0x7fffffffbd78;
(* vmovdqa %ymm15,-0xb0(%rbp)                      #! EA = L0x7fffffffbea0; PC = 0x555555578ce1 *)
mov L0x7fffffffbea0 ymm15_0;
mov L0x7fffffffbea8 ymm15_1;
mov L0x7fffffffbeb0 ymm15_2;
mov L0x7fffffffbeb8 ymm15_3;
(* vpor   %ymm2,%ymm0,%ymm0                        #! PC = 0x555555578ce9 *)
or ymm0_0@uint64 ymm0_0 ymm2_0;
or ymm0_1@uint64 ymm0_1 ymm2_1;
or ymm0_2@uint64 ymm0_2 ymm2_2;
or ymm0_3@uint64 ymm0_3 ymm2_3;
(* vpsrlq $0x1c,%ymm1,%ymm2                        #! PC = 0x555555578ced *)
shr ymm2_0 ymm1_0 0x1c@uint64;
shr ymm2_1 ymm1_1 0x1c@uint64;
shr ymm2_2 ymm1_2 0x1c@uint64;
shr ymm2_3 ymm1_3 0x1c@uint64;
(* vpsllq $0x24,%ymm1,%ymm1                        #! PC = 0x555555578cf2 *)
shl ymm1_0 ymm1_0 0x24@uint64;
shl ymm1_1 ymm1_1 0x24@uint64;
shl ymm1_2 ymm1_2 0x24@uint64;
shl ymm1_3 ymm1_3 0x24@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x555555578cf7 *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpor   %ymm3,%ymm7,%ymm2                        #! PC = 0x555555578cfb *)
or ymm2_0@uint64 ymm7_0 ymm3_0;
or ymm2_1@uint64 ymm7_1 ymm3_1;
or ymm2_2@uint64 ymm7_2 ymm3_2;
or ymm2_3@uint64 ymm7_3 ymm3_3;
(* vpandn %ymm2,%ymm1,%ymm3                        #! PC = 0x555555578cff *)
not ymm1_0n@uint64 ymm1_0;
and ymm3_0@uint64 ymm1_0n ymm2_0;
not ymm1_1n@uint64 ymm1_1;
and ymm3_1@uint64 ymm1_1n ymm2_1;
not ymm1_2n@uint64 ymm1_2;
and ymm3_2@uint64 ymm1_2n ymm2_2;
not ymm1_3n@uint64 ymm1_3;
and ymm3_3@uint64 ymm1_3n ymm2_3;
(* vpxor  %ymm0,%ymm3,%ymm7                        #! PC = 0x555555578d03 *)
xor ymm7_0@uint64 ymm3_0 ymm0_0;
xor ymm7_1@uint64 ymm3_1 ymm0_1;
xor ymm7_2@uint64 ymm3_2 ymm0_2;
xor ymm7_3@uint64 ymm3_3 ymm0_3;
(* vpsrlq $0x31,%ymm10,%ymm3                       #! PC = 0x555555578d07 *)
shr ymm3_0 ymm10_0 0x31@uint64;
shr ymm3_1 ymm10_1 0x31@uint64;
shr ymm3_2 ymm10_2 0x31@uint64;
shr ymm3_3 ymm10_3 0x31@uint64;
(* vpsllq $0xf,%ymm10,%ymm10                       #! PC = 0x555555578d0d *)
shl ymm10_0 ymm10_0 0xf@uint64;
shl ymm10_1 ymm10_1 0xf@uint64;
shl ymm10_2 ymm10_2 0xf@uint64;
shl ymm10_3 ymm10_3 0xf@uint64;
(* vmovdqa %ymm7,%ymm12                            #! PC = 0x555555578d13 *)
mov ymm12_0 ymm7_0;
mov ymm12_1 ymm7_1;
mov ymm12_2 ymm7_2;
mov ymm12_3 ymm7_3;
(* vpor   %ymm3,%ymm10,%ymm10                      #! PC = 0x555555578d17 *)
or ymm10_0@uint64 ymm10_0 ymm3_0;
or ymm10_1@uint64 ymm10_1 ymm3_1;
or ymm10_2@uint64 ymm10_2 ymm3_2;
or ymm10_3@uint64 ymm10_3 ymm3_3;
(* vpxor  -0x2f0(%rbp),%ymm13,%ymm3                #! EA = L0x7fffffffbc60; Value = 0xe5dc85ffad78d9b0; PC = 0x555555578d1b *)
xor ymm3_0@uint64 ymm13_0 L0x7fffffffbc60;
xor ymm3_1@uint64 ymm13_1 L0x7fffffffbc68;
xor ymm3_2@uint64 ymm13_2 L0x7fffffffbc70;
xor ymm3_3@uint64 ymm13_3 L0x7fffffffbc78;
(* vpxor  -0x1b0(%rbp),%ymm13,%ymm13               #! EA = L0x7fffffffbda0; Value = 0xd95c0a4ec94ad619; PC = 0x555555578d23 *)
xor ymm13_0@uint64 ymm13_0 L0x7fffffffbda0;
xor ymm13_1@uint64 ymm13_1 L0x7fffffffbda8;
xor ymm13_2@uint64 ymm13_2 L0x7fffffffbdb0;
xor ymm13_3@uint64 ymm13_3 L0x7fffffffbdb8;
(* vmovdqa %ymm12,-0x350(%rbp)                     #! EA = L0x7fffffffbc00; PC = 0x555555578d2b *)
mov L0x7fffffffbc00 ymm12_0;
mov L0x7fffffffbc08 ymm12_1;
mov L0x7fffffffbc10 ymm12_2;
mov L0x7fffffffbc18 ymm12_3;
(* vpxor  -0x150(%rbp),%ymm11,%ymm11               #! EA = L0x7fffffffbe00; Value = 0xa1c3cee067ab9f52; PC = 0x555555578d33 *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbe00;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbe08;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbe10;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbe18;
(* vpandn %ymm10,%ymm2,%ymm7                       #! PC = 0x555555578d3b *)
not ymm2_0n@uint64 ymm2_0;
and ymm7_0@uint64 ymm2_0n ymm10_0;
not ymm2_1n@uint64 ymm2_1;
and ymm7_1@uint64 ymm2_1n ymm10_1;
not ymm2_2n@uint64 ymm2_2;
and ymm7_2@uint64 ymm2_2n ymm10_2;
not ymm2_3n@uint64 ymm2_3;
and ymm7_3@uint64 ymm2_3n ymm10_3;
(* vpxor  -0x2d0(%rbp),%ymm14,%ymm14               #! EA = L0x7fffffffbc80; Value = 0x6db98725571f1604; PC = 0x555555578d40 *)
xor ymm14_0@uint64 ymm14_0 L0x7fffffffbc80;
xor ymm14_1@uint64 ymm14_1 L0x7fffffffbc88;
xor ymm14_2@uint64 ymm14_2 L0x7fffffffbc90;
xor ymm14_3@uint64 ymm14_3 L0x7fffffffbc98;
(* vpshufb 0x5518f(%rip),%ymm3,%ymm3        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x555555578d48 *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm3_0, ymm3_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm3_2, ymm3_3, tmp_2, tmp_3);
mov ymm3_0 tmp_0;
mov ymm3_1 tmp_1;
mov ymm3_2 tmp_2;
mov ymm3_3 tmp_3;
(* vpxor  %ymm1,%ymm7,%ymm7                        #! PC = 0x555555578d51 *)
xor ymm7_0@uint64 ymm7_0 ymm1_0;
xor ymm7_1@uint64 ymm7_1 ymm1_1;
xor ymm7_2@uint64 ymm7_2 ymm1_2;
xor ymm7_3@uint64 ymm7_3 ymm1_3;
(* vpxor  -0x330(%rbp),%ymm5,%ymm5                 #! EA = L0x7fffffffbc20; Value = 0xb8e36f84d019b15f; PC = 0x555555578d55 *)
xor ymm5_0@uint64 ymm5_0 L0x7fffffffbc20;
xor ymm5_1@uint64 ymm5_1 L0x7fffffffbc28;
xor ymm5_2@uint64 ymm5_2 L0x7fffffffbc30;
xor ymm5_3@uint64 ymm5_3 L0x7fffffffbc38;
(* vpandn %ymm3,%ymm10,%ymm6                       #! PC = 0x555555578d5d *)
not ymm10_0n@uint64 ymm10_0;
and ymm6_0@uint64 ymm10_0n ymm3_0;
not ymm10_1n@uint64 ymm10_1;
and ymm6_1@uint64 ymm10_1n ymm3_1;
not ymm10_2n@uint64 ymm10_2;
and ymm6_2@uint64 ymm10_2n ymm3_2;
not ymm10_3n@uint64 ymm10_3;
and ymm6_3@uint64 ymm10_3n ymm3_3;
(* vpxor  -0x310(%rbp),%ymm4,%ymm4                 #! EA = L0x7fffffffbc40; Value = 0xab89b3623f5f6964; PC = 0x555555578d61 *)
xor ymm4_0@uint64 ymm4_0 L0x7fffffffbc40;
xor ymm4_1@uint64 ymm4_1 L0x7fffffffbc48;
xor ymm4_2@uint64 ymm4_2 L0x7fffffffbc50;
xor ymm4_3@uint64 ymm4_3 L0x7fffffffbc58;
(* vpxor  %ymm2,%ymm6,%ymm15                       #! PC = 0x555555578d69 *)
xor ymm15_0@uint64 ymm6_0 ymm2_0;
xor ymm15_1@uint64 ymm6_1 ymm2_1;
xor ymm15_2@uint64 ymm6_2 ymm2_2;
xor ymm15_3@uint64 ymm6_3 ymm2_3;
(* vpandn %ymm0,%ymm3,%ymm2                        #! PC = 0x555555578d6d *)
not ymm3_0n@uint64 ymm3_0;
and ymm2_0@uint64 ymm3_0n ymm0_0;
not ymm3_1n@uint64 ymm3_1;
and ymm2_1@uint64 ymm3_1n ymm0_1;
not ymm3_2n@uint64 ymm3_2;
and ymm2_2@uint64 ymm3_2n ymm0_2;
not ymm3_3n@uint64 ymm3_3;
and ymm2_3@uint64 ymm3_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x555555578d71 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm3,%ymm0,%ymm6                        #! PC = 0x555555578d75 *)
xor ymm6_0@uint64 ymm0_0 ymm3_0;
xor ymm6_1@uint64 ymm0_1 ymm3_1;
xor ymm6_2@uint64 ymm0_2 ymm3_2;
xor ymm6_3@uint64 ymm0_3 ymm3_3;
(* vpsrlq $0x9,%ymm13,%ymm0                        #! PC = 0x555555578d79 *)
shr ymm0_0 ymm13_0 0x9@uint64;
shr ymm0_1 ymm13_1 0x9@uint64;
shr ymm0_2 ymm13_2 0x9@uint64;
shr ymm0_3 ymm13_3 0x9@uint64;
(* vmovdqa %ymm15,-0x70(%rbp)                      #! EA = L0x7fffffffbee0; PC = 0x555555578d7f *)
mov L0x7fffffffbee0 ymm15_0;
mov L0x7fffffffbee8 ymm15_1;
mov L0x7fffffffbef0 ymm15_2;
mov L0x7fffffffbef8 ymm15_3;
(* vpxor  %ymm10,%ymm2,%ymm10                      #! PC = 0x555555578d84 *)
xor ymm10_0@uint64 ymm2_0 ymm10_0;
xor ymm10_1@uint64 ymm2_1 ymm10_1;
xor ymm10_2@uint64 ymm2_2 ymm10_2;
xor ymm10_3@uint64 ymm2_3 ymm10_3;
(* vpsllq $0x37,%ymm13,%ymm13                      #! PC = 0x555555578d89 *)
shl ymm13_0 ymm13_0 0x37@uint64;
shl ymm13_1 ymm13_1 0x37@uint64;
shl ymm13_2 ymm13_2 0x37@uint64;
shl ymm13_3 ymm13_3 0x37@uint64;
(* vpsrlq $0x2,%ymm14,%ymm1                        #! PC = 0x555555578d8f *)
shr ymm1_0 ymm14_0 0x2@uint64;
shr ymm1_1 ymm14_1 0x2@uint64;
shr ymm1_2 ymm14_2 0x2@uint64;
shr ymm1_3 ymm14_3 0x2@uint64;
(* vmovdqa %ymm6,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x555555578d95 *)
mov L0x7fffffffbf00 ymm6_0;
mov L0x7fffffffbf08 ymm6_1;
mov L0x7fffffffbf10 ymm6_2;
mov L0x7fffffffbf18 ymm6_3;
(* vmovdqa -0xd0(%rbp),%ymm15                      #! EA = L0x7fffffffbe80; Value = 0x2d6cbeb27c4bc219; PC = 0x555555578d9a *)
mov ymm15_0 L0x7fffffffbe80;
mov ymm15_1 L0x7fffffffbe88;
mov ymm15_2 L0x7fffffffbe90;
mov ymm15_3 L0x7fffffffbe98;
(* vpor   %ymm0,%ymm13,%ymm13                      #! PC = 0x555555578da2 *)
or ymm13_0@uint64 ymm13_0 ymm0_0;
or ymm13_1@uint64 ymm13_1 ymm0_1;
or ymm13_2@uint64 ymm13_2 ymm0_2;
or ymm13_3@uint64 ymm13_3 ymm0_3;
(* vpsrlq $0x19,%ymm11,%ymm0                       #! PC = 0x555555578da6 *)
shr ymm0_0 ymm11_0 0x19@uint64;
shr ymm0_1 ymm11_1 0x19@uint64;
shr ymm0_2 ymm11_2 0x19@uint64;
shr ymm0_3 ymm11_3 0x19@uint64;
(* vpsllq $0x27,%ymm11,%ymm11                      #! PC = 0x555555578dac *)
shl ymm11_0 ymm11_0 0x27@uint64;
shl ymm11_1 ymm11_1 0x27@uint64;
shl ymm11_2 ymm11_2 0x27@uint64;
shl ymm11_3 ymm11_3 0x27@uint64;
(* vpsllq $0x3e,%ymm14,%ymm14                      #! PC = 0x555555578db2 *)
shl ymm14_0 ymm14_0 0x3e@uint64;
shl ymm14_1 ymm14_1 0x3e@uint64;
shl ymm14_2 ymm14_2 0x3e@uint64;
shl ymm14_3 ymm14_3 0x3e@uint64;
(* vpor   %ymm0,%ymm11,%ymm11                      #! PC = 0x555555578db8 *)
or ymm11_0@uint64 ymm11_0 ymm0_0;
or ymm11_1@uint64 ymm11_1 ymm0_1;
or ymm11_2@uint64 ymm11_2 ymm0_2;
or ymm11_3@uint64 ymm11_3 ymm0_3;
(* vpor   %ymm1,%ymm14,%ymm3                       #! PC = 0x555555578dbc *)
or ymm3_0@uint64 ymm14_0 ymm1_0;
or ymm3_1@uint64 ymm14_1 ymm1_1;
or ymm3_2@uint64 ymm14_2 ymm1_2;
or ymm3_3@uint64 ymm14_3 ymm1_3;
(* vpxor  -0x230(%rbp),%ymm15,%ymm0                #! EA = L0x7fffffffbd20; Value = 0x0b5033314f45c5cb; PC = 0x555555578dc0 *)
xor ymm0_0@uint64 ymm15_0 L0x7fffffffbd20;
xor ymm0_1@uint64 ymm15_1 L0x7fffffffbd28;
xor ymm0_2@uint64 ymm15_2 L0x7fffffffbd30;
xor ymm0_3@uint64 ymm15_3 L0x7fffffffbd38;
(* vpandn %ymm11,%ymm13,%ymm6                      #! PC = 0x555555578dc8 *)
not ymm13_0n@uint64 ymm13_0;
and ymm6_0@uint64 ymm13_0n ymm11_0;
not ymm13_1n@uint64 ymm13_1;
and ymm6_1@uint64 ymm13_1n ymm11_1;
not ymm13_2n@uint64 ymm13_2;
and ymm6_2@uint64 ymm13_2n ymm11_2;
not ymm13_3n@uint64 ymm13_3;
and ymm6_3@uint64 ymm13_3n ymm11_3;
(* vpsrlq $0x3e,%ymm4,%ymm1                        #! PC = 0x555555578dcd *)
shr ymm1_0 ymm4_0 0x3e@uint64;
shr ymm1_1 ymm4_1 0x3e@uint64;
shr ymm1_2 ymm4_2 0x3e@uint64;
shr ymm1_3 ymm4_3 0x3e@uint64;
(* vpxor  -0x70(%rbp),%ymm9,%ymm15                 #! EA = L0x7fffffffbee0; Value = 0x375463e874af271c; PC = 0x555555578dd2 *)
xor ymm15_0@uint64 ymm9_0 L0x7fffffffbee0;
xor ymm15_1@uint64 ymm9_1 L0x7fffffffbee8;
xor ymm15_2@uint64 ymm9_2 L0x7fffffffbef0;
xor ymm15_3@uint64 ymm9_3 L0x7fffffffbef8;
(* vpxor  %ymm3,%ymm6,%ymm6                        #! PC = 0x555555578dd7 *)
xor ymm6_0@uint64 ymm6_0 ymm3_0;
xor ymm6_1@uint64 ymm6_1 ymm3_1;
xor ymm6_2@uint64 ymm6_2 ymm3_2;
xor ymm6_3@uint64 ymm6_3 ymm3_3;
(* vpsllq $0x2,%ymm4,%ymm4                         #! PC = 0x555555578ddb *)
shl ymm4_0 ymm4_0 0x2@uint64;
shl ymm4_1 ymm4_1 0x2@uint64;
shl ymm4_2 ymm4_2 0x2@uint64;
shl ymm4_3 ymm4_3 0x2@uint64;
(* vpxor  %ymm12,%ymm6,%ymm2                       #! PC = 0x555555578de0 *)
xor ymm2_0@uint64 ymm6_0 ymm12_0;
xor ymm2_1@uint64 ymm6_1 ymm12_1;
xor ymm2_2@uint64 ymm6_2 ymm12_2;
xor ymm2_3@uint64 ymm6_3 ymm12_3;
(* vpxor  %ymm0,%ymm2,%ymm2                        #! PC = 0x555555578de5 *)
xor ymm2_0@uint64 ymm2_0 ymm0_0;
xor ymm2_1@uint64 ymm2_1 ymm0_1;
xor ymm2_2@uint64 ymm2_2 ymm0_2;
xor ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x17,%ymm5,%ymm0                        #! PC = 0x555555578de9 *)
shr ymm0_0 ymm5_0 0x17@uint64;
shr ymm0_1 ymm5_1 0x17@uint64;
shr ymm0_2 ymm5_2 0x17@uint64;
shr ymm0_3 ymm5_3 0x17@uint64;
(* vpxor  -0x90(%rbp),%ymm2,%ymm2                  #! EA = L0x7fffffffbec0; Value = 0x43e30b96ff110a58; PC = 0x555555578dee *)
xor ymm2_0@uint64 ymm2_0 L0x7fffffffbec0;
xor ymm2_1@uint64 ymm2_1 L0x7fffffffbec8;
xor ymm2_2@uint64 ymm2_2 L0x7fffffffbed0;
xor ymm2_3@uint64 ymm2_3 L0x7fffffffbed8;
(* vpsllq $0x29,%ymm5,%ymm5                        #! PC = 0x555555578df6 *)
shl ymm5_0 ymm5_0 0x29@uint64;
shl ymm5_1 ymm5_1 0x29@uint64;
shl ymm5_2 ymm5_2 0x29@uint64;
shl ymm5_3 ymm5_3 0x29@uint64;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x555555578dfb *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpandn %ymm5,%ymm11,%ymm0                       #! PC = 0x555555578dff *)
not ymm11_0n@uint64 ymm11_0;
and ymm0_0@uint64 ymm11_0n ymm5_0;
not ymm11_1n@uint64 ymm11_1;
and ymm0_1@uint64 ymm11_1n ymm5_1;
not ymm11_2n@uint64 ymm11_2;
and ymm0_2@uint64 ymm11_2n ymm5_2;
not ymm11_3n@uint64 ymm11_3;
and ymm0_3@uint64 ymm11_3n ymm5_3;
(* vpxor  %ymm13,%ymm0,%ymm14                      #! PC = 0x555555578e03 *)
xor ymm14_0@uint64 ymm0_0 ymm13_0;
xor ymm14_1@uint64 ymm0_1 ymm13_1;
xor ymm14_2@uint64 ymm0_2 ymm13_2;
xor ymm14_3@uint64 ymm0_3 ymm13_3;
(* vpxor  -0x290(%rbp),%ymm8,%ymm0                 #! EA = L0x7fffffffbcc0; Value = 0xd642c7df22b4173c; PC = 0x555555578e08 *)
xor ymm0_0@uint64 ymm8_0 L0x7fffffffbcc0;
xor ymm0_1@uint64 ymm8_1 L0x7fffffffbcc8;
xor ymm0_2@uint64 ymm8_2 L0x7fffffffbcd0;
xor ymm0_3@uint64 ymm8_3 L0x7fffffffbcd8;
(* vmovdqa %ymm14,%ymm12                           #! PC = 0x555555578e10 *)
mov ymm12_0 ymm14_0;
mov ymm12_1 ymm14_1;
mov ymm12_2 ymm14_2;
mov ymm12_3 ymm14_3;
(* vpxor  -0x1d0(%rbp),%ymm7,%ymm14                #! EA = L0x7fffffffbd80; Value = 0xa58159b967a3ad93; PC = 0x555555578e15 *)
xor ymm14_0@uint64 ymm7_0 L0x7fffffffbd80;
xor ymm14_1@uint64 ymm7_1 L0x7fffffffbd88;
xor ymm14_2@uint64 ymm7_2 L0x7fffffffbd90;
xor ymm14_3@uint64 ymm7_3 L0x7fffffffbd98;
(* vmovdqa %ymm12,-0x330(%rbp)                     #! EA = L0x7fffffffbc20; PC = 0x555555578e1d *)
mov L0x7fffffffbc20 ymm12_0;
mov L0x7fffffffbc28 ymm12_1;
mov L0x7fffffffbc30 ymm12_2;
mov L0x7fffffffbc38 ymm12_3;
(* vpxor  %ymm0,%ymm14,%ymm14                      #! PC = 0x555555578e25 *)
xor ymm14_0@uint64 ymm14_0 ymm0_0;
xor ymm14_1@uint64 ymm14_1 ymm0_1;
xor ymm14_2@uint64 ymm14_2 ymm0_2;
xor ymm14_3@uint64 ymm14_3 ymm0_3;
(* vpor   %ymm1,%ymm4,%ymm0                        #! PC = 0x555555578e29 *)
or ymm0_0@uint64 ymm4_0 ymm1_0;
or ymm0_1@uint64 ymm4_1 ymm1_1;
or ymm0_2@uint64 ymm4_2 ymm1_2;
or ymm0_3@uint64 ymm4_3 ymm1_3;
(* vpandn %ymm13,%ymm3,%ymm4                       #! PC = 0x555555578e2d *)
not ymm3_0n@uint64 ymm3_0;
and ymm4_0@uint64 ymm3_0n ymm13_0;
not ymm3_1n@uint64 ymm3_1;
and ymm4_1@uint64 ymm3_1n ymm13_1;
not ymm3_2n@uint64 ymm3_2;
and ymm4_2@uint64 ymm3_2n ymm13_2;
not ymm3_3n@uint64 ymm3_3;
and ymm4_3@uint64 ymm3_3n ymm13_3;
(* vmovdqa -0xf0(%rbp),%ymm13                      #! EA = L0x7fffffffbe60; Value = 0x7a8e099fe258e5d8; PC = 0x555555578e32 *)
mov ymm13_0 L0x7fffffffbe60;
mov ymm13_1 L0x7fffffffbe68;
mov ymm13_2 L0x7fffffffbe70;
mov ymm13_3 L0x7fffffffbe78;
(* vpandn %ymm0,%ymm5,%ymm1                        #! PC = 0x555555578e3a *)
not ymm5_0n@uint64 ymm5_0;
and ymm1_0@uint64 ymm5_0n ymm0_0;
not ymm5_1n@uint64 ymm5_1;
and ymm1_1@uint64 ymm5_1n ymm0_1;
not ymm5_2n@uint64 ymm5_2;
and ymm1_2@uint64 ymm5_2n ymm0_2;
not ymm5_3n@uint64 ymm5_3;
and ymm1_3@uint64 ymm5_3n ymm0_3;
(* vpxor  %ymm12,%ymm14,%ymm14                     #! PC = 0x555555578e3e *)
xor ymm14_0@uint64 ymm14_0 ymm12_0;
xor ymm14_1@uint64 ymm14_1 ymm12_1;
xor ymm14_2@uint64 ymm14_2 ymm12_2;
xor ymm14_3@uint64 ymm14_3 ymm12_3;
(* vpxor  %ymm0,%ymm4,%ymm4                        #! PC = 0x555555578e43 *)
xor ymm4_0@uint64 ymm4_0 ymm0_0;
xor ymm4_1@uint64 ymm4_1 ymm0_1;
xor ymm4_2@uint64 ymm4_2 ymm0_2;
xor ymm4_3@uint64 ymm4_3 ymm0_3;
(* vpxor  %ymm11,%ymm1,%ymm11                      #! PC = 0x555555578e47 *)
xor ymm11_0@uint64 ymm1_0 ymm11_0;
xor ymm11_1@uint64 ymm1_1 ymm11_1;
xor ymm11_2@uint64 ymm1_2 ymm11_2;
xor ymm11_3@uint64 ymm1_3 ymm11_3;
(* vmovdqa -0x110(%rbp),%ymm1                      #! EA = L0x7fffffffbe40; Value = 0x34fe5a6214181ed1; PC = 0x555555578e4c *)
mov ymm1_0 L0x7fffffffbe40;
mov ymm1_1 L0x7fffffffbe48;
mov ymm1_2 L0x7fffffffbe50;
mov ymm1_3 L0x7fffffffbe58;
(* vpxor  -0x270(%rbp),%ymm1,%ymm1                 #! EA = L0x7fffffffbce0; Value = 0xbfd660dfe2e0051d; PC = 0x555555578e54 *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbce0;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbce8;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbcf0;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbcf8;
(* vpxor  %ymm1,%ymm15,%ymm15                      #! PC = 0x555555578e5c *)
xor ymm15_0@uint64 ymm15_0 ymm1_0;
xor ymm15_1@uint64 ymm15_1 ymm1_1;
xor ymm15_2@uint64 ymm15_2 ymm1_2;
xor ymm15_3@uint64 ymm15_3 ymm1_3;
(* vpandn %ymm3,%ymm0,%ymm1                        #! PC = 0x555555578e60 *)
not ymm0_0n@uint64 ymm0_0;
and ymm1_0@uint64 ymm0_0n ymm3_0;
not ymm0_1n@uint64 ymm0_1;
and ymm1_1@uint64 ymm0_1n ymm3_1;
not ymm0_2n@uint64 ymm0_2;
and ymm1_2@uint64 ymm0_2n ymm3_2;
not ymm0_3n@uint64 ymm0_3;
and ymm1_3@uint64 ymm0_3n ymm3_3;
(* vpxor  -0x250(%rbp),%ymm13,%ymm0                #! EA = L0x7fffffffbd00; Value = 0x37c05e405b01af0c; PC = 0x555555578e64 *)
xor ymm0_0@uint64 ymm13_0 L0x7fffffffbd00;
xor ymm0_1@uint64 ymm13_1 L0x7fffffffbd08;
xor ymm0_2@uint64 ymm13_2 L0x7fffffffbd10;
xor ymm0_3@uint64 ymm13_3 L0x7fffffffbd18;
(* vpxor  %ymm5,%ymm1,%ymm1                        #! PC = 0x555555578e6c *)
xor ymm1_0@uint64 ymm1_0 ymm5_0;
xor ymm1_1@uint64 ymm1_1 ymm5_1;
xor ymm1_2@uint64 ymm1_2 ymm5_2;
xor ymm1_3@uint64 ymm1_3 ymm5_3;
(* vpsllq $0x1,%ymm14,%ymm5                        #! PC = 0x555555578e70 *)
shl ymm5_0 ymm14_0 0x1@uint64;
shl ymm5_1 ymm14_1 0x1@uint64;
shl ymm5_2 ymm14_2 0x1@uint64;
shl ymm5_3 ymm14_3 0x1@uint64;
(* vpxor  %ymm11,%ymm15,%ymm15                     #! PC = 0x555555578e76 *)
xor ymm15_0@uint64 ymm15_0 ymm11_0;
xor ymm15_1@uint64 ymm15_1 ymm11_1;
xor ymm15_2@uint64 ymm15_2 ymm11_2;
xor ymm15_3@uint64 ymm15_3 ymm11_3;
(* vpxor  %ymm1,%ymm10,%ymm12                      #! PC = 0x555555578e7b *)
xor ymm12_0@uint64 ymm10_0 ymm1_0;
xor ymm12_1@uint64 ymm10_1 ymm1_1;
xor ymm12_2@uint64 ymm10_2 ymm1_2;
xor ymm12_3@uint64 ymm10_3 ymm1_3;
(* vmovdqa %ymm1,-0x2b0(%rbp)                      #! EA = L0x7fffffffbca0; PC = 0x555555578e7f *)
mov L0x7fffffffbca0 ymm1_0;
mov L0x7fffffffbca8 ymm1_1;
mov L0x7fffffffbcb0 ymm1_2;
mov L0x7fffffffbcb8 ymm1_3;
(* vpsllq $0x1,%ymm15,%ymm3                        #! PC = 0x555555578e87 *)
shl ymm3_0 ymm15_0 0x1@uint64;
shl ymm3_1 ymm15_1 0x1@uint64;
shl ymm3_2 ymm15_2 0x1@uint64;
shl ymm3_3 ymm15_3 0x1@uint64;
(* vmovdqa -0x210(%rbp),%ymm1                      #! EA = L0x7fffffffbd40; Value = 0xb42abc5de738ddd5; PC = 0x555555578e8d *)
mov ymm1_0 L0x7fffffffbd40;
mov ymm1_1 L0x7fffffffbd48;
mov ymm1_2 L0x7fffffffbd50;
mov ymm1_3 L0x7fffffffbd58;
(* vpxor  -0x130(%rbp),%ymm1,%ymm1                 #! EA = L0x7fffffffbe20; Value = 0xa303b734f55677e6; PC = 0x555555578e95 *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbe20;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbe28;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbe30;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbe38;
(* vpxor  %ymm1,%ymm12,%ymm12                      #! PC = 0x555555578e9d *)
xor ymm12_0@uint64 ymm12_0 ymm1_0;
xor ymm12_1@uint64 ymm12_1 ymm1_1;
xor ymm12_2@uint64 ymm12_2 ymm1_2;
xor ymm12_3@uint64 ymm12_3 ymm1_3;
(* vpxor  -0x170(%rbp),%ymm4,%ymm1                 #! EA = L0x7fffffffbde0; Value = 0x5e8f6ad5d330526e; PC = 0x555555578ea1 *)
xor ymm1_0@uint64 ymm4_0 L0x7fffffffbde0;
xor ymm1_1@uint64 ymm4_1 L0x7fffffffbde8;
xor ymm1_2@uint64 ymm4_2 L0x7fffffffbdf0;
xor ymm1_3@uint64 ymm4_3 L0x7fffffffbdf8;
(* vpxor  -0xb0(%rbp),%ymm12,%ymm12                #! EA = L0x7fffffffbea0; Value = 0x5f252e8e367f3dc8; PC = 0x555555578ea9 *)
xor ymm12_0@uint64 ymm12_0 L0x7fffffffbea0;
xor ymm12_1@uint64 ymm12_1 L0x7fffffffbea8;
xor ymm12_2@uint64 ymm12_2 L0x7fffffffbeb0;
xor ymm12_3@uint64 ymm12_3 L0x7fffffffbeb8;
(* vpxor  %ymm0,%ymm1,%ymm1                        #! PC = 0x555555578eb1 *)
xor ymm1_0@uint64 ymm1_0 ymm0_0;
xor ymm1_1@uint64 ymm1_1 ymm0_1;
xor ymm1_2@uint64 ymm1_2 ymm0_2;
xor ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3f,%ymm14,%ymm0                       #! PC = 0x555555578eb5 *)
shr ymm0_0 ymm14_0 0x3f@uint64;
shr ymm0_1 ymm14_1 0x3f@uint64;
shr ymm0_2 ymm14_2 0x3f@uint64;
shr ymm0_3 ymm14_3 0x3f@uint64;
(* vpxor  -0x50(%rbp),%ymm1,%ymm1                  #! EA = L0x7fffffffbf00; Value = 0x36e67b423707dcb0; PC = 0x555555578ebb *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbf00;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbf08;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbf10;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbf18;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x555555578ec0 *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpsrlq $0x3f,%ymm15,%ymm0                       #! PC = 0x555555578ec4 *)
shr ymm0_0 ymm15_0 0x3f@uint64;
shr ymm0_1 ymm15_1 0x3f@uint64;
shr ymm0_2 ymm15_2 0x3f@uint64;
shr ymm0_3 ymm15_3 0x3f@uint64;
(* vpsrlq $0x3f,%ymm12,%ymm13                      #! PC = 0x555555578eca *)
shr ymm13_0 ymm12_0 0x3f@uint64;
shr ymm13_1 ymm12_1 0x3f@uint64;
shr ymm13_2 ymm12_2 0x3f@uint64;
shr ymm13_3 ymm12_3 0x3f@uint64;
(* vpor   %ymm0,%ymm3,%ymm3                        #! PC = 0x555555578ed0 *)
or ymm3_0@uint64 ymm3_0 ymm0_0;
or ymm3_1@uint64 ymm3_1 ymm0_1;
or ymm3_2@uint64 ymm3_2 ymm0_2;
or ymm3_3@uint64 ymm3_3 ymm0_3;
(* vpxor  %ymm1,%ymm5,%ymm5                        #! PC = 0x555555578ed4 *)
xor ymm5_0@uint64 ymm5_0 ymm1_0;
xor ymm5_1@uint64 ymm5_1 ymm1_1;
xor ymm5_2@uint64 ymm5_2 ymm1_2;
xor ymm5_3@uint64 ymm5_3 ymm1_3;
(* vpsllq $0x1,%ymm12,%ymm0                        #! PC = 0x555555578ed8 *)
shl ymm0_0 ymm12_0 0x1@uint64;
shl ymm0_1 ymm12_1 0x1@uint64;
shl ymm0_2 ymm12_2 0x1@uint64;
shl ymm0_3 ymm12_3 0x1@uint64;
(* vpxor  %ymm2,%ymm3,%ymm3                        #! PC = 0x555555578ede *)
xor ymm3_0@uint64 ymm3_0 ymm2_0;
xor ymm3_1@uint64 ymm3_1 ymm2_1;
xor ymm3_2@uint64 ymm3_2 ymm2_2;
xor ymm3_3@uint64 ymm3_3 ymm2_3;
(* vpxor  %ymm6,%ymm5,%ymm6                        #! PC = 0x555555578ee2 *)
xor ymm6_0@uint64 ymm5_0 ymm6_0;
xor ymm6_1@uint64 ymm5_1 ymm6_1;
xor ymm6_2@uint64 ymm5_2 ymm6_2;
xor ymm6_3@uint64 ymm5_3 ymm6_3;
(* vpor   %ymm13,%ymm0,%ymm0                       #! PC = 0x555555578ee6 *)
or ymm0_0@uint64 ymm0_0 ymm13_0;
or ymm0_1@uint64 ymm0_1 ymm13_1;
or ymm0_2@uint64 ymm0_2 ymm13_2;
or ymm0_3@uint64 ymm0_3 ymm13_3;
(* vpxor  %ymm8,%ymm3,%ymm8                        #! PC = 0x555555578eeb *)
xor ymm8_0@uint64 ymm3_0 ymm8_0;
xor ymm8_1@uint64 ymm3_1 ymm8_1;
xor ymm8_2@uint64 ymm3_2 ymm8_2;
xor ymm8_3@uint64 ymm3_3 ymm8_3;
(* vpxor  %ymm7,%ymm3,%ymm7                        #! PC = 0x555555578ef0 *)
xor ymm7_0@uint64 ymm3_0 ymm7_0;
xor ymm7_1@uint64 ymm3_1 ymm7_1;
xor ymm7_2@uint64 ymm3_2 ymm7_2;
xor ymm7_3@uint64 ymm3_3 ymm7_3;
(* vpxor  %ymm14,%ymm0,%ymm14                      #! PC = 0x555555578ef4 *)
xor ymm14_0@uint64 ymm0_0 ymm14_0;
xor ymm14_1@uint64 ymm0_1 ymm14_1;
xor ymm14_2@uint64 ymm0_2 ymm14_2;
xor ymm14_3@uint64 ymm0_3 ymm14_3;
(* vpsrlq $0x3f,%ymm1,%ymm0                        #! PC = 0x555555578ef9 *)
shr ymm0_0 ymm1_0 0x3f@uint64;
shr ymm0_1 ymm1_1 0x3f@uint64;
shr ymm0_2 ymm1_2 0x3f@uint64;
shr ymm0_3 ymm1_3 0x3f@uint64;
(* vpsllq $0x1,%ymm1,%ymm1                         #! PC = 0x555555578efe *)
shl ymm1_0 ymm1_0 0x1@uint64;
shl ymm1_1 ymm1_1 0x1@uint64;
shl ymm1_2 ymm1_2 0x1@uint64;
shl ymm1_3 ymm1_3 0x1@uint64;
(* vpxor  %ymm9,%ymm14,%ymm9                       #! PC = 0x555555578f03 *)
xor ymm9_0@uint64 ymm14_0 ymm9_0;
xor ymm9_1@uint64 ymm14_1 ymm9_1;
xor ymm9_2@uint64 ymm14_2 ymm9_2;
xor ymm9_3@uint64 ymm14_3 ymm9_3;
(* vpxor  %ymm11,%ymm14,%ymm11                     #! PC = 0x555555578f08 *)
xor ymm11_0@uint64 ymm14_0 ymm11_0;
xor ymm11_1@uint64 ymm14_1 ymm11_1;
xor ymm11_2@uint64 ymm14_2 ymm11_2;
xor ymm11_3@uint64 ymm14_3 ymm11_3;
(* vpor   %ymm0,%ymm1,%ymm1                        #! PC = 0x555555578f0d *)
or ymm1_0@uint64 ymm1_0 ymm0_0;
or ymm1_1@uint64 ymm1_1 ymm0_1;
or ymm1_2@uint64 ymm1_2 ymm0_2;
or ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3f,%ymm2,%ymm0                        #! PC = 0x555555578f11 *)
shr ymm0_0 ymm2_0 0x3f@uint64;
shr ymm0_1 ymm2_1 0x3f@uint64;
shr ymm0_2 ymm2_2 0x3f@uint64;
shr ymm0_3 ymm2_3 0x3f@uint64;
(* vpsllq $0x1,%ymm2,%ymm2                         #! PC = 0x555555578f16 *)
shl ymm2_0 ymm2_0 0x1@uint64;
shl ymm2_1 ymm2_1 0x1@uint64;
shl ymm2_2 ymm2_2 0x1@uint64;
shl ymm2_3 ymm2_3 0x1@uint64;
(* vpxor  %ymm15,%ymm1,%ymm15                      #! PC = 0x555555578f1b *)
xor ymm15_0@uint64 ymm1_0 ymm15_0;
xor ymm15_1@uint64 ymm1_1 ymm15_1;
xor ymm15_2@uint64 ymm1_2 ymm15_2;
xor ymm15_3@uint64 ymm1_3 ymm15_3;
(* vpxor  -0x90(%rbp),%ymm5,%ymm1                  #! EA = L0x7fffffffbec0; Value = 0x43e30b96ff110a58; PC = 0x555555578f20 *)
xor ymm1_0@uint64 ymm5_0 L0x7fffffffbec0;
xor ymm1_1@uint64 ymm5_1 L0x7fffffffbec8;
xor ymm1_2@uint64 ymm5_2 L0x7fffffffbed0;
xor ymm1_3@uint64 ymm5_3 L0x7fffffffbed8;
(* vpor   %ymm0,%ymm2,%ymm2                        #! PC = 0x555555578f28 *)
or ymm2_0@uint64 ymm2_0 ymm0_0;
or ymm2_1@uint64 ymm2_1 ymm0_1;
or ymm2_2@uint64 ymm2_2 ymm0_2;
or ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x14,%ymm8,%ymm0                        #! PC = 0x555555578f2c *)
shr ymm0_0 ymm8_0 0x14@uint64;
shr ymm0_1 ymm8_1 0x14@uint64;
shr ymm0_2 ymm8_2 0x14@uint64;
shr ymm0_3 ymm8_3 0x14@uint64;
(* vpxor  %ymm10,%ymm15,%ymm10                     #! PC = 0x555555578f32 *)
xor ymm10_0@uint64 ymm15_0 ymm10_0;
xor ymm10_1@uint64 ymm15_1 ymm10_1;
xor ymm10_2@uint64 ymm15_2 ymm10_2;
xor ymm10_3@uint64 ymm15_3 ymm10_3;
(* vpsllq $0x2c,%ymm8,%ymm8                        #! PC = 0x555555578f37 *)
shl ymm8_0 ymm8_0 0x2c@uint64;
shl ymm8_1 ymm8_1 0x2c@uint64;
shl ymm8_2 ymm8_2 0x2c@uint64;
shl ymm8_3 ymm8_3 0x2c@uint64;
(* vpxor  %ymm12,%ymm2,%ymm12                      #! PC = 0x555555578f3d *)
xor ymm12_0@uint64 ymm2_0 ymm12_0;
xor ymm12_1@uint64 ymm2_1 ymm12_1;
xor ymm12_2@uint64 ymm2_2 ymm12_2;
xor ymm12_3@uint64 ymm2_3 ymm12_3;
(* vpor   %ymm0,%ymm8,%ymm8                        #! PC = 0x555555578f42 *)
or ymm8_0@uint64 ymm8_0 ymm0_0;
or ymm8_1@uint64 ymm8_1 ymm0_1;
or ymm8_2@uint64 ymm8_2 ymm0_2;
or ymm8_3@uint64 ymm8_3 ymm0_3;
(* vpsrlq $0x15,%ymm9,%ymm0                        #! PC = 0x555555578f46 *)
shr ymm0_0 ymm9_0 0x15@uint64;
shr ymm0_1 ymm9_1 0x15@uint64;
shr ymm0_2 ymm9_2 0x15@uint64;
shr ymm0_3 ymm9_3 0x15@uint64;
(* vpxor  %ymm4,%ymm12,%ymm4                       #! PC = 0x555555578f4c *)
xor ymm4_0@uint64 ymm12_0 ymm4_0;
xor ymm4_1@uint64 ymm12_1 ymm4_1;
xor ymm4_2@uint64 ymm12_2 ymm4_2;
xor ymm4_3@uint64 ymm12_3 ymm4_3;
(* vpsllq $0x2b,%ymm9,%ymm9                        #! PC = 0x555555578f50 *)
shl ymm9_0 ymm9_0 0x2b@uint64;
shl ymm9_1 ymm9_1 0x2b@uint64;
shl ymm9_2 ymm9_2 0x2b@uint64;
shl ymm9_3 ymm9_3 0x2b@uint64;
(* vpor   %ymm0,%ymm9,%ymm9                        #! PC = 0x555555578f56 *)
or ymm9_0@uint64 ymm9_0 ymm0_0;
or ymm9_1@uint64 ymm9_1 ymm0_1;
or ymm9_2@uint64 ymm9_2 ymm0_2;
or ymm9_3@uint64 ymm9_3 ymm0_3;
(* vmovq  %r10,%xmm0                               #! PC = 0x555555578f5a *)
mov xmm0_0 r10;
mov xmm0_1 0@uint64;
(* vpandn %ymm9,%ymm8,%ymm2                        #! PC = 0x555555578f5f *)
not ymm8_0n@uint64 ymm8_0;
and ymm2_0@uint64 ymm8_0n ymm9_0;
not ymm8_1n@uint64 ymm8_1;
and ymm2_1@uint64 ymm8_1n ymm9_1;
not ymm8_2n@uint64 ymm8_2;
and ymm2_2@uint64 ymm8_2n ymm9_2;
not ymm8_3n@uint64 ymm8_3;
and ymm2_3@uint64 ymm8_3n ymm9_3;
(* vpbroadcastq %xmm0,%ymm0                        #! PC = 0x555555578f64 *)
mov ymm0_0 xmm0_0;
mov ymm0_1 xmm0_0;
mov ymm0_2 xmm0_0;
mov ymm0_3 xmm0_0;
(* vpxor  %ymm2,%ymm0,%ymm0                        #! PC = 0x555555578f69 *)
xor ymm0_0@uint64 ymm0_0 ymm2_0;
xor ymm0_1@uint64 ymm0_1 ymm2_1;
xor ymm0_2@uint64 ymm0_2 ymm2_2;
xor ymm0_3@uint64 ymm0_3 ymm2_3;
(* vpxor  %ymm1,%ymm0,%ymm2                        #! PC = 0x555555578f6d *)
xor ymm2_0@uint64 ymm0_0 ymm1_0;
xor ymm2_1@uint64 ymm0_1 ymm1_1;
xor ymm2_2@uint64 ymm0_2 ymm1_2;
xor ymm2_3@uint64 ymm0_3 ymm1_3;
(* vpsrlq $0x2b,%ymm10,%ymm0                       #! PC = 0x555555578f71 *)
shr ymm0_0 ymm10_0 0x2b@uint64;
shr ymm0_1 ymm10_1 0x2b@uint64;
shr ymm0_2 ymm10_2 0x2b@uint64;
shr ymm0_3 ymm10_3 0x2b@uint64;
(* vpsllq $0x15,%ymm10,%ymm10                      #! PC = 0x555555578f77 *)
shl ymm10_0 ymm10_0 0x15@uint64;
shl ymm10_1 ymm10_1 0x15@uint64;
shl ymm10_2 ymm10_2 0x15@uint64;
shl ymm10_3 ymm10_3 0x15@uint64;
(* vmovdqa %ymm2,-0x90(%rbp)                       #! EA = L0x7fffffffbec0; PC = 0x555555578f7d *)
mov L0x7fffffffbec0 ymm2_0;
mov L0x7fffffffbec8 ymm2_1;
mov L0x7fffffffbed0 ymm2_2;
mov L0x7fffffffbed8 ymm2_3;
(* vpor   %ymm0,%ymm10,%ymm10                      #! PC = 0x555555578f85 *)
or ymm10_0@uint64 ymm10_0 ymm0_0;
or ymm10_1@uint64 ymm10_1 ymm0_1;
or ymm10_2@uint64 ymm10_2 ymm0_2;
or ymm10_3@uint64 ymm10_3 ymm0_3;
(* vpandn %ymm10,%ymm9,%ymm0                       #! PC = 0x555555578f89 *)
not ymm9_0n@uint64 ymm9_0;
and ymm0_0@uint64 ymm9_0n ymm10_0;
not ymm9_1n@uint64 ymm9_1;
and ymm0_1@uint64 ymm9_1n ymm10_1;
not ymm9_2n@uint64 ymm9_2;
and ymm0_2@uint64 ymm9_2n ymm10_2;
not ymm9_3n@uint64 ymm9_3;
and ymm0_3@uint64 ymm9_3n ymm10_3;
(* vpxor  %ymm8,%ymm0,%ymm0                        #! PC = 0x555555578f8e *)
xor ymm0_0@uint64 ymm0_0 ymm8_0;
xor ymm0_1@uint64 ymm0_1 ymm8_1;
xor ymm0_2@uint64 ymm0_2 ymm8_2;
xor ymm0_3@uint64 ymm0_3 ymm8_3;
(* vpandn %ymm8,%ymm1,%ymm8                        #! PC = 0x555555578f93 *)
not ymm1_0n@uint64 ymm1_0;
and ymm8_0@uint64 ymm1_0n ymm8_0;
not ymm1_1n@uint64 ymm1_1;
and ymm8_1@uint64 ymm1_1n ymm8_1;
not ymm1_2n@uint64 ymm1_2;
and ymm8_2@uint64 ymm1_2n ymm8_2;
not ymm1_3n@uint64 ymm1_3;
and ymm8_3@uint64 ymm1_3n ymm8_3;
(* vmovdqa %ymm0,-0x1f0(%rbp)                      #! EA = L0x7fffffffbd60; PC = 0x555555578f98 *)
mov L0x7fffffffbd60 ymm0_0;
mov L0x7fffffffbd68 ymm0_1;
mov L0x7fffffffbd70 ymm0_2;
mov L0x7fffffffbd78 ymm0_3;
(* vpsrlq $0x32,%ymm4,%ymm0                        #! PC = 0x555555578fa0 *)
shr ymm0_0 ymm4_0 0x32@uint64;
shr ymm0_1 ymm4_1 0x32@uint64;
shr ymm0_2 ymm4_2 0x32@uint64;
shr ymm0_3 ymm4_3 0x32@uint64;
(* vpsllq $0xe,%ymm4,%ymm4                         #! PC = 0x555555578fa5 *)
shl ymm4_0 ymm4_0 0xe@uint64;
shl ymm4_1 ymm4_1 0xe@uint64;
shl ymm4_2 ymm4_2 0xe@uint64;
shl ymm4_3 ymm4_3 0xe@uint64;
(* vpor   %ymm0,%ymm4,%ymm4                        #! PC = 0x555555578faa *)
or ymm4_0@uint64 ymm4_0 ymm0_0;
or ymm4_1@uint64 ymm4_1 ymm0_1;
or ymm4_2@uint64 ymm4_2 ymm0_2;
or ymm4_3@uint64 ymm4_3 ymm0_3;
(* vpandn %ymm4,%ymm10,%ymm0                       #! PC = 0x555555578fae *)
not ymm10_0n@uint64 ymm10_0;
and ymm0_0@uint64 ymm10_0n ymm4_0;
not ymm10_1n@uint64 ymm10_1;
and ymm0_1@uint64 ymm10_1n ymm4_1;
not ymm10_2n@uint64 ymm10_2;
and ymm0_2@uint64 ymm10_2n ymm4_2;
not ymm10_3n@uint64 ymm10_3;
and ymm0_3@uint64 ymm10_3n ymm4_3;
(* vpxor  %ymm4,%ymm8,%ymm8                        #! PC = 0x555555578fb2 *)
xor ymm8_0@uint64 ymm8_0 ymm4_0;
xor ymm8_1@uint64 ymm8_1 ymm4_1;
xor ymm8_2@uint64 ymm8_2 ymm4_2;
xor ymm8_3@uint64 ymm8_3 ymm4_3;
(* vpxor  %ymm9,%ymm0,%ymm9                        #! PC = 0x555555578fb6 *)
xor ymm9_0@uint64 ymm0_0 ymm9_0;
xor ymm9_1@uint64 ymm0_1 ymm9_1;
xor ymm9_2@uint64 ymm0_2 ymm9_2;
xor ymm9_3@uint64 ymm0_3 ymm9_3;
(* vpandn %ymm1,%ymm4,%ymm0                        #! PC = 0x555555578fbb *)
not ymm4_0n@uint64 ymm4_0;
and ymm0_0@uint64 ymm4_0n ymm1_0;
not ymm4_1n@uint64 ymm4_1;
and ymm0_1@uint64 ymm4_1n ymm1_1;
not ymm4_2n@uint64 ymm4_2;
and ymm0_2@uint64 ymm4_2n ymm1_2;
not ymm4_3n@uint64 ymm4_3;
and ymm0_3@uint64 ymm4_3n ymm1_3;
(* vmovdqa %ymm9,-0x310(%rbp)                      #! EA = L0x7fffffffbc40; PC = 0x555555578fbf *)
mov L0x7fffffffbc40 ymm9_0;
mov L0x7fffffffbc48 ymm9_1;
mov L0x7fffffffbc50 ymm9_2;
mov L0x7fffffffbc58 ymm9_3;
(* vpxor  %ymm10,%ymm0,%ymm9                       #! PC = 0x555555578fc7 *)
xor ymm9_0@uint64 ymm0_0 ymm10_0;
xor ymm9_1@uint64 ymm0_1 ymm10_1;
xor ymm9_2@uint64 ymm0_2 ymm10_2;
xor ymm9_3@uint64 ymm0_3 ymm10_3;
(* vmovdqa %ymm9,-0x150(%rbp)                      #! EA = L0x7fffffffbe00; PC = 0x555555578fcc *)
mov L0x7fffffffbe00 ymm9_0;
mov L0x7fffffffbe08 ymm9_1;
mov L0x7fffffffbe10 ymm9_2;
mov L0x7fffffffbe18 ymm9_3;
(* vmovdqa %ymm8,-0x2f0(%rbp)                      #! EA = L0x7fffffffbc60; PC = 0x555555578fd4 *)
mov L0x7fffffffbc60 ymm8_0;
mov L0x7fffffffbc68 ymm8_1;
mov L0x7fffffffbc70 ymm8_2;
mov L0x7fffffffbc78 ymm8_3;
(* vpxor  -0x130(%rbp),%ymm15,%ymm10               #! EA = L0x7fffffffbe20; Value = 0xa303b734f55677e6; PC = 0x555555578fdc *)
xor ymm10_0@uint64 ymm15_0 L0x7fffffffbe20;
xor ymm10_1@uint64 ymm15_1 L0x7fffffffbe28;
xor ymm10_2@uint64 ymm15_2 L0x7fffffffbe30;
xor ymm10_3@uint64 ymm15_3 L0x7fffffffbe38;
(* vpxor  -0xb0(%rbp),%ymm15,%ymm9                 #! EA = L0x7fffffffbea0; Value = 0x5f252e8e367f3dc8; PC = 0x555555578fe4 *)
xor ymm9_0@uint64 ymm15_0 L0x7fffffffbea0;
xor ymm9_1@uint64 ymm15_1 L0x7fffffffbea8;
xor ymm9_2@uint64 ymm15_2 L0x7fffffffbeb0;
xor ymm9_3@uint64 ymm15_3 L0x7fffffffbeb8;
(* vpsrlq $0x24,%ymm10,%ymm0                       #! PC = 0x555555578fec *)
shr ymm0_0 ymm10_0 0x24@uint64;
shr ymm0_1 ymm10_1 0x24@uint64;
shr ymm0_2 ymm10_2 0x24@uint64;
shr ymm0_3 ymm10_3 0x24@uint64;
(* vpsllq $0x1c,%ymm10,%ymm1                       #! PC = 0x555555578ff2 *)
shl ymm1_0 ymm10_0 0x1c@uint64;
shl ymm1_1 ymm10_1 0x1c@uint64;
shl ymm1_2 ymm10_2 0x1c@uint64;
shl ymm1_3 ymm10_3 0x1c@uint64;
(* vpor   %ymm0,%ymm1,%ymm1                        #! PC = 0x555555578ff8 *)
or ymm1_0@uint64 ymm1_0 ymm0_0;
or ymm1_1@uint64 ymm1_1 ymm0_1;
or ymm1_2@uint64 ymm1_2 ymm0_2;
or ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpxor  -0xf0(%rbp),%ymm12,%ymm0                 #! EA = L0x7fffffffbe60; Value = 0x7a8e099fe258e5d8; PC = 0x555555578ffc *)
xor ymm0_0@uint64 ymm12_0 L0x7fffffffbe60;
xor ymm0_1@uint64 ymm12_1 L0x7fffffffbe68;
xor ymm0_2@uint64 ymm12_2 L0x7fffffffbe70;
xor ymm0_3@uint64 ymm12_3 L0x7fffffffbe78;
(* vpsrlq $0x2c,%ymm0,%ymm2                        #! PC = 0x555555579004 *)
shr ymm2_0 ymm0_0 0x2c@uint64;
shr ymm2_1 ymm0_1 0x2c@uint64;
shr ymm2_2 ymm0_2 0x2c@uint64;
shr ymm2_3 ymm0_3 0x2c@uint64;
(* vpsllq $0x14,%ymm0,%ymm0                        #! PC = 0x555555579009 *)
shl ymm0_0 ymm0_0 0x14@uint64;
shl ymm0_1 ymm0_1 0x14@uint64;
shl ymm0_2 ymm0_2 0x14@uint64;
shl ymm0_3 ymm0_3 0x14@uint64;
(* vpor   %ymm2,%ymm0,%ymm0                        #! PC = 0x55555557900e *)
or ymm0_0@uint64 ymm0_0 ymm2_0;
or ymm0_1@uint64 ymm0_1 ymm2_1;
or ymm0_2@uint64 ymm0_2 ymm2_2;
or ymm0_3@uint64 ymm0_3 ymm2_3;
(* vpxor  -0xd0(%rbp),%ymm5,%ymm2                  #! EA = L0x7fffffffbe80; Value = 0x2d6cbeb27c4bc219; PC = 0x555555579012 *)
xor ymm2_0@uint64 ymm5_0 L0x7fffffffbe80;
xor ymm2_1@uint64 ymm5_1 L0x7fffffffbe88;
xor ymm2_2@uint64 ymm5_2 L0x7fffffffbe90;
xor ymm2_3@uint64 ymm5_3 L0x7fffffffbe98;
(* vpsrlq $0x3d,%ymm2,%ymm4                        #! PC = 0x55555557901a *)
shr ymm4_0 ymm2_0 0x3d@uint64;
shr ymm4_1 ymm2_1 0x3d@uint64;
shr ymm4_2 ymm2_2 0x3d@uint64;
shr ymm4_3 ymm2_3 0x3d@uint64;
(* vpsllq $0x3,%ymm2,%ymm2                         #! PC = 0x55555557901f *)
shl ymm2_0 ymm2_0 0x3@uint64;
shl ymm2_1 ymm2_1 0x3@uint64;
shl ymm2_2 ymm2_2 0x3@uint64;
shl ymm2_3 ymm2_3 0x3@uint64;
(* vpor   %ymm4,%ymm2,%ymm2                        #! PC = 0x555555579024 *)
or ymm2_0@uint64 ymm2_0 ymm4_0;
or ymm2_1@uint64 ymm2_1 ymm4_1;
or ymm2_2@uint64 ymm2_2 ymm4_2;
or ymm2_3@uint64 ymm2_3 ymm4_3;
(* vpsrlq $0x13,%ymm7,%ymm4                        #! PC = 0x555555579028 *)
shr ymm4_0 ymm7_0 0x13@uint64;
shr ymm4_1 ymm7_1 0x13@uint64;
shr ymm4_2 ymm7_2 0x13@uint64;
shr ymm4_3 ymm7_3 0x13@uint64;
(* vpandn %ymm2,%ymm0,%ymm10                       #! PC = 0x55555557902d *)
not ymm0_0n@uint64 ymm0_0;
and ymm10_0@uint64 ymm0_0n ymm2_0;
not ymm0_1n@uint64 ymm0_1;
and ymm10_1@uint64 ymm0_1n ymm2_1;
not ymm0_2n@uint64 ymm0_2;
and ymm10_2@uint64 ymm0_2n ymm2_2;
not ymm0_3n@uint64 ymm0_3;
and ymm10_3@uint64 ymm0_3n ymm2_3;
(* vpsllq $0x2d,%ymm7,%ymm7                        #! PC = 0x555555579031 *)
shl ymm7_0 ymm7_0 0x2d@uint64;
shl ymm7_1 ymm7_1 0x2d@uint64;
shl ymm7_2 ymm7_2 0x2d@uint64;
shl ymm7_3 ymm7_3 0x2d@uint64;
(* vpxor  %ymm1,%ymm10,%ymm13                      #! PC = 0x555555579036 *)
xor ymm13_0@uint64 ymm10_0 ymm1_0;
xor ymm13_1@uint64 ymm10_1 ymm1_1;
xor ymm13_2@uint64 ymm10_2 ymm1_2;
xor ymm13_3@uint64 ymm10_3 ymm1_3;
(* vmovdqa %ymm13,-0x2d0(%rbp)                     #! EA = L0x7fffffffbc80; PC = 0x55555557903a *)
mov L0x7fffffffbc80 ymm13_0;
mov L0x7fffffffbc88 ymm13_1;
mov L0x7fffffffbc90 ymm13_2;
mov L0x7fffffffbc98 ymm13_3;
(* vpor   %ymm4,%ymm7,%ymm13                       #! PC = 0x555555579042 *)
or ymm13_0@uint64 ymm7_0 ymm4_0;
or ymm13_1@uint64 ymm7_1 ymm4_1;
or ymm13_2@uint64 ymm7_2 ymm4_2;
or ymm13_3@uint64 ymm7_3 ymm4_3;
(* vpsrlq $0x3,%ymm11,%ymm4                        #! PC = 0x555555579046 *)
shr ymm4_0 ymm11_0 0x3@uint64;
shr ymm4_1 ymm11_1 0x3@uint64;
shr ymm4_2 ymm11_2 0x3@uint64;
shr ymm4_3 ymm11_3 0x3@uint64;
(* vpsllq $0x3d,%ymm11,%ymm11                      #! PC = 0x55555557904c *)
shl ymm11_0 ymm11_0 0x3d@uint64;
shl ymm11_1 ymm11_1 0x3d@uint64;
shl ymm11_2 ymm11_2 0x3d@uint64;
shl ymm11_3 ymm11_3 0x3d@uint64;
(* vpandn %ymm13,%ymm2,%ymm8                       #! PC = 0x555555579052 *)
not ymm2_0n@uint64 ymm2_0;
and ymm8_0@uint64 ymm2_0n ymm13_0;
not ymm2_1n@uint64 ymm2_1;
and ymm8_1@uint64 ymm2_1n ymm13_1;
not ymm2_2n@uint64 ymm2_2;
and ymm8_2@uint64 ymm2_2n ymm13_2;
not ymm2_3n@uint64 ymm2_3;
and ymm8_3@uint64 ymm2_3n ymm13_3;
(* vpor   %ymm4,%ymm11,%ymm11                      #! PC = 0x555555579057 *)
or ymm11_0@uint64 ymm11_0 ymm4_0;
or ymm11_1@uint64 ymm11_1 ymm4_1;
or ymm11_2@uint64 ymm11_2 ymm4_2;
or ymm11_3@uint64 ymm11_3 ymm4_3;
(* vpxor  %ymm0,%ymm8,%ymm8                        #! PC = 0x55555557905b *)
xor ymm8_0@uint64 ymm8_0 ymm0_0;
xor ymm8_1@uint64 ymm8_1 ymm0_1;
xor ymm8_2@uint64 ymm8_2 ymm0_2;
xor ymm8_3@uint64 ymm8_3 ymm0_3;
(* vpandn %ymm11,%ymm13,%ymm4                      #! PC = 0x55555557905f *)
not ymm13_0n@uint64 ymm13_0;
and ymm4_0@uint64 ymm13_0n ymm11_0;
not ymm13_1n@uint64 ymm13_1;
and ymm4_1@uint64 ymm13_1n ymm11_1;
not ymm13_2n@uint64 ymm13_2;
and ymm4_2@uint64 ymm13_2n ymm11_2;
not ymm13_3n@uint64 ymm13_3;
and ymm4_3@uint64 ymm13_3n ymm11_3;
(* vpxor  %ymm2,%ymm4,%ymm7                        #! PC = 0x555555579064 *)
xor ymm7_0@uint64 ymm4_0 ymm2_0;
xor ymm7_1@uint64 ymm4_1 ymm2_1;
xor ymm7_2@uint64 ymm4_2 ymm2_2;
xor ymm7_3@uint64 ymm4_3 ymm2_3;
(* vpsrlq $0x27,%ymm9,%ymm4                        #! PC = 0x555555579068 *)
shr ymm4_0 ymm9_0 0x27@uint64;
shr ymm4_1 ymm9_1 0x27@uint64;
shr ymm4_2 ymm9_2 0x27@uint64;
shr ymm4_3 ymm9_3 0x27@uint64;
(* vpandn %ymm1,%ymm11,%ymm2                       #! PC = 0x55555557906e *)
not ymm11_0n@uint64 ymm11_0;
and ymm2_0@uint64 ymm11_0n ymm1_0;
not ymm11_1n@uint64 ymm11_1;
and ymm2_1@uint64 ymm11_1n ymm1_1;
not ymm11_2n@uint64 ymm11_2;
and ymm2_2@uint64 ymm11_2n ymm1_2;
not ymm11_3n@uint64 ymm11_3;
and ymm2_3@uint64 ymm11_3n ymm1_3;
(* vpandn %ymm0,%ymm1,%ymm1                        #! PC = 0x555555579072 *)
not ymm1_0n@uint64 ymm1_0;
and ymm1_0@uint64 ymm1_0n ymm0_0;
not ymm1_1n@uint64 ymm1_1;
and ymm1_1@uint64 ymm1_1n ymm0_1;
not ymm1_2n@uint64 ymm1_2;
and ymm1_2@uint64 ymm1_2n ymm0_2;
not ymm1_3n@uint64 ymm1_3;
and ymm1_3@uint64 ymm1_3n ymm0_3;
(* vpxor  %ymm13,%ymm2,%ymm13                      #! PC = 0x555555579076 *)
xor ymm13_0@uint64 ymm2_0 ymm13_0;
xor ymm13_1@uint64 ymm2_1 ymm13_1;
xor ymm13_2@uint64 ymm2_2 ymm13_2;
xor ymm13_3@uint64 ymm2_3 ymm13_3;
(* vpxor  -0x110(%rbp),%ymm14,%ymm2                #! EA = L0x7fffffffbe40; Value = 0x34fe5a6214181ed1; PC = 0x55555557907b *)
xor ymm2_0@uint64 ymm14_0 L0x7fffffffbe40;
xor ymm2_1@uint64 ymm14_1 L0x7fffffffbe48;
xor ymm2_2@uint64 ymm14_2 L0x7fffffffbe50;
xor ymm2_3@uint64 ymm14_3 L0x7fffffffbe58;
(* vmovdqa %ymm7,-0x1b0(%rbp)                      #! EA = L0x7fffffffbda0; PC = 0x555555579083 *)
mov L0x7fffffffbda0 ymm7_0;
mov L0x7fffffffbda8 ymm7_1;
mov L0x7fffffffbdb0 ymm7_2;
mov L0x7fffffffbdb8 ymm7_3;
(* vpxor  %ymm11,%ymm1,%ymm11                      #! PC = 0x55555557908b *)
xor ymm11_0@uint64 ymm1_0 ymm11_0;
xor ymm11_1@uint64 ymm1_1 ymm11_1;
xor ymm11_2@uint64 ymm1_2 ymm11_2;
xor ymm11_3@uint64 ymm1_3 ymm11_3;
(* vpxor  -0x290(%rbp),%ymm3,%ymm1                 #! EA = L0x7fffffffbcc0; Value = 0xd642c7df22b4173c; PC = 0x555555579090 *)
xor ymm1_0@uint64 ymm3_0 L0x7fffffffbcc0;
xor ymm1_1@uint64 ymm3_1 L0x7fffffffbcc8;
xor ymm1_2@uint64 ymm3_2 L0x7fffffffbcd0;
xor ymm1_3@uint64 ymm3_3 L0x7fffffffbcd8;
(* vmovdqa %ymm13,-0x190(%rbp)                     #! EA = L0x7fffffffbdc0; PC = 0x555555579098 *)
mov L0x7fffffffbdc0 ymm13_0;
mov L0x7fffffffbdc8 ymm13_1;
mov L0x7fffffffbdd0 ymm13_2;
mov L0x7fffffffbdd8 ymm13_3;
(* vpsllq $0x19,%ymm9,%ymm9                        #! PC = 0x5555555790a0 *)
shl ymm9_0 ymm9_0 0x19@uint64;
shl ymm9_1 ymm9_1 0x19@uint64;
shl ymm9_2 ymm9_2 0x19@uint64;
shl ymm9_3 ymm9_3 0x19@uint64;
(* vmovdqa %ymm11,-0x130(%rbp)                     #! EA = L0x7fffffffbe20; PC = 0x5555555790a6 *)
mov L0x7fffffffbe20 ymm11_0;
mov L0x7fffffffbe28 ymm11_1;
mov L0x7fffffffbe30 ymm11_2;
mov L0x7fffffffbe38 ymm11_3;
(* vpsrlq $0x3f,%ymm1,%ymm0                        #! PC = 0x5555555790ae *)
shr ymm0_0 ymm1_0 0x3f@uint64;
shr ymm0_1 ymm1_1 0x3f@uint64;
shr ymm0_2 ymm1_2 0x3f@uint64;
shr ymm0_3 ymm1_3 0x3f@uint64;
(* vpsllq $0x1,%ymm1,%ymm1                         #! PC = 0x5555555790b3 *)
shl ymm1_0 ymm1_0 0x1@uint64;
shl ymm1_1 ymm1_1 0x1@uint64;
shl ymm1_2 ymm1_2 0x1@uint64;
shl ymm1_3 ymm1_3 0x1@uint64;
(* vpor   %ymm0,%ymm1,%ymm1                        #! PC = 0x5555555790b8 *)
or ymm1_0@uint64 ymm1_0 ymm0_0;
or ymm1_1@uint64 ymm1_1 ymm0_1;
or ymm1_2@uint64 ymm1_2 ymm0_2;
or ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3a,%ymm2,%ymm0                        #! PC = 0x5555555790bc *)
shr ymm0_0 ymm2_0 0x3a@uint64;
shr ymm0_1 ymm2_1 0x3a@uint64;
shr ymm0_2 ymm2_2 0x3a@uint64;
shr ymm0_3 ymm2_3 0x3a@uint64;
(* vpsllq $0x6,%ymm2,%ymm2                         #! PC = 0x5555555790c1 *)
shl ymm2_0 ymm2_0 0x6@uint64;
shl ymm2_1 ymm2_1 0x6@uint64;
shl ymm2_2 ymm2_2 0x6@uint64;
shl ymm2_3 ymm2_3 0x6@uint64;
(* vpor   %ymm0,%ymm2,%ymm2                        #! PC = 0x5555555790c6 *)
or ymm2_0@uint64 ymm2_0 ymm0_0;
or ymm2_1@uint64 ymm2_1 ymm0_1;
or ymm2_2@uint64 ymm2_2 ymm0_2;
or ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpor   %ymm4,%ymm9,%ymm0                        #! PC = 0x5555555790ca *)
or ymm0_0@uint64 ymm9_0 ymm4_0;
or ymm0_1@uint64 ymm9_1 ymm4_1;
or ymm0_2@uint64 ymm9_2 ymm4_2;
or ymm0_3@uint64 ymm9_3 ymm4_3;
(* vpandn %ymm0,%ymm2,%ymm4                        #! PC = 0x5555555790ce *)
not ymm2_0n@uint64 ymm2_0;
and ymm4_0@uint64 ymm2_0n ymm0_0;
not ymm2_1n@uint64 ymm2_1;
and ymm4_1@uint64 ymm2_1n ymm0_1;
not ymm2_2n@uint64 ymm2_2;
and ymm4_2@uint64 ymm2_2n ymm0_2;
not ymm2_3n@uint64 ymm2_3;
and ymm4_3@uint64 ymm2_3n ymm0_3;
(* vpxor  %ymm1,%ymm4,%ymm11                       #! PC = 0x5555555790d2 *)
xor ymm11_0@uint64 ymm4_0 ymm1_0;
xor ymm11_1@uint64 ymm4_1 ymm1_1;
xor ymm11_2@uint64 ymm4_2 ymm1_2;
xor ymm11_3@uint64 ymm4_3 ymm1_3;
(* vpxor  -0x50(%rbp),%ymm12,%ymm4                 #! EA = L0x7fffffffbf00; Value = 0x36e67b423707dcb0; PC = 0x5555555790d6 *)
xor ymm4_0@uint64 ymm12_0 L0x7fffffffbf00;
xor ymm4_1@uint64 ymm12_1 L0x7fffffffbf08;
xor ymm4_2@uint64 ymm12_2 L0x7fffffffbf10;
xor ymm4_3@uint64 ymm12_3 L0x7fffffffbf18;
(* vmovdqa %ymm11,%ymm13                           #! PC = 0x5555555790db *)
mov ymm13_0 ymm11_0;
mov ymm13_1 ymm11_1;
mov ymm13_2 ymm11_2;
mov ymm13_3 ymm11_3;
(* vpshufb 0x54e17(%rip),%ymm4,%ymm4        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x5555555790e0 *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm4_0, ymm4_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm4_2, ymm4_3, tmp_2, tmp_3);
mov ymm4_0 tmp_0;
mov ymm4_1 tmp_1;
mov ymm4_2 tmp_2;
mov ymm4_3 tmp_3;
(* vpandn %ymm4,%ymm0,%ymm7                        #! PC = 0x5555555790e9 *)
not ymm0_0n@uint64 ymm0_0;
and ymm7_0@uint64 ymm0_0n ymm4_0;
not ymm0_1n@uint64 ymm0_1;
and ymm7_1@uint64 ymm0_1n ymm4_1;
not ymm0_2n@uint64 ymm0_2;
and ymm7_2@uint64 ymm0_2n ymm4_2;
not ymm0_3n@uint64 ymm0_3;
and ymm7_3@uint64 ymm0_3n ymm4_3;
(* vpxor  %ymm2,%ymm7,%ymm7                        #! PC = 0x5555555790ed *)
xor ymm7_0@uint64 ymm7_0 ymm2_0;
xor ymm7_1@uint64 ymm7_1 ymm2_1;
xor ymm7_2@uint64 ymm7_2 ymm2_2;
xor ymm7_3@uint64 ymm7_3 ymm2_3;
(* vmovdqa %ymm7,-0x110(%rbp)                      #! EA = L0x7fffffffbe40; PC = 0x5555555790f1 *)
mov L0x7fffffffbe40 ymm7_0;
mov L0x7fffffffbe48 ymm7_1;
mov L0x7fffffffbe50 ymm7_2;
mov L0x7fffffffbe58 ymm7_3;
(* vpsrlq $0x2e,%ymm6,%ymm7                        #! PC = 0x5555555790f9 *)
shr ymm7_0 ymm6_0 0x2e@uint64;
shr ymm7_1 ymm6_1 0x2e@uint64;
shr ymm7_2 ymm6_2 0x2e@uint64;
shr ymm7_3 ymm6_3 0x2e@uint64;
(* vpsllq $0x12,%ymm6,%ymm6                        #! PC = 0x5555555790fe *)
shl ymm6_0 ymm6_0 0x12@uint64;
shl ymm6_1 ymm6_1 0x12@uint64;
shl ymm6_2 ymm6_2 0x12@uint64;
shl ymm6_3 ymm6_3 0x12@uint64;
(* vpor   %ymm7,%ymm6,%ymm11                       #! PC = 0x555555579103 *)
or ymm11_0@uint64 ymm6_0 ymm7_0;
or ymm11_1@uint64 ymm6_1 ymm7_1;
or ymm11_2@uint64 ymm6_2 ymm7_2;
or ymm11_3@uint64 ymm6_3 ymm7_3;
(* vpxor  -0x1d0(%rbp),%ymm3,%ymm7                 #! EA = L0x7fffffffbd80; Value = 0xa58159b967a3ad93; PC = 0x555555579107 *)
xor ymm7_0@uint64 ymm3_0 L0x7fffffffbd80;
xor ymm7_1@uint64 ymm3_1 L0x7fffffffbd88;
xor ymm7_2@uint64 ymm3_2 L0x7fffffffbd90;
xor ymm7_3@uint64 ymm3_3 L0x7fffffffbd98;
(* vmovdqa %ymm13,-0x1d0(%rbp)                     #! EA = L0x7fffffffbd80; PC = 0x55555557910f *)
mov L0x7fffffffbd80 ymm13_0;
mov L0x7fffffffbd88 ymm13_1;
mov L0x7fffffffbd90 ymm13_2;
mov L0x7fffffffbd98 ymm13_3;
(* vpandn %ymm11,%ymm4,%ymm9                       #! PC = 0x555555579117 *)
not ymm4_0n@uint64 ymm4_0;
and ymm9_0@uint64 ymm4_0n ymm11_0;
not ymm4_1n@uint64 ymm4_1;
and ymm9_1@uint64 ymm4_1n ymm11_1;
not ymm4_2n@uint64 ymm4_2;
and ymm9_2@uint64 ymm4_2n ymm11_2;
not ymm4_3n@uint64 ymm4_3;
and ymm9_3@uint64 ymm4_3n ymm11_3;
(* vpxor  %ymm0,%ymm9,%ymm9                        #! PC = 0x55555557911c *)
xor ymm9_0@uint64 ymm9_0 ymm0_0;
xor ymm9_1@uint64 ymm9_1 ymm0_1;
xor ymm9_2@uint64 ymm9_2 ymm0_2;
xor ymm9_3@uint64 ymm9_3 ymm0_3;
(* vpandn %ymm1,%ymm11,%ymm0                       #! PC = 0x555555579120 *)
not ymm11_0n@uint64 ymm11_0;
and ymm0_0@uint64 ymm11_0n ymm1_0;
not ymm11_1n@uint64 ymm11_1;
and ymm0_1@uint64 ymm11_1n ymm1_1;
not ymm11_2n@uint64 ymm11_2;
and ymm0_2@uint64 ymm11_2n ymm1_2;
not ymm11_3n@uint64 ymm11_3;
and ymm0_3@uint64 ymm11_3n ymm1_3;
(* vpandn %ymm2,%ymm1,%ymm1                        #! PC = 0x555555579124 *)
not ymm1_0n@uint64 ymm1_0;
and ymm1_0@uint64 ymm1_0n ymm2_0;
not ymm1_1n@uint64 ymm1_1;
and ymm1_1@uint64 ymm1_1n ymm2_1;
not ymm1_2n@uint64 ymm1_2;
and ymm1_2@uint64 ymm1_2n ymm2_2;
not ymm1_3n@uint64 ymm1_3;
and ymm1_3@uint64 ymm1_3n ymm2_3;
(* vpxor  %ymm4,%ymm0,%ymm10                       #! PC = 0x555555579128 *)
xor ymm10_0@uint64 ymm0_0 ymm4_0;
xor ymm10_1@uint64 ymm0_1 ymm4_1;
xor ymm10_2@uint64 ymm0_2 ymm4_2;
xor ymm10_3@uint64 ymm0_3 ymm4_3;
(* vpxor  -0x250(%rbp),%ymm12,%ymm0                #! EA = L0x7fffffffbd00; Value = 0x37c05e405b01af0c; PC = 0x55555557912c *)
xor ymm0_0@uint64 ymm12_0 L0x7fffffffbd00;
xor ymm0_1@uint64 ymm12_1 L0x7fffffffbd08;
xor ymm0_2@uint64 ymm12_2 L0x7fffffffbd10;
xor ymm0_3@uint64 ymm12_3 L0x7fffffffbd18;
(* vpxor  %ymm11,%ymm1,%ymm11                      #! PC = 0x555555579134 *)
xor ymm11_0@uint64 ymm1_0 ymm11_0;
xor ymm11_1@uint64 ymm1_1 ymm11_1;
xor ymm11_2@uint64 ymm1_2 ymm11_2;
xor ymm11_3@uint64 ymm1_3 ymm11_3;
(* vmovdqa %ymm11,-0xd0(%rbp)                      #! EA = L0x7fffffffbe80; PC = 0x555555579139 *)
mov L0x7fffffffbe80 ymm11_0;
mov L0x7fffffffbe88 ymm11_1;
mov L0x7fffffffbe90 ymm11_2;
mov L0x7fffffffbe98 ymm11_3;
(* vpxor  -0x170(%rbp),%ymm12,%ymm12               #! EA = L0x7fffffffbde0; Value = 0x5e8f6ad5d330526e; PC = 0x555555579141 *)
xor ymm12_0@uint64 ymm12_0 L0x7fffffffbde0;
xor ymm12_1@uint64 ymm12_1 L0x7fffffffbde8;
xor ymm12_2@uint64 ymm12_2 L0x7fffffffbdf0;
xor ymm12_3@uint64 ymm12_3 L0x7fffffffbdf8;
(* vpsrlq $0x25,%ymm0,%ymm1                        #! PC = 0x555555579149 *)
shr ymm1_0 ymm0_0 0x25@uint64;
shr ymm1_1 ymm0_1 0x25@uint64;
shr ymm1_2 ymm0_2 0x25@uint64;
shr ymm1_3 ymm0_3 0x25@uint64;
(* vpsllq $0x1b,%ymm0,%ymm0                        #! PC = 0x55555557914e *)
shl ymm0_0 ymm0_0 0x1b@uint64;
shl ymm0_1 ymm0_1 0x1b@uint64;
shl ymm0_2 ymm0_2 0x1b@uint64;
shl ymm0_3 ymm0_3 0x1b@uint64;
(* vmovdqa %ymm10,-0xf0(%rbp)                      #! EA = L0x7fffffffbe60; PC = 0x555555579153 *)
mov L0x7fffffffbe60 ymm10_0;
mov L0x7fffffffbe68 ymm10_1;
mov L0x7fffffffbe70 ymm10_2;
mov L0x7fffffffbe78 ymm10_3;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x55555557915b *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x230(%rbp),%ymm5,%ymm1                 #! EA = L0x7fffffffbd20; Value = 0x0b5033314f45c5cb; PC = 0x55555557915f *)
xor ymm1_0@uint64 ymm5_0 L0x7fffffffbd20;
xor ymm1_1@uint64 ymm5_1 L0x7fffffffbd28;
xor ymm1_2@uint64 ymm5_2 L0x7fffffffbd30;
xor ymm1_3@uint64 ymm5_3 L0x7fffffffbd38;
(* vpxor  -0x350(%rbp),%ymm5,%ymm5                 #! EA = L0x7fffffffbc00; Value = 0x4fcb128386812f6e; PC = 0x555555579167 *)
xor ymm5_0@uint64 ymm5_0 L0x7fffffffbc00;
xor ymm5_1@uint64 ymm5_1 L0x7fffffffbc08;
xor ymm5_2@uint64 ymm5_2 L0x7fffffffbc10;
xor ymm5_3@uint64 ymm5_3 L0x7fffffffbc18;
(* vpsrlq $0x1c,%ymm1,%ymm2                        #! PC = 0x55555557916f *)
shr ymm2_0 ymm1_0 0x1c@uint64;
shr ymm2_1 ymm1_1 0x1c@uint64;
shr ymm2_2 ymm1_2 0x1c@uint64;
shr ymm2_3 ymm1_3 0x1c@uint64;
(* vpsllq $0x24,%ymm1,%ymm1                        #! PC = 0x555555579174 *)
shl ymm1_0 ymm1_0 0x24@uint64;
shl ymm1_1 ymm1_1 0x24@uint64;
shl ymm1_2 ymm1_2 0x24@uint64;
shl ymm1_3 ymm1_3 0x24@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x555555579179 *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpsrlq $0x36,%ymm7,%ymm2                        #! PC = 0x55555557917d *)
shr ymm2_0 ymm7_0 0x36@uint64;
shr ymm2_1 ymm7_1 0x36@uint64;
shr ymm2_2 ymm7_2 0x36@uint64;
shr ymm2_3 ymm7_3 0x36@uint64;
(* vpsllq $0xa,%ymm7,%ymm7                         #! PC = 0x555555579182 *)
shl ymm7_0 ymm7_0 0xa@uint64;
shl ymm7_1 ymm7_1 0xa@uint64;
shl ymm7_2 ymm7_2 0xa@uint64;
shl ymm7_3 ymm7_3 0xa@uint64;
(* vpor   %ymm2,%ymm7,%ymm11                       #! PC = 0x555555579187 *)
or ymm11_0@uint64 ymm7_0 ymm2_0;
or ymm11_1@uint64 ymm7_1 ymm2_1;
or ymm11_2@uint64 ymm7_2 ymm2_2;
or ymm11_3@uint64 ymm7_3 ymm2_3;
(* vpxor  -0x70(%rbp),%ymm14,%ymm2                 #! EA = L0x7fffffffbee0; Value = 0x375463e874af271c; PC = 0x55555557918b *)
xor ymm2_0@uint64 ymm14_0 L0x7fffffffbee0;
xor ymm2_1@uint64 ymm14_1 L0x7fffffffbee8;
xor ymm2_2@uint64 ymm14_2 L0x7fffffffbef0;
xor ymm2_3@uint64 ymm14_3 L0x7fffffffbef8;
(* vpxor  -0x270(%rbp),%ymm14,%ymm14               #! EA = L0x7fffffffbce0; Value = 0xbfd660dfe2e0051d; PC = 0x555555579190 *)
xor ymm14_0@uint64 ymm14_0 L0x7fffffffbce0;
xor ymm14_1@uint64 ymm14_1 L0x7fffffffbce8;
xor ymm14_2@uint64 ymm14_2 L0x7fffffffbcf0;
xor ymm14_3@uint64 ymm14_3 L0x7fffffffbcf8;
(* vpandn %ymm11,%ymm1,%ymm4                       #! PC = 0x555555579198 *)
not ymm1_0n@uint64 ymm1_0;
and ymm4_0@uint64 ymm1_0n ymm11_0;
not ymm1_1n@uint64 ymm1_1;
and ymm4_1@uint64 ymm1_1n ymm11_1;
not ymm1_2n@uint64 ymm1_2;
and ymm4_2@uint64 ymm1_2n ymm11_2;
not ymm1_3n@uint64 ymm1_3;
and ymm4_3@uint64 ymm1_3n ymm11_3;
(* vpxor  %ymm0,%ymm4,%ymm10                       #! PC = 0x55555557919d *)
xor ymm10_0@uint64 ymm4_0 ymm0_0;
xor ymm10_1@uint64 ymm4_1 ymm0_1;
xor ymm10_2@uint64 ymm4_2 ymm0_2;
xor ymm10_3@uint64 ymm4_3 ymm0_3;
(* vpsrlq $0x31,%ymm2,%ymm4                        #! PC = 0x5555555791a1 *)
shr ymm4_0 ymm2_0 0x31@uint64;
shr ymm4_1 ymm2_1 0x31@uint64;
shr ymm4_2 ymm2_2 0x31@uint64;
shr ymm4_3 ymm2_3 0x31@uint64;
(* vpsllq $0xf,%ymm2,%ymm2                         #! PC = 0x5555555791a6 *)
shl ymm2_0 ymm2_0 0xf@uint64;
shl ymm2_1 ymm2_1 0xf@uint64;
shl ymm2_2 ymm2_2 0xf@uint64;
shl ymm2_3 ymm2_3 0xf@uint64;
(* vmovdqa %ymm10,-0xb0(%rbp)                      #! EA = L0x7fffffffbea0; PC = 0x5555555791ab *)
mov L0x7fffffffbea0 ymm10_0;
mov L0x7fffffffbea8 ymm10_1;
mov L0x7fffffffbeb0 ymm10_2;
mov L0x7fffffffbeb8 ymm10_3;
(* vpor   %ymm4,%ymm2,%ymm2                        #! PC = 0x5555555791b3 *)
or ymm2_0@uint64 ymm2_0 ymm4_0;
or ymm2_1@uint64 ymm2_1 ymm4_1;
or ymm2_2@uint64 ymm2_2 ymm4_2;
or ymm2_3@uint64 ymm2_3 ymm4_3;
(* vpxor  -0x2b0(%rbp),%ymm15,%ymm4                #! EA = L0x7fffffffbca0; Value = 0xb913d9d348a8e89a; PC = 0x5555555791b7 *)
xor ymm4_0@uint64 ymm15_0 L0x7fffffffbca0;
xor ymm4_1@uint64 ymm15_1 L0x7fffffffbca8;
xor ymm4_2@uint64 ymm15_2 L0x7fffffffbcb0;
xor ymm4_3@uint64 ymm15_3 L0x7fffffffbcb8;
(* vpxor  -0x210(%rbp),%ymm15,%ymm15               #! EA = L0x7fffffffbd40; Value = 0xb42abc5de738ddd5; PC = 0x5555555791bf *)
xor ymm15_0@uint64 ymm15_0 L0x7fffffffbd40;
xor ymm15_1@uint64 ymm15_1 L0x7fffffffbd48;
xor ymm15_2@uint64 ymm15_2 L0x7fffffffbd50;
xor ymm15_3@uint64 ymm15_3 L0x7fffffffbd58;
(* vpandn %ymm2,%ymm11,%ymm7                       #! PC = 0x5555555791c7 *)
not ymm11_0n@uint64 ymm11_0;
and ymm7_0@uint64 ymm11_0n ymm2_0;
not ymm11_1n@uint64 ymm11_1;
and ymm7_1@uint64 ymm11_1n ymm2_1;
not ymm11_2n@uint64 ymm11_2;
and ymm7_2@uint64 ymm11_2n ymm2_2;
not ymm11_3n@uint64 ymm11_3;
and ymm7_3@uint64 ymm11_3n ymm2_3;
(* vpshufb 0x54d0c(%rip),%ymm4,%ymm4        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x5555555791cb *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm4_0, ymm4_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm4_2, ymm4_3, tmp_2, tmp_3);
mov ymm4_0 tmp_0;
mov ymm4_1 tmp_1;
mov ymm4_2 tmp_2;
mov ymm4_3 tmp_3;
(* vpxor  %ymm1,%ymm7,%ymm7                        #! PC = 0x5555555791d4 *)
xor ymm7_0@uint64 ymm7_0 ymm1_0;
xor ymm7_1@uint64 ymm7_1 ymm1_1;
xor ymm7_2@uint64 ymm7_2 ymm1_2;
xor ymm7_3@uint64 ymm7_3 ymm1_3;
(* vpandn %ymm4,%ymm2,%ymm6                        #! PC = 0x5555555791d8 *)
not ymm2_0n@uint64 ymm2_0;
and ymm6_0@uint64 ymm2_0n ymm4_0;
not ymm2_1n@uint64 ymm2_1;
and ymm6_1@uint64 ymm2_1n ymm4_1;
not ymm2_2n@uint64 ymm2_2;
and ymm6_2@uint64 ymm2_2n ymm4_2;
not ymm2_3n@uint64 ymm2_3;
and ymm6_3@uint64 ymm2_3n ymm4_3;
(* vpandn %ymm0,%ymm4,%ymm10                       #! PC = 0x5555555791dc *)
not ymm4_0n@uint64 ymm4_0;
and ymm10_0@uint64 ymm4_0n ymm0_0;
not ymm4_1n@uint64 ymm4_1;
and ymm10_1@uint64 ymm4_1n ymm0_1;
not ymm4_2n@uint64 ymm4_2;
and ymm10_2@uint64 ymm4_2n ymm0_2;
not ymm4_3n@uint64 ymm4_3;
and ymm10_3@uint64 ymm4_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x5555555791e0 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm11,%ymm6,%ymm6                       #! PC = 0x5555555791e4 *)
xor ymm6_0@uint64 ymm6_0 ymm11_0;
xor ymm6_1@uint64 ymm6_1 ymm11_1;
xor ymm6_2@uint64 ymm6_2 ymm11_2;
xor ymm6_3@uint64 ymm6_3 ymm11_3;
(* vpsrlq $0x19,%ymm12,%ymm1                       #! PC = 0x5555555791e9 *)
shr ymm1_0 ymm12_0 0x19@uint64;
shr ymm1_1 ymm12_1 0x19@uint64;
shr ymm1_2 ymm12_2 0x19@uint64;
shr ymm1_3 ymm12_3 0x19@uint64;
(* vpxor  %ymm2,%ymm10,%ymm10                      #! PC = 0x5555555791ef *)
xor ymm10_0@uint64 ymm10_0 ymm2_0;
xor ymm10_1@uint64 ymm10_1 ymm2_1;
xor ymm10_2@uint64 ymm10_2 ymm2_2;
xor ymm10_3@uint64 ymm10_3 ymm2_3;
(* vmovdqa %ymm6,-0x70(%rbp)                       #! EA = L0x7fffffffbee0; PC = 0x5555555791f3 *)
mov L0x7fffffffbee0 ymm6_0;
mov L0x7fffffffbee8 ymm6_1;
mov L0x7fffffffbef0 ymm6_2;
mov L0x7fffffffbef8 ymm6_3;
(* vpsllq $0x27,%ymm12,%ymm12                      #! PC = 0x5555555791f8 *)
shl ymm12_0 ymm12_0 0x27@uint64;
shl ymm12_1 ymm12_1 0x27@uint64;
shl ymm12_2 ymm12_2 0x27@uint64;
shl ymm12_3 ymm12_3 0x27@uint64;
(* vpxor  %ymm4,%ymm0,%ymm6                        #! PC = 0x5555555791fe *)
xor ymm6_0@uint64 ymm0_0 ymm4_0;
xor ymm6_1@uint64 ymm0_1 ymm4_1;
xor ymm6_2@uint64 ymm0_2 ymm4_2;
xor ymm6_3@uint64 ymm0_3 ymm4_3;
(* vpsrlq $0x9,%ymm15,%ymm0                        #! PC = 0x555555579202 *)
shr ymm0_0 ymm15_0 0x9@uint64;
shr ymm0_1 ymm15_1 0x9@uint64;
shr ymm0_2 ymm15_2 0x9@uint64;
shr ymm0_3 ymm15_3 0x9@uint64;
(* vpsllq $0x37,%ymm15,%ymm15                      #! PC = 0x555555579208 *)
shl ymm15_0 ymm15_0 0x37@uint64;
shl ymm15_1 ymm15_1 0x37@uint64;
shl ymm15_2 ymm15_2 0x37@uint64;
shl ymm15_3 ymm15_3 0x37@uint64;
(* vpor   %ymm1,%ymm12,%ymm12                      #! PC = 0x55555557920e *)
or ymm12_0@uint64 ymm12_0 ymm1_0;
or ymm12_1@uint64 ymm12_1 ymm1_1;
or ymm12_2@uint64 ymm12_2 ymm1_2;
or ymm12_3@uint64 ymm12_3 ymm1_3;
(* vmovdqa %ymm6,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x555555579212 *)
mov L0x7fffffffbf00 ymm6_0;
mov L0x7fffffffbf08 ymm6_1;
mov L0x7fffffffbf10 ymm6_2;
mov L0x7fffffffbf18 ymm6_3;
(* vpor   %ymm0,%ymm15,%ymm15                      #! PC = 0x555555579217 *)
or ymm15_0@uint64 ymm15_0 ymm0_0;
or ymm15_1@uint64 ymm15_1 ymm0_1;
or ymm15_2@uint64 ymm15_2 ymm0_2;
or ymm15_3@uint64 ymm15_3 ymm0_3;
(* vpsrlq $0x2,%ymm14,%ymm2                        #! PC = 0x55555557921b *)
shr ymm2_0 ymm14_0 0x2@uint64;
shr ymm2_1 ymm14_1 0x2@uint64;
shr ymm2_2 ymm14_2 0x2@uint64;
shr ymm2_3 ymm14_3 0x2@uint64;
(* vpxor  -0x2d0(%rbp),%ymm13,%ymm1                #! EA = L0x7fffffffbc80; Value = 0x42b502eff3fce7df; PC = 0x555555579221 *)
xor ymm1_0@uint64 ymm13_0 L0x7fffffffbc80;
xor ymm1_1@uint64 ymm13_1 L0x7fffffffbc88;
xor ymm1_2@uint64 ymm13_2 L0x7fffffffbc90;
xor ymm1_3@uint64 ymm13_3 L0x7fffffffbc98;
(* vpsllq $0x3e,%ymm14,%ymm14                      #! PC = 0x555555579229 *)
shl ymm14_0 ymm14_0 0x3e@uint64;
shl ymm14_1 ymm14_1 0x3e@uint64;
shl ymm14_2 ymm14_2 0x3e@uint64;
shl ymm14_3 ymm14_3 0x3e@uint64;
(* vpandn %ymm12,%ymm15,%ymm6                      #! PC = 0x55555557922f *)
not ymm15_0n@uint64 ymm15_0;
and ymm6_0@uint64 ymm15_0n ymm12_0;
not ymm15_1n@uint64 ymm15_1;
and ymm6_1@uint64 ymm15_1n ymm12_1;
not ymm15_2n@uint64 ymm15_2;
and ymm6_2@uint64 ymm15_2n ymm12_2;
not ymm15_3n@uint64 ymm15_3;
and ymm6_3@uint64 ymm15_3n ymm12_3;
(* vpor   %ymm2,%ymm14,%ymm4                       #! PC = 0x555555579234 *)
or ymm4_0@uint64 ymm14_0 ymm2_0;
or ymm4_1@uint64 ymm14_1 ymm2_1;
or ymm4_2@uint64 ymm14_2 ymm2_2;
or ymm4_3@uint64 ymm14_3 ymm2_3;
(* vpxor  %ymm4,%ymm6,%ymm6                        #! PC = 0x555555579238 *)
xor ymm6_0@uint64 ymm6_0 ymm4_0;
xor ymm6_1@uint64 ymm6_1 ymm4_1;
xor ymm6_2@uint64 ymm6_2 ymm4_2;
xor ymm6_3@uint64 ymm6_3 ymm4_3;
(* vpxor  -0xb0(%rbp),%ymm6,%ymm0                  #! EA = L0x7fffffffbea0; Value = 0xaf39326eb3d059f6; PC = 0x55555557923c *)
xor ymm0_0@uint64 ymm6_0 L0x7fffffffbea0;
xor ymm0_1@uint64 ymm6_1 L0x7fffffffbea8;
xor ymm0_2@uint64 ymm6_2 L0x7fffffffbeb0;
xor ymm0_3@uint64 ymm6_3 L0x7fffffffbeb8;
(* vpxor  %ymm1,%ymm0,%ymm0                        #! PC = 0x555555579244 *)
xor ymm0_0@uint64 ymm0_0 ymm1_0;
xor ymm0_1@uint64 ymm0_1 ymm1_1;
xor ymm0_2@uint64 ymm0_2 ymm1_2;
xor ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpsrlq $0x17,%ymm5,%ymm1                        #! PC = 0x555555579248 *)
shr ymm1_0 ymm5_0 0x17@uint64;
shr ymm1_1 ymm5_1 0x17@uint64;
shr ymm1_2 ymm5_2 0x17@uint64;
shr ymm1_3 ymm5_3 0x17@uint64;
(* vpxor  -0x90(%rbp),%ymm0,%ymm0                  #! EA = L0x7fffffffbec0; Value = 0x0c77f40134a70cbc; PC = 0x55555557924d *)
xor ymm0_0@uint64 ymm0_0 L0x7fffffffbec0;
xor ymm0_1@uint64 ymm0_1 L0x7fffffffbec8;
xor ymm0_2@uint64 ymm0_2 L0x7fffffffbed0;
xor ymm0_3@uint64 ymm0_3 L0x7fffffffbed8;
(* vpsllq $0x29,%ymm5,%ymm5                        #! PC = 0x555555579255 *)
shl ymm5_0 ymm5_0 0x29@uint64;
shl ymm5_1 ymm5_1 0x29@uint64;
shl ymm5_2 ymm5_2 0x29@uint64;
shl ymm5_3 ymm5_3 0x29@uint64;
(* vpxor  -0x110(%rbp),%ymm7,%ymm14                #! EA = L0x7fffffffbe40; Value = 0x28bb4a3ea45041a4; PC = 0x55555557925a *)
xor ymm14_0@uint64 ymm7_0 L0x7fffffffbe40;
xor ymm14_1@uint64 ymm7_1 L0x7fffffffbe48;
xor ymm14_2@uint64 ymm7_2 L0x7fffffffbe50;
xor ymm14_3@uint64 ymm7_3 L0x7fffffffbe58;
(* vpxor  -0x330(%rbp),%ymm3,%ymm3                 #! EA = L0x7fffffffbc20; Value = 0xcfcd9cecd3ade3d5; PC = 0x555555579262 *)
xor ymm3_0@uint64 ymm3_0 L0x7fffffffbc20;
xor ymm3_1@uint64 ymm3_1 L0x7fffffffbc28;
xor ymm3_2@uint64 ymm3_2 L0x7fffffffbc30;
xor ymm3_3@uint64 ymm3_3 L0x7fffffffbc38;
(* vpor   %ymm1,%ymm5,%ymm11                       #! PC = 0x55555557926a *)
or ymm11_0@uint64 ymm5_0 ymm1_0;
or ymm11_1@uint64 ymm5_1 ymm1_1;
or ymm11_2@uint64 ymm5_2 ymm1_2;
or ymm11_3@uint64 ymm5_3 ymm1_3;
(* vpxor  -0x1f0(%rbp),%ymm8,%ymm5                 #! EA = L0x7fffffffbd60; Value = 0x6f650cbffc57ab01; PC = 0x55555557926e *)
xor ymm5_0@uint64 ymm8_0 L0x7fffffffbd60;
xor ymm5_1@uint64 ymm8_1 L0x7fffffffbd68;
xor ymm5_2@uint64 ymm8_2 L0x7fffffffbd70;
xor ymm5_3@uint64 ymm8_3 L0x7fffffffbd78;
(* vpxor  -0x70(%rbp),%ymm9,%ymm13                 #! EA = L0x7fffffffbee0; Value = 0x3874ebc3cf5c4aef; PC = 0x555555579276 *)
xor ymm13_0@uint64 ymm9_0 L0x7fffffffbee0;
xor ymm13_1@uint64 ymm9_1 L0x7fffffffbee8;
xor ymm13_2@uint64 ymm9_2 L0x7fffffffbef0;
xor ymm13_3@uint64 ymm9_3 L0x7fffffffbef8;
(* vpandn %ymm11,%ymm12,%ymm1                      #! PC = 0x55555557927b *)
not ymm12_0n@uint64 ymm12_0;
and ymm1_0@uint64 ymm12_0n ymm11_0;
not ymm12_1n@uint64 ymm12_1;
and ymm1_1@uint64 ymm12_1n ymm11_1;
not ymm12_2n@uint64 ymm12_2;
and ymm1_2@uint64 ymm12_2n ymm11_2;
not ymm12_3n@uint64 ymm12_3;
and ymm1_3@uint64 ymm12_3n ymm11_3;
(* vpxor  %ymm15,%ymm1,%ymm2                       #! PC = 0x555555579280 *)
xor ymm2_0@uint64 ymm1_0 ymm15_0;
xor ymm2_1@uint64 ymm1_1 ymm15_1;
xor ymm2_2@uint64 ymm1_2 ymm15_2;
xor ymm2_3@uint64 ymm1_3 ymm15_3;
(* vpsrlq $0x3e,%ymm3,%ymm1                        #! PC = 0x555555579285 *)
shr ymm1_0 ymm3_0 0x3e@uint64;
shr ymm1_1 ymm3_1 0x3e@uint64;
shr ymm1_2 ymm3_2 0x3e@uint64;
shr ymm1_3 ymm3_3 0x3e@uint64;
(* vpxor  %ymm5,%ymm14,%ymm14                      #! PC = 0x55555557928a *)
xor ymm14_0@uint64 ymm14_0 ymm5_0;
xor ymm14_1@uint64 ymm14_1 ymm5_1;
xor ymm14_2@uint64 ymm14_2 ymm5_2;
xor ymm14_3@uint64 ymm14_3 ymm5_3;
(* vmovdqa -0x1b0(%rbp),%ymm5                      #! EA = L0x7fffffffbda0; Value = 0x5640794d9b8e0201; PC = 0x55555557928e *)
mov ymm5_0 L0x7fffffffbda0;
mov ymm5_1 L0x7fffffffbda8;
mov ymm5_2 L0x7fffffffbdb0;
mov ymm5_3 L0x7fffffffbdb8;
(* vpsllq $0x2,%ymm3,%ymm3                         #! PC = 0x555555579296 *)
shl ymm3_0 ymm3_0 0x2@uint64;
shl ymm3_1 ymm3_1 0x2@uint64;
shl ymm3_2 ymm3_2 0x2@uint64;
shl ymm3_3 ymm3_3 0x2@uint64;
(* vpxor  %ymm2,%ymm14,%ymm14                      #! PC = 0x55555557929b *)
xor ymm14_0@uint64 ymm14_0 ymm2_0;
xor ymm14_1@uint64 ymm14_1 ymm2_1;
xor ymm14_2@uint64 ymm14_2 ymm2_2;
xor ymm14_3@uint64 ymm14_3 ymm2_3;
(* vmovdqa %ymm2,-0x350(%rbp)                      #! EA = L0x7fffffffbc00; PC = 0x55555557929f *)
mov L0x7fffffffbc00 ymm2_0;
mov L0x7fffffffbc08 ymm2_1;
mov L0x7fffffffbc10 ymm2_2;
mov L0x7fffffffbc18 ymm2_3;
(* vpor   %ymm1,%ymm3,%ymm2                        #! PC = 0x5555555792a7 *)
or ymm2_0@uint64 ymm3_0 ymm1_0;
or ymm2_1@uint64 ymm3_1 ymm1_1;
or ymm2_2@uint64 ymm3_2 ymm1_2;
or ymm2_3@uint64 ymm3_3 ymm1_3;
(* vmovdqa -0x190(%rbp),%ymm3                      #! EA = L0x7fffffffbdc0; Value = 0x2acf091a0c121018; PC = 0x5555555792ab *)
mov ymm3_0 L0x7fffffffbdc0;
mov ymm3_1 L0x7fffffffbdc8;
mov ymm3_2 L0x7fffffffbdd0;
mov ymm3_3 L0x7fffffffbdd8;
(* vpandn %ymm2,%ymm11,%ymm1                       #! PC = 0x5555555792b3 *)
not ymm11_0n@uint64 ymm11_0;
and ymm1_0@uint64 ymm11_0n ymm2_0;
not ymm11_1n@uint64 ymm11_1;
and ymm1_1@uint64 ymm11_1n ymm2_1;
not ymm11_2n@uint64 ymm11_2;
and ymm1_2@uint64 ymm11_2n ymm2_2;
not ymm11_3n@uint64 ymm11_3;
and ymm1_3@uint64 ymm11_3n ymm2_3;
(* vpxor  %ymm12,%ymm1,%ymm12                      #! PC = 0x5555555792b7 *)
xor ymm12_0@uint64 ymm1_0 ymm12_0;
xor ymm12_1@uint64 ymm1_1 ymm12_1;
xor ymm12_2@uint64 ymm1_2 ymm12_2;
xor ymm12_3@uint64 ymm1_3 ymm12_3;
(* vpxor  -0x310(%rbp),%ymm5,%ymm1                 #! EA = L0x7fffffffbc40; Value = 0x6964c6213cd3202d; PC = 0x5555555792bc *)
xor ymm1_0@uint64 ymm5_0 L0x7fffffffbc40;
xor ymm1_1@uint64 ymm5_1 L0x7fffffffbc48;
xor ymm1_2@uint64 ymm5_2 L0x7fffffffbc50;
xor ymm1_3@uint64 ymm5_3 L0x7fffffffbc58;
(* vpandn %ymm4,%ymm2,%ymm5                        #! PC = 0x5555555792c4 *)
not ymm2_0n@uint64 ymm2_0;
and ymm5_0@uint64 ymm2_0n ymm4_0;
not ymm2_1n@uint64 ymm2_1;
and ymm5_1@uint64 ymm2_1n ymm4_1;
not ymm2_2n@uint64 ymm2_2;
and ymm5_2@uint64 ymm2_2n ymm4_2;
not ymm2_3n@uint64 ymm2_3;
and ymm5_3@uint64 ymm2_3n ymm4_3;
(* vpxor  %ymm11,%ymm5,%ymm11                      #! PC = 0x5555555792c8 *)
xor ymm11_0@uint64 ymm5_0 ymm11_0;
xor ymm11_1@uint64 ymm5_1 ymm11_1;
xor ymm11_2@uint64 ymm5_2 ymm11_2;
xor ymm11_3@uint64 ymm5_3 ymm11_3;
(* vpxor  %ymm1,%ymm13,%ymm13                      #! PC = 0x5555555792cd *)
xor ymm13_0@uint64 ymm13_0 ymm1_0;
xor ymm13_1@uint64 ymm13_1 ymm1_1;
xor ymm13_2@uint64 ymm13_2 ymm1_2;
xor ymm13_3@uint64 ymm13_3 ymm1_3;
(* vpxor  -0x150(%rbp),%ymm3,%ymm1                 #! EA = L0x7fffffffbe00; Value = 0xf5841db03d841837; PC = 0x5555555792d1 *)
xor ymm1_0@uint64 ymm3_0 L0x7fffffffbe00;
xor ymm1_1@uint64 ymm3_1 L0x7fffffffbe08;
xor ymm1_2@uint64 ymm3_2 L0x7fffffffbe10;
xor ymm1_3@uint64 ymm3_3 L0x7fffffffbe18;
(* vpandn %ymm15,%ymm4,%ymm3                       #! PC = 0x5555555792d9 *)
not ymm4_0n@uint64 ymm4_0;
and ymm3_0@uint64 ymm4_0n ymm15_0;
not ymm4_1n@uint64 ymm4_1;
and ymm3_1@uint64 ymm4_1n ymm15_1;
not ymm4_2n@uint64 ymm4_2;
and ymm3_2@uint64 ymm4_2n ymm15_2;
not ymm4_3n@uint64 ymm4_3;
and ymm3_3@uint64 ymm4_3n ymm15_3;
(* vmovdqa %ymm11,-0x330(%rbp)                     #! EA = L0x7fffffffbc20; PC = 0x5555555792de *)
mov L0x7fffffffbc20 ymm11_0;
mov L0x7fffffffbc28 ymm11_1;
mov L0x7fffffffbc30 ymm11_2;
mov L0x7fffffffbc38 ymm11_3;
(* vpxor  %ymm11,%ymm10,%ymm11                     #! PC = 0x5555555792e6 *)
xor ymm11_0@uint64 ymm10_0 ymm11_0;
xor ymm11_1@uint64 ymm10_1 ymm11_1;
xor ymm11_2@uint64 ymm10_2 ymm11_2;
xor ymm11_3@uint64 ymm10_3 ymm11_3;
(* vmovdqa -0x130(%rbp),%ymm15                     #! EA = L0x7fffffffbe20; Value = 0xe7c314743c763a96; PC = 0x5555555792eb *)
mov ymm15_0 L0x7fffffffbe20;
mov ymm15_1 L0x7fffffffbe28;
mov ymm15_2 L0x7fffffffbe30;
mov ymm15_3 L0x7fffffffbe38;
(* vpxor  %ymm2,%ymm3,%ymm3                        #! PC = 0x5555555792f3 *)
xor ymm3_0@uint64 ymm3_0 ymm2_0;
xor ymm3_1@uint64 ymm3_1 ymm2_1;
xor ymm3_2@uint64 ymm3_2 ymm2_2;
xor ymm3_3@uint64 ymm3_3 ymm2_3;
(* vpxor  -0x2f0(%rbp),%ymm15,%ymm2                #! EA = L0x7fffffffbc60; Value = 0xa7158bb1a7922376; PC = 0x5555555792f7 *)
xor ymm2_0@uint64 ymm15_0 L0x7fffffffbc60;
xor ymm2_1@uint64 ymm15_1 L0x7fffffffbc68;
xor ymm2_2@uint64 ymm15_2 L0x7fffffffbc70;
xor ymm2_3@uint64 ymm15_3 L0x7fffffffbc78;
(* vpxor  %ymm1,%ymm11,%ymm11                      #! PC = 0x5555555792ff *)
xor ymm11_0@uint64 ymm11_0 ymm1_0;
xor ymm11_1@uint64 ymm11_1 ymm1_1;
xor ymm11_2@uint64 ymm11_2 ymm1_2;
xor ymm11_3@uint64 ymm11_3 ymm1_3;
(* vpxor  -0xd0(%rbp),%ymm3,%ymm1                  #! EA = L0x7fffffffbe80; Value = 0x14bc213555dcb238; PC = 0x555555579303 *)
xor ymm1_0@uint64 ymm3_0 L0x7fffffffbe80;
xor ymm1_1@uint64 ymm3_1 L0x7fffffffbe88;
xor ymm1_2@uint64 ymm3_2 L0x7fffffffbe90;
xor ymm1_3@uint64 ymm3_3 L0x7fffffffbe98;
(* vpxor  %ymm12,%ymm13,%ymm13                     #! PC = 0x55555557930b *)
xor ymm13_0@uint64 ymm13_0 ymm12_0;
xor ymm13_1@uint64 ymm13_1 ymm12_1;
xor ymm13_2@uint64 ymm13_2 ymm12_2;
xor ymm13_3@uint64 ymm13_3 ymm12_3;
(* vpxor  -0xf0(%rbp),%ymm11,%ymm11                #! EA = L0x7fffffffbe60; Value = 0x6989c2111a451228; PC = 0x555555579310 *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbe60;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbe68;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbe70;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbe78;
(* vpsllq $0x1,%ymm14,%ymm4                        #! PC = 0x555555579318 *)
shl ymm4_0 ymm14_0 0x1@uint64;
shl ymm4_1 ymm14_1 0x1@uint64;
shl ymm4_2 ymm14_2 0x1@uint64;
shl ymm4_3 ymm14_3 0x1@uint64;
(* vpxor  %ymm2,%ymm1,%ymm1                        #! PC = 0x55555557931e *)
xor ymm1_0@uint64 ymm1_0 ymm2_0;
xor ymm1_1@uint64 ymm1_1 ymm2_1;
xor ymm1_2@uint64 ymm1_2 ymm2_2;
xor ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpsrlq $0x3f,%ymm14,%ymm2                       #! PC = 0x555555579322 *)
shr ymm2_0 ymm14_0 0x3f@uint64;
shr ymm2_1 ymm14_1 0x3f@uint64;
shr ymm2_2 ymm14_2 0x3f@uint64;
shr ymm2_3 ymm14_3 0x3f@uint64;
(* vpxor  -0x50(%rbp),%ymm1,%ymm1                  #! EA = L0x7fffffffbf00; Value = 0x41ad501b5eaa2c21; PC = 0x555555579328 *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbf00;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbf08;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbf10;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbf18;
(* vpsrlq $0x3f,%ymm13,%ymm5                       #! PC = 0x55555557932d *)
shr ymm5_0 ymm13_0 0x3f@uint64;
shr ymm5_1 ymm13_1 0x3f@uint64;
shr ymm5_2 ymm13_2 0x3f@uint64;
shr ymm5_3 ymm13_3 0x3f@uint64;
(* vpsrlq $0x3f,%ymm11,%ymm15                      #! PC = 0x555555579333 *)
shr ymm15_0 ymm11_0 0x3f@uint64;
shr ymm15_1 ymm11_1 0x3f@uint64;
shr ymm15_2 ymm11_2 0x3f@uint64;
shr ymm15_3 ymm11_3 0x3f@uint64;
(* vpor   %ymm2,%ymm4,%ymm4                        #! PC = 0x555555579339 *)
or ymm4_0@uint64 ymm4_0 ymm2_0;
or ymm4_1@uint64 ymm4_1 ymm2_1;
or ymm4_2@uint64 ymm4_2 ymm2_2;
or ymm4_3@uint64 ymm4_3 ymm2_3;
(* vpsllq $0x1,%ymm13,%ymm2                        #! PC = 0x55555557933d *)
shl ymm2_0 ymm13_0 0x1@uint64;
shl ymm2_1 ymm13_1 0x1@uint64;
shl ymm2_2 ymm13_2 0x1@uint64;
shl ymm2_3 ymm13_3 0x1@uint64;
(* vpxor  %ymm1,%ymm4,%ymm4                        #! PC = 0x555555579343 *)
xor ymm4_0@uint64 ymm4_0 ymm1_0;
xor ymm4_1@uint64 ymm4_1 ymm1_1;
xor ymm4_2@uint64 ymm4_2 ymm1_2;
xor ymm4_3@uint64 ymm4_3 ymm1_3;
(* vpor   %ymm5,%ymm2,%ymm2                        #! PC = 0x555555579347 *)
or ymm2_0@uint64 ymm2_0 ymm5_0;
or ymm2_1@uint64 ymm2_1 ymm5_1;
or ymm2_2@uint64 ymm2_2 ymm5_2;
or ymm2_3@uint64 ymm2_3 ymm5_3;
(* vpsllq $0x1,%ymm11,%ymm5                        #! PC = 0x55555557934b *)
shl ymm5_0 ymm11_0 0x1@uint64;
shl ymm5_1 ymm11_1 0x1@uint64;
shl ymm5_2 ymm11_2 0x1@uint64;
shl ymm5_3 ymm11_3 0x1@uint64;
(* vpxor  %ymm6,%ymm4,%ymm6                        #! PC = 0x555555579351 *)
xor ymm6_0@uint64 ymm4_0 ymm6_0;
xor ymm6_1@uint64 ymm4_1 ymm6_1;
xor ymm6_2@uint64 ymm4_2 ymm6_2;
xor ymm6_3@uint64 ymm4_3 ymm6_3;
(* vpor   %ymm15,%ymm5,%ymm5                       #! PC = 0x555555579355 *)
or ymm5_0@uint64 ymm5_0 ymm15_0;
or ymm5_1@uint64 ymm5_1 ymm15_1;
or ymm5_2@uint64 ymm5_2 ymm15_2;
or ymm5_3@uint64 ymm5_3 ymm15_3;
(* vpxor  %ymm0,%ymm2,%ymm2                        #! PC = 0x55555557935a *)
xor ymm2_0@uint64 ymm2_0 ymm0_0;
xor ymm2_1@uint64 ymm2_1 ymm0_1;
xor ymm2_2@uint64 ymm2_2 ymm0_2;
xor ymm2_3@uint64 ymm2_3 ymm0_3;
(* vmovq  %r11,%xmm15                              #! PC = 0x55555557935e *)
mov xmm15_0 r11;
mov xmm15_1 0@uint64;
(* vpxor  %ymm14,%ymm5,%ymm14                      #! PC = 0x555555579363 *)
xor ymm14_0@uint64 ymm5_0 ymm14_0;
xor ymm14_1@uint64 ymm5_1 ymm14_1;
xor ymm14_2@uint64 ymm5_2 ymm14_2;
xor ymm14_3@uint64 ymm5_3 ymm14_3;
(* vpsrlq $0x3f,%ymm1,%ymm5                        #! PC = 0x555555579368 *)
shr ymm5_0 ymm1_0 0x3f@uint64;
shr ymm5_1 ymm1_1 0x3f@uint64;
shr ymm5_2 ymm1_2 0x3f@uint64;
shr ymm5_3 ymm1_3 0x3f@uint64;
(* vpxor  %ymm8,%ymm2,%ymm8                        #! PC = 0x55555557936d *)
xor ymm8_0@uint64 ymm2_0 ymm8_0;
xor ymm8_1@uint64 ymm2_1 ymm8_1;
xor ymm8_2@uint64 ymm2_2 ymm8_2;
xor ymm8_3@uint64 ymm2_3 ymm8_3;
(* vpsllq $0x1,%ymm1,%ymm1                         #! PC = 0x555555579372 *)
shl ymm1_0 ymm1_0 0x1@uint64;
shl ymm1_1 ymm1_1 0x1@uint64;
shl ymm1_2 ymm1_2 0x1@uint64;
shl ymm1_3 ymm1_3 0x1@uint64;
(* vpxor  %ymm9,%ymm14,%ymm9                       #! PC = 0x555555579377 *)
xor ymm9_0@uint64 ymm14_0 ymm9_0;
xor ymm9_1@uint64 ymm14_1 ymm9_1;
xor ymm9_2@uint64 ymm14_2 ymm9_2;
xor ymm9_3@uint64 ymm14_3 ymm9_3;
(* vpxor  %ymm7,%ymm2,%ymm7                        #! PC = 0x55555557937c *)
xor ymm7_0@uint64 ymm2_0 ymm7_0;
xor ymm7_1@uint64 ymm2_1 ymm7_1;
xor ymm7_2@uint64 ymm2_2 ymm7_2;
xor ymm7_3@uint64 ymm2_3 ymm7_3;
(* vpor   %ymm5,%ymm1,%ymm1                        #! PC = 0x555555579380 *)
or ymm1_0@uint64 ymm1_0 ymm5_0;
or ymm1_1@uint64 ymm1_1 ymm5_1;
or ymm1_2@uint64 ymm1_2 ymm5_2;
or ymm1_3@uint64 ymm1_3 ymm5_3;
(* vpxor  %ymm12,%ymm14,%ymm12                     #! PC = 0x555555579384 *)
xor ymm12_0@uint64 ymm14_0 ymm12_0;
xor ymm12_1@uint64 ymm14_1 ymm12_1;
xor ymm12_2@uint64 ymm14_2 ymm12_2;
xor ymm12_3@uint64 ymm14_3 ymm12_3;
(* vpxor  %ymm13,%ymm1,%ymm13                      #! PC = 0x555555579389 *)
xor ymm13_0@uint64 ymm1_0 ymm13_0;
xor ymm13_1@uint64 ymm1_1 ymm13_1;
xor ymm13_2@uint64 ymm1_2 ymm13_2;
xor ymm13_3@uint64 ymm1_3 ymm13_3;
(* vpsrlq $0x3f,%ymm0,%ymm1                        #! PC = 0x55555557938e *)
shr ymm1_0 ymm0_0 0x3f@uint64;
shr ymm1_1 ymm0_1 0x3f@uint64;
shr ymm1_2 ymm0_2 0x3f@uint64;
shr ymm1_3 ymm0_3 0x3f@uint64;
(* vpsllq $0x1,%ymm0,%ymm0                         #! PC = 0x555555579393 *)
shl ymm0_0 ymm0_0 0x1@uint64;
shl ymm0_1 ymm0_1 0x1@uint64;
shl ymm0_2 ymm0_2 0x1@uint64;
shl ymm0_3 ymm0_3 0x1@uint64;
(* vpxor  %ymm10,%ymm13,%ymm10                     #! PC = 0x555555579398 *)
xor ymm10_0@uint64 ymm13_0 ymm10_0;
xor ymm10_1@uint64 ymm13_1 ymm10_1;
xor ymm10_2@uint64 ymm13_2 ymm10_2;
xor ymm10_3@uint64 ymm13_3 ymm10_3;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x55555557939d *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpsrlq $0x14,%ymm8,%ymm1                        #! PC = 0x5555555793a1 *)
shr ymm1_0 ymm8_0 0x14@uint64;
shr ymm1_1 ymm8_1 0x14@uint64;
shr ymm1_2 ymm8_2 0x14@uint64;
shr ymm1_3 ymm8_3 0x14@uint64;
(* vpsllq $0x2c,%ymm8,%ymm8                        #! PC = 0x5555555793a7 *)
shl ymm8_0 ymm8_0 0x2c@uint64;
shl ymm8_1 ymm8_1 0x2c@uint64;
shl ymm8_2 ymm8_2 0x2c@uint64;
shl ymm8_3 ymm8_3 0x2c@uint64;
(* vpxor  %ymm11,%ymm0,%ymm11                      #! PC = 0x5555555793ad *)
xor ymm11_0@uint64 ymm0_0 ymm11_0;
xor ymm11_1@uint64 ymm0_1 ymm11_1;
xor ymm11_2@uint64 ymm0_2 ymm11_2;
xor ymm11_3@uint64 ymm0_3 ymm11_3;
(* vpxor  -0x90(%rbp),%ymm4,%ymm0                  #! EA = L0x7fffffffbec0; Value = 0x0c77f40134a70cbc; PC = 0x5555555793b2 *)
xor ymm0_0@uint64 ymm4_0 L0x7fffffffbec0;
xor ymm0_1@uint64 ymm4_1 L0x7fffffffbec8;
xor ymm0_2@uint64 ymm4_2 L0x7fffffffbed0;
xor ymm0_3@uint64 ymm4_3 L0x7fffffffbed8;
(* vpor   %ymm1,%ymm8,%ymm8                        #! PC = 0x5555555793ba *)
or ymm8_0@uint64 ymm8_0 ymm1_0;
or ymm8_1@uint64 ymm8_1 ymm1_1;
or ymm8_2@uint64 ymm8_2 ymm1_2;
or ymm8_3@uint64 ymm8_3 ymm1_3;
(* vpsrlq $0x15,%ymm9,%ymm1                        #! PC = 0x5555555793be *)
shr ymm1_0 ymm9_0 0x15@uint64;
shr ymm1_1 ymm9_1 0x15@uint64;
shr ymm1_2 ymm9_2 0x15@uint64;
shr ymm1_3 ymm9_3 0x15@uint64;
(* vpxor  %ymm3,%ymm11,%ymm3                       #! PC = 0x5555555793c4 *)
xor ymm3_0@uint64 ymm11_0 ymm3_0;
xor ymm3_1@uint64 ymm11_1 ymm3_1;
xor ymm3_2@uint64 ymm11_2 ymm3_2;
xor ymm3_3@uint64 ymm11_3 ymm3_3;
(* vpsllq $0x2b,%ymm9,%ymm9                        #! PC = 0x5555555793c8 *)
shl ymm9_0 ymm9_0 0x2b@uint64;
shl ymm9_1 ymm9_1 0x2b@uint64;
shl ymm9_2 ymm9_2 0x2b@uint64;
shl ymm9_3 ymm9_3 0x2b@uint64;
(* vpor   %ymm1,%ymm9,%ymm9                        #! PC = 0x5555555793ce *)
or ymm9_0@uint64 ymm9_0 ymm1_0;
or ymm9_1@uint64 ymm9_1 ymm1_1;
or ymm9_2@uint64 ymm9_2 ymm1_2;
or ymm9_3@uint64 ymm9_3 ymm1_3;
(* vpbroadcastq %xmm15,%ymm1                       #! PC = 0x5555555793d2 *)
mov ymm1_0 xmm15_0;
mov ymm1_1 xmm15_0;
mov ymm1_2 xmm15_0;
mov ymm1_3 xmm15_0;
(* vpandn %ymm9,%ymm8,%ymm5                        #! PC = 0x5555555793d7 *)
not ymm8_0n@uint64 ymm8_0;
and ymm5_0@uint64 ymm8_0n ymm9_0;
not ymm8_1n@uint64 ymm8_1;
and ymm5_1@uint64 ymm8_1n ymm9_1;
not ymm8_2n@uint64 ymm8_2;
and ymm5_2@uint64 ymm8_2n ymm9_2;
not ymm8_3n@uint64 ymm8_3;
and ymm5_3@uint64 ymm8_3n ymm9_3;
(* vpxor  %ymm5,%ymm1,%ymm1                        #! PC = 0x5555555793dc *)
xor ymm1_0@uint64 ymm1_0 ymm5_0;
xor ymm1_1@uint64 ymm1_1 ymm5_1;
xor ymm1_2@uint64 ymm1_2 ymm5_2;
xor ymm1_3@uint64 ymm1_3 ymm5_3;
(* vpxor  %ymm0,%ymm1,%ymm15                       #! PC = 0x5555555793e0 *)
xor ymm15_0@uint64 ymm1_0 ymm0_0;
xor ymm15_1@uint64 ymm1_1 ymm0_1;
xor ymm15_2@uint64 ymm1_2 ymm0_2;
xor ymm15_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x2b,%ymm10,%ymm1                       #! PC = 0x5555555793e4 *)
shr ymm1_0 ymm10_0 0x2b@uint64;
shr ymm1_1 ymm10_1 0x2b@uint64;
shr ymm1_2 ymm10_2 0x2b@uint64;
shr ymm1_3 ymm10_3 0x2b@uint64;
(* vpsllq $0x15,%ymm10,%ymm10                      #! PC = 0x5555555793ea *)
shl ymm10_0 ymm10_0 0x15@uint64;
shl ymm10_1 ymm10_1 0x15@uint64;
shl ymm10_2 ymm10_2 0x15@uint64;
shl ymm10_3 ymm10_3 0x15@uint64;
(* vmovdqa %ymm15,-0x90(%rbp)                      #! EA = L0x7fffffffbec0; PC = 0x5555555793f0 *)
mov L0x7fffffffbec0 ymm15_0;
mov L0x7fffffffbec8 ymm15_1;
mov L0x7fffffffbed0 ymm15_2;
mov L0x7fffffffbed8 ymm15_3;
(* vpor   %ymm1,%ymm10,%ymm10                      #! PC = 0x5555555793f8 *)
or ymm10_0@uint64 ymm10_0 ymm1_0;
or ymm10_1@uint64 ymm10_1 ymm1_1;
or ymm10_2@uint64 ymm10_2 ymm1_2;
or ymm10_3@uint64 ymm10_3 ymm1_3;
(* vpandn %ymm10,%ymm9,%ymm1                       #! PC = 0x5555555793fc *)
not ymm9_0n@uint64 ymm9_0;
and ymm1_0@uint64 ymm9_0n ymm10_0;
not ymm9_1n@uint64 ymm9_1;
and ymm1_1@uint64 ymm9_1n ymm10_1;
not ymm9_2n@uint64 ymm9_2;
and ymm1_2@uint64 ymm9_2n ymm10_2;
not ymm9_3n@uint64 ymm9_3;
and ymm1_3@uint64 ymm9_3n ymm10_3;
(* vpxor  %ymm8,%ymm1,%ymm1                        #! PC = 0x555555579401 *)
xor ymm1_0@uint64 ymm1_0 ymm8_0;
xor ymm1_1@uint64 ymm1_1 ymm8_1;
xor ymm1_2@uint64 ymm1_2 ymm8_2;
xor ymm1_3@uint64 ymm1_3 ymm8_3;
(* vpandn %ymm8,%ymm0,%ymm8                        #! PC = 0x555555579406 *)
not ymm0_0n@uint64 ymm0_0;
and ymm8_0@uint64 ymm0_0n ymm8_0;
not ymm0_1n@uint64 ymm0_1;
and ymm8_1@uint64 ymm0_1n ymm8_1;
not ymm0_2n@uint64 ymm0_2;
and ymm8_2@uint64 ymm0_2n ymm8_2;
not ymm0_3n@uint64 ymm0_3;
and ymm8_3@uint64 ymm0_3n ymm8_3;
(* vmovdqa %ymm1,-0x2b0(%rbp)                      #! EA = L0x7fffffffbca0; PC = 0x55555557940b *)
mov L0x7fffffffbca0 ymm1_0;
mov L0x7fffffffbca8 ymm1_1;
mov L0x7fffffffbcb0 ymm1_2;
mov L0x7fffffffbcb8 ymm1_3;
(* vpsrlq $0x32,%ymm3,%ymm1                        #! PC = 0x555555579413 *)
shr ymm1_0 ymm3_0 0x32@uint64;
shr ymm1_1 ymm3_1 0x32@uint64;
shr ymm1_2 ymm3_2 0x32@uint64;
shr ymm1_3 ymm3_3 0x32@uint64;
(* vpsllq $0xe,%ymm3,%ymm3                         #! PC = 0x555555579418 *)
shl ymm3_0 ymm3_0 0xe@uint64;
shl ymm3_1 ymm3_1 0xe@uint64;
shl ymm3_2 ymm3_2 0xe@uint64;
shl ymm3_3 ymm3_3 0xe@uint64;
(* vpor   %ymm1,%ymm3,%ymm3                        #! PC = 0x55555557941d *)
or ymm3_0@uint64 ymm3_0 ymm1_0;
or ymm3_1@uint64 ymm3_1 ymm1_1;
or ymm3_2@uint64 ymm3_2 ymm1_2;
or ymm3_3@uint64 ymm3_3 ymm1_3;
(* vpandn %ymm3,%ymm10,%ymm1                       #! PC = 0x555555579421 *)
not ymm10_0n@uint64 ymm10_0;
and ymm1_0@uint64 ymm10_0n ymm3_0;
not ymm10_1n@uint64 ymm10_1;
and ymm1_1@uint64 ymm10_1n ymm3_1;
not ymm10_2n@uint64 ymm10_2;
and ymm1_2@uint64 ymm10_2n ymm3_2;
not ymm10_3n@uint64 ymm10_3;
and ymm1_3@uint64 ymm10_3n ymm3_3;
(* vpxor  %ymm3,%ymm8,%ymm8                        #! PC = 0x555555579425 *)
xor ymm8_0@uint64 ymm8_0 ymm3_0;
xor ymm8_1@uint64 ymm8_1 ymm3_1;
xor ymm8_2@uint64 ymm8_2 ymm3_2;
xor ymm8_3@uint64 ymm8_3 ymm3_3;
(* vpxor  %ymm9,%ymm1,%ymm9                        #! PC = 0x555555579429 *)
xor ymm9_0@uint64 ymm1_0 ymm9_0;
xor ymm9_1@uint64 ymm1_1 ymm9_1;
xor ymm9_2@uint64 ymm1_2 ymm9_2;
xor ymm9_3@uint64 ymm1_3 ymm9_3;
(* vpandn %ymm0,%ymm3,%ymm1                        #! PC = 0x55555557942e *)
not ymm3_0n@uint64 ymm3_0;
and ymm1_0@uint64 ymm3_0n ymm0_0;
not ymm3_1n@uint64 ymm3_1;
and ymm1_1@uint64 ymm3_1n ymm0_1;
not ymm3_2n@uint64 ymm3_2;
and ymm1_2@uint64 ymm3_2n ymm0_2;
not ymm3_3n@uint64 ymm3_3;
and ymm1_3@uint64 ymm3_3n ymm0_3;
(* vmovdqa %ymm8,-0x270(%rbp)                      #! EA = L0x7fffffffbce0; PC = 0x555555579432 *)
mov L0x7fffffffbce0 ymm8_0;
mov L0x7fffffffbce8 ymm8_1;
mov L0x7fffffffbcf0 ymm8_2;
mov L0x7fffffffbcf8 ymm8_3;
(* vmovdqa %ymm9,-0x290(%rbp)                      #! EA = L0x7fffffffbcc0; PC = 0x55555557943a *)
mov L0x7fffffffbcc0 ymm9_0;
mov L0x7fffffffbcc8 ymm9_1;
mov L0x7fffffffbcd0 ymm9_2;
mov L0x7fffffffbcd8 ymm9_3;
(* vpxor  %ymm10,%ymm1,%ymm9                       #! PC = 0x555555579442 *)
xor ymm9_0@uint64 ymm1_0 ymm10_0;
xor ymm9_1@uint64 ymm1_1 ymm10_1;
xor ymm9_2@uint64 ymm1_2 ymm10_2;
xor ymm9_3@uint64 ymm1_3 ymm10_3;
(* vpxor  -0x150(%rbp),%ymm13,%ymm10               #! EA = L0x7fffffffbe00; Value = 0xf5841db03d841837; PC = 0x555555579447 *)
xor ymm10_0@uint64 ymm13_0 L0x7fffffffbe00;
xor ymm10_1@uint64 ymm13_1 L0x7fffffffbe08;
xor ymm10_2@uint64 ymm13_2 L0x7fffffffbe10;
xor ymm10_3@uint64 ymm13_3 L0x7fffffffbe18;
(* vmovdqa %ymm9,-0x170(%rbp)                      #! EA = L0x7fffffffbde0; PC = 0x55555557944f *)
mov L0x7fffffffbde0 ymm9_0;
mov L0x7fffffffbde8 ymm9_1;
mov L0x7fffffffbdf0 ymm9_2;
mov L0x7fffffffbdf8 ymm9_3;
(* vpxor  -0xf0(%rbp),%ymm13,%ymm9                 #! EA = L0x7fffffffbe60; Value = 0x6989c2111a451228; PC = 0x555555579457 *)
xor ymm9_0@uint64 ymm13_0 L0x7fffffffbe60;
xor ymm9_1@uint64 ymm13_1 L0x7fffffffbe68;
xor ymm9_2@uint64 ymm13_2 L0x7fffffffbe70;
xor ymm9_3@uint64 ymm13_3 L0x7fffffffbe78;
(* vpsrlq $0x24,%ymm10,%ymm1                       #! PC = 0x55555557945f *)
shr ymm1_0 ymm10_0 0x24@uint64;
shr ymm1_1 ymm10_1 0x24@uint64;
shr ymm1_2 ymm10_2 0x24@uint64;
shr ymm1_3 ymm10_3 0x24@uint64;
(* vpsllq $0x1c,%ymm10,%ymm0                       #! PC = 0x555555579465 *)
shl ymm0_0 ymm10_0 0x1c@uint64;
shl ymm0_1 ymm10_1 0x1c@uint64;
shl ymm0_2 ymm10_2 0x1c@uint64;
shl ymm0_3 ymm10_3 0x1c@uint64;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x55555557946b *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x130(%rbp),%ymm11,%ymm1                #! EA = L0x7fffffffbe20; Value = 0xe7c314743c763a96; PC = 0x55555557946f *)
xor ymm1_0@uint64 ymm11_0 L0x7fffffffbe20;
xor ymm1_1@uint64 ymm11_1 L0x7fffffffbe28;
xor ymm1_2@uint64 ymm11_2 L0x7fffffffbe30;
xor ymm1_3@uint64 ymm11_3 L0x7fffffffbe38;
(* vpsrlq $0x2c,%ymm1,%ymm3                        #! PC = 0x555555579477 *)
shr ymm3_0 ymm1_0 0x2c@uint64;
shr ymm3_1 ymm1_1 0x2c@uint64;
shr ymm3_2 ymm1_2 0x2c@uint64;
shr ymm3_3 ymm1_3 0x2c@uint64;
(* vpsllq $0x14,%ymm1,%ymm1                        #! PC = 0x55555557947c *)
shl ymm1_0 ymm1_0 0x14@uint64;
shl ymm1_1 ymm1_1 0x14@uint64;
shl ymm1_2 ymm1_2 0x14@uint64;
shl ymm1_3 ymm1_3 0x14@uint64;
(* vpor   %ymm3,%ymm1,%ymm1                        #! PC = 0x555555579481 *)
or ymm1_0@uint64 ymm1_0 ymm3_0;
or ymm1_1@uint64 ymm1_1 ymm3_1;
or ymm1_2@uint64 ymm1_2 ymm3_2;
or ymm1_3@uint64 ymm1_3 ymm3_3;
(* vpxor  -0x1d0(%rbp),%ymm4,%ymm3                 #! EA = L0x7fffffffbd80; Value = 0x179b677365470e31; PC = 0x555555579485 *)
xor ymm3_0@uint64 ymm4_0 L0x7fffffffbd80;
xor ymm3_1@uint64 ymm4_1 L0x7fffffffbd88;
xor ymm3_2@uint64 ymm4_2 L0x7fffffffbd90;
xor ymm3_3@uint64 ymm4_3 L0x7fffffffbd98;
(* vpsrlq $0x3d,%ymm3,%ymm5                        #! PC = 0x55555557948d *)
shr ymm5_0 ymm3_0 0x3d@uint64;
shr ymm5_1 ymm3_1 0x3d@uint64;
shr ymm5_2 ymm3_2 0x3d@uint64;
shr ymm5_3 ymm3_3 0x3d@uint64;
(* vpsllq $0x3,%ymm3,%ymm3                         #! PC = 0x555555579492 *)
shl ymm3_0 ymm3_0 0x3@uint64;
shl ymm3_1 ymm3_1 0x3@uint64;
shl ymm3_2 ymm3_2 0x3@uint64;
shl ymm3_3 ymm3_3 0x3@uint64;
(* vpor   %ymm5,%ymm3,%ymm3                        #! PC = 0x555555579497 *)
or ymm3_0@uint64 ymm3_0 ymm5_0;
or ymm3_1@uint64 ymm3_1 ymm5_1;
or ymm3_2@uint64 ymm3_2 ymm5_2;
or ymm3_3@uint64 ymm3_3 ymm5_3;
(* vpsrlq $0x13,%ymm7,%ymm5                        #! PC = 0x55555557949b *)
shr ymm5_0 ymm7_0 0x13@uint64;
shr ymm5_1 ymm7_1 0x13@uint64;
shr ymm5_2 ymm7_2 0x13@uint64;
shr ymm5_3 ymm7_3 0x13@uint64;
(* vpsllq $0x2d,%ymm7,%ymm7                        #! PC = 0x5555555794a0 *)
shl ymm7_0 ymm7_0 0x2d@uint64;
shl ymm7_1 ymm7_1 0x2d@uint64;
shl ymm7_2 ymm7_2 0x2d@uint64;
shl ymm7_3 ymm7_3 0x2d@uint64;
(* vpandn %ymm3,%ymm1,%ymm10                       #! PC = 0x5555555794a5 *)
not ymm1_0n@uint64 ymm1_0;
and ymm10_0@uint64 ymm1_0n ymm3_0;
not ymm1_1n@uint64 ymm1_1;
and ymm10_1@uint64 ymm1_1n ymm3_1;
not ymm1_2n@uint64 ymm1_2;
and ymm10_2@uint64 ymm1_2n ymm3_2;
not ymm1_3n@uint64 ymm1_3;
and ymm10_3@uint64 ymm1_3n ymm3_3;
(* vpor   %ymm5,%ymm7,%ymm15                       #! PC = 0x5555555794a9 *)
or ymm15_0@uint64 ymm7_0 ymm5_0;
or ymm15_1@uint64 ymm7_1 ymm5_1;
or ymm15_2@uint64 ymm7_2 ymm5_2;
or ymm15_3@uint64 ymm7_3 ymm5_3;
(* vpsrlq $0x3,%ymm12,%ymm5                        #! PC = 0x5555555794ad *)
shr ymm5_0 ymm12_0 0x3@uint64;
shr ymm5_1 ymm12_1 0x3@uint64;
shr ymm5_2 ymm12_2 0x3@uint64;
shr ymm5_3 ymm12_3 0x3@uint64;
(* vpxor  %ymm0,%ymm10,%ymm8                       #! PC = 0x5555555794b3 *)
xor ymm8_0@uint64 ymm10_0 ymm0_0;
xor ymm8_1@uint64 ymm10_1 ymm0_1;
xor ymm8_2@uint64 ymm10_2 ymm0_2;
xor ymm8_3@uint64 ymm10_3 ymm0_3;
(* vpsllq $0x3d,%ymm12,%ymm12                      #! PC = 0x5555555794b7 *)
shl ymm12_0 ymm12_0 0x3d@uint64;
shl ymm12_1 ymm12_1 0x3d@uint64;
shl ymm12_2 ymm12_2 0x3d@uint64;
shl ymm12_3 ymm12_3 0x3d@uint64;
(* vmovdqa %ymm8,-0x210(%rbp)                      #! EA = L0x7fffffffbd40; PC = 0x5555555794bd *)
mov L0x7fffffffbd40 ymm8_0;
mov L0x7fffffffbd48 ymm8_1;
mov L0x7fffffffbd50 ymm8_2;
mov L0x7fffffffbd58 ymm8_3;
(* vpandn %ymm15,%ymm3,%ymm8                       #! PC = 0x5555555794c5 *)
not ymm3_0n@uint64 ymm3_0;
and ymm8_0@uint64 ymm3_0n ymm15_0;
not ymm3_1n@uint64 ymm3_1;
and ymm8_1@uint64 ymm3_1n ymm15_1;
not ymm3_2n@uint64 ymm3_2;
and ymm8_2@uint64 ymm3_2n ymm15_2;
not ymm3_3n@uint64 ymm3_3;
and ymm8_3@uint64 ymm3_3n ymm15_3;
(* vpor   %ymm5,%ymm12,%ymm12                      #! PC = 0x5555555794ca *)
or ymm12_0@uint64 ymm12_0 ymm5_0;
or ymm12_1@uint64 ymm12_1 ymm5_1;
or ymm12_2@uint64 ymm12_2 ymm5_2;
or ymm12_3@uint64 ymm12_3 ymm5_3;
(* vpxor  %ymm1,%ymm8,%ymm8                        #! PC = 0x5555555794ce *)
xor ymm8_0@uint64 ymm8_0 ymm1_0;
xor ymm8_1@uint64 ymm8_1 ymm1_1;
xor ymm8_2@uint64 ymm8_2 ymm1_2;
xor ymm8_3@uint64 ymm8_3 ymm1_3;
(* vpandn %ymm12,%ymm15,%ymm5                      #! PC = 0x5555555794d2 *)
not ymm15_0n@uint64 ymm15_0;
and ymm5_0@uint64 ymm15_0n ymm12_0;
not ymm15_1n@uint64 ymm15_1;
and ymm5_1@uint64 ymm15_1n ymm12_1;
not ymm15_2n@uint64 ymm15_2;
and ymm5_2@uint64 ymm15_2n ymm12_2;
not ymm15_3n@uint64 ymm15_3;
and ymm5_3@uint64 ymm15_3n ymm12_3;
(* vpxor  %ymm3,%ymm5,%ymm7                        #! PC = 0x5555555794d7 *)
xor ymm7_0@uint64 ymm5_0 ymm3_0;
xor ymm7_1@uint64 ymm5_1 ymm3_1;
xor ymm7_2@uint64 ymm5_2 ymm3_2;
xor ymm7_3@uint64 ymm5_3 ymm3_3;
(* vpsrlq $0x27,%ymm9,%ymm5                        #! PC = 0x5555555794db *)
shr ymm5_0 ymm9_0 0x27@uint64;
shr ymm5_1 ymm9_1 0x27@uint64;
shr ymm5_2 ymm9_2 0x27@uint64;
shr ymm5_3 ymm9_3 0x27@uint64;
(* vpandn %ymm0,%ymm12,%ymm3                       #! PC = 0x5555555794e1 *)
not ymm12_0n@uint64 ymm12_0;
and ymm3_0@uint64 ymm12_0n ymm0_0;
not ymm12_1n@uint64 ymm12_1;
and ymm3_1@uint64 ymm12_1n ymm0_1;
not ymm12_2n@uint64 ymm12_2;
and ymm3_2@uint64 ymm12_2n ymm0_2;
not ymm12_3n@uint64 ymm12_3;
and ymm3_3@uint64 ymm12_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x5555555794e5 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm15,%ymm3,%ymm10                      #! PC = 0x5555555794e9 *)
xor ymm10_0@uint64 ymm3_0 ymm15_0;
xor ymm10_1@uint64 ymm3_1 ymm15_1;
xor ymm10_2@uint64 ymm3_2 ymm15_2;
xor ymm10_3@uint64 ymm3_3 ymm15_3;
(* vmovdqa %ymm7,-0x1d0(%rbp)                      #! EA = L0x7fffffffbd80; PC = 0x5555555794ee *)
mov L0x7fffffffbd80 ymm7_0;
mov L0x7fffffffbd88 ymm7_1;
mov L0x7fffffffbd90 ymm7_2;
mov L0x7fffffffbd98 ymm7_3;
(* vpxor  %ymm12,%ymm0,%ymm12                      #! PC = 0x5555555794f6 *)
xor ymm12_0@uint64 ymm0_0 ymm12_0;
xor ymm12_1@uint64 ymm0_1 ymm12_1;
xor ymm12_2@uint64 ymm0_2 ymm12_2;
xor ymm12_3@uint64 ymm0_3 ymm12_3;
(* vpxor  -0x1f0(%rbp),%ymm2,%ymm0                 #! EA = L0x7fffffffbd60; Value = 0x6f650cbffc57ab01; PC = 0x5555555794fb *)
xor ymm0_0@uint64 ymm2_0 L0x7fffffffbd60;
xor ymm0_1@uint64 ymm2_1 L0x7fffffffbd68;
xor ymm0_2@uint64 ymm2_2 L0x7fffffffbd70;
xor ymm0_3@uint64 ymm2_3 L0x7fffffffbd78;
(* vmovdqa %ymm10,-0x250(%rbp)                     #! EA = L0x7fffffffbd00; PC = 0x555555579503 *)
mov L0x7fffffffbd00 ymm10_0;
mov L0x7fffffffbd08 ymm10_1;
mov L0x7fffffffbd10 ymm10_2;
mov L0x7fffffffbd18 ymm10_3;
(* vpsllq $0x19,%ymm9,%ymm9                        #! PC = 0x55555557950b *)
shl ymm9_0 ymm9_0 0x19@uint64;
shl ymm9_1 ymm9_1 0x19@uint64;
shl ymm9_2 ymm9_2 0x19@uint64;
shl ymm9_3 ymm9_3 0x19@uint64;
(* vmovdqa %ymm12,-0x150(%rbp)                     #! EA = L0x7fffffffbe00; PC = 0x555555579511 *)
mov L0x7fffffffbe00 ymm12_0;
mov L0x7fffffffbe08 ymm12_1;
mov L0x7fffffffbe10 ymm12_2;
mov L0x7fffffffbe18 ymm12_3;
(* vpsrlq $0x3f,%ymm0,%ymm1                        #! PC = 0x555555579519 *)
shr ymm1_0 ymm0_0 0x3f@uint64;
shr ymm1_1 ymm0_1 0x3f@uint64;
shr ymm1_2 ymm0_2 0x3f@uint64;
shr ymm1_3 ymm0_3 0x3f@uint64;
(* vpsllq $0x1,%ymm0,%ymm0                         #! PC = 0x55555557951e *)
shl ymm0_0 ymm0_0 0x1@uint64;
shl ymm0_1 ymm0_1 0x1@uint64;
shl ymm0_2 ymm0_2 0x1@uint64;
shl ymm0_3 ymm0_3 0x1@uint64;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x555555579523 *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x1b0(%rbp),%ymm14,%ymm1                #! EA = L0x7fffffffbda0; Value = 0x5640794d9b8e0201; PC = 0x555555579527 *)
xor ymm1_0@uint64 ymm14_0 L0x7fffffffbda0;
xor ymm1_1@uint64 ymm14_1 L0x7fffffffbda8;
xor ymm1_2@uint64 ymm14_2 L0x7fffffffbdb0;
xor ymm1_3@uint64 ymm14_3 L0x7fffffffbdb8;
(* vpsrlq $0x3a,%ymm1,%ymm3                        #! PC = 0x55555557952f *)
shr ymm3_0 ymm1_0 0x3a@uint64;
shr ymm3_1 ymm1_1 0x3a@uint64;
shr ymm3_2 ymm1_2 0x3a@uint64;
shr ymm3_3 ymm1_3 0x3a@uint64;
(* vpsllq $0x6,%ymm1,%ymm1                         #! PC = 0x555555579534 *)
shl ymm1_0 ymm1_0 0x6@uint64;
shl ymm1_1 ymm1_1 0x6@uint64;
shl ymm1_2 ymm1_2 0x6@uint64;
shl ymm1_3 ymm1_3 0x6@uint64;
(* vpor   %ymm3,%ymm1,%ymm1                        #! PC = 0x555555579539 *)
or ymm1_0@uint64 ymm1_0 ymm3_0;
or ymm1_1@uint64 ymm1_1 ymm3_1;
or ymm1_2@uint64 ymm1_2 ymm3_2;
or ymm1_3@uint64 ymm1_3 ymm3_3;
(* vpor   %ymm5,%ymm9,%ymm3                        #! PC = 0x55555557953d *)
or ymm3_0@uint64 ymm9_0 ymm5_0;
or ymm3_1@uint64 ymm9_1 ymm5_1;
or ymm3_2@uint64 ymm9_2 ymm5_2;
or ymm3_3@uint64 ymm9_3 ymm5_3;
(* vpandn %ymm3,%ymm1,%ymm5                        #! PC = 0x555555579541 *)
not ymm1_0n@uint64 ymm1_0;
and ymm5_0@uint64 ymm1_0n ymm3_0;
not ymm1_1n@uint64 ymm1_1;
and ymm5_1@uint64 ymm1_1n ymm3_1;
not ymm1_2n@uint64 ymm1_2;
and ymm5_2@uint64 ymm1_2n ymm3_2;
not ymm1_3n@uint64 ymm1_3;
and ymm5_3@uint64 ymm1_3n ymm3_3;
(* vpxor  %ymm0,%ymm5,%ymm12                       #! PC = 0x555555579545 *)
xor ymm12_0@uint64 ymm5_0 ymm0_0;
xor ymm12_1@uint64 ymm5_1 ymm0_1;
xor ymm12_2@uint64 ymm5_2 ymm0_2;
xor ymm12_3@uint64 ymm5_3 ymm0_3;
(* vmovdqa %ymm12,-0x130(%rbp)                     #! EA = L0x7fffffffbe20; PC = 0x555555579549 *)
mov L0x7fffffffbe20 ymm12_0;
mov L0x7fffffffbe28 ymm12_1;
mov L0x7fffffffbe30 ymm12_2;
mov L0x7fffffffbe38 ymm12_3;
(* vpxor  -0x50(%rbp),%ymm11,%ymm5                 #! EA = L0x7fffffffbf00; Value = 0x41ad501b5eaa2c21; PC = 0x555555579551 *)
xor ymm5_0@uint64 ymm11_0 L0x7fffffffbf00;
xor ymm5_1@uint64 ymm11_1 L0x7fffffffbf08;
xor ymm5_2@uint64 ymm11_2 L0x7fffffffbf10;
xor ymm5_3@uint64 ymm11_3 L0x7fffffffbf18;
(* vpshufb 0x549a1(%rip),%ymm5,%ymm5        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x555555579556 *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm5_0, ymm5_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm5_2, ymm5_3, tmp_2, tmp_3);
mov ymm5_0 tmp_0;
mov ymm5_1 tmp_1;
mov ymm5_2 tmp_2;
mov ymm5_3 tmp_3;
(* vpandn %ymm5,%ymm3,%ymm7                        #! PC = 0x55555557955f *)
not ymm3_0n@uint64 ymm3_0;
and ymm7_0@uint64 ymm3_0n ymm5_0;
not ymm3_1n@uint64 ymm3_1;
and ymm7_1@uint64 ymm3_1n ymm5_1;
not ymm3_2n@uint64 ymm3_2;
and ymm7_2@uint64 ymm3_2n ymm5_2;
not ymm3_3n@uint64 ymm3_3;
and ymm7_3@uint64 ymm3_3n ymm5_3;
(* vpxor  %ymm1,%ymm7,%ymm15                       #! PC = 0x555555579563 *)
xor ymm15_0@uint64 ymm7_0 ymm1_0;
xor ymm15_1@uint64 ymm7_1 ymm1_1;
xor ymm15_2@uint64 ymm7_2 ymm1_2;
xor ymm15_3@uint64 ymm7_3 ymm1_3;
(* vpsrlq $0x2e,%ymm6,%ymm7                        #! PC = 0x555555579567 *)
shr ymm7_0 ymm6_0 0x2e@uint64;
shr ymm7_1 ymm6_1 0x2e@uint64;
shr ymm7_2 ymm6_2 0x2e@uint64;
shr ymm7_3 ymm6_3 0x2e@uint64;
(* vpsllq $0x12,%ymm6,%ymm6                        #! PC = 0x55555557956c *)
shl ymm6_0 ymm6_0 0x12@uint64;
shl ymm6_1 ymm6_1 0x12@uint64;
shl ymm6_2 ymm6_2 0x12@uint64;
shl ymm6_3 ymm6_3 0x12@uint64;
(* vmovdqa %ymm15,-0x1f0(%rbp)                     #! EA = L0x7fffffffbd60; PC = 0x555555579571 *)
mov L0x7fffffffbd60 ymm15_0;
mov L0x7fffffffbd68 ymm15_1;
mov L0x7fffffffbd70 ymm15_2;
mov L0x7fffffffbd78 ymm15_3;
(* vpor   %ymm7,%ymm6,%ymm12                       #! PC = 0x555555579579 *)
or ymm12_0@uint64 ymm6_0 ymm7_0;
or ymm12_1@uint64 ymm6_1 ymm7_1;
or ymm12_2@uint64 ymm6_2 ymm7_2;
or ymm12_3@uint64 ymm6_3 ymm7_3;
(* vpxor  -0x110(%rbp),%ymm2,%ymm7                 #! EA = L0x7fffffffbe40; Value = 0x28bb4a3ea45041a4; PC = 0x55555557957d *)
xor ymm7_0@uint64 ymm2_0 L0x7fffffffbe40;
xor ymm7_1@uint64 ymm2_1 L0x7fffffffbe48;
xor ymm7_2@uint64 ymm2_2 L0x7fffffffbe50;
xor ymm7_3@uint64 ymm2_3 L0x7fffffffbe58;
(* vpandn %ymm12,%ymm5,%ymm9                       #! PC = 0x555555579585 *)
not ymm5_0n@uint64 ymm5_0;
and ymm9_0@uint64 ymm5_0n ymm12_0;
not ymm5_1n@uint64 ymm5_1;
and ymm9_1@uint64 ymm5_1n ymm12_1;
not ymm5_2n@uint64 ymm5_2;
and ymm9_2@uint64 ymm5_2n ymm12_2;
not ymm5_3n@uint64 ymm5_3;
and ymm9_3@uint64 ymm5_3n ymm12_3;
(* vpxor  %ymm3,%ymm9,%ymm9                        #! PC = 0x55555557958a *)
xor ymm9_0@uint64 ymm9_0 ymm3_0;
xor ymm9_1@uint64 ymm9_1 ymm3_1;
xor ymm9_2@uint64 ymm9_2 ymm3_2;
xor ymm9_3@uint64 ymm9_3 ymm3_3;
(* vpandn %ymm0,%ymm12,%ymm3                       #! PC = 0x55555557958e *)
not ymm12_0n@uint64 ymm12_0;
and ymm3_0@uint64 ymm12_0n ymm0_0;
not ymm12_1n@uint64 ymm12_1;
and ymm3_1@uint64 ymm12_1n ymm0_1;
not ymm12_2n@uint64 ymm12_2;
and ymm3_2@uint64 ymm12_2n ymm0_2;
not ymm12_3n@uint64 ymm12_3;
and ymm3_3@uint64 ymm12_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x555555579592 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  -0x2f0(%rbp),%ymm11,%ymm1                #! EA = L0x7fffffffbc60; Value = 0xa7158bb1a7922376; PC = 0x555555579596 *)
xor ymm1_0@uint64 ymm11_0 L0x7fffffffbc60;
xor ymm1_1@uint64 ymm11_1 L0x7fffffffbc68;
xor ymm1_2@uint64 ymm11_2 L0x7fffffffbc70;
xor ymm1_3@uint64 ymm11_3 L0x7fffffffbc78;
(* vpxor  %ymm12,%ymm0,%ymm0                       #! PC = 0x55555557959e *)
xor ymm0_0@uint64 ymm0_0 ymm12_0;
xor ymm0_1@uint64 ymm0_1 ymm12_1;
xor ymm0_2@uint64 ymm0_2 ymm12_2;
xor ymm0_3@uint64 ymm0_3 ymm12_3;
(* vpxor  %ymm5,%ymm3,%ymm10                       #! PC = 0x5555555795a3 *)
xor ymm10_0@uint64 ymm3_0 ymm5_0;
xor ymm10_1@uint64 ymm3_1 ymm5_1;
xor ymm10_2@uint64 ymm3_2 ymm5_2;
xor ymm10_3@uint64 ymm3_3 ymm5_3;
(* vmovdqa %ymm0,-0x230(%rbp)                      #! EA = L0x7fffffffbd20; PC = 0x5555555795a7 *)
mov L0x7fffffffbd20 ymm0_0;
mov L0x7fffffffbd28 ymm0_1;
mov L0x7fffffffbd30 ymm0_2;
mov L0x7fffffffbd38 ymm0_3;
(* vpsrlq $0x36,%ymm7,%ymm12                       #! PC = 0x5555555795af *)
shr ymm12_0 ymm7_0 0x36@uint64;
shr ymm12_1 ymm7_1 0x36@uint64;
shr ymm12_2 ymm7_2 0x36@uint64;
shr ymm12_3 ymm7_3 0x36@uint64;
(* vpsllq $0xa,%ymm7,%ymm7                         #! PC = 0x5555555795b4 *)
shl ymm7_0 ymm7_0 0xa@uint64;
shl ymm7_1 ymm7_1 0xa@uint64;
shl ymm7_2 ymm7_2 0xa@uint64;
shl ymm7_3 ymm7_3 0xa@uint64;
(* vpxor  -0xd0(%rbp),%ymm11,%ymm11                #! EA = L0x7fffffffbe80; Value = 0x14bc213555dcb238; PC = 0x5555555795b9 *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbe80;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbe88;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbe90;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbe98;
(* vpsrlq $0x25,%ymm1,%ymm3                        #! PC = 0x5555555795c1 *)
shr ymm3_0 ymm1_0 0x25@uint64;
shr ymm3_1 ymm1_1 0x25@uint64;
shr ymm3_2 ymm1_2 0x25@uint64;
shr ymm3_3 ymm1_3 0x25@uint64;
(* vpsllq $0x1b,%ymm1,%ymm0                        #! PC = 0x5555555795c6 *)
shl ymm0_0 ymm1_0 0x1b@uint64;
shl ymm0_1 ymm1_1 0x1b@uint64;
shl ymm0_2 ymm1_2 0x1b@uint64;
shl ymm0_3 ymm1_3 0x1b@uint64;
(* vpxor  -0x2d0(%rbp),%ymm4,%ymm1                 #! EA = L0x7fffffffbc80; Value = 0x42b502eff3fce7df; PC = 0x5555555795cb *)
xor ymm1_0@uint64 ymm4_0 L0x7fffffffbc80;
xor ymm1_1@uint64 ymm4_1 L0x7fffffffbc88;
xor ymm1_2@uint64 ymm4_2 L0x7fffffffbc90;
xor ymm1_3@uint64 ymm4_3 L0x7fffffffbc98;
(* vmovdqa %ymm10,-0x1b0(%rbp)                     #! EA = L0x7fffffffbda0; PC = 0x5555555795d3 *)
mov L0x7fffffffbda0 ymm10_0;
mov L0x7fffffffbda8 ymm10_1;
mov L0x7fffffffbdb0 ymm10_2;
mov L0x7fffffffbdb8 ymm10_3;
(* vpor   %ymm7,%ymm12,%ymm12                      #! PC = 0x5555555795db *)
or ymm12_0@uint64 ymm12_0 ymm7_0;
or ymm12_1@uint64 ymm12_1 ymm7_1;
or ymm12_2@uint64 ymm12_2 ymm7_2;
or ymm12_3@uint64 ymm12_3 ymm7_3;
(* vpor   %ymm3,%ymm0,%ymm0                        #! PC = 0x5555555795df *)
or ymm0_0@uint64 ymm0_0 ymm3_0;
or ymm0_1@uint64 ymm0_1 ymm3_1;
or ymm0_2@uint64 ymm0_2 ymm3_2;
or ymm0_3@uint64 ymm0_3 ymm3_3;
(* vpxor  -0xb0(%rbp),%ymm4,%ymm4                  #! EA = L0x7fffffffbea0; Value = 0xaf39326eb3d059f6; PC = 0x5555555795e3 *)
xor ymm4_0@uint64 ymm4_0 L0x7fffffffbea0;
xor ymm4_1@uint64 ymm4_1 L0x7fffffffbea8;
xor ymm4_2@uint64 ymm4_2 L0x7fffffffbeb0;
xor ymm4_3@uint64 ymm4_3 L0x7fffffffbeb8;
(* vpsrlq $0x1c,%ymm1,%ymm10                       #! PC = 0x5555555795eb *)
shr ymm10_0 ymm1_0 0x1c@uint64;
shr ymm10_1 ymm1_1 0x1c@uint64;
shr ymm10_2 ymm1_2 0x1c@uint64;
shr ymm10_3 ymm1_3 0x1c@uint64;
(* vpsllq $0x24,%ymm1,%ymm1                        #! PC = 0x5555555795f0 *)
shl ymm1_0 ymm1_0 0x24@uint64;
shl ymm1_1 ymm1_1 0x24@uint64;
shl ymm1_2 ymm1_2 0x24@uint64;
shl ymm1_3 ymm1_3 0x24@uint64;
(* vpor   %ymm1,%ymm10,%ymm10                      #! PC = 0x5555555795f5 *)
or ymm10_0@uint64 ymm10_0 ymm1_0;
or ymm10_1@uint64 ymm10_1 ymm1_1;
or ymm10_2@uint64 ymm10_2 ymm1_2;
or ymm10_3@uint64 ymm10_3 ymm1_3;
(* vpxor  -0x330(%rbp),%ymm13,%ymm1                #! EA = L0x7fffffffbc20; Value = 0xefd2f940cf583917; PC = 0x5555555795f9 *)
xor ymm1_0@uint64 ymm13_0 L0x7fffffffbc20;
xor ymm1_1@uint64 ymm13_1 L0x7fffffffbc28;
xor ymm1_2@uint64 ymm13_2 L0x7fffffffbc30;
xor ymm1_3@uint64 ymm13_3 L0x7fffffffbc38;
(* vpxor  -0x190(%rbp),%ymm13,%ymm13               #! EA = L0x7fffffffbdc0; Value = 0x2acf091a0c121018; PC = 0x555555579601 *)
xor ymm13_0@uint64 ymm13_0 L0x7fffffffbdc0;
xor ymm13_1@uint64 ymm13_1 L0x7fffffffbdc8;
xor ymm13_2@uint64 ymm13_2 L0x7fffffffbdd0;
xor ymm13_3@uint64 ymm13_3 L0x7fffffffbdd8;
(* vpandn %ymm12,%ymm10,%ymm3                      #! PC = 0x555555579609 *)
not ymm10_0n@uint64 ymm10_0;
and ymm3_0@uint64 ymm10_0n ymm12_0;
not ymm10_1n@uint64 ymm10_1;
and ymm3_1@uint64 ymm10_1n ymm12_1;
not ymm10_2n@uint64 ymm10_2;
and ymm3_2@uint64 ymm10_2n ymm12_2;
not ymm10_3n@uint64 ymm10_3;
and ymm3_3@uint64 ymm10_3n ymm12_3;
(* vpshufb 0x548c9(%rip),%ymm1,%ymm1        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x55555557960e *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm1_0, ymm1_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm1_2, ymm1_3, tmp_2, tmp_3);
mov ymm1_0 tmp_0;
mov ymm1_1 tmp_1;
mov ymm1_2 tmp_2;
mov ymm1_3 tmp_3;
(* vpxor  %ymm0,%ymm3,%ymm6                        #! PC = 0x555555579617 *)
xor ymm6_0@uint64 ymm3_0 ymm0_0;
xor ymm6_1@uint64 ymm3_1 ymm0_1;
xor ymm6_2@uint64 ymm3_2 ymm0_2;
xor ymm6_3@uint64 ymm3_3 ymm0_3;
(* vmovdqa %ymm6,%ymm15                            #! PC = 0x55555557961b *)
mov ymm15_0 ymm6_0;
mov ymm15_1 ymm6_1;
mov ymm15_2 ymm6_2;
mov ymm15_3 ymm6_3;
(* vpxor  -0x70(%rbp),%ymm14,%ymm6                 #! EA = L0x7fffffffbee0; Value = 0x3874ebc3cf5c4aef; PC = 0x55555557961f *)
xor ymm6_0@uint64 ymm14_0 L0x7fffffffbee0;
xor ymm6_1@uint64 ymm14_1 L0x7fffffffbee8;
xor ymm6_2@uint64 ymm14_2 L0x7fffffffbef0;
xor ymm6_3@uint64 ymm14_3 L0x7fffffffbef8;
(* vpxor  -0x310(%rbp),%ymm14,%ymm14               #! EA = L0x7fffffffbc40; Value = 0x6964c6213cd3202d; PC = 0x555555579624 *)
xor ymm14_0@uint64 ymm14_0 L0x7fffffffbc40;
xor ymm14_1@uint64 ymm14_1 L0x7fffffffbc48;
xor ymm14_2@uint64 ymm14_2 L0x7fffffffbc50;
xor ymm14_3@uint64 ymm14_3 L0x7fffffffbc58;
(* vmovdqa %ymm15,-0x2d0(%rbp)                     #! EA = L0x7fffffffbc80; PC = 0x55555557962c *)
mov L0x7fffffffbc80 ymm15_0;
mov L0x7fffffffbc88 ymm15_1;
mov L0x7fffffffbc90 ymm15_2;
mov L0x7fffffffbc98 ymm15_3;
(* vpsrlq $0x31,%ymm6,%ymm7                        #! PC = 0x555555579634 *)
shr ymm7_0 ymm6_0 0x31@uint64;
shr ymm7_1 ymm6_1 0x31@uint64;
shr ymm7_2 ymm6_2 0x31@uint64;
shr ymm7_3 ymm6_3 0x31@uint64;
(* vpsllq $0xf,%ymm6,%ymm6                         #! PC = 0x555555579639 *)
shl ymm6_0 ymm6_0 0xf@uint64;
shl ymm6_1 ymm6_1 0xf@uint64;
shl ymm6_2 ymm6_2 0xf@uint64;
shl ymm6_3 ymm6_3 0xf@uint64;
(* vpor   %ymm6,%ymm7,%ymm3                        #! PC = 0x55555557963e *)
or ymm3_0@uint64 ymm7_0 ymm6_0;
or ymm3_1@uint64 ymm7_1 ymm6_1;
or ymm3_2@uint64 ymm7_2 ymm6_2;
or ymm3_3@uint64 ymm7_3 ymm6_3;
(* vpandn %ymm1,%ymm3,%ymm5                        #! PC = 0x555555579642 *)
not ymm3_0n@uint64 ymm3_0;
and ymm5_0@uint64 ymm3_0n ymm1_0;
not ymm3_1n@uint64 ymm3_1;
and ymm5_1@uint64 ymm3_1n ymm1_1;
not ymm3_2n@uint64 ymm3_2;
and ymm5_2@uint64 ymm3_2n ymm1_2;
not ymm3_3n@uint64 ymm3_3;
and ymm5_3@uint64 ymm3_3n ymm1_3;
(* vpandn %ymm3,%ymm12,%ymm6                       #! PC = 0x555555579646 *)
not ymm12_0n@uint64 ymm12_0;
and ymm6_0@uint64 ymm12_0n ymm3_0;
not ymm12_1n@uint64 ymm12_1;
and ymm6_1@uint64 ymm12_1n ymm3_1;
not ymm12_2n@uint64 ymm12_2;
and ymm6_2@uint64 ymm12_2n ymm3_2;
not ymm12_3n@uint64 ymm12_3;
and ymm6_3@uint64 ymm12_3n ymm3_3;
(* vpxor  %ymm12,%ymm5,%ymm7                       #! PC = 0x55555557964a *)
xor ymm7_0@uint64 ymm5_0 ymm12_0;
xor ymm7_1@uint64 ymm5_1 ymm12_1;
xor ymm7_2@uint64 ymm5_2 ymm12_2;
xor ymm7_3@uint64 ymm5_3 ymm12_3;
(* vpsrlq $0x19,%ymm11,%ymm5                       #! PC = 0x55555557964f *)
shr ymm5_0 ymm11_0 0x19@uint64;
shr ymm5_1 ymm11_1 0x19@uint64;
shr ymm5_2 ymm11_2 0x19@uint64;
shr ymm5_3 ymm11_3 0x19@uint64;
(* vpxor  %ymm10,%ymm6,%ymm6                       #! PC = 0x555555579655 *)
xor ymm6_0@uint64 ymm6_0 ymm10_0;
xor ymm6_1@uint64 ymm6_1 ymm10_1;
xor ymm6_2@uint64 ymm6_2 ymm10_2;
xor ymm6_3@uint64 ymm6_3 ymm10_3;
(* vmovdqa %ymm7,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x55555557965a *)
mov L0x7fffffffbf00 ymm7_0;
mov L0x7fffffffbf08 ymm7_1;
mov L0x7fffffffbf10 ymm7_2;
mov L0x7fffffffbf18 ymm7_3;
(* vpandn %ymm0,%ymm1,%ymm7                        #! PC = 0x55555557965f *)
not ymm1_0n@uint64 ymm1_0;
and ymm7_0@uint64 ymm1_0n ymm0_0;
not ymm1_1n@uint64 ymm1_1;
and ymm7_1@uint64 ymm1_1n ymm0_1;
not ymm1_2n@uint64 ymm1_2;
and ymm7_2@uint64 ymm1_2n ymm0_2;
not ymm1_3n@uint64 ymm1_3;
and ymm7_3@uint64 ymm1_3n ymm0_3;
(* vpandn %ymm10,%ymm0,%ymm0                       #! PC = 0x555555579663 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm10_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm10_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm10_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm10_3;
(* vpxor  %ymm1,%ymm0,%ymm1                        #! PC = 0x555555579668 *)
xor ymm1_0@uint64 ymm0_0 ymm1_0;
xor ymm1_1@uint64 ymm0_1 ymm1_1;
xor ymm1_2@uint64 ymm0_2 ymm1_2;
xor ymm1_3@uint64 ymm0_3 ymm1_3;
(* vpsllq $0x27,%ymm11,%ymm11                      #! PC = 0x55555557966c *)
shl ymm11_0 ymm11_0 0x27@uint64;
shl ymm11_1 ymm11_1 0x27@uint64;
shl ymm11_2 ymm11_2 0x27@uint64;
shl ymm11_3 ymm11_3 0x27@uint64;
(* vpxor  %ymm3,%ymm7,%ymm7                        #! PC = 0x555555579672 *)
xor ymm7_0@uint64 ymm7_0 ymm3_0;
xor ymm7_1@uint64 ymm7_1 ymm3_1;
xor ymm7_2@uint64 ymm7_2 ymm3_2;
xor ymm7_3@uint64 ymm7_3 ymm3_3;
(* vpsrlq $0x9,%ymm13,%ymm0                        #! PC = 0x555555579676 *)
shr ymm0_0 ymm13_0 0x9@uint64;
shr ymm0_1 ymm13_1 0x9@uint64;
shr ymm0_2 ymm13_2 0x9@uint64;
shr ymm0_3 ymm13_3 0x9@uint64;
(* vpsllq $0x37,%ymm13,%ymm13                      #! PC = 0x55555557967c *)
shl ymm13_0 ymm13_0 0x37@uint64;
shl ymm13_1 ymm13_1 0x37@uint64;
shl ymm13_2 ymm13_2 0x37@uint64;
shl ymm13_3 ymm13_3 0x37@uint64;
(* vpor   %ymm11,%ymm5,%ymm11                      #! PC = 0x555555579682 *)
or ymm11_0@uint64 ymm5_0 ymm11_0;
or ymm11_1@uint64 ymm5_1 ymm11_1;
or ymm11_2@uint64 ymm5_2 ymm11_2;
or ymm11_3@uint64 ymm5_3 ymm11_3;
(* vmovdqa %ymm1,-0x70(%rbp)                       #! EA = L0x7fffffffbee0; PC = 0x555555579687 *)
mov L0x7fffffffbee0 ymm1_0;
mov L0x7fffffffbee8 ymm1_1;
mov L0x7fffffffbef0 ymm1_2;
mov L0x7fffffffbef8 ymm1_3;
(* vpor   %ymm13,%ymm0,%ymm13                      #! PC = 0x55555557968c *)
or ymm13_0@uint64 ymm0_0 ymm13_0;
or ymm13_1@uint64 ymm0_1 ymm13_1;
or ymm13_2@uint64 ymm0_2 ymm13_2;
or ymm13_3@uint64 ymm0_3 ymm13_3;
(* vpsrlq $0x2,%ymm14,%ymm3                        #! PC = 0x555555579691 *)
shr ymm3_0 ymm14_0 0x2@uint64;
shr ymm3_1 ymm14_1 0x2@uint64;
shr ymm3_2 ymm14_2 0x2@uint64;
shr ymm3_3 ymm14_3 0x2@uint64;
(* vpxor  -0x350(%rbp),%ymm2,%ymm1                 #! EA = L0x7fffffffbc00; Value = 0x4f40bfbf5114b70e; PC = 0x555555579697 *)
xor ymm1_0@uint64 ymm2_0 L0x7fffffffbc00;
xor ymm1_1@uint64 ymm2_1 L0x7fffffffbc08;
xor ymm1_2@uint64 ymm2_2 L0x7fffffffbc10;
xor ymm1_3@uint64 ymm2_3 L0x7fffffffbc18;
(* vpsllq $0x3e,%ymm14,%ymm14                      #! PC = 0x55555557969f *)
shl ymm14_0 ymm14_0 0x3e@uint64;
shl ymm14_1 ymm14_1 0x3e@uint64;
shl ymm14_2 ymm14_2 0x3e@uint64;
shl ymm14_3 ymm14_3 0x3e@uint64;
(* vpandn %ymm11,%ymm13,%ymm5                      #! PC = 0x5555555796a5 *)
not ymm13_0n@uint64 ymm13_0;
and ymm5_0@uint64 ymm13_0n ymm11_0;
not ymm13_1n@uint64 ymm13_1;
and ymm5_1@uint64 ymm13_1n ymm11_1;
not ymm13_2n@uint64 ymm13_2;
and ymm5_2@uint64 ymm13_2n ymm11_2;
not ymm13_3n@uint64 ymm13_3;
and ymm5_3@uint64 ymm13_3n ymm11_3;
(* vpor   %ymm14,%ymm3,%ymm3                       #! PC = 0x5555555796aa *)
or ymm3_0@uint64 ymm3_0 ymm14_0;
or ymm3_1@uint64 ymm3_1 ymm14_1;
or ymm3_2@uint64 ymm3_2 ymm14_2;
or ymm3_3@uint64 ymm3_3 ymm14_3;
(* vpsllq $0x2,%ymm1,%ymm2                         #! PC = 0x5555555796af *)
shl ymm2_0 ymm1_0 0x2@uint64;
shl ymm2_1 ymm1_1 0x2@uint64;
shl ymm2_2 ymm1_2 0x2@uint64;
shl ymm2_3 ymm1_3 0x2@uint64;
(* vpxor  %ymm3,%ymm5,%ymm5                        #! PC = 0x5555555796b4 *)
xor ymm5_0@uint64 ymm5_0 ymm3_0;
xor ymm5_1@uint64 ymm5_1 ymm3_1;
xor ymm5_2@uint64 ymm5_2 ymm3_2;
xor ymm5_3@uint64 ymm5_3 ymm3_3;
(* vpxor  %ymm15,%ymm5,%ymm10                      #! PC = 0x5555555796b8 *)
xor ymm10_0@uint64 ymm5_0 ymm15_0;
xor ymm10_1@uint64 ymm5_1 ymm15_1;
xor ymm10_2@uint64 ymm5_2 ymm15_2;
xor ymm10_3@uint64 ymm5_3 ymm15_3;
(* vmovdqa -0x130(%rbp),%ymm15                     #! EA = L0x7fffffffbe20; Value = 0xd0316755555ceec9; PC = 0x5555555796bd *)
mov ymm15_0 L0x7fffffffbe20;
mov ymm15_1 L0x7fffffffbe28;
mov ymm15_2 L0x7fffffffbe30;
mov ymm15_3 L0x7fffffffbe38;
(* vpxor  -0x210(%rbp),%ymm15,%ymm0                #! EA = L0x7fffffffbd40; Value = 0x9debe90720187a5e; PC = 0x5555555796c5 *)
xor ymm0_0@uint64 ymm15_0 L0x7fffffffbd40;
xor ymm0_1@uint64 ymm15_1 L0x7fffffffbd48;
xor ymm0_2@uint64 ymm15_2 L0x7fffffffbd50;
xor ymm0_3@uint64 ymm15_3 L0x7fffffffbd58;
(* vpxor  -0x50(%rbp),%ymm9,%ymm15                 #! EA = L0x7fffffffbf00; Value = 0x641e17838270a7af; PC = 0x5555555796cd *)
xor ymm15_0@uint64 ymm9_0 L0x7fffffffbf00;
xor ymm15_1@uint64 ymm9_1 L0x7fffffffbf08;
xor ymm15_2@uint64 ymm9_2 L0x7fffffffbf10;
xor ymm15_3@uint64 ymm9_3 L0x7fffffffbf18;
(* vpxor  %ymm0,%ymm10,%ymm10                      #! PC = 0x5555555796d2 *)
xor ymm10_0@uint64 ymm10_0 ymm0_0;
xor ymm10_1@uint64 ymm10_1 ymm0_1;
xor ymm10_2@uint64 ymm10_2 ymm0_2;
xor ymm10_3@uint64 ymm10_3 ymm0_3;
(* vpsrlq $0x17,%ymm4,%ymm0                        #! PC = 0x5555555796d6 *)
shr ymm0_0 ymm4_0 0x17@uint64;
shr ymm0_1 ymm4_1 0x17@uint64;
shr ymm0_2 ymm4_2 0x17@uint64;
shr ymm0_3 ymm4_3 0x17@uint64;
(* vpxor  -0x90(%rbp),%ymm10,%ymm10                #! EA = L0x7fffffffbec0; Value = 0xe71990f66e5c3199; PC = 0x5555555796db *)
xor ymm10_0@uint64 ymm10_0 L0x7fffffffbec0;
xor ymm10_1@uint64 ymm10_1 L0x7fffffffbec8;
xor ymm10_2@uint64 ymm10_2 L0x7fffffffbed0;
xor ymm10_3@uint64 ymm10_3 L0x7fffffffbed8;
(* vpsllq $0x29,%ymm4,%ymm4                        #! PC = 0x5555555796e3 *)
shl ymm4_0 ymm4_0 0x29@uint64;
shl ymm4_1 ymm4_1 0x29@uint64;
shl ymm4_2 ymm4_2 0x29@uint64;
shl ymm4_3 ymm4_3 0x29@uint64;
(* vpor   %ymm4,%ymm0,%ymm4                        #! PC = 0x5555555796e8 *)
or ymm4_0@uint64 ymm0_0 ymm4_0;
or ymm4_1@uint64 ymm0_1 ymm4_1;
or ymm4_2@uint64 ymm0_2 ymm4_2;
or ymm4_3@uint64 ymm0_3 ymm4_3;
(* vpandn %ymm4,%ymm11,%ymm0                       #! PC = 0x5555555796ec *)
not ymm11_0n@uint64 ymm11_0;
and ymm0_0@uint64 ymm11_0n ymm4_0;
not ymm11_1n@uint64 ymm11_1;
and ymm0_1@uint64 ymm11_1n ymm4_1;
not ymm11_2n@uint64 ymm11_2;
and ymm0_2@uint64 ymm11_2n ymm4_2;
not ymm11_3n@uint64 ymm11_3;
and ymm0_3@uint64 ymm11_3n ymm4_3;
(* vpxor  %ymm13,%ymm0,%ymm14                      #! PC = 0x5555555796f0 *)
xor ymm14_0@uint64 ymm0_0 ymm13_0;
xor ymm14_1@uint64 ymm0_1 ymm13_1;
xor ymm14_2@uint64 ymm0_2 ymm13_2;
xor ymm14_3@uint64 ymm0_3 ymm13_3;
(* vpxor  -0x2b0(%rbp),%ymm8,%ymm0                 #! EA = L0x7fffffffbca0; Value = 0x9815a7fefc970812; PC = 0x5555555796f5 *)
xor ymm0_0@uint64 ymm8_0 L0x7fffffffbca0;
xor ymm0_1@uint64 ymm8_1 L0x7fffffffbca8;
xor ymm0_2@uint64 ymm8_2 L0x7fffffffbcb0;
xor ymm0_3@uint64 ymm8_3 L0x7fffffffbcb8;
(* vmovdqa %ymm14,%ymm12                           #! PC = 0x5555555796fd *)
mov ymm12_0 ymm14_0;
mov ymm12_1 ymm14_1;
mov ymm12_2 ymm14_2;
mov ymm12_3 ymm14_3;
(* vpxor  -0x1f0(%rbp),%ymm6,%ymm14                #! EA = L0x7fffffffbd60; Value = 0x5dcd9aa3fcdad534; PC = 0x555555579702 *)
xor ymm14_0@uint64 ymm6_0 L0x7fffffffbd60;
xor ymm14_1@uint64 ymm6_1 L0x7fffffffbd68;
xor ymm14_2@uint64 ymm6_2 L0x7fffffffbd70;
xor ymm14_3@uint64 ymm6_3 L0x7fffffffbd78;
(* vmovdqa %ymm12,-0x2f0(%rbp)                     #! EA = L0x7fffffffbc60; PC = 0x55555557970a *)
mov L0x7fffffffbc60 ymm12_0;
mov L0x7fffffffbc68 ymm12_1;
mov L0x7fffffffbc70 ymm12_2;
mov L0x7fffffffbc78 ymm12_3;
(* vpxor  %ymm0,%ymm14,%ymm14                      #! PC = 0x555555579712 *)
xor ymm14_0@uint64 ymm14_0 ymm0_0;
xor ymm14_1@uint64 ymm14_1 ymm0_1;
xor ymm14_2@uint64 ymm14_2 ymm0_2;
xor ymm14_3@uint64 ymm14_3 ymm0_3;
(* vpsrlq $0x3e,%ymm1,%ymm0                        #! PC = 0x555555579716 *)
shr ymm0_0 ymm1_0 0x3e@uint64;
shr ymm0_1 ymm1_1 0x3e@uint64;
shr ymm0_2 ymm1_2 0x3e@uint64;
shr ymm0_3 ymm1_3 0x3e@uint64;
(* vpor   %ymm2,%ymm0,%ymm0                        #! PC = 0x55555557971b *)
or ymm0_0@uint64 ymm0_0 ymm2_0;
or ymm0_1@uint64 ymm0_1 ymm2_1;
or ymm0_2@uint64 ymm0_2 ymm2_2;
or ymm0_3@uint64 ymm0_3 ymm2_3;
(* vpxor  %ymm12,%ymm14,%ymm14                     #! PC = 0x55555557971f *)
xor ymm14_0@uint64 ymm14_0 ymm12_0;
xor ymm14_1@uint64 ymm14_1 ymm12_1;
xor ymm14_2@uint64 ymm14_2 ymm12_2;
xor ymm14_3@uint64 ymm14_3 ymm12_3;
(* vmovdqa -0x1d0(%rbp),%ymm12                     #! EA = L0x7fffffffbd80; Value = 0x71fa1d117d2f0d2b; PC = 0x555555579724 *)
mov ymm12_0 L0x7fffffffbd80;
mov ymm12_1 L0x7fffffffbd88;
mov ymm12_2 L0x7fffffffbd90;
mov ymm12_3 L0x7fffffffbd98;
(* vpandn %ymm13,%ymm3,%ymm2                       #! PC = 0x55555557972c *)
not ymm3_0n@uint64 ymm3_0;
and ymm2_0@uint64 ymm3_0n ymm13_0;
not ymm3_1n@uint64 ymm3_1;
and ymm2_1@uint64 ymm3_1n ymm13_1;
not ymm3_2n@uint64 ymm3_2;
and ymm2_2@uint64 ymm3_2n ymm13_2;
not ymm3_3n@uint64 ymm3_3;
and ymm2_3@uint64 ymm3_3n ymm13_3;
(* vpandn %ymm0,%ymm4,%ymm1                        #! PC = 0x555555579731 *)
not ymm4_0n@uint64 ymm4_0;
and ymm1_0@uint64 ymm4_0n ymm0_0;
not ymm4_1n@uint64 ymm4_1;
and ymm1_1@uint64 ymm4_1n ymm0_1;
not ymm4_2n@uint64 ymm4_2;
and ymm1_2@uint64 ymm4_2n ymm0_2;
not ymm4_3n@uint64 ymm4_3;
and ymm1_3@uint64 ymm4_3n ymm0_3;
(* vpxor  %ymm0,%ymm2,%ymm2                        #! PC = 0x555555579735 *)
xor ymm2_0@uint64 ymm2_0 ymm0_0;
xor ymm2_1@uint64 ymm2_1 ymm0_1;
xor ymm2_2@uint64 ymm2_2 ymm0_2;
xor ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpxor  %ymm11,%ymm1,%ymm11                      #! PC = 0x555555579739 *)
xor ymm11_0@uint64 ymm1_0 ymm11_0;
xor ymm11_1@uint64 ymm1_1 ymm11_1;
xor ymm11_2@uint64 ymm1_2 ymm11_2;
xor ymm11_3@uint64 ymm1_3 ymm11_3;
(* vpxor  -0x290(%rbp),%ymm12,%ymm1                #! EA = L0x7fffffffbcc0; Value = 0x7adeb6308d077a89; PC = 0x55555557973e *)
xor ymm1_0@uint64 ymm12_0 L0x7fffffffbcc0;
xor ymm1_1@uint64 ymm12_1 L0x7fffffffbcc8;
xor ymm1_2@uint64 ymm12_2 L0x7fffffffbcd0;
xor ymm1_3@uint64 ymm12_3 L0x7fffffffbcd8;
(* vpxor  %ymm1,%ymm15,%ymm15                      #! PC = 0x555555579746 *)
xor ymm15_0@uint64 ymm15_0 ymm1_0;
xor ymm15_1@uint64 ymm15_1 ymm1_1;
xor ymm15_2@uint64 ymm15_2 ymm1_2;
xor ymm15_3@uint64 ymm15_3 ymm1_3;
(* vpandn %ymm3,%ymm0,%ymm1                        #! PC = 0x55555557974a *)
not ymm0_0n@uint64 ymm0_0;
and ymm1_0@uint64 ymm0_0n ymm3_0;
not ymm0_1n@uint64 ymm0_1;
and ymm1_1@uint64 ymm0_1n ymm3_1;
not ymm0_2n@uint64 ymm0_2;
and ymm1_2@uint64 ymm0_2n ymm3_2;
not ymm0_3n@uint64 ymm0_3;
and ymm1_3@uint64 ymm0_3n ymm3_3;
(* vpxor  %ymm4,%ymm1,%ymm4                        #! PC = 0x55555557974e *)
xor ymm4_0@uint64 ymm1_0 ymm4_0;
xor ymm4_1@uint64 ymm1_1 ymm4_1;
xor ymm4_2@uint64 ymm1_2 ymm4_2;
xor ymm4_3@uint64 ymm1_3 ymm4_3;
(* vmovdqa -0x250(%rbp),%ymm1                      #! EA = L0x7fffffffbd00; Value = 0x7e2d04cc17121756; PC = 0x555555579752 *)
mov ymm1_0 L0x7fffffffbd00;
mov ymm1_1 L0x7fffffffbd08;
mov ymm1_2 L0x7fffffffbd10;
mov ymm1_3 L0x7fffffffbd18;
(* vpxor  %ymm11,%ymm15,%ymm15                     #! PC = 0x55555557975a *)
xor ymm15_0@uint64 ymm15_0 ymm11_0;
xor ymm15_1@uint64 ymm15_1 ymm11_1;
xor ymm15_2@uint64 ymm15_2 ymm11_2;
xor ymm15_3@uint64 ymm15_3 ymm11_3;
(* vpxor  -0x170(%rbp),%ymm1,%ymm1                 #! EA = L0x7fffffffbde0; Value = 0x1bcf81ec65528647; PC = 0x55555557975f *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbde0;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbde8;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbdf0;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbdf8;
(* vpxor  %ymm4,%ymm7,%ymm12                       #! PC = 0x555555579767 *)
xor ymm12_0@uint64 ymm7_0 ymm4_0;
xor ymm12_1@uint64 ymm7_1 ymm4_1;
xor ymm12_2@uint64 ymm7_2 ymm4_2;
xor ymm12_3@uint64 ymm7_3 ymm4_3;
(* vmovdqa %ymm4,-0x310(%rbp)                      #! EA = L0x7fffffffbc40; PC = 0x55555557976b *)
mov L0x7fffffffbc40 ymm4_0;
mov L0x7fffffffbc48 ymm4_1;
mov L0x7fffffffbc50 ymm4_2;
mov L0x7fffffffbc58 ymm4_3;
(* vpsrlq $0x3f,%ymm14,%ymm3                       #! PC = 0x555555579773 *)
shr ymm3_0 ymm14_0 0x3f@uint64;
shr ymm3_1 ymm14_1 0x3f@uint64;
shr ymm3_2 ymm14_2 0x3f@uint64;
shr ymm3_3 ymm14_3 0x3f@uint64;
(* vpxor  %ymm1,%ymm12,%ymm12                      #! PC = 0x555555579779 *)
xor ymm12_0@uint64 ymm12_0 ymm1_0;
xor ymm12_1@uint64 ymm12_1 ymm1_1;
xor ymm12_2@uint64 ymm12_2 ymm1_2;
xor ymm12_3@uint64 ymm12_3 ymm1_3;
(* vpxor  -0x1b0(%rbp),%ymm12,%ymm12               #! EA = L0x7fffffffbda0; Value = 0x2a1c42a54f48b263; PC = 0x55555557977d *)
xor ymm12_0@uint64 ymm12_0 L0x7fffffffbda0;
xor ymm12_1@uint64 ymm12_1 L0x7fffffffbda8;
xor ymm12_2@uint64 ymm12_2 L0x7fffffffbdb0;
xor ymm12_3@uint64 ymm12_3 L0x7fffffffbdb8;
(* vpxor  -0x230(%rbp),%ymm2,%ymm4                 #! EA = L0x7fffffffbd20; Value = 0x439eb6c088b33bb8; PC = 0x555555579785 *)
xor ymm4_0@uint64 ymm2_0 L0x7fffffffbd20;
xor ymm4_1@uint64 ymm2_1 L0x7fffffffbd28;
xor ymm4_2@uint64 ymm2_2 L0x7fffffffbd30;
xor ymm4_3@uint64 ymm2_3 L0x7fffffffbd38;
(* vmovdqa -0x150(%rbp),%ymm0                      #! EA = L0x7fffffffbe00; Value = 0xc31e4540d69f88c9; PC = 0x55555557978d *)
mov ymm0_0 L0x7fffffffbe00;
mov ymm0_1 L0x7fffffffbe08;
mov ymm0_2 L0x7fffffffbe10;
mov ymm0_3 L0x7fffffffbe18;
(* vpxor  -0x270(%rbp),%ymm0,%ymm0                 #! EA = L0x7fffffffbce0; Value = 0xfb3f99d0c00b9d77; PC = 0x555555579795 *)
xor ymm0_0@uint64 ymm0_0 L0x7fffffffbce0;
xor ymm0_1@uint64 ymm0_1 L0x7fffffffbce8;
xor ymm0_2@uint64 ymm0_2 L0x7fffffffbcf0;
xor ymm0_3@uint64 ymm0_3 L0x7fffffffbcf8;
(* vpsrlq $0x3f,%ymm15,%ymm1                       #! PC = 0x55555557979d *)
shr ymm1_0 ymm15_0 0x3f@uint64;
shr ymm1_1 ymm15_1 0x3f@uint64;
shr ymm1_2 ymm15_2 0x3f@uint64;
shr ymm1_3 ymm15_3 0x3f@uint64;
(* vpsllq $0x1,%ymm12,%ymm13                       #! PC = 0x5555555797a3 *)
shl ymm13_0 ymm12_0 0x1@uint64;
shl ymm13_1 ymm12_1 0x1@uint64;
shl ymm13_2 ymm12_2 0x1@uint64;
shl ymm13_3 ymm12_3 0x1@uint64;
(* vpxor  %ymm0,%ymm4,%ymm4                        #! PC = 0x5555555797a9 *)
xor ymm4_0@uint64 ymm4_0 ymm0_0;
xor ymm4_1@uint64 ymm4_1 ymm0_1;
xor ymm4_2@uint64 ymm4_2 ymm0_2;
xor ymm4_3@uint64 ymm4_3 ymm0_3;
(* vpsllq $0x1,%ymm14,%ymm0                        #! PC = 0x5555555797ad *)
shl ymm0_0 ymm14_0 0x1@uint64;
shl ymm0_1 ymm14_1 0x1@uint64;
shl ymm0_2 ymm14_2 0x1@uint64;
shl ymm0_3 ymm14_3 0x1@uint64;
(* vpxor  -0x70(%rbp),%ymm4,%ymm4                  #! EA = L0x7fffffffbee0; Value = 0xdb137cc98c3c8239; PC = 0x5555555797b3 *)
xor ymm4_0@uint64 ymm4_0 L0x7fffffffbee0;
xor ymm4_1@uint64 ymm4_1 L0x7fffffffbee8;
xor ymm4_2@uint64 ymm4_2 L0x7fffffffbef0;
xor ymm4_3@uint64 ymm4_3 L0x7fffffffbef8;
(* vpor   %ymm0,%ymm3,%ymm3                        #! PC = 0x5555555797b8 *)
or ymm3_0@uint64 ymm3_0 ymm0_0;
or ymm3_1@uint64 ymm3_1 ymm0_1;
or ymm3_2@uint64 ymm3_2 ymm0_2;
or ymm3_3@uint64 ymm3_3 ymm0_3;
(* vpsllq $0x1,%ymm15,%ymm0                        #! PC = 0x5555555797bc *)
shl ymm0_0 ymm15_0 0x1@uint64;
shl ymm0_1 ymm15_1 0x1@uint64;
shl ymm0_2 ymm15_2 0x1@uint64;
shl ymm0_3 ymm15_3 0x1@uint64;
(* vpor   %ymm0,%ymm1,%ymm1                        #! PC = 0x5555555797c2 *)
or ymm1_0@uint64 ymm1_0 ymm0_0;
or ymm1_1@uint64 ymm1_1 ymm0_1;
or ymm1_2@uint64 ymm1_2 ymm0_2;
or ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3f,%ymm12,%ymm0                       #! PC = 0x5555555797c6 *)
shr ymm0_0 ymm12_0 0x3f@uint64;
shr ymm0_1 ymm12_1 0x3f@uint64;
shr ymm0_2 ymm12_2 0x3f@uint64;
shr ymm0_3 ymm12_3 0x3f@uint64;
(* vpxor  %ymm4,%ymm3,%ymm3                        #! PC = 0x5555555797cc *)
xor ymm3_0@uint64 ymm3_0 ymm4_0;
xor ymm3_1@uint64 ymm3_1 ymm4_1;
xor ymm3_2@uint64 ymm3_2 ymm4_2;
xor ymm3_3@uint64 ymm3_3 ymm4_3;
(* vpor   %ymm13,%ymm0,%ymm0                       #! PC = 0x5555555797d0 *)
or ymm0_0@uint64 ymm0_0 ymm13_0;
or ymm0_1@uint64 ymm0_1 ymm13_1;
or ymm0_2@uint64 ymm0_2 ymm13_2;
or ymm0_3@uint64 ymm0_3 ymm13_3;
(* vpxor  %ymm10,%ymm1,%ymm1                       #! PC = 0x5555555797d5 *)
xor ymm1_0@uint64 ymm1_0 ymm10_0;
xor ymm1_1@uint64 ymm1_1 ymm10_1;
xor ymm1_2@uint64 ymm1_2 ymm10_2;
xor ymm1_3@uint64 ymm1_3 ymm10_3;
(* vpxor  %ymm3,%ymm5,%ymm5                        #! PC = 0x5555555797da *)
xor ymm5_0@uint64 ymm5_0 ymm3_0;
xor ymm5_1@uint64 ymm5_1 ymm3_1;
xor ymm5_2@uint64 ymm5_2 ymm3_2;
xor ymm5_3@uint64 ymm5_3 ymm3_3;
(* vpxor  %ymm14,%ymm0,%ymm14                      #! PC = 0x5555555797de *)
xor ymm14_0@uint64 ymm0_0 ymm14_0;
xor ymm14_1@uint64 ymm0_1 ymm14_1;
xor ymm14_2@uint64 ymm0_2 ymm14_2;
xor ymm14_3@uint64 ymm0_3 ymm14_3;
(* vpsrlq $0x3f,%ymm4,%ymm0                        #! PC = 0x5555555797e3 *)
shr ymm0_0 ymm4_0 0x3f@uint64;
shr ymm0_1 ymm4_1 0x3f@uint64;
shr ymm0_2 ymm4_2 0x3f@uint64;
shr ymm0_3 ymm4_3 0x3f@uint64;
(* vpxor  %ymm8,%ymm1,%ymm8                        #! PC = 0x5555555797e8 *)
xor ymm8_0@uint64 ymm1_0 ymm8_0;
xor ymm8_1@uint64 ymm1_1 ymm8_1;
xor ymm8_2@uint64 ymm1_2 ymm8_2;
xor ymm8_3@uint64 ymm1_3 ymm8_3;
(* vpsllq $0x1,%ymm4,%ymm4                         #! PC = 0x5555555797ed *)
shl ymm4_0 ymm4_0 0x1@uint64;
shl ymm4_1 ymm4_1 0x1@uint64;
shl ymm4_2 ymm4_2 0x1@uint64;
shl ymm4_3 ymm4_3 0x1@uint64;
(* vpxor  %ymm9,%ymm14,%ymm9                       #! PC = 0x5555555797f2 *)
xor ymm9_0@uint64 ymm14_0 ymm9_0;
xor ymm9_1@uint64 ymm14_1 ymm9_1;
xor ymm9_2@uint64 ymm14_2 ymm9_2;
xor ymm9_3@uint64 ymm14_3 ymm9_3;
(* vpxor  %ymm1,%ymm6,%ymm6                        #! PC = 0x5555555797f7 *)
xor ymm6_0@uint64 ymm6_0 ymm1_0;
xor ymm6_1@uint64 ymm6_1 ymm1_1;
xor ymm6_2@uint64 ymm6_2 ymm1_2;
xor ymm6_3@uint64 ymm6_3 ymm1_3;
(* vpor   %ymm4,%ymm0,%ymm4                        #! PC = 0x5555555797fb *)
or ymm4_0@uint64 ymm0_0 ymm4_0;
or ymm4_1@uint64 ymm0_1 ymm4_1;
or ymm4_2@uint64 ymm0_2 ymm4_2;
or ymm4_3@uint64 ymm0_3 ymm4_3;
(* vpsrlq $0x3f,%ymm10,%ymm0                       #! PC = 0x5555555797ff *)
shr ymm0_0 ymm10_0 0x3f@uint64;
shr ymm0_1 ymm10_1 0x3f@uint64;
shr ymm0_2 ymm10_2 0x3f@uint64;
shr ymm0_3 ymm10_3 0x3f@uint64;
(* vpxor  %ymm14,%ymm11,%ymm11                     #! PC = 0x555555579805 *)
xor ymm11_0@uint64 ymm11_0 ymm14_0;
xor ymm11_1@uint64 ymm11_1 ymm14_1;
xor ymm11_2@uint64 ymm11_2 ymm14_2;
xor ymm11_3@uint64 ymm11_3 ymm14_3;
(* vpxor  %ymm15,%ymm4,%ymm15                      #! PC = 0x55555557980a *)
xor ymm15_0@uint64 ymm4_0 ymm15_0;
xor ymm15_1@uint64 ymm4_1 ymm15_1;
xor ymm15_2@uint64 ymm4_2 ymm15_2;
xor ymm15_3@uint64 ymm4_3 ymm15_3;
(* vpsllq $0x1,%ymm10,%ymm10                       #! PC = 0x55555557980f *)
shl ymm10_0 ymm10_0 0x1@uint64;
shl ymm10_1 ymm10_1 0x1@uint64;
shl ymm10_2 ymm10_2 0x1@uint64;
shl ymm10_3 ymm10_3 0x1@uint64;
(* vpsrlq $0x14,%ymm8,%ymm4                        #! PC = 0x555555579815 *)
shr ymm4_0 ymm8_0 0x14@uint64;
shr ymm4_1 ymm8_1 0x14@uint64;
shr ymm4_2 ymm8_2 0x14@uint64;
shr ymm4_3 ymm8_3 0x14@uint64;
(* vpsllq $0x2c,%ymm8,%ymm8                        #! PC = 0x55555557981b *)
shl ymm8_0 ymm8_0 0x2c@uint64;
shl ymm8_1 ymm8_1 0x2c@uint64;
shl ymm8_2 ymm8_2 0x2c@uint64;
shl ymm8_3 ymm8_3 0x2c@uint64;
(* vpor   %ymm10,%ymm0,%ymm10                      #! PC = 0x555555579821 *)
or ymm10_0@uint64 ymm0_0 ymm10_0;
or ymm10_1@uint64 ymm0_1 ymm10_1;
or ymm10_2@uint64 ymm0_2 ymm10_2;
or ymm10_3@uint64 ymm0_3 ymm10_3;
(* vpor   %ymm8,%ymm4,%ymm8                        #! PC = 0x555555579826 *)
or ymm8_0@uint64 ymm4_0 ymm8_0;
or ymm8_1@uint64 ymm4_1 ymm8_1;
or ymm8_2@uint64 ymm4_2 ymm8_2;
or ymm8_3@uint64 ymm4_3 ymm8_3;
(* vpxor  %ymm12,%ymm10,%ymm12                     #! PC = 0x55555557982b *)
xor ymm12_0@uint64 ymm10_0 ymm12_0;
xor ymm12_1@uint64 ymm10_1 ymm12_1;
xor ymm12_2@uint64 ymm10_2 ymm12_2;
xor ymm12_3@uint64 ymm10_3 ymm12_3;
(* vmovq  %rbx,%xmm10                              #! PC = 0x555555579830 *)
mov xmm10_0 rbx;
mov xmm10_1 0@uint64;
(* vpsrlq $0x15,%ymm9,%ymm4                        #! PC = 0x555555579835 *)
shr ymm4_0 ymm9_0 0x15@uint64;
shr ymm4_1 ymm9_1 0x15@uint64;
shr ymm4_2 ymm9_2 0x15@uint64;
shr ymm4_3 ymm9_3 0x15@uint64;
(* vpsllq $0x2b,%ymm9,%ymm9                        #! PC = 0x55555557983b *)
shl ymm9_0 ymm9_0 0x2b@uint64;
shl ymm9_1 ymm9_1 0x2b@uint64;
shl ymm9_2 ymm9_2 0x2b@uint64;
shl ymm9_3 ymm9_3 0x2b@uint64;
(* vpxor  %ymm15,%ymm7,%ymm7                       #! PC = 0x555555579841 *)
xor ymm7_0@uint64 ymm7_0 ymm15_0;
xor ymm7_1@uint64 ymm7_1 ymm15_1;
xor ymm7_2@uint64 ymm7_2 ymm15_2;
xor ymm7_3@uint64 ymm7_3 ymm15_3;
(* vpor   %ymm9,%ymm4,%ymm9                        #! PC = 0x555555579846 *)
or ymm9_0@uint64 ymm4_0 ymm9_0;
or ymm9_1@uint64 ymm4_1 ymm9_1;
or ymm9_2@uint64 ymm4_2 ymm9_2;
or ymm9_3@uint64 ymm4_3 ymm9_3;
(* vpbroadcastq %xmm10,%ymm10                      #! PC = 0x55555557984b *)
mov ymm10_0 xmm10_0;
mov ymm10_1 xmm10_0;
mov ymm10_2 xmm10_0;
mov ymm10_3 xmm10_0;
(* vpxor  %ymm12,%ymm2,%ymm2                       #! PC = 0x555555579850 *)
xor ymm2_0@uint64 ymm2_0 ymm12_0;
xor ymm2_1@uint64 ymm2_1 ymm12_1;
xor ymm2_2@uint64 ymm2_2 ymm12_2;
xor ymm2_3@uint64 ymm2_3 ymm12_3;
(* vpandn %ymm9,%ymm8,%ymm4                        #! PC = 0x555555579855 *)
not ymm8_0n@uint64 ymm8_0;
and ymm4_0@uint64 ymm8_0n ymm9_0;
not ymm8_1n@uint64 ymm8_1;
and ymm4_1@uint64 ymm8_1n ymm9_1;
not ymm8_2n@uint64 ymm8_2;
and ymm4_2@uint64 ymm8_2n ymm9_2;
not ymm8_3n@uint64 ymm8_3;
and ymm4_3@uint64 ymm8_3n ymm9_3;
(* vpxor  -0x90(%rbp),%ymm3,%ymm0                  #! EA = L0x7fffffffbec0; Value = 0xe71990f66e5c3199; PC = 0x55555557985a *)
xor ymm0_0@uint64 ymm3_0 L0x7fffffffbec0;
xor ymm0_1@uint64 ymm3_1 L0x7fffffffbec8;
xor ymm0_2@uint64 ymm3_2 L0x7fffffffbed0;
xor ymm0_3@uint64 ymm3_3 L0x7fffffffbed8;
(* vpxor  %ymm10,%ymm4,%ymm10                      #! PC = 0x555555579862 *)
xor ymm10_0@uint64 ymm4_0 ymm10_0;
xor ymm10_1@uint64 ymm4_1 ymm10_1;
xor ymm10_2@uint64 ymm4_2 ymm10_2;
xor ymm10_3@uint64 ymm4_3 ymm10_3;
(* vpsrlq $0x2b,%ymm7,%ymm4                        #! PC = 0x555555579867 *)
shr ymm4_0 ymm7_0 0x2b@uint64;
shr ymm4_1 ymm7_1 0x2b@uint64;
shr ymm4_2 ymm7_2 0x2b@uint64;
shr ymm4_3 ymm7_3 0x2b@uint64;
(* vpsllq $0x15,%ymm7,%ymm7                        #! PC = 0x55555557986c *)
shl ymm7_0 ymm7_0 0x15@uint64;
shl ymm7_1 ymm7_1 0x15@uint64;
shl ymm7_2 ymm7_2 0x15@uint64;
shl ymm7_3 ymm7_3 0x15@uint64;
(* vpxor  %ymm0,%ymm10,%ymm10                      #! PC = 0x555555579871 *)
xor ymm10_0@uint64 ymm10_0 ymm0_0;
xor ymm10_1@uint64 ymm10_1 ymm0_1;
xor ymm10_2@uint64 ymm10_2 ymm0_2;
xor ymm10_3@uint64 ymm10_3 ymm0_3;
(* vpor   %ymm7,%ymm4,%ymm7                        #! PC = 0x555555579875 *)
or ymm7_0@uint64 ymm4_0 ymm7_0;
or ymm7_1@uint64 ymm4_1 ymm7_1;
or ymm7_2@uint64 ymm4_2 ymm7_2;
or ymm7_3@uint64 ymm4_3 ymm7_3;
(* vmovdqa %ymm10,-0x90(%rbp)                      #! EA = L0x7fffffffbec0; PC = 0x555555579879 *)
mov L0x7fffffffbec0 ymm10_0;
mov L0x7fffffffbec8 ymm10_1;
mov L0x7fffffffbed0 ymm10_2;
mov L0x7fffffffbed8 ymm10_3;
(* vpandn %ymm7,%ymm9,%ymm4                        #! PC = 0x555555579881 *)
not ymm9_0n@uint64 ymm9_0;
and ymm4_0@uint64 ymm9_0n ymm7_0;
not ymm9_1n@uint64 ymm9_1;
and ymm4_1@uint64 ymm9_1n ymm7_1;
not ymm9_2n@uint64 ymm9_2;
and ymm4_2@uint64 ymm9_2n ymm7_2;
not ymm9_3n@uint64 ymm9_3;
and ymm4_3@uint64 ymm9_3n ymm7_3;
(* vpxor  %ymm8,%ymm4,%ymm13                       #! PC = 0x555555579885 *)
xor ymm13_0@uint64 ymm4_0 ymm8_0;
xor ymm13_1@uint64 ymm4_1 ymm8_1;
xor ymm13_2@uint64 ymm4_2 ymm8_2;
xor ymm13_3@uint64 ymm4_3 ymm8_3;
(* vpsrlq $0x32,%ymm2,%ymm4                        #! PC = 0x55555557988a *)
shr ymm4_0 ymm2_0 0x32@uint64;
shr ymm4_1 ymm2_1 0x32@uint64;
shr ymm4_2 ymm2_2 0x32@uint64;
shr ymm4_3 ymm2_3 0x32@uint64;
(* vpsllq $0xe,%ymm2,%ymm2                         #! PC = 0x55555557988f *)
shl ymm2_0 ymm2_0 0xe@uint64;
shl ymm2_1 ymm2_1 0xe@uint64;
shl ymm2_2 ymm2_2 0xe@uint64;
shl ymm2_3 ymm2_3 0xe@uint64;
(* vmovdqa %ymm13,-0xb0(%rbp)                      #! EA = L0x7fffffffbea0; PC = 0x555555579894 *)
mov L0x7fffffffbea0 ymm13_0;
mov L0x7fffffffbea8 ymm13_1;
mov L0x7fffffffbeb0 ymm13_2;
mov L0x7fffffffbeb8 ymm13_3;
(* vpsrlq $0x13,%ymm6,%ymm13                       #! PC = 0x55555557989c *)
shr ymm13_0 ymm6_0 0x13@uint64;
shr ymm13_1 ymm6_1 0x13@uint64;
shr ymm13_2 ymm6_2 0x13@uint64;
shr ymm13_3 ymm6_3 0x13@uint64;
(* vpor   %ymm2,%ymm4,%ymm2                        #! PC = 0x5555555798a1 *)
or ymm2_0@uint64 ymm4_0 ymm2_0;
or ymm2_1@uint64 ymm4_1 ymm2_1;
or ymm2_2@uint64 ymm4_2 ymm2_2;
or ymm2_3@uint64 ymm4_3 ymm2_3;
(* vpsllq $0x2d,%ymm6,%ymm6                        #! PC = 0x5555555798a5 *)
shl ymm6_0 ymm6_0 0x2d@uint64;
shl ymm6_1 ymm6_1 0x2d@uint64;
shl ymm6_2 ymm6_2 0x2d@uint64;
shl ymm6_3 ymm6_3 0x2d@uint64;
(* vpandn %ymm2,%ymm7,%ymm4                        #! PC = 0x5555555798aa *)
not ymm7_0n@uint64 ymm7_0;
and ymm4_0@uint64 ymm7_0n ymm2_0;
not ymm7_1n@uint64 ymm7_1;
and ymm4_1@uint64 ymm7_1n ymm2_1;
not ymm7_2n@uint64 ymm7_2;
and ymm4_2@uint64 ymm7_2n ymm2_2;
not ymm7_3n@uint64 ymm7_3;
and ymm4_3@uint64 ymm7_3n ymm2_3;
(* vpor   %ymm6,%ymm13,%ymm13                      #! PC = 0x5555555798ae *)
or ymm13_0@uint64 ymm13_0 ymm6_0;
or ymm13_1@uint64 ymm13_1 ymm6_1;
or ymm13_2@uint64 ymm13_2 ymm6_2;
or ymm13_3@uint64 ymm13_3 ymm6_3;
(* vpxor  %ymm9,%ymm4,%ymm9                        #! PC = 0x5555555798b2 *)
xor ymm9_0@uint64 ymm4_0 ymm9_0;
xor ymm9_1@uint64 ymm4_1 ymm9_1;
xor ymm9_2@uint64 ymm4_2 ymm9_2;
xor ymm9_3@uint64 ymm4_3 ymm9_3;
(* vpandn %ymm0,%ymm2,%ymm4                        #! PC = 0x5555555798b7 *)
not ymm2_0n@uint64 ymm2_0;
and ymm4_0@uint64 ymm2_0n ymm0_0;
not ymm2_1n@uint64 ymm2_1;
and ymm4_1@uint64 ymm2_1n ymm0_1;
not ymm2_2n@uint64 ymm2_2;
and ymm4_2@uint64 ymm2_2n ymm0_2;
not ymm2_3n@uint64 ymm2_3;
and ymm4_3@uint64 ymm2_3n ymm0_3;
(* vpandn %ymm8,%ymm0,%ymm0                        #! PC = 0x5555555798bb *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm8_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm8_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm8_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm8_3;
(* vpxor  %ymm2,%ymm0,%ymm8                        #! PC = 0x5555555798c0 *)
xor ymm8_0@uint64 ymm0_0 ymm2_0;
xor ymm8_1@uint64 ymm0_1 ymm2_1;
xor ymm8_2@uint64 ymm0_2 ymm2_2;
xor ymm8_3@uint64 ymm0_3 ymm2_3;
(* vpxor  -0x170(%rbp),%ymm15,%ymm0                #! EA = L0x7fffffffbde0; Value = 0x1bcf81ec65528647; PC = 0x5555555798c4 *)
xor ymm0_0@uint64 ymm15_0 L0x7fffffffbde0;
xor ymm0_1@uint64 ymm15_1 L0x7fffffffbde8;
xor ymm0_2@uint64 ymm15_2 L0x7fffffffbdf0;
xor ymm0_3@uint64 ymm15_3 L0x7fffffffbdf8;
(* vpxor  %ymm7,%ymm4,%ymm7                        #! PC = 0x5555555798cc *)
xor ymm7_0@uint64 ymm4_0 ymm7_0;
xor ymm7_1@uint64 ymm4_1 ymm7_1;
xor ymm7_2@uint64 ymm4_2 ymm7_2;
xor ymm7_3@uint64 ymm4_3 ymm7_3;
(* vmovdqa %ymm9,-0xd0(%rbp)                       #! EA = L0x7fffffffbe80; PC = 0x5555555798d0 *)
mov L0x7fffffffbe80 ymm9_0;
mov L0x7fffffffbe88 ymm9_1;
mov L0x7fffffffbe90 ymm9_2;
mov L0x7fffffffbe98 ymm9_3;
(* vpxor  -0x150(%rbp),%ymm12,%ymm2                #! EA = L0x7fffffffbe00; Value = 0xc31e4540d69f88c9; PC = 0x5555555798d8 *)
xor ymm2_0@uint64 ymm12_0 L0x7fffffffbe00;
xor ymm2_1@uint64 ymm12_1 L0x7fffffffbe08;
xor ymm2_2@uint64 ymm12_2 L0x7fffffffbe10;
xor ymm2_3@uint64 ymm12_3 L0x7fffffffbe18;
(* vpxor  -0x130(%rbp),%ymm3,%ymm4                 #! EA = L0x7fffffffbe20; Value = 0xd0316755555ceec9; PC = 0x5555555798e0 *)
xor ymm4_0@uint64 ymm3_0 L0x7fffffffbe20;
xor ymm4_1@uint64 ymm3_1 L0x7fffffffbe28;
xor ymm4_2@uint64 ymm3_2 L0x7fffffffbe30;
xor ymm4_3@uint64 ymm3_3 L0x7fffffffbe38;
(* vmovdqa %ymm7,-0xf0(%rbp)                       #! EA = L0x7fffffffbe60; PC = 0x5555555798e8 *)
mov L0x7fffffffbe60 ymm7_0;
mov L0x7fffffffbe68 ymm7_1;
mov L0x7fffffffbe70 ymm7_2;
mov L0x7fffffffbe78 ymm7_3;
(* vpsrlq $0x24,%ymm0,%ymm10                       #! PC = 0x5555555798f0 *)
shr ymm10_0 ymm0_0 0x24@uint64;
shr ymm10_1 ymm0_1 0x24@uint64;
shr ymm10_2 ymm0_2 0x24@uint64;
shr ymm10_3 ymm0_3 0x24@uint64;
(* vpsllq $0x1c,%ymm0,%ymm0                        #! PC = 0x5555555798f5 *)
shl ymm0_0 ymm0_0 0x1c@uint64;
shl ymm0_1 ymm0_1 0x1c@uint64;
shl ymm0_2 ymm0_2 0x1c@uint64;
shl ymm0_3 ymm0_3 0x1c@uint64;
(* vmovdqa %ymm8,-0x110(%rbp)                      #! EA = L0x7fffffffbe40; PC = 0x5555555798fa *)
mov L0x7fffffffbe40 ymm8_0;
mov L0x7fffffffbe48 ymm8_1;
mov L0x7fffffffbe50 ymm8_2;
mov L0x7fffffffbe58 ymm8_3;
(* vpor   %ymm0,%ymm10,%ymm10                      #! PC = 0x555555579902 *)
or ymm10_0@uint64 ymm10_0 ymm0_0;
or ymm10_1@uint64 ymm10_1 ymm0_1;
or ymm10_2@uint64 ymm10_2 ymm0_2;
or ymm10_3@uint64 ymm10_3 ymm0_3;
(* vpsrlq $0x2c,%ymm2,%ymm0                        #! PC = 0x555555579906 *)
shr ymm0_0 ymm2_0 0x2c@uint64;
shr ymm0_1 ymm2_1 0x2c@uint64;
shr ymm0_2 ymm2_2 0x2c@uint64;
shr ymm0_3 ymm2_3 0x2c@uint64;
(* vpsllq $0x14,%ymm2,%ymm2                        #! PC = 0x55555557990b *)
shl ymm2_0 ymm2_0 0x14@uint64;
shl ymm2_1 ymm2_1 0x14@uint64;
shl ymm2_2 ymm2_2 0x14@uint64;
shl ymm2_3 ymm2_3 0x14@uint64;
(* vpor   %ymm2,%ymm0,%ymm0                        #! PC = 0x555555579910 *)
or ymm0_0@uint64 ymm0_0 ymm2_0;
or ymm0_1@uint64 ymm0_1 ymm2_1;
or ymm0_2@uint64 ymm0_2 ymm2_2;
or ymm0_3@uint64 ymm0_3 ymm2_3;
(* vpsrlq $0x3d,%ymm4,%ymm2                        #! PC = 0x555555579914 *)
shr ymm2_0 ymm4_0 0x3d@uint64;
shr ymm2_1 ymm4_1 0x3d@uint64;
shr ymm2_2 ymm4_2 0x3d@uint64;
shr ymm2_3 ymm4_3 0x3d@uint64;
(* vpsllq $0x3,%ymm4,%ymm4                         #! PC = 0x555555579919 *)
shl ymm4_0 ymm4_0 0x3@uint64;
shl ymm4_1 ymm4_1 0x3@uint64;
shl ymm4_2 ymm4_2 0x3@uint64;
shl ymm4_3 ymm4_3 0x3@uint64;
(* vpor   %ymm4,%ymm2,%ymm2                        #! PC = 0x55555557991e *)
or ymm2_0@uint64 ymm2_0 ymm4_0;
or ymm2_1@uint64 ymm2_1 ymm4_1;
or ymm2_2@uint64 ymm2_2 ymm4_2;
or ymm2_3@uint64 ymm2_3 ymm4_3;
(* vpsrlq $0x3,%ymm11,%ymm4                        #! PC = 0x555555579922 *)
shr ymm4_0 ymm11_0 0x3@uint64;
shr ymm4_1 ymm11_1 0x3@uint64;
shr ymm4_2 ymm11_2 0x3@uint64;
shr ymm4_3 ymm11_3 0x3@uint64;
(* vpsllq $0x3d,%ymm11,%ymm11                      #! PC = 0x555555579928 *)
shl ymm11_0 ymm11_0 0x3d@uint64;
shl ymm11_1 ymm11_1 0x3d@uint64;
shl ymm11_2 ymm11_2 0x3d@uint64;
shl ymm11_3 ymm11_3 0x3d@uint64;
(* vpandn %ymm2,%ymm0,%ymm9                        #! PC = 0x55555557992e *)
not ymm0_0n@uint64 ymm0_0;
and ymm9_0@uint64 ymm0_0n ymm2_0;
not ymm0_1n@uint64 ymm0_1;
and ymm9_1@uint64 ymm0_1n ymm2_1;
not ymm0_2n@uint64 ymm0_2;
and ymm9_2@uint64 ymm0_2n ymm2_2;
not ymm0_3n@uint64 ymm0_3;
and ymm9_3@uint64 ymm0_3n ymm2_3;
(* vpor   %ymm11,%ymm4,%ymm11                      #! PC = 0x555555579932 *)
or ymm11_0@uint64 ymm4_0 ymm11_0;
or ymm11_1@uint64 ymm4_1 ymm11_1;
or ymm11_2@uint64 ymm4_2 ymm11_2;
or ymm11_3@uint64 ymm4_3 ymm11_3;
(* vpxor  %ymm10,%ymm9,%ymm7                       #! PC = 0x555555579937 *)
xor ymm7_0@uint64 ymm9_0 ymm10_0;
xor ymm7_1@uint64 ymm9_1 ymm10_1;
xor ymm7_2@uint64 ymm9_2 ymm10_2;
xor ymm7_3@uint64 ymm9_3 ymm10_3;
(* vpandn %ymm11,%ymm13,%ymm4                      #! PC = 0x55555557993c *)
not ymm13_0n@uint64 ymm13_0;
and ymm4_0@uint64 ymm13_0n ymm11_0;
not ymm13_1n@uint64 ymm13_1;
and ymm4_1@uint64 ymm13_1n ymm11_1;
not ymm13_2n@uint64 ymm13_2;
and ymm4_2@uint64 ymm13_2n ymm11_2;
not ymm13_3n@uint64 ymm13_3;
and ymm4_3@uint64 ymm13_3n ymm11_3;
(* vmovdqa %ymm7,-0x130(%rbp)                      #! EA = L0x7fffffffbe20; PC = 0x555555579941 *)
mov L0x7fffffffbe20 ymm7_0;
mov L0x7fffffffbe28 ymm7_1;
mov L0x7fffffffbe30 ymm7_2;
mov L0x7fffffffbe38 ymm7_3;
(* vpandn %ymm13,%ymm2,%ymm7                       #! PC = 0x555555579949 *)
not ymm2_0n@uint64 ymm2_0;
and ymm7_0@uint64 ymm2_0n ymm13_0;
not ymm2_1n@uint64 ymm2_1;
and ymm7_1@uint64 ymm2_1n ymm13_1;
not ymm2_2n@uint64 ymm2_2;
and ymm7_2@uint64 ymm2_2n ymm13_2;
not ymm2_3n@uint64 ymm2_3;
and ymm7_3@uint64 ymm2_3n ymm13_3;
(* vpxor  %ymm2,%ymm4,%ymm8                        #! PC = 0x55555557994e *)
xor ymm8_0@uint64 ymm4_0 ymm2_0;
xor ymm8_1@uint64 ymm4_1 ymm2_1;
xor ymm8_2@uint64 ymm4_2 ymm2_2;
xor ymm8_3@uint64 ymm4_3 ymm2_3;
(* vpandn %ymm10,%ymm11,%ymm2                      #! PC = 0x555555579952 *)
not ymm11_0n@uint64 ymm11_0;
and ymm2_0@uint64 ymm11_0n ymm10_0;
not ymm11_1n@uint64 ymm11_1;
and ymm2_1@uint64 ymm11_1n ymm10_1;
not ymm11_2n@uint64 ymm11_2;
and ymm2_2@uint64 ymm11_2n ymm10_2;
not ymm11_3n@uint64 ymm11_3;
and ymm2_3@uint64 ymm11_3n ymm10_3;
(* vpxor  -0x1d0(%rbp),%ymm14,%ymm4                #! EA = L0x7fffffffbd80; Value = 0x71fa1d117d2f0d2b; PC = 0x555555579957 *)
xor ymm4_0@uint64 ymm14_0 L0x7fffffffbd80;
xor ymm4_1@uint64 ymm14_1 L0x7fffffffbd88;
xor ymm4_2@uint64 ymm14_2 L0x7fffffffbd90;
xor ymm4_3@uint64 ymm14_3 L0x7fffffffbd98;
(* vpxor  %ymm13,%ymm2,%ymm9                       #! PC = 0x55555557995f *)
xor ymm9_0@uint64 ymm2_0 ymm13_0;
xor ymm9_1@uint64 ymm2_1 ymm13_1;
xor ymm9_2@uint64 ymm2_2 ymm13_2;
xor ymm9_3@uint64 ymm2_3 ymm13_3;
(* vpxor  %ymm0,%ymm7,%ymm7                        #! PC = 0x555555579964 *)
xor ymm7_0@uint64 ymm7_0 ymm0_0;
xor ymm7_1@uint64 ymm7_1 ymm0_1;
xor ymm7_2@uint64 ymm7_2 ymm0_2;
xor ymm7_3@uint64 ymm7_3 ymm0_3;
(* vpxor  -0x2b0(%rbp),%ymm1,%ymm2                 #! EA = L0x7fffffffbca0; Value = 0x9815a7fefc970812; PC = 0x555555579968 *)
xor ymm2_0@uint64 ymm1_0 L0x7fffffffbca0;
xor ymm2_1@uint64 ymm1_1 L0x7fffffffbca8;
xor ymm2_2@uint64 ymm1_2 L0x7fffffffbcb0;
xor ymm2_3@uint64 ymm1_3 L0x7fffffffbcb8;
(* vmovdqa %ymm8,-0x150(%rbp)                      #! EA = L0x7fffffffbe00; PC = 0x555555579970 *)
mov L0x7fffffffbe00 ymm8_0;
mov L0x7fffffffbe08 ymm8_1;
mov L0x7fffffffbe10 ymm8_2;
mov L0x7fffffffbe18 ymm8_3;
(* vpandn %ymm0,%ymm10,%ymm10                      #! PC = 0x555555579978 *)
not ymm10_0n@uint64 ymm10_0;
and ymm10_0@uint64 ymm10_0n ymm0_0;
not ymm10_1n@uint64 ymm10_1;
and ymm10_1@uint64 ymm10_1n ymm0_1;
not ymm10_2n@uint64 ymm10_2;
and ymm10_2@uint64 ymm10_2n ymm0_2;
not ymm10_3n@uint64 ymm10_3;
and ymm10_3@uint64 ymm10_3n ymm0_3;
(* vmovdqa %ymm9,-0x170(%rbp)                      #! EA = L0x7fffffffbde0; PC = 0x55555557997c *)
mov L0x7fffffffbde0 ymm9_0;
mov L0x7fffffffbde8 ymm9_1;
mov L0x7fffffffbdf0 ymm9_2;
mov L0x7fffffffbdf8 ymm9_3;
(* vpsrlq $0x3f,%ymm2,%ymm0                        #! PC = 0x555555579984 *)
shr ymm0_0 ymm2_0 0x3f@uint64;
shr ymm0_1 ymm2_1 0x3f@uint64;
shr ymm0_2 ymm2_2 0x3f@uint64;
shr ymm0_3 ymm2_3 0x3f@uint64;
(* vpsllq $0x1,%ymm2,%ymm2                         #! PC = 0x555555579989 *)
shl ymm2_0 ymm2_0 0x1@uint64;
shl ymm2_1 ymm2_1 0x1@uint64;
shl ymm2_2 ymm2_2 0x1@uint64;
shl ymm2_3 ymm2_3 0x1@uint64;
(* vpxor  %ymm11,%ymm10,%ymm11                     #! PC = 0x55555557998e *)
xor ymm11_0@uint64 ymm10_0 ymm11_0;
xor ymm11_1@uint64 ymm10_1 ymm11_1;
xor ymm11_2@uint64 ymm10_2 ymm11_2;
xor ymm11_3@uint64 ymm10_3 ymm11_3;
(* vpor   %ymm2,%ymm0,%ymm0                        #! PC = 0x555555579993 *)
or ymm0_0@uint64 ymm0_0 ymm2_0;
or ymm0_1@uint64 ymm0_1 ymm2_1;
or ymm0_2@uint64 ymm0_2 ymm2_2;
or ymm0_3@uint64 ymm0_3 ymm2_3;
(* vpsrlq $0x3a,%ymm4,%ymm2                        #! PC = 0x555555579997 *)
shr ymm2_0 ymm4_0 0x3a@uint64;
shr ymm2_1 ymm4_1 0x3a@uint64;
shr ymm2_2 ymm4_2 0x3a@uint64;
shr ymm2_3 ymm4_3 0x3a@uint64;
(* vmovdqa %ymm11,-0x190(%rbp)                     #! EA = L0x7fffffffbdc0; PC = 0x55555557999c *)
mov L0x7fffffffbdc0 ymm11_0;
mov L0x7fffffffbdc8 ymm11_1;
mov L0x7fffffffbdd0 ymm11_2;
mov L0x7fffffffbdd8 ymm11_3;
(* vpsllq $0x6,%ymm4,%ymm4                         #! PC = 0x5555555799a4 *)
shl ymm4_0 ymm4_0 0x6@uint64;
shl ymm4_1 ymm4_1 0x6@uint64;
shl ymm4_2 ymm4_2 0x6@uint64;
shl ymm4_3 ymm4_3 0x6@uint64;
(* vpor   %ymm4,%ymm2,%ymm2                        #! PC = 0x5555555799a9 *)
or ymm2_0@uint64 ymm2_0 ymm4_0;
or ymm2_1@uint64 ymm2_1 ymm4_1;
or ymm2_2@uint64 ymm2_2 ymm4_2;
or ymm2_3@uint64 ymm2_3 ymm4_3;
(* vpxor  -0x1b0(%rbp),%ymm15,%ymm4                #! EA = L0x7fffffffbda0; Value = 0x2a1c42a54f48b263; PC = 0x5555555799ad *)
xor ymm4_0@uint64 ymm15_0 L0x7fffffffbda0;
xor ymm4_1@uint64 ymm15_1 L0x7fffffffbda8;
xor ymm4_2@uint64 ymm15_2 L0x7fffffffbdb0;
xor ymm4_3@uint64 ymm15_3 L0x7fffffffbdb8;
(* vpsrlq $0x27,%ymm4,%ymm8                        #! PC = 0x5555555799b5 *)
shr ymm8_0 ymm4_0 0x27@uint64;
shr ymm8_1 ymm4_1 0x27@uint64;
shr ymm8_2 ymm4_2 0x27@uint64;
shr ymm8_3 ymm4_3 0x27@uint64;
(* vpsllq $0x19,%ymm4,%ymm6                        #! PC = 0x5555555799ba *)
shl ymm6_0 ymm4_0 0x19@uint64;
shl ymm6_1 ymm4_1 0x19@uint64;
shl ymm6_2 ymm4_2 0x19@uint64;
shl ymm6_3 ymm4_3 0x19@uint64;
(* vpor   %ymm6,%ymm8,%ymm6                        #! PC = 0x5555555799bf *)
or ymm6_0@uint64 ymm8_0 ymm6_0;
or ymm6_1@uint64 ymm8_1 ymm6_1;
or ymm6_2@uint64 ymm8_2 ymm6_2;
or ymm6_3@uint64 ymm8_3 ymm6_3;
(* vpandn %ymm6,%ymm2,%ymm4                        #! PC = 0x5555555799c3 *)
not ymm2_0n@uint64 ymm2_0;
and ymm4_0@uint64 ymm2_0n ymm6_0;
not ymm2_1n@uint64 ymm2_1;
and ymm4_1@uint64 ymm2_1n ymm6_1;
not ymm2_2n@uint64 ymm2_2;
and ymm4_2@uint64 ymm2_2n ymm6_2;
not ymm2_3n@uint64 ymm2_3;
and ymm4_3@uint64 ymm2_3n ymm6_3;
(* vpxor  %ymm0,%ymm4,%ymm11                       #! PC = 0x5555555799c7 *)
xor ymm11_0@uint64 ymm4_0 ymm0_0;
xor ymm11_1@uint64 ymm4_1 ymm0_1;
xor ymm11_2@uint64 ymm4_2 ymm0_2;
xor ymm11_3@uint64 ymm4_3 ymm0_3;
(* vpxor  -0x70(%rbp),%ymm12,%ymm4                 #! EA = L0x7fffffffbee0; Value = 0xdb137cc98c3c8239; PC = 0x5555555799cb *)
xor ymm4_0@uint64 ymm12_0 L0x7fffffffbee0;
xor ymm4_1@uint64 ymm12_1 L0x7fffffffbee8;
xor ymm4_2@uint64 ymm12_2 L0x7fffffffbef0;
xor ymm4_3@uint64 ymm12_3 L0x7fffffffbef8;
(* vmovdqa %ymm11,%ymm13                           #! PC = 0x5555555799d0 *)
mov ymm13_0 ymm11_0;
mov ymm13_1 ymm11_1;
mov ymm13_2 ymm11_2;
mov ymm13_3 ymm11_3;
(* vpsrlq $0x2e,%ymm5,%ymm11                       #! PC = 0x5555555799d5 *)
shr ymm11_0 ymm5_0 0x2e@uint64;
shr ymm11_1 ymm5_1 0x2e@uint64;
shr ymm11_2 ymm5_2 0x2e@uint64;
shr ymm11_3 ymm5_3 0x2e@uint64;
(* vpshufb 0x5451d(%rip),%ymm4,%ymm4        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x5555555799da *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm4_0, ymm4_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm4_2, ymm4_3, tmp_2, tmp_3);
mov ymm4_0 tmp_0;
mov ymm4_1 tmp_1;
mov ymm4_2 tmp_2;
mov ymm4_3 tmp_3;
(* vpsllq $0x12,%ymm5,%ymm5                        #! PC = 0x5555555799e3 *)
shl ymm5_0 ymm5_0 0x12@uint64;
shl ymm5_1 ymm5_1 0x12@uint64;
shl ymm5_2 ymm5_2 0x12@uint64;
shl ymm5_3 ymm5_3 0x12@uint64;
(* vpandn %ymm4,%ymm6,%ymm10                       #! PC = 0x5555555799e8 *)
not ymm6_0n@uint64 ymm6_0;
and ymm10_0@uint64 ymm6_0n ymm4_0;
not ymm6_1n@uint64 ymm6_1;
and ymm10_1@uint64 ymm6_1n ymm4_1;
not ymm6_2n@uint64 ymm6_2;
and ymm10_2@uint64 ymm6_2n ymm4_2;
not ymm6_3n@uint64 ymm6_3;
and ymm10_3@uint64 ymm6_3n ymm4_3;
(* vpor   %ymm5,%ymm11,%ymm5                       #! PC = 0x5555555799ec *)
or ymm5_0@uint64 ymm11_0 ymm5_0;
or ymm5_1@uint64 ymm11_1 ymm5_1;
or ymm5_2@uint64 ymm11_2 ymm5_2;
or ymm5_3@uint64 ymm11_3 ymm5_3;
(* vpxor  %ymm2,%ymm10,%ymm8                       #! PC = 0x5555555799f0 *)
xor ymm8_0@uint64 ymm10_0 ymm2_0;
xor ymm8_1@uint64 ymm10_1 ymm2_1;
xor ymm8_2@uint64 ymm10_2 ymm2_2;
xor ymm8_3@uint64 ymm10_3 ymm2_3;
(* vmovdqa %ymm8,-0x70(%rbp)                       #! EA = L0x7fffffffbee0; PC = 0x5555555799f4 *)
mov L0x7fffffffbee0 ymm8_0;
mov L0x7fffffffbee8 ymm8_1;
mov L0x7fffffffbef0 ymm8_2;
mov L0x7fffffffbef8 ymm8_3;
(* vpandn %ymm5,%ymm4,%ymm8                        #! PC = 0x5555555799f9 *)
not ymm4_0n@uint64 ymm4_0;
and ymm8_0@uint64 ymm4_0n ymm5_0;
not ymm4_1n@uint64 ymm4_1;
and ymm8_1@uint64 ymm4_1n ymm5_1;
not ymm4_2n@uint64 ymm4_2;
and ymm8_2@uint64 ymm4_2n ymm5_2;
not ymm4_3n@uint64 ymm4_3;
and ymm8_3@uint64 ymm4_3n ymm5_3;
(* vpxor  %ymm6,%ymm8,%ymm8                        #! PC = 0x5555555799fd *)
xor ymm8_0@uint64 ymm8_0 ymm6_0;
xor ymm8_1@uint64 ymm8_1 ymm6_1;
xor ymm8_2@uint64 ymm8_2 ymm6_2;
xor ymm8_3@uint64 ymm8_3 ymm6_3;
(* vpandn %ymm0,%ymm5,%ymm6                        #! PC = 0x555555579a01 *)
not ymm5_0n@uint64 ymm5_0;
and ymm6_0@uint64 ymm5_0n ymm0_0;
not ymm5_1n@uint64 ymm5_1;
and ymm6_1@uint64 ymm5_1n ymm0_1;
not ymm5_2n@uint64 ymm5_2;
and ymm6_2@uint64 ymm5_2n ymm0_2;
not ymm5_3n@uint64 ymm5_3;
and ymm6_3@uint64 ymm5_3n ymm0_3;
(* vpandn %ymm2,%ymm0,%ymm0                        #! PC = 0x555555579a05 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm2_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm2_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm2_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm2_3;
(* vpxor  %ymm5,%ymm0,%ymm5                        #! PC = 0x555555579a09 *)
xor ymm5_0@uint64 ymm0_0 ymm5_0;
xor ymm5_1@uint64 ymm0_1 ymm5_1;
xor ymm5_2@uint64 ymm0_2 ymm5_2;
xor ymm5_3@uint64 ymm0_3 ymm5_3;
(* vpxor  -0x270(%rbp),%ymm12,%ymm0                #! EA = L0x7fffffffbce0; Value = 0xfb3f99d0c00b9d77; PC = 0x555555579a0d *)
xor ymm0_0@uint64 ymm12_0 L0x7fffffffbce0;
xor ymm0_1@uint64 ymm12_1 L0x7fffffffbce8;
xor ymm0_2@uint64 ymm12_2 L0x7fffffffbcf0;
xor ymm0_3@uint64 ymm12_3 L0x7fffffffbcf8;
(* vpxor  %ymm4,%ymm6,%ymm11                       #! PC = 0x555555579a15 *)
xor ymm11_0@uint64 ymm6_0 ymm4_0;
xor ymm11_1@uint64 ymm6_1 ymm4_1;
xor ymm11_2@uint64 ymm6_2 ymm4_2;
xor ymm11_3@uint64 ymm6_3 ymm4_3;
(* vmovdqa %ymm11,-0x1b0(%rbp)                     #! EA = L0x7fffffffbda0; PC = 0x555555579a19 *)
mov L0x7fffffffbda0 ymm11_0;
mov L0x7fffffffbda8 ymm11_1;
mov L0x7fffffffbdb0 ymm11_2;
mov L0x7fffffffbdb8 ymm11_3;
(* vpsrlq $0x25,%ymm0,%ymm9                        #! PC = 0x555555579a21 *)
shr ymm9_0 ymm0_0 0x25@uint64;
shr ymm9_1 ymm0_1 0x25@uint64;
shr ymm9_2 ymm0_2 0x25@uint64;
shr ymm9_3 ymm0_3 0x25@uint64;
(* vpsllq $0x1b,%ymm0,%ymm0                        #! PC = 0x555555579a26 *)
shl ymm0_0 ymm0_0 0x1b@uint64;
shl ymm0_1 ymm0_1 0x1b@uint64;
shl ymm0_2 ymm0_2 0x1b@uint64;
shl ymm0_3 ymm0_3 0x1b@uint64;
(* vmovdqa %ymm5,-0x1d0(%rbp)                      #! EA = L0x7fffffffbd80; PC = 0x555555579a2b *)
mov L0x7fffffffbd80 ymm5_0;
mov L0x7fffffffbd88 ymm5_1;
mov L0x7fffffffbd90 ymm5_2;
mov L0x7fffffffbd98 ymm5_3;
(* vpxor  -0x50(%rbp),%ymm14,%ymm5                 #! EA = L0x7fffffffbf00; Value = 0x641e17838270a7af; PC = 0x555555579a33 *)
xor ymm5_0@uint64 ymm14_0 L0x7fffffffbf00;
xor ymm5_1@uint64 ymm14_1 L0x7fffffffbf08;
xor ymm5_2@uint64 ymm14_2 L0x7fffffffbf10;
xor ymm5_3@uint64 ymm14_3 L0x7fffffffbf18;
(* vpor   %ymm0,%ymm9,%ymm9                        #! PC = 0x555555579a38 *)
or ymm9_0@uint64 ymm9_0 ymm0_0;
or ymm9_1@uint64 ymm9_1 ymm0_1;
or ymm9_2@uint64 ymm9_2 ymm0_2;
or ymm9_3@uint64 ymm9_3 ymm0_3;
(* vpxor  -0x210(%rbp),%ymm3,%ymm0                 #! EA = L0x7fffffffbd40; Value = 0x9debe90720187a5e; PC = 0x555555579a3c *)
xor ymm0_0@uint64 ymm3_0 L0x7fffffffbd40;
xor ymm0_1@uint64 ymm3_1 L0x7fffffffbd48;
xor ymm0_2@uint64 ymm3_2 L0x7fffffffbd50;
xor ymm0_3@uint64 ymm3_3 L0x7fffffffbd58;
(* vpsrlq $0x31,%ymm5,%ymm6                        #! PC = 0x555555579a44 *)
shr ymm6_0 ymm5_0 0x31@uint64;
shr ymm6_1 ymm5_1 0x31@uint64;
shr ymm6_2 ymm5_2 0x31@uint64;
shr ymm6_3 ymm5_3 0x31@uint64;
(* vpsllq $0xf,%ymm5,%ymm5                         #! PC = 0x555555579a49 *)
shl ymm5_0 ymm5_0 0xf@uint64;
shl ymm5_1 ymm5_1 0xf@uint64;
shl ymm5_2 ymm5_2 0xf@uint64;
shl ymm5_3 ymm5_3 0xf@uint64;
(* vpsrlq $0x1c,%ymm0,%ymm10                       #! PC = 0x555555579a4e *)
shr ymm10_0 ymm0_0 0x1c@uint64;
shr ymm10_1 ymm0_1 0x1c@uint64;
shr ymm10_2 ymm0_2 0x1c@uint64;
shr ymm10_3 ymm0_3 0x1c@uint64;
(* vpsllq $0x24,%ymm0,%ymm0                        #! PC = 0x555555579a53 *)
shl ymm0_0 ymm0_0 0x24@uint64;
shl ymm0_1 ymm0_1 0x24@uint64;
shl ymm0_2 ymm0_2 0x24@uint64;
shl ymm0_3 ymm0_3 0x24@uint64;
(* vpor   %ymm0,%ymm10,%ymm10                      #! PC = 0x555555579a58 *)
or ymm10_0@uint64 ymm10_0 ymm0_0;
or ymm10_1@uint64 ymm10_1 ymm0_1;
or ymm10_2@uint64 ymm10_2 ymm0_2;
or ymm10_3@uint64 ymm10_3 ymm0_3;
(* vpxor  -0x1f0(%rbp),%ymm1,%ymm0                 #! EA = L0x7fffffffbd60; Value = 0x5dcd9aa3fcdad534; PC = 0x555555579a5c *)
xor ymm0_0@uint64 ymm1_0 L0x7fffffffbd60;
xor ymm0_1@uint64 ymm1_1 L0x7fffffffbd68;
xor ymm0_2@uint64 ymm1_2 L0x7fffffffbd70;
xor ymm0_3@uint64 ymm1_3 L0x7fffffffbd78;
(* vpsrlq $0x36,%ymm0,%ymm11                       #! PC = 0x555555579a64 *)
shr ymm11_0 ymm0_0 0x36@uint64;
shr ymm11_1 ymm0_1 0x36@uint64;
shr ymm11_2 ymm0_2 0x36@uint64;
shr ymm11_3 ymm0_3 0x36@uint64;
(* vpsllq $0xa,%ymm0,%ymm0                         #! PC = 0x555555579a69 *)
shl ymm0_0 ymm0_0 0xa@uint64;
shl ymm0_1 ymm0_1 0xa@uint64;
shl ymm0_2 ymm0_2 0xa@uint64;
shl ymm0_3 ymm0_3 0xa@uint64;
(* vpor   %ymm0,%ymm11,%ymm11                      #! PC = 0x555555579a6e *)
or ymm11_0@uint64 ymm11_0 ymm0_0;
or ymm11_1@uint64 ymm11_1 ymm0_1;
or ymm11_2@uint64 ymm11_2 ymm0_2;
or ymm11_3@uint64 ymm11_3 ymm0_3;
(* vpandn %ymm11,%ymm10,%ymm2                      #! PC = 0x555555579a72 *)
not ymm10_0n@uint64 ymm10_0;
and ymm2_0@uint64 ymm10_0n ymm11_0;
not ymm10_1n@uint64 ymm10_1;
and ymm2_1@uint64 ymm10_1n ymm11_1;
not ymm10_2n@uint64 ymm10_2;
and ymm2_2@uint64 ymm10_2n ymm11_2;
not ymm10_3n@uint64 ymm10_3;
and ymm2_3@uint64 ymm10_3n ymm11_3;
(* vpxor  %ymm9,%ymm2,%ymm0                        #! PC = 0x555555579a77 *)
xor ymm0_0@uint64 ymm2_0 ymm9_0;
xor ymm0_1@uint64 ymm2_1 ymm9_1;
xor ymm0_2@uint64 ymm2_2 ymm9_2;
xor ymm0_3@uint64 ymm2_3 ymm9_3;
(* vpor   %ymm5,%ymm6,%ymm2                        #! PC = 0x555555579a7c *)
or ymm2_0@uint64 ymm6_0 ymm5_0;
or ymm2_1@uint64 ymm6_1 ymm5_1;
or ymm2_2@uint64 ymm6_2 ymm5_2;
or ymm2_3@uint64 ymm6_3 ymm5_3;
(* vmovdqa %ymm0,-0x1f0(%rbp)                      #! EA = L0x7fffffffbd60; PC = 0x555555579a80 *)
mov L0x7fffffffbd60 ymm0_0;
mov L0x7fffffffbd68 ymm0_1;
mov L0x7fffffffbd70 ymm0_2;
mov L0x7fffffffbd78 ymm0_3;
(* vpxor  -0x310(%rbp),%ymm15,%ymm0                #! EA = L0x7fffffffbc40; Value = 0x5e28c4ce78bd2190; PC = 0x555555579a88 *)
xor ymm0_0@uint64 ymm15_0 L0x7fffffffbc40;
xor ymm0_1@uint64 ymm15_1 L0x7fffffffbc48;
xor ymm0_2@uint64 ymm15_2 L0x7fffffffbc50;
xor ymm0_3@uint64 ymm15_3 L0x7fffffffbc58;
(* vpandn %ymm2,%ymm11,%ymm5                       #! PC = 0x555555579a90 *)
not ymm11_0n@uint64 ymm11_0;
and ymm5_0@uint64 ymm11_0n ymm2_0;
not ymm11_1n@uint64 ymm11_1;
and ymm5_1@uint64 ymm11_1n ymm2_1;
not ymm11_2n@uint64 ymm11_2;
and ymm5_2@uint64 ymm11_2n ymm2_2;
not ymm11_3n@uint64 ymm11_3;
and ymm5_3@uint64 ymm11_3n ymm2_3;
(* vpxor  %ymm10,%ymm5,%ymm5                       #! PC = 0x555555579a94 *)
xor ymm5_0@uint64 ymm5_0 ymm10_0;
xor ymm5_1@uint64 ymm5_1 ymm10_1;
xor ymm5_2@uint64 ymm5_2 ymm10_2;
xor ymm5_3@uint64 ymm5_3 ymm10_3;
(* vpshufb 0x5443e(%rip),%ymm0,%ymm0        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x555555579a99 *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm0_0, ymm0_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm0_2, ymm0_3, tmp_2, tmp_3);
mov ymm0_0 tmp_0;
mov ymm0_1 tmp_1;
mov ymm0_2 tmp_2;
mov ymm0_3 tmp_3;
(* vpandn %ymm0,%ymm2,%ymm4                        #! PC = 0x555555579aa2 *)
not ymm2_0n@uint64 ymm2_0;
and ymm4_0@uint64 ymm2_0n ymm0_0;
not ymm2_1n@uint64 ymm2_1;
and ymm4_1@uint64 ymm2_1n ymm0_1;
not ymm2_2n@uint64 ymm2_2;
and ymm4_2@uint64 ymm2_2n ymm0_2;
not ymm2_3n@uint64 ymm2_3;
and ymm4_3@uint64 ymm2_3n ymm0_3;
(* vpandn %ymm9,%ymm0,%ymm6                        #! PC = 0x555555579aa6 *)
not ymm0_0n@uint64 ymm0_0;
and ymm6_0@uint64 ymm0_0n ymm9_0;
not ymm0_1n@uint64 ymm0_1;
and ymm6_1@uint64 ymm0_1n ymm9_1;
not ymm0_2n@uint64 ymm0_2;
and ymm6_2@uint64 ymm0_2n ymm9_2;
not ymm0_3n@uint64 ymm0_3;
and ymm6_3@uint64 ymm0_3n ymm9_3;
(* vpandn %ymm10,%ymm9,%ymm9                       #! PC = 0x555555579aab *)
not ymm9_0n@uint64 ymm9_0;
and ymm9_0@uint64 ymm9_0n ymm10_0;
not ymm9_1n@uint64 ymm9_1;
and ymm9_1@uint64 ymm9_1n ymm10_1;
not ymm9_2n@uint64 ymm9_2;
and ymm9_2@uint64 ymm9_2n ymm10_2;
not ymm9_3n@uint64 ymm9_3;
and ymm9_3@uint64 ymm9_3n ymm10_3;
(* vpxor  %ymm11,%ymm4,%ymm4                       #! PC = 0x555555579ab0 *)
xor ymm4_0@uint64 ymm4_0 ymm11_0;
xor ymm4_1@uint64 ymm4_1 ymm11_1;
xor ymm4_2@uint64 ymm4_2 ymm11_2;
xor ymm4_3@uint64 ymm4_3 ymm11_3;
(* vpxor  %ymm2,%ymm6,%ymm6                        #! PC = 0x555555579ab5 *)
xor ymm6_0@uint64 ymm6_0 ymm2_0;
xor ymm6_1@uint64 ymm6_1 ymm2_1;
xor ymm6_2@uint64 ymm6_2 ymm2_2;
xor ymm6_3@uint64 ymm6_3 ymm2_3;
(* vmovdqa %ymm4,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x555555579ab9 *)
mov L0x7fffffffbf00 ymm4_0;
mov L0x7fffffffbf08 ymm4_1;
mov L0x7fffffffbf10 ymm4_2;
mov L0x7fffffffbf18 ymm4_3;
(* vpxor  -0x250(%rbp),%ymm15,%ymm15               #! EA = L0x7fffffffbd00; Value = 0x7e2d04cc17121756; PC = 0x555555579abe *)
xor ymm15_0@uint64 ymm15_0 L0x7fffffffbd00;
xor ymm15_1@uint64 ymm15_1 L0x7fffffffbd08;
xor ymm15_2@uint64 ymm15_2 L0x7fffffffbd10;
xor ymm15_3@uint64 ymm15_3 L0x7fffffffbd18;
(* vpxor  -0x230(%rbp),%ymm12,%ymm12               #! EA = L0x7fffffffbd20; Value = 0x439eb6c088b33bb8; PC = 0x555555579ac6 *)
xor ymm12_0@uint64 ymm12_0 L0x7fffffffbd20;
xor ymm12_1@uint64 ymm12_1 L0x7fffffffbd28;
xor ymm12_2@uint64 ymm12_2 L0x7fffffffbd30;
xor ymm12_3@uint64 ymm12_3 L0x7fffffffbd38;
(* vpxor  %ymm0,%ymm9,%ymm4                        #! PC = 0x555555579ace *)
xor ymm4_0@uint64 ymm9_0 ymm0_0;
xor ymm4_1@uint64 ymm9_1 ymm0_1;
xor ymm4_2@uint64 ymm9_2 ymm0_2;
xor ymm4_3@uint64 ymm9_3 ymm0_3;
(* vpxor  -0x290(%rbp),%ymm14,%ymm14               #! EA = L0x7fffffffbcc0; Value = 0x7adeb6308d077a89; PC = 0x555555579ad2 *)
xor ymm14_0@uint64 ymm14_0 L0x7fffffffbcc0;
xor ymm14_1@uint64 ymm14_1 L0x7fffffffbcc8;
xor ymm14_2@uint64 ymm14_2 L0x7fffffffbcd0;
xor ymm14_3@uint64 ymm14_3 L0x7fffffffbcd8;
(* vpxor  -0x2d0(%rbp),%ymm3,%ymm3                 #! EA = L0x7fffffffbc80; Value = 0x3de51a3100441002; PC = 0x555555579ada *)
xor ymm3_0@uint64 ymm3_0 L0x7fffffffbc80;
xor ymm3_1@uint64 ymm3_1 L0x7fffffffbc88;
xor ymm3_2@uint64 ymm3_2 L0x7fffffffbc90;
xor ymm3_3@uint64 ymm3_3 L0x7fffffffbc98;
(* vmovdqa %ymm4,-0x210(%rbp)                      #! EA = L0x7fffffffbd40; PC = 0x555555579ae2 *)
mov L0x7fffffffbd40 ymm4_0;
mov L0x7fffffffbd48 ymm4_1;
mov L0x7fffffffbd50 ymm4_2;
mov L0x7fffffffbd58 ymm4_3;
(* vpsrlq $0x9,%ymm15,%ymm0                        #! PC = 0x555555579aea *)
shr ymm0_0 ymm15_0 0x9@uint64;
shr ymm0_1 ymm15_1 0x9@uint64;
shr ymm0_2 ymm15_2 0x9@uint64;
shr ymm0_3 ymm15_3 0x9@uint64;
(* vpsrlq $0x19,%ymm12,%ymm4                       #! PC = 0x555555579af0 *)
shr ymm4_0 ymm12_0 0x19@uint64;
shr ymm4_1 ymm12_1 0x19@uint64;
shr ymm4_2 ymm12_2 0x19@uint64;
shr ymm4_3 ymm12_3 0x19@uint64;
(* vmovdqa %ymm13,-0x2b0(%rbp)                     #! EA = L0x7fffffffbca0; PC = 0x555555579af6 *)
mov L0x7fffffffbca0 ymm13_0;
mov L0x7fffffffbca8 ymm13_1;
mov L0x7fffffffbcb0 ymm13_2;
mov L0x7fffffffbcb8 ymm13_3;
(* vpsllq $0x37,%ymm15,%ymm15                      #! PC = 0x555555579afe *)
shl ymm15_0 ymm15_0 0x37@uint64;
shl ymm15_1 ymm15_1 0x37@uint64;
shl ymm15_2 ymm15_2 0x37@uint64;
shl ymm15_3 ymm15_3 0x37@uint64;
(* vpsllq $0x27,%ymm12,%ymm12                      #! PC = 0x555555579b04 *)
shl ymm12_0 ymm12_0 0x27@uint64;
shl ymm12_1 ymm12_1 0x27@uint64;
shl ymm12_2 ymm12_2 0x27@uint64;
shl ymm12_3 ymm12_3 0x27@uint64;
(* vpor   %ymm15,%ymm0,%ymm15                      #! PC = 0x555555579b0a *)
or ymm15_0@uint64 ymm0_0 ymm15_0;
or ymm15_1@uint64 ymm0_1 ymm15_1;
or ymm15_2@uint64 ymm0_2 ymm15_2;
or ymm15_3@uint64 ymm0_3 ymm15_3;
(* vpsrlq $0x2,%ymm14,%ymm2                        #! PC = 0x555555579b0f *)
shr ymm2_0 ymm14_0 0x2@uint64;
shr ymm2_1 ymm14_1 0x2@uint64;
shr ymm2_2 ymm14_2 0x2@uint64;
shr ymm2_3 ymm14_3 0x2@uint64;
(* vpor   %ymm12,%ymm4,%ymm12                      #! PC = 0x555555579b15 *)
or ymm12_0@uint64 ymm4_0 ymm12_0;
or ymm12_1@uint64 ymm4_1 ymm12_1;
or ymm12_2@uint64 ymm4_2 ymm12_2;
or ymm12_3@uint64 ymm4_3 ymm12_3;
(* vpsllq $0x3e,%ymm14,%ymm14                      #! PC = 0x555555579b1a *)
shl ymm14_0 ymm14_0 0x3e@uint64;
shl ymm14_1 ymm14_1 0x3e@uint64;
shl ymm14_2 ymm14_2 0x3e@uint64;
shl ymm14_3 ymm14_3 0x3e@uint64;
(* vpsrlq $0x17,%ymm3,%ymm11                       #! PC = 0x555555579b20 *)
shr ymm11_0 ymm3_0 0x17@uint64;
shr ymm11_1 ymm3_1 0x17@uint64;
shr ymm11_2 ymm3_2 0x17@uint64;
shr ymm11_3 ymm3_3 0x17@uint64;
(* vpandn %ymm12,%ymm15,%ymm4                      #! PC = 0x555555579b25 *)
not ymm15_0n@uint64 ymm15_0;
and ymm4_0@uint64 ymm15_0n ymm12_0;
not ymm15_1n@uint64 ymm15_1;
and ymm4_1@uint64 ymm15_1n ymm12_1;
not ymm15_2n@uint64 ymm15_2;
and ymm4_2@uint64 ymm15_2n ymm12_2;
not ymm15_3n@uint64 ymm15_3;
and ymm4_3@uint64 ymm15_3n ymm12_3;
(* vpsllq $0x29,%ymm3,%ymm3                        #! PC = 0x555555579b2a *)
shl ymm3_0 ymm3_0 0x29@uint64;
shl ymm3_1 ymm3_1 0x29@uint64;
shl ymm3_2 ymm3_2 0x29@uint64;
shl ymm3_3 ymm3_3 0x29@uint64;
(* vpor   %ymm14,%ymm2,%ymm2                       #! PC = 0x555555579b2f *)
or ymm2_0@uint64 ymm2_0 ymm14_0;
or ymm2_1@uint64 ymm2_1 ymm14_1;
or ymm2_2@uint64 ymm2_2 ymm14_2;
or ymm2_3@uint64 ymm2_3 ymm14_3;
(* vpxor  -0x70(%rbp),%ymm5,%ymm14                 #! EA = L0x7fffffffbee0; Value = 0x479dabb00fd1751d; PC = 0x555555579b34 *)
xor ymm14_0@uint64 ymm5_0 L0x7fffffffbee0;
xor ymm14_1@uint64 ymm5_1 L0x7fffffffbee8;
xor ymm14_2@uint64 ymm5_2 L0x7fffffffbef0;
xor ymm14_3@uint64 ymm5_3 L0x7fffffffbef8;
(* vpor   %ymm3,%ymm11,%ymm11                      #! PC = 0x555555579b39 *)
or ymm11_0@uint64 ymm11_0 ymm3_0;
or ymm11_1@uint64 ymm11_1 ymm3_1;
or ymm11_2@uint64 ymm11_2 ymm3_2;
or ymm11_3@uint64 ymm11_3 ymm3_3;
(* vpxor  -0xb0(%rbp),%ymm7,%ymm3                  #! EA = L0x7fffffffbea0; Value = 0x96cce37672278288; PC = 0x555555579b3d *)
xor ymm3_0@uint64 ymm7_0 L0x7fffffffbea0;
xor ymm3_1@uint64 ymm7_1 L0x7fffffffbea8;
xor ymm3_2@uint64 ymm7_2 L0x7fffffffbeb0;
xor ymm3_3@uint64 ymm7_3 L0x7fffffffbeb8;
(* vpxor  -0x130(%rbp),%ymm13,%ymm0                #! EA = L0x7fffffffbe20; Value = 0x8c004295c6f66cea; PC = 0x555555579b45 *)
xor ymm0_0@uint64 ymm13_0 L0x7fffffffbe20;
xor ymm0_1@uint64 ymm13_1 L0x7fffffffbe28;
xor ymm0_2@uint64 ymm13_2 L0x7fffffffbe30;
xor ymm0_3@uint64 ymm13_3 L0x7fffffffbe38;
(* vpxor  %ymm2,%ymm4,%ymm4                        #! PC = 0x555555579b4d *)
xor ymm4_0@uint64 ymm4_0 ymm2_0;
xor ymm4_1@uint64 ymm4_1 ymm2_1;
xor ymm4_2@uint64 ymm4_2 ymm2_2;
xor ymm4_3@uint64 ymm4_3 ymm2_3;
(* vpxor  -0x1f0(%rbp),%ymm4,%ymm9                 #! EA = L0x7fffffffbd60; Value = 0x31a6e97ccae2238d; PC = 0x555555579b51 *)
xor ymm9_0@uint64 ymm4_0 L0x7fffffffbd60;
xor ymm9_1@uint64 ymm4_1 L0x7fffffffbd68;
xor ymm9_2@uint64 ymm4_2 L0x7fffffffbd70;
xor ymm9_3@uint64 ymm4_3 L0x7fffffffbd78;
(* vpxor  %ymm3,%ymm14,%ymm14                      #! PC = 0x555555579b59 *)
xor ymm14_0@uint64 ymm14_0 ymm3_0;
xor ymm14_1@uint64 ymm14_1 ymm3_1;
xor ymm14_2@uint64 ymm14_2 ymm3_2;
xor ymm14_3@uint64 ymm14_3 ymm3_3;
(* vpxor  -0x2f0(%rbp),%ymm1,%ymm3                 #! EA = L0x7fffffffbc60; Value = 0x6b62b259d8ff954c; PC = 0x555555579b5d *)
xor ymm3_0@uint64 ymm1_0 L0x7fffffffbc60;
xor ymm3_1@uint64 ymm1_1 L0x7fffffffbc68;
xor ymm3_2@uint64 ymm1_2 L0x7fffffffbc70;
xor ymm3_3@uint64 ymm1_3 L0x7fffffffbc78;
(* vpxor  %ymm0,%ymm9,%ymm9                        #! PC = 0x555555579b65 *)
xor ymm9_0@uint64 ymm9_0 ymm0_0;
xor ymm9_1@uint64 ymm9_1 ymm0_1;
xor ymm9_2@uint64 ymm9_2 ymm0_2;
xor ymm9_3@uint64 ymm9_3 ymm0_3;
(* vpandn %ymm11,%ymm12,%ymm0                      #! PC = 0x555555579b69 *)
not ymm12_0n@uint64 ymm12_0;
and ymm0_0@uint64 ymm12_0n ymm11_0;
not ymm12_1n@uint64 ymm12_1;
and ymm0_1@uint64 ymm12_1n ymm11_1;
not ymm12_2n@uint64 ymm12_2;
and ymm0_2@uint64 ymm12_2n ymm11_2;
not ymm12_3n@uint64 ymm12_3;
and ymm0_3@uint64 ymm12_3n ymm11_3;
(* vpxor  -0x90(%rbp),%ymm9,%ymm9                  #! EA = L0x7fffffffbec0; Value = 0x309632a97ced9ce8; PC = 0x555555579b6e *)
xor ymm9_0@uint64 ymm9_0 L0x7fffffffbec0;
xor ymm9_1@uint64 ymm9_1 L0x7fffffffbec8;
xor ymm9_2@uint64 ymm9_2 L0x7fffffffbed0;
xor ymm9_3@uint64 ymm9_3 L0x7fffffffbed8;
(* vpxor  %ymm15,%ymm0,%ymm13                      #! PC = 0x555555579b76 *)
xor ymm13_0@uint64 ymm0_0 ymm15_0;
xor ymm13_1@uint64 ymm0_1 ymm15_1;
xor ymm13_2@uint64 ymm0_2 ymm15_2;
xor ymm13_3@uint64 ymm0_3 ymm15_3;
(* vpsllq $0x2,%ymm3,%ymm1                         #! PC = 0x555555579b7b *)
shl ymm1_0 ymm3_0 0x2@uint64;
shl ymm1_1 ymm3_1 0x2@uint64;
shl ymm1_2 ymm3_2 0x2@uint64;
shl ymm1_3 ymm3_3 0x2@uint64;
(* vpsrlq $0x3e,%ymm3,%ymm0                        #! PC = 0x555555579b80 *)
shr ymm0_0 ymm3_0 0x3e@uint64;
shr ymm0_1 ymm3_1 0x3e@uint64;
shr ymm0_2 ymm3_2 0x3e@uint64;
shr ymm0_3 ymm3_3 0x3e@uint64;
(* vpxor  %ymm13,%ymm14,%ymm14                     #! PC = 0x555555579b85 *)
xor ymm14_0@uint64 ymm14_0 ymm13_0;
xor ymm14_1@uint64 ymm14_1 ymm13_1;
xor ymm14_2@uint64 ymm14_2 ymm13_2;
xor ymm14_3@uint64 ymm14_3 ymm13_3;
(* vmovdqa %ymm13,-0x310(%rbp)                     #! EA = L0x7fffffffbc40; PC = 0x555555579b8a *)
mov L0x7fffffffbc40 ymm13_0;
mov L0x7fffffffbc48 ymm13_1;
mov L0x7fffffffbc50 ymm13_2;
mov L0x7fffffffbc58 ymm13_3;
(* vpxor  -0x50(%rbp),%ymm8,%ymm13                 #! EA = L0x7fffffffbf00; Value = 0x4a71a7f66478dd99; PC = 0x555555579b92 *)
xor ymm13_0@uint64 ymm8_0 L0x7fffffffbf00;
xor ymm13_1@uint64 ymm8_1 L0x7fffffffbf08;
xor ymm13_2@uint64 ymm8_2 L0x7fffffffbf10;
xor ymm13_3@uint64 ymm8_3 L0x7fffffffbf18;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x555555579b97 *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpandn %ymm0,%ymm11,%ymm1                       #! PC = 0x555555579b9b *)
not ymm11_0n@uint64 ymm11_0;
and ymm1_0@uint64 ymm11_0n ymm0_0;
not ymm11_1n@uint64 ymm11_1;
and ymm1_1@uint64 ymm11_1n ymm0_1;
not ymm11_2n@uint64 ymm11_2;
and ymm1_2@uint64 ymm11_2n ymm0_2;
not ymm11_3n@uint64 ymm11_3;
and ymm1_3@uint64 ymm11_3n ymm0_3;
(* vpandn %ymm2,%ymm0,%ymm3                        #! PC = 0x555555579b9f *)
not ymm0_0n@uint64 ymm0_0;
and ymm3_0@uint64 ymm0_0n ymm2_0;
not ymm0_1n@uint64 ymm0_1;
and ymm3_1@uint64 ymm0_1n ymm2_1;
not ymm0_2n@uint64 ymm0_2;
and ymm3_2@uint64 ymm0_2n ymm2_2;
not ymm0_3n@uint64 ymm0_3;
and ymm3_3@uint64 ymm0_3n ymm2_3;
(* vpxor  %ymm12,%ymm1,%ymm12                      #! PC = 0x555555579ba3 *)
xor ymm12_0@uint64 ymm1_0 ymm12_0;
xor ymm12_1@uint64 ymm1_1 ymm12_1;
xor ymm12_2@uint64 ymm1_2 ymm12_2;
xor ymm12_3@uint64 ymm1_3 ymm12_3;
(* vmovdqa -0x150(%rbp),%ymm1                      #! EA = L0x7fffffffbe00; Value = 0x5c202896127a3201; PC = 0x555555579ba8 *)
mov ymm1_0 L0x7fffffffbe00;
mov ymm1_1 L0x7fffffffbe08;
mov ymm1_2 L0x7fffffffbe10;
mov ymm1_3 L0x7fffffffbe18;
(* vpxor  -0xd0(%rbp),%ymm1,%ymm1                  #! EA = L0x7fffffffbe80; Value = 0xb8b4102d555463a0; PC = 0x555555579bb0 *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbe80;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbe88;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbe90;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbe98;
(* vpxor  %ymm1,%ymm13,%ymm13                      #! PC = 0x555555579bb8 *)
xor ymm13_0@uint64 ymm13_0 ymm1_0;
xor ymm13_1@uint64 ymm13_1 ymm1_1;
xor ymm13_2@uint64 ymm13_2 ymm1_2;
xor ymm13_3@uint64 ymm13_3 ymm1_3;
(* vpxor  %ymm11,%ymm3,%ymm1                       #! PC = 0x555555579bbc *)
xor ymm1_0@uint64 ymm3_0 ymm11_0;
xor ymm1_1@uint64 ymm3_1 ymm11_1;
xor ymm1_2@uint64 ymm3_2 ymm11_2;
xor ymm1_3@uint64 ymm3_3 ymm11_3;
(* vmovdqa -0x170(%rbp),%ymm3                      #! EA = L0x7fffffffbde0; Value = 0xde117f77f405d44b; PC = 0x555555579bc1 *)
mov ymm3_0 L0x7fffffffbde0;
mov ymm3_1 L0x7fffffffbde8;
mov ymm3_2 L0x7fffffffbdf0;
mov ymm3_3 L0x7fffffffbdf8;
(* vpxor  %ymm1,%ymm6,%ymm11                       #! PC = 0x555555579bc9 *)
xor ymm11_0@uint64 ymm6_0 ymm1_0;
xor ymm11_1@uint64 ymm6_1 ymm1_1;
xor ymm11_2@uint64 ymm6_2 ymm1_2;
xor ymm11_3@uint64 ymm6_3 ymm1_3;
(* vmovdqa %ymm1,-0x2f0(%rbp)                      #! EA = L0x7fffffffbc60; PC = 0x555555579bcd *)
mov L0x7fffffffbc60 ymm1_0;
mov L0x7fffffffbc68 ymm1_1;
mov L0x7fffffffbc70 ymm1_2;
mov L0x7fffffffbc78 ymm1_3;
(* vpxor  %ymm12,%ymm13,%ymm13                     #! PC = 0x555555579bd5 *)
xor ymm13_0@uint64 ymm13_0 ymm12_0;
xor ymm13_1@uint64 ymm13_1 ymm12_1;
xor ymm13_2@uint64 ymm13_2 ymm12_2;
xor ymm13_3@uint64 ymm13_3 ymm12_3;
(* vpxor  -0xf0(%rbp),%ymm3,%ymm1                  #! EA = L0x7fffffffbe60; Value = 0x781cb76c1b390af6; PC = 0x555555579bda *)
xor ymm1_0@uint64 ymm3_0 L0x7fffffffbe60;
xor ymm1_1@uint64 ymm3_1 L0x7fffffffbe68;
xor ymm1_2@uint64 ymm3_2 L0x7fffffffbe70;
xor ymm1_3@uint64 ymm3_3 L0x7fffffffbe78;
(* vpsllq $0x1,%ymm13,%ymm3                        #! PC = 0x555555579be2 *)
shl ymm3_0 ymm13_0 0x1@uint64;
shl ymm3_1 ymm13_1 0x1@uint64;
shl ymm3_2 ymm13_2 0x1@uint64;
shl ymm3_3 ymm13_3 0x1@uint64;
(* vpxor  %ymm1,%ymm11,%ymm11                      #! PC = 0x555555579be8 *)
xor ymm11_0@uint64 ymm11_0 ymm1_0;
xor ymm11_1@uint64 ymm11_1 ymm1_1;
xor ymm11_2@uint64 ymm11_2 ymm1_2;
xor ymm11_3@uint64 ymm11_3 ymm1_3;
(* vpandn %ymm15,%ymm2,%ymm1                       #! PC = 0x555555579bec *)
not ymm2_0n@uint64 ymm2_0;
and ymm1_0@uint64 ymm2_0n ymm15_0;
not ymm2_1n@uint64 ymm2_1;
and ymm1_1@uint64 ymm2_1n ymm15_1;
not ymm2_2n@uint64 ymm2_2;
and ymm1_2@uint64 ymm2_2n ymm15_2;
not ymm2_3n@uint64 ymm2_3;
and ymm1_3@uint64 ymm2_3n ymm15_3;
(* vmovdqa -0x190(%rbp),%ymm15                     #! EA = L0x7fffffffbdc0; Value = 0x93350e8d9591bc9e; PC = 0x555555579bf1 *)
mov ymm15_0 L0x7fffffffbdc0;
mov ymm15_1 L0x7fffffffbdc8;
mov ymm15_2 L0x7fffffffbdd0;
mov ymm15_3 L0x7fffffffbdd8;
(* vpxor  -0x1b0(%rbp),%ymm11,%ymm11               #! EA = L0x7fffffffbda0; Value = 0xe4c33c7972e95e5c; PC = 0x555555579bf9 *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbda0;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbda8;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbdb0;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbdb8;
(* vpxor  %ymm0,%ymm1,%ymm1                        #! PC = 0x555555579c01 *)
xor ymm1_0@uint64 ymm1_0 ymm0_0;
xor ymm1_1@uint64 ymm1_1 ymm0_1;
xor ymm1_2@uint64 ymm1_2 ymm0_2;
xor ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpxor  -0x110(%rbp),%ymm15,%ymm0                #! EA = L0x7fffffffbe40; Value = 0x5efe9079fbd06374; PC = 0x555555579c05 *)
xor ymm0_0@uint64 ymm15_0 L0x7fffffffbe40;
xor ymm0_1@uint64 ymm15_1 L0x7fffffffbe48;
xor ymm0_2@uint64 ymm15_2 L0x7fffffffbe50;
xor ymm0_3@uint64 ymm15_3 L0x7fffffffbe58;
(* vpxor  -0x1d0(%rbp),%ymm1,%ymm10                #! EA = L0x7fffffffbd80; Value = 0x525049f077812bc0; PC = 0x555555579c0d *)
xor ymm10_0@uint64 ymm1_0 L0x7fffffffbd80;
xor ymm10_1@uint64 ymm1_1 L0x7fffffffbd88;
xor ymm10_2@uint64 ymm1_2 L0x7fffffffbd90;
xor ymm10_3@uint64 ymm1_3 L0x7fffffffbd98;
(* vpsrlq $0x3f,%ymm14,%ymm2                       #! PC = 0x555555579c15 *)
shr ymm2_0 ymm14_0 0x3f@uint64;
shr ymm2_1 ymm14_1 0x3f@uint64;
shr ymm2_2 ymm14_2 0x3f@uint64;
shr ymm2_3 ymm14_3 0x3f@uint64;
(* vpsllq $0x1,%ymm11,%ymm15                       #! PC = 0x555555579c1b *)
shl ymm15_0 ymm11_0 0x1@uint64;
shl ymm15_1 ymm11_1 0x1@uint64;
shl ymm15_2 ymm11_2 0x1@uint64;
shl ymm15_3 ymm11_3 0x1@uint64;
(* vpxor  %ymm0,%ymm10,%ymm10                      #! PC = 0x555555579c21 *)
xor ymm10_0@uint64 ymm10_0 ymm0_0;
xor ymm10_1@uint64 ymm10_1 ymm0_1;
xor ymm10_2@uint64 ymm10_2 ymm0_2;
xor ymm10_3@uint64 ymm10_3 ymm0_3;
(* vpsllq $0x1,%ymm14,%ymm0                        #! PC = 0x555555579c25 *)
shl ymm0_0 ymm14_0 0x1@uint64;
shl ymm0_1 ymm14_1 0x1@uint64;
shl ymm0_2 ymm14_2 0x1@uint64;
shl ymm0_3 ymm14_3 0x1@uint64;
(* vpxor  -0x210(%rbp),%ymm10,%ymm10               #! EA = L0x7fffffffbd40; Value = 0xe9231008af083f0c; PC = 0x555555579c2b *)
xor ymm10_0@uint64 ymm10_0 L0x7fffffffbd40;
xor ymm10_1@uint64 ymm10_1 L0x7fffffffbd48;
xor ymm10_2@uint64 ymm10_2 L0x7fffffffbd50;
xor ymm10_3@uint64 ymm10_3 L0x7fffffffbd58;
(* vpor   %ymm0,%ymm2,%ymm2                        #! PC = 0x555555579c33 *)
or ymm2_0@uint64 ymm2_0 ymm0_0;
or ymm2_1@uint64 ymm2_1 ymm0_1;
or ymm2_2@uint64 ymm2_2 ymm0_2;
or ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x3f,%ymm13,%ymm0                       #! PC = 0x555555579c37 *)
shr ymm0_0 ymm13_0 0x3f@uint64;
shr ymm0_1 ymm13_1 0x3f@uint64;
shr ymm0_2 ymm13_2 0x3f@uint64;
shr ymm0_3 ymm13_3 0x3f@uint64;
(* vpor   %ymm3,%ymm0,%ymm0                        #! PC = 0x555555579c3d *)
or ymm0_0@uint64 ymm0_0 ymm3_0;
or ymm0_1@uint64 ymm0_1 ymm3_1;
or ymm0_2@uint64 ymm0_2 ymm3_2;
or ymm0_3@uint64 ymm0_3 ymm3_3;
(* vpsrlq $0x3f,%ymm11,%ymm3                       #! PC = 0x555555579c41 *)
shr ymm3_0 ymm11_0 0x3f@uint64;
shr ymm3_1 ymm11_1 0x3f@uint64;
shr ymm3_2 ymm11_2 0x3f@uint64;
shr ymm3_3 ymm11_3 0x3f@uint64;
(* vpxor  %ymm10,%ymm2,%ymm2                       #! PC = 0x555555579c47 *)
xor ymm2_0@uint64 ymm2_0 ymm10_0;
xor ymm2_1@uint64 ymm2_1 ymm10_1;
xor ymm2_2@uint64 ymm2_2 ymm10_2;
xor ymm2_3@uint64 ymm2_3 ymm10_3;
(* vpor   %ymm15,%ymm3,%ymm3                       #! PC = 0x555555579c4c *)
or ymm3_0@uint64 ymm3_0 ymm15_0;
or ymm3_1@uint64 ymm3_1 ymm15_1;
or ymm3_2@uint64 ymm3_2 ymm15_2;
or ymm3_3@uint64 ymm3_3 ymm15_3;
(* vpxor  %ymm9,%ymm0,%ymm0                        #! PC = 0x555555579c51 *)
xor ymm0_0@uint64 ymm0_0 ymm9_0;
xor ymm0_1@uint64 ymm0_1 ymm9_1;
xor ymm0_2@uint64 ymm0_2 ymm9_2;
xor ymm0_3@uint64 ymm0_3 ymm9_3;
(* vpxor  %ymm2,%ymm4,%ymm4                        #! PC = 0x555555579c56 *)
xor ymm4_0@uint64 ymm4_0 ymm2_0;
xor ymm4_1@uint64 ymm4_1 ymm2_1;
xor ymm4_2@uint64 ymm4_2 ymm2_2;
xor ymm4_3@uint64 ymm4_3 ymm2_3;
(* vpxor  %ymm14,%ymm3,%ymm14                      #! PC = 0x555555579c5a *)
xor ymm14_0@uint64 ymm3_0 ymm14_0;
xor ymm14_1@uint64 ymm3_1 ymm14_1;
xor ymm14_2@uint64 ymm3_2 ymm14_2;
xor ymm14_3@uint64 ymm3_3 ymm14_3;
(* vpsrlq $0x3f,%ymm10,%ymm3                       #! PC = 0x555555579c5f *)
shr ymm3_0 ymm10_0 0x3f@uint64;
shr ymm3_1 ymm10_1 0x3f@uint64;
shr ymm3_2 ymm10_2 0x3f@uint64;
shr ymm3_3 ymm10_3 0x3f@uint64;
(* vpxor  %ymm0,%ymm7,%ymm7                        #! PC = 0x555555579c65 *)
xor ymm7_0@uint64 ymm7_0 ymm0_0;
xor ymm7_1@uint64 ymm7_1 ymm0_1;
xor ymm7_2@uint64 ymm7_2 ymm0_2;
xor ymm7_3@uint64 ymm7_3 ymm0_3;
(* vpsllq $0x1,%ymm10,%ymm10                       #! PC = 0x555555579c69 *)
shl ymm10_0 ymm10_0 0x1@uint64;
shl ymm10_1 ymm10_1 0x1@uint64;
shl ymm10_2 ymm10_2 0x1@uint64;
shl ymm10_3 ymm10_3 0x1@uint64;
(* vpxor  %ymm14,%ymm8,%ymm8                       #! PC = 0x555555579c6f *)
xor ymm8_0@uint64 ymm8_0 ymm14_0;
xor ymm8_1@uint64 ymm8_1 ymm14_1;
xor ymm8_2@uint64 ymm8_2 ymm14_2;
xor ymm8_3@uint64 ymm8_3 ymm14_3;
(* vpxor  %ymm0,%ymm5,%ymm5                        #! PC = 0x555555579c74 *)
xor ymm5_0@uint64 ymm5_0 ymm0_0;
xor ymm5_1@uint64 ymm5_1 ymm0_1;
xor ymm5_2@uint64 ymm5_2 ymm0_2;
xor ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpor   %ymm10,%ymm3,%ymm10                      #! PC = 0x555555579c78 *)
or ymm10_0@uint64 ymm3_0 ymm10_0;
or ymm10_1@uint64 ymm3_1 ymm10_1;
or ymm10_2@uint64 ymm3_2 ymm10_2;
or ymm10_3@uint64 ymm3_3 ymm10_3;
(* vpsrlq $0x3f,%ymm9,%ymm3                        #! PC = 0x555555579c7d *)
shr ymm3_0 ymm9_0 0x3f@uint64;
shr ymm3_1 ymm9_1 0x3f@uint64;
shr ymm3_2 ymm9_2 0x3f@uint64;
shr ymm3_3 ymm9_3 0x3f@uint64;
(* vpxor  %ymm14,%ymm12,%ymm12                     #! PC = 0x555555579c83 *)
xor ymm12_0@uint64 ymm12_0 ymm14_0;
xor ymm12_1@uint64 ymm12_1 ymm14_1;
xor ymm12_2@uint64 ymm12_2 ymm14_2;
xor ymm12_3@uint64 ymm12_3 ymm14_3;
(* vpsllq $0x1,%ymm9,%ymm9                         #! PC = 0x555555579c88 *)
shl ymm9_0 ymm9_0 0x1@uint64;
shl ymm9_1 ymm9_1 0x1@uint64;
shl ymm9_2 ymm9_2 0x1@uint64;
shl ymm9_3 ymm9_3 0x1@uint64;
(* vpxor  %ymm13,%ymm10,%ymm13                     #! PC = 0x555555579c8e *)
xor ymm13_0@uint64 ymm10_0 ymm13_0;
xor ymm13_1@uint64 ymm10_1 ymm13_1;
xor ymm13_2@uint64 ymm10_2 ymm13_2;
xor ymm13_3@uint64 ymm10_3 ymm13_3;
(* vpxor  -0x90(%rbp),%ymm2,%ymm10                 #! EA = L0x7fffffffbec0; Value = 0x309632a97ced9ce8; PC = 0x555555579c93 *)
xor ymm10_0@uint64 ymm2_0 L0x7fffffffbec0;
xor ymm10_1@uint64 ymm2_1 L0x7fffffffbec8;
xor ymm10_2@uint64 ymm2_2 L0x7fffffffbed0;
xor ymm10_3@uint64 ymm2_3 L0x7fffffffbed8;
(* vpor   %ymm9,%ymm3,%ymm9                        #! PC = 0x555555579c9b *)
or ymm9_0@uint64 ymm3_0 ymm9_0;
or ymm9_1@uint64 ymm3_1 ymm9_1;
or ymm9_2@uint64 ymm3_2 ymm9_2;
or ymm9_3@uint64 ymm3_3 ymm9_3;
(* vpsrlq $0x14,%ymm7,%ymm3                        #! PC = 0x555555579ca0 *)
shr ymm3_0 ymm7_0 0x14@uint64;
shr ymm3_1 ymm7_1 0x14@uint64;
shr ymm3_2 ymm7_2 0x14@uint64;
shr ymm3_3 ymm7_3 0x14@uint64;
(* vpxor  %ymm13,%ymm6,%ymm6                       #! PC = 0x555555579ca5 *)
xor ymm6_0@uint64 ymm6_0 ymm13_0;
xor ymm6_1@uint64 ymm6_1 ymm13_1;
xor ymm6_2@uint64 ymm6_2 ymm13_2;
xor ymm6_3@uint64 ymm6_3 ymm13_3;
(* vpsllq $0x2c,%ymm7,%ymm7                        #! PC = 0x555555579caa *)
shl ymm7_0 ymm7_0 0x2c@uint64;
shl ymm7_1 ymm7_1 0x2c@uint64;
shl ymm7_2 ymm7_2 0x2c@uint64;
shl ymm7_3 ymm7_3 0x2c@uint64;
(* vpxor  %ymm11,%ymm9,%ymm11                      #! PC = 0x555555579caf *)
xor ymm11_0@uint64 ymm9_0 ymm11_0;
xor ymm11_1@uint64 ymm9_1 ymm11_1;
xor ymm11_2@uint64 ymm9_2 ymm11_2;
xor ymm11_3@uint64 ymm9_3 ymm11_3;
(* vmovq  %r12,%xmm9                               #! PC = 0x555555579cb4 *)
mov xmm9_0 r12;
mov xmm9_1 0@uint64;
(* vpor   %ymm7,%ymm3,%ymm7                        #! PC = 0x555555579cb9 *)
or ymm7_0@uint64 ymm3_0 ymm7_0;
or ymm7_1@uint64 ymm3_1 ymm7_1;
or ymm7_2@uint64 ymm3_2 ymm7_2;
or ymm7_3@uint64 ymm3_3 ymm7_3;
(* vpsrlq $0x15,%ymm8,%ymm3                        #! PC = 0x555555579cbd *)
shr ymm3_0 ymm8_0 0x15@uint64;
shr ymm3_1 ymm8_1 0x15@uint64;
shr ymm3_2 ymm8_2 0x15@uint64;
shr ymm3_3 ymm8_3 0x15@uint64;
(* vpbroadcastq %xmm9,%ymm9                        #! PC = 0x555555579cc3 *)
mov ymm9_0 xmm9_0;
mov ymm9_1 xmm9_0;
mov ymm9_2 xmm9_0;
mov ymm9_3 xmm9_0;
(* vpsllq $0x2b,%ymm8,%ymm8                        #! PC = 0x555555579cc8 *)
shl ymm8_0 ymm8_0 0x2b@uint64;
shl ymm8_1 ymm8_1 0x2b@uint64;
shl ymm8_2 ymm8_2 0x2b@uint64;
shl ymm8_3 ymm8_3 0x2b@uint64;
(* vpxor  %ymm11,%ymm1,%ymm1                       #! PC = 0x555555579cce *)
xor ymm1_0@uint64 ymm1_0 ymm11_0;
xor ymm1_1@uint64 ymm1_1 ymm11_1;
xor ymm1_2@uint64 ymm1_2 ymm11_2;
xor ymm1_3@uint64 ymm1_3 ymm11_3;
(* vpor   %ymm8,%ymm3,%ymm8                        #! PC = 0x555555579cd3 *)
or ymm8_0@uint64 ymm3_0 ymm8_0;
or ymm8_1@uint64 ymm3_1 ymm8_1;
or ymm8_2@uint64 ymm3_2 ymm8_2;
or ymm8_3@uint64 ymm3_3 ymm8_3;
(* vpandn %ymm8,%ymm7,%ymm3                        #! PC = 0x555555579cd8 *)
not ymm7_0n@uint64 ymm7_0;
and ymm3_0@uint64 ymm7_0n ymm8_0;
not ymm7_1n@uint64 ymm7_1;
and ymm3_1@uint64 ymm7_1n ymm8_1;
not ymm7_2n@uint64 ymm7_2;
and ymm3_2@uint64 ymm7_2n ymm8_2;
not ymm7_3n@uint64 ymm7_3;
and ymm3_3@uint64 ymm7_3n ymm8_3;
(* vpxor  %ymm9,%ymm3,%ymm9                        #! PC = 0x555555579cdd *)
xor ymm9_0@uint64 ymm3_0 ymm9_0;
xor ymm9_1@uint64 ymm3_1 ymm9_1;
xor ymm9_2@uint64 ymm3_2 ymm9_2;
xor ymm9_3@uint64 ymm3_3 ymm9_3;
(* vpsrlq $0x2b,%ymm6,%ymm3                        #! PC = 0x555555579ce2 *)
shr ymm3_0 ymm6_0 0x2b@uint64;
shr ymm3_1 ymm6_1 0x2b@uint64;
shr ymm3_2 ymm6_2 0x2b@uint64;
shr ymm3_3 ymm6_3 0x2b@uint64;
(* vpsllq $0x15,%ymm6,%ymm6                        #! PC = 0x555555579ce7 *)
shl ymm6_0 ymm6_0 0x15@uint64;
shl ymm6_1 ymm6_1 0x15@uint64;
shl ymm6_2 ymm6_2 0x15@uint64;
shl ymm6_3 ymm6_3 0x15@uint64;
(* vpxor  %ymm10,%ymm9,%ymm9                       #! PC = 0x555555579cec *)
xor ymm9_0@uint64 ymm9_0 ymm10_0;
xor ymm9_1@uint64 ymm9_1 ymm10_1;
xor ymm9_2@uint64 ymm9_2 ymm10_2;
xor ymm9_3@uint64 ymm9_3 ymm10_3;
(* vpor   %ymm6,%ymm3,%ymm6                        #! PC = 0x555555579cf1 *)
or ymm6_0@uint64 ymm3_0 ymm6_0;
or ymm6_1@uint64 ymm3_1 ymm6_1;
or ymm6_2@uint64 ymm3_2 ymm6_2;
or ymm6_3@uint64 ymm3_3 ymm6_3;
(* vmovdqa %ymm9,-0x90(%rbp)                       #! EA = L0x7fffffffbec0; PC = 0x555555579cf5 *)
mov L0x7fffffffbec0 ymm9_0;
mov L0x7fffffffbec8 ymm9_1;
mov L0x7fffffffbed0 ymm9_2;
mov L0x7fffffffbed8 ymm9_3;
(* vpandn %ymm6,%ymm8,%ymm3                        #! PC = 0x555555579cfd *)
not ymm8_0n@uint64 ymm8_0;
and ymm3_0@uint64 ymm8_0n ymm6_0;
not ymm8_1n@uint64 ymm8_1;
and ymm3_1@uint64 ymm8_1n ymm6_1;
not ymm8_2n@uint64 ymm8_2;
and ymm3_2@uint64 ymm8_2n ymm6_2;
not ymm8_3n@uint64 ymm8_3;
and ymm3_3@uint64 ymm8_3n ymm6_3;
(* vpxor  %ymm7,%ymm3,%ymm9                        #! PC = 0x555555579d01 *)
xor ymm9_0@uint64 ymm3_0 ymm7_0;
xor ymm9_1@uint64 ymm3_1 ymm7_1;
xor ymm9_2@uint64 ymm3_2 ymm7_2;
xor ymm9_3@uint64 ymm3_3 ymm7_3;
(* vpsrlq $0x32,%ymm1,%ymm3                        #! PC = 0x555555579d05 *)
shr ymm3_0 ymm1_0 0x32@uint64;
shr ymm3_1 ymm1_1 0x32@uint64;
shr ymm3_2 ymm1_2 0x32@uint64;
shr ymm3_3 ymm1_3 0x32@uint64;
(* vpsllq $0xe,%ymm1,%ymm1                         #! PC = 0x555555579d0a *)
shl ymm1_0 ymm1_0 0xe@uint64;
shl ymm1_1 ymm1_1 0xe@uint64;
shl ymm1_2 ymm1_2 0xe@uint64;
shl ymm1_3 ymm1_3 0xe@uint64;
(* vmovdqa %ymm9,-0x230(%rbp)                      #! EA = L0x7fffffffbd20; PC = 0x555555579d0f *)
mov L0x7fffffffbd20 ymm9_0;
mov L0x7fffffffbd28 ymm9_1;
mov L0x7fffffffbd30 ymm9_2;
mov L0x7fffffffbd38 ymm9_3;
(* vpor   %ymm1,%ymm3,%ymm1                        #! PC = 0x555555579d17 *)
or ymm1_0@uint64 ymm3_0 ymm1_0;
or ymm1_1@uint64 ymm3_1 ymm1_1;
or ymm1_2@uint64 ymm3_2 ymm1_2;
or ymm1_3@uint64 ymm3_3 ymm1_3;
(* vpandn %ymm1,%ymm6,%ymm3                        #! PC = 0x555555579d1b *)
not ymm6_0n@uint64 ymm6_0;
and ymm3_0@uint64 ymm6_0n ymm1_0;
not ymm6_1n@uint64 ymm6_1;
and ymm3_1@uint64 ymm6_1n ymm1_1;
not ymm6_2n@uint64 ymm6_2;
and ymm3_2@uint64 ymm6_2n ymm1_2;
not ymm6_3n@uint64 ymm6_3;
and ymm3_3@uint64 ymm6_3n ymm1_3;
(* vpxor  %ymm8,%ymm3,%ymm8                        #! PC = 0x555555579d1f *)
xor ymm8_0@uint64 ymm3_0 ymm8_0;
xor ymm8_1@uint64 ymm3_1 ymm8_1;
xor ymm8_2@uint64 ymm3_2 ymm8_2;
xor ymm8_3@uint64 ymm3_3 ymm8_3;
(* vpandn %ymm10,%ymm1,%ymm3                       #! PC = 0x555555579d24 *)
not ymm1_0n@uint64 ymm1_0;
and ymm3_0@uint64 ymm1_0n ymm10_0;
not ymm1_1n@uint64 ymm1_1;
and ymm3_1@uint64 ymm1_1n ymm10_1;
not ymm1_2n@uint64 ymm1_2;
and ymm3_2@uint64 ymm1_2n ymm10_2;
not ymm1_3n@uint64 ymm1_3;
and ymm3_3@uint64 ymm1_3n ymm10_3;
(* vpandn %ymm7,%ymm10,%ymm10                      #! PC = 0x555555579d29 *)
not ymm10_0n@uint64 ymm10_0;
and ymm10_0@uint64 ymm10_0n ymm7_0;
not ymm10_1n@uint64 ymm10_1;
and ymm10_1@uint64 ymm10_1n ymm7_1;
not ymm10_2n@uint64 ymm10_2;
and ymm10_2@uint64 ymm10_2n ymm7_2;
not ymm10_3n@uint64 ymm10_3;
and ymm10_3@uint64 ymm10_3n ymm7_3;
(* vpxor  %ymm1,%ymm10,%ymm7                       #! PC = 0x555555579d2d *)
xor ymm7_0@uint64 ymm10_0 ymm1_0;
xor ymm7_1@uint64 ymm10_1 ymm1_1;
xor ymm7_2@uint64 ymm10_2 ymm1_2;
xor ymm7_3@uint64 ymm10_3 ymm1_3;
(* vpxor  -0xf0(%rbp),%ymm13,%ymm1                 #! EA = L0x7fffffffbe60; Value = 0x781cb76c1b390af6; PC = 0x555555579d31 *)
xor ymm1_0@uint64 ymm13_0 L0x7fffffffbe60;
xor ymm1_1@uint64 ymm13_1 L0x7fffffffbe68;
xor ymm1_2@uint64 ymm13_2 L0x7fffffffbe70;
xor ymm1_3@uint64 ymm13_3 L0x7fffffffbe78;
(* vmovdqa %ymm8,-0x250(%rbp)                      #! EA = L0x7fffffffbd00; PC = 0x555555579d39 *)
mov L0x7fffffffbd00 ymm8_0;
mov L0x7fffffffbd08 ymm8_1;
mov L0x7fffffffbd10 ymm8_2;
mov L0x7fffffffbd18 ymm8_3;
(* vpxor  %ymm6,%ymm3,%ymm8                        #! PC = 0x555555579d41 *)
xor ymm8_0@uint64 ymm3_0 ymm6_0;
xor ymm8_1@uint64 ymm3_1 ymm6_1;
xor ymm8_2@uint64 ymm3_2 ymm6_2;
xor ymm8_3@uint64 ymm3_3 ymm6_3;
(* vpxor  -0x190(%rbp),%ymm11,%ymm3                #! EA = L0x7fffffffbdc0; Value = 0x93350e8d9591bc9e; PC = 0x555555579d45 *)
xor ymm3_0@uint64 ymm11_0 L0x7fffffffbdc0;
xor ymm3_1@uint64 ymm11_1 L0x7fffffffbdc8;
xor ymm3_2@uint64 ymm11_2 L0x7fffffffbdd0;
xor ymm3_3@uint64 ymm11_3 L0x7fffffffbdd8;
(* vmovdqa %ymm8,-0x270(%rbp)                      #! EA = L0x7fffffffbce0; PC = 0x555555579d4d *)
mov L0x7fffffffbce0 ymm8_0;
mov L0x7fffffffbce8 ymm8_1;
mov L0x7fffffffbcf0 ymm8_2;
mov L0x7fffffffbcf8 ymm8_3;
(* vmovdqa %ymm7,-0x290(%rbp)                      #! EA = L0x7fffffffbcc0; PC = 0x555555579d55 *)
mov L0x7fffffffbcc0 ymm7_0;
mov L0x7fffffffbcc8 ymm7_1;
mov L0x7fffffffbcd0 ymm7_2;
mov L0x7fffffffbcd8 ymm7_3;
(* vpxor  -0x2b0(%rbp),%ymm2,%ymm6                 #! EA = L0x7fffffffbca0; Value = 0x4735c6634ea08aa2; PC = 0x555555579d5d *)
xor ymm6_0@uint64 ymm2_0 L0x7fffffffbca0;
xor ymm6_1@uint64 ymm2_1 L0x7fffffffbca8;
xor ymm6_2@uint64 ymm2_2 L0x7fffffffbcb0;
xor ymm6_3@uint64 ymm2_3 L0x7fffffffbcb8;
(* vpsrlq $0x24,%ymm1,%ymm7                        #! PC = 0x555555579d65 *)
shr ymm7_0 ymm1_0 0x24@uint64;
shr ymm7_1 ymm1_1 0x24@uint64;
shr ymm7_2 ymm1_2 0x24@uint64;
shr ymm7_3 ymm1_3 0x24@uint64;
(* vpsllq $0x1c,%ymm1,%ymm1                        #! PC = 0x555555579d6a *)
shl ymm1_0 ymm1_0 0x1c@uint64;
shl ymm1_1 ymm1_1 0x1c@uint64;
shl ymm1_2 ymm1_2 0x1c@uint64;
shl ymm1_3 ymm1_3 0x1c@uint64;
(* vpor   %ymm1,%ymm7,%ymm7                        #! PC = 0x555555579d6f *)
or ymm7_0@uint64 ymm7_0 ymm1_0;
or ymm7_1@uint64 ymm7_1 ymm1_1;
or ymm7_2@uint64 ymm7_2 ymm1_2;
or ymm7_3@uint64 ymm7_3 ymm1_3;
(* vpsrlq $0x2c,%ymm3,%ymm1                        #! PC = 0x555555579d73 *)
shr ymm1_0 ymm3_0 0x2c@uint64;
shr ymm1_1 ymm3_1 0x2c@uint64;
shr ymm1_2 ymm3_2 0x2c@uint64;
shr ymm1_3 ymm3_3 0x2c@uint64;
(* vpsllq $0x14,%ymm3,%ymm3                        #! PC = 0x555555579d78 *)
shl ymm3_0 ymm3_0 0x14@uint64;
shl ymm3_1 ymm3_1 0x14@uint64;
shl ymm3_2 ymm3_2 0x14@uint64;
shl ymm3_3 ymm3_3 0x14@uint64;
(* vpor   %ymm3,%ymm1,%ymm1                        #! PC = 0x555555579d7d *)
or ymm1_0@uint64 ymm1_0 ymm3_0;
or ymm1_1@uint64 ymm1_1 ymm3_1;
or ymm1_2@uint64 ymm1_2 ymm3_2;
or ymm1_3@uint64 ymm1_3 ymm3_3;
(* vpsrlq $0x3d,%ymm6,%ymm3                        #! PC = 0x555555579d81 *)
shr ymm3_0 ymm6_0 0x3d@uint64;
shr ymm3_1 ymm6_1 0x3d@uint64;
shr ymm3_2 ymm6_2 0x3d@uint64;
shr ymm3_3 ymm6_3 0x3d@uint64;
(* vpsllq $0x3,%ymm6,%ymm6                         #! PC = 0x555555579d86 *)
shl ymm6_0 ymm6_0 0x3@uint64;
shl ymm6_1 ymm6_1 0x3@uint64;
shl ymm6_2 ymm6_2 0x3@uint64;
shl ymm6_3 ymm6_3 0x3@uint64;
(* vpor   %ymm6,%ymm3,%ymm3                        #! PC = 0x555555579d8b *)
or ymm3_0@uint64 ymm3_0 ymm6_0;
or ymm3_1@uint64 ymm3_1 ymm6_1;
or ymm3_2@uint64 ymm3_2 ymm6_2;
or ymm3_3@uint64 ymm3_3 ymm6_3;
(* vpandn %ymm3,%ymm1,%ymm6                        #! PC = 0x555555579d8f *)
not ymm1_0n@uint64 ymm1_0;
and ymm6_0@uint64 ymm1_0n ymm3_0;
not ymm1_1n@uint64 ymm1_1;
and ymm6_1@uint64 ymm1_1n ymm3_1;
not ymm1_2n@uint64 ymm1_2;
and ymm6_2@uint64 ymm1_2n ymm3_2;
not ymm1_3n@uint64 ymm1_3;
and ymm6_3@uint64 ymm1_3n ymm3_3;
(* vpxor  %ymm7,%ymm6,%ymm15                       #! PC = 0x555555579d93 *)
xor ymm15_0@uint64 ymm6_0 ymm7_0;
xor ymm15_1@uint64 ymm6_1 ymm7_1;
xor ymm15_2@uint64 ymm6_2 ymm7_2;
xor ymm15_3@uint64 ymm6_3 ymm7_3;
(* vmovdqa %ymm15,-0xf0(%rbp)                      #! EA = L0x7fffffffbe60; PC = 0x555555579d97 *)
mov L0x7fffffffbe60 ymm15_0;
mov L0x7fffffffbe68 ymm15_1;
mov L0x7fffffffbe70 ymm15_2;
mov L0x7fffffffbe78 ymm15_3;
(* vpsrlq $0x13,%ymm5,%ymm15                       #! PC = 0x555555579d9f *)
shr ymm15_0 ymm5_0 0x13@uint64;
shr ymm15_1 ymm5_1 0x13@uint64;
shr ymm15_2 ymm5_2 0x13@uint64;
shr ymm15_3 ymm5_3 0x13@uint64;
(* vpsllq $0x2d,%ymm5,%ymm5                        #! PC = 0x555555579da4 *)
shl ymm5_0 ymm5_0 0x2d@uint64;
shl ymm5_1 ymm5_1 0x2d@uint64;
shl ymm5_2 ymm5_2 0x2d@uint64;
shl ymm5_3 ymm5_3 0x2d@uint64;
(* vpor   %ymm5,%ymm15,%ymm15                      #! PC = 0x555555579da9 *)
or ymm15_0@uint64 ymm15_0 ymm5_0;
or ymm15_1@uint64 ymm15_1 ymm5_1;
or ymm15_2@uint64 ymm15_2 ymm5_2;
or ymm15_3@uint64 ymm15_3 ymm5_3;
(* vpsrlq $0x3,%ymm12,%ymm5                        #! PC = 0x555555579dad *)
shr ymm5_0 ymm12_0 0x3@uint64;
shr ymm5_1 ymm12_1 0x3@uint64;
shr ymm5_2 ymm12_2 0x3@uint64;
shr ymm5_3 ymm12_3 0x3@uint64;
(* vpsllq $0x3d,%ymm12,%ymm12                      #! PC = 0x555555579db3 *)
shl ymm12_0 ymm12_0 0x3d@uint64;
shl ymm12_1 ymm12_1 0x3d@uint64;
shl ymm12_2 ymm12_2 0x3d@uint64;
shl ymm12_3 ymm12_3 0x3d@uint64;
(* vpandn %ymm15,%ymm3,%ymm8                       #! PC = 0x555555579db9 *)
not ymm3_0n@uint64 ymm3_0;
and ymm8_0@uint64 ymm3_0n ymm15_0;
not ymm3_1n@uint64 ymm3_1;
and ymm8_1@uint64 ymm3_1n ymm15_1;
not ymm3_2n@uint64 ymm3_2;
and ymm8_2@uint64 ymm3_2n ymm15_2;
not ymm3_3n@uint64 ymm3_3;
and ymm8_3@uint64 ymm3_3n ymm15_3;
(* vpor   %ymm12,%ymm5,%ymm12                      #! PC = 0x555555579dbe *)
or ymm12_0@uint64 ymm5_0 ymm12_0;
or ymm12_1@uint64 ymm5_1 ymm12_1;
or ymm12_2@uint64 ymm5_2 ymm12_2;
or ymm12_3@uint64 ymm5_3 ymm12_3;
(* vpxor  %ymm1,%ymm8,%ymm8                        #! PC = 0x555555579dc3 *)
xor ymm8_0@uint64 ymm8_0 ymm1_0;
xor ymm8_1@uint64 ymm8_1 ymm1_1;
xor ymm8_2@uint64 ymm8_2 ymm1_2;
xor ymm8_3@uint64 ymm8_3 ymm1_3;
(* vpandn %ymm12,%ymm15,%ymm5                      #! PC = 0x555555579dc7 *)
not ymm15_0n@uint64 ymm15_0;
and ymm5_0@uint64 ymm15_0n ymm12_0;
not ymm15_1n@uint64 ymm15_1;
and ymm5_1@uint64 ymm15_1n ymm12_1;
not ymm15_2n@uint64 ymm15_2;
and ymm5_2@uint64 ymm15_2n ymm12_2;
not ymm15_3n@uint64 ymm15_3;
and ymm5_3@uint64 ymm15_3n ymm12_3;
(* vpxor  %ymm3,%ymm5,%ymm5                        #! PC = 0x555555579dcc *)
xor ymm5_0@uint64 ymm5_0 ymm3_0;
xor ymm5_1@uint64 ymm5_1 ymm3_1;
xor ymm5_2@uint64 ymm5_2 ymm3_2;
xor ymm5_3@uint64 ymm5_3 ymm3_3;
(* vpandn %ymm7,%ymm12,%ymm3                       #! PC = 0x555555579dd0 *)
not ymm12_0n@uint64 ymm12_0;
and ymm3_0@uint64 ymm12_0n ymm7_0;
not ymm12_1n@uint64 ymm12_1;
and ymm3_1@uint64 ymm12_1n ymm7_1;
not ymm12_2n@uint64 ymm12_2;
and ymm3_2@uint64 ymm12_2n ymm7_2;
not ymm12_3n@uint64 ymm12_3;
and ymm3_3@uint64 ymm12_3n ymm7_3;
(* vpandn %ymm1,%ymm7,%ymm7                        #! PC = 0x555555579dd4 *)
not ymm7_0n@uint64 ymm7_0;
and ymm7_0@uint64 ymm7_0n ymm1_0;
not ymm7_1n@uint64 ymm7_1;
and ymm7_1@uint64 ymm7_1n ymm1_1;
not ymm7_2n@uint64 ymm7_2;
and ymm7_2@uint64 ymm7_2n ymm1_2;
not ymm7_3n@uint64 ymm7_3;
and ymm7_3@uint64 ymm7_3n ymm1_3;
(* vpxor  %ymm15,%ymm3,%ymm15                      #! PC = 0x555555579dd8 *)
xor ymm15_0@uint64 ymm3_0 ymm15_0;
xor ymm15_1@uint64 ymm3_1 ymm15_1;
xor ymm15_2@uint64 ymm3_2 ymm15_2;
xor ymm15_3@uint64 ymm3_3 ymm15_3;
(* vpxor  -0xb0(%rbp),%ymm0,%ymm3                  #! EA = L0x7fffffffbea0; Value = 0x96cce37672278288; PC = 0x555555579ddd *)
xor ymm3_0@uint64 ymm0_0 L0x7fffffffbea0;
xor ymm3_1@uint64 ymm0_1 L0x7fffffffbea8;
xor ymm3_2@uint64 ymm0_2 L0x7fffffffbeb0;
xor ymm3_3@uint64 ymm0_3 L0x7fffffffbeb8;
(* vmovdqa %ymm5,-0x190(%rbp)                      #! EA = L0x7fffffffbdc0; PC = 0x555555579de5 *)
mov L0x7fffffffbdc0 ymm5_0;
mov L0x7fffffffbdc8 ymm5_1;
mov L0x7fffffffbdd0 ymm5_2;
mov L0x7fffffffbdd8 ymm5_3;
(* vpxor  %ymm12,%ymm7,%ymm12                      #! PC = 0x555555579ded *)
xor ymm12_0@uint64 ymm7_0 ymm12_0;
xor ymm12_1@uint64 ymm7_1 ymm12_1;
xor ymm12_2@uint64 ymm7_2 ymm12_2;
xor ymm12_3@uint64 ymm7_3 ymm12_3;
(* vpxor  -0x150(%rbp),%ymm14,%ymm5                #! EA = L0x7fffffffbe00; Value = 0x5c202896127a3201; PC = 0x555555579df2 *)
xor ymm5_0@uint64 ymm14_0 L0x7fffffffbe00;
xor ymm5_1@uint64 ymm14_1 L0x7fffffffbe08;
xor ymm5_2@uint64 ymm14_2 L0x7fffffffbe10;
xor ymm5_3@uint64 ymm14_3 L0x7fffffffbe18;
(* vmovdqa %ymm12,-0x2d0(%rbp)                     #! EA = L0x7fffffffbc80; PC = 0x555555579dfa *)
mov L0x7fffffffbc80 ymm12_0;
mov L0x7fffffffbc88 ymm12_1;
mov L0x7fffffffbc90 ymm12_2;
mov L0x7fffffffbc98 ymm12_3;
(* vpsrlq $0x2e,%ymm4,%ymm7                        #! PC = 0x555555579e02 *)
shr ymm7_0 ymm4_0 0x2e@uint64;
shr ymm7_1 ymm4_1 0x2e@uint64;
shr ymm7_2 ymm4_2 0x2e@uint64;
shr ymm7_3 ymm4_3 0x2e@uint64;
(* vpsrlq $0x3f,%ymm3,%ymm1                        #! PC = 0x555555579e07 *)
shr ymm1_0 ymm3_0 0x3f@uint64;
shr ymm1_1 ymm3_1 0x3f@uint64;
shr ymm1_2 ymm3_2 0x3f@uint64;
shr ymm1_3 ymm3_3 0x3f@uint64;
(* vpsllq $0x1,%ymm3,%ymm3                         #! PC = 0x555555579e0c *)
shl ymm3_0 ymm3_0 0x1@uint64;
shl ymm3_1 ymm3_1 0x1@uint64;
shl ymm3_2 ymm3_2 0x1@uint64;
shl ymm3_3 ymm3_3 0x1@uint64;
(* vmovdqa %ymm15,-0x2b0(%rbp)                     #! EA = L0x7fffffffbca0; PC = 0x555555579e11 *)
mov L0x7fffffffbca0 ymm15_0;
mov L0x7fffffffbca8 ymm15_1;
mov L0x7fffffffbcb0 ymm15_2;
mov L0x7fffffffbcb8 ymm15_3;
(* vpor   %ymm3,%ymm1,%ymm1                        #! PC = 0x555555579e19 *)
or ymm1_0@uint64 ymm1_0 ymm3_0;
or ymm1_1@uint64 ymm1_1 ymm3_1;
or ymm1_2@uint64 ymm1_2 ymm3_2;
or ymm1_3@uint64 ymm1_3 ymm3_3;
(* vpsrlq $0x3a,%ymm5,%ymm3                        #! PC = 0x555555579e1d *)
shr ymm3_0 ymm5_0 0x3a@uint64;
shr ymm3_1 ymm5_1 0x3a@uint64;
shr ymm3_2 ymm5_2 0x3a@uint64;
shr ymm3_3 ymm5_3 0x3a@uint64;
(* vpsllq $0x6,%ymm5,%ymm5                         #! PC = 0x555555579e22 *)
shl ymm5_0 ymm5_0 0x6@uint64;
shl ymm5_1 ymm5_1 0x6@uint64;
shl ymm5_2 ymm5_2 0x6@uint64;
shl ymm5_3 ymm5_3 0x6@uint64;
(* vpsllq $0x12,%ymm4,%ymm4                        #! PC = 0x555555579e27 *)
shl ymm4_0 ymm4_0 0x12@uint64;
shl ymm4_1 ymm4_1 0x12@uint64;
shl ymm4_2 ymm4_2 0x12@uint64;
shl ymm4_3 ymm4_3 0x12@uint64;
(* vpor   %ymm5,%ymm3,%ymm3                        #! PC = 0x555555579e2c *)
or ymm3_0@uint64 ymm3_0 ymm5_0;
or ymm3_1@uint64 ymm3_1 ymm5_1;
or ymm3_2@uint64 ymm3_2 ymm5_2;
or ymm3_3@uint64 ymm3_3 ymm5_3;
(* vpxor  -0x1b0(%rbp),%ymm13,%ymm5                #! EA = L0x7fffffffbda0; Value = 0xe4c33c7972e95e5c; PC = 0x555555579e30 *)
xor ymm5_0@uint64 ymm13_0 L0x7fffffffbda0;
xor ymm5_1@uint64 ymm13_1 L0x7fffffffbda8;
xor ymm5_2@uint64 ymm13_2 L0x7fffffffbdb0;
xor ymm5_3@uint64 ymm13_3 L0x7fffffffbdb8;
(* vpor   %ymm4,%ymm7,%ymm4                        #! PC = 0x555555579e38 *)
or ymm4_0@uint64 ymm7_0 ymm4_0;
or ymm4_1@uint64 ymm7_1 ymm4_1;
or ymm4_2@uint64 ymm7_2 ymm4_2;
or ymm4_3@uint64 ymm7_3 ymm4_3;
(* vpsrlq $0x27,%ymm5,%ymm12                       #! PC = 0x555555579e3c *)
shr ymm12_0 ymm5_0 0x27@uint64;
shr ymm12_1 ymm5_1 0x27@uint64;
shr ymm12_2 ymm5_2 0x27@uint64;
shr ymm12_3 ymm5_3 0x27@uint64;
(* vpsllq $0x19,%ymm5,%ymm6                        #! PC = 0x555555579e41 *)
shl ymm6_0 ymm5_0 0x19@uint64;
shl ymm6_1 ymm5_1 0x19@uint64;
shl ymm6_2 ymm5_2 0x19@uint64;
shl ymm6_3 ymm5_3 0x19@uint64;
(* vpor   %ymm6,%ymm12,%ymm6                       #! PC = 0x555555579e46 *)
or ymm6_0@uint64 ymm12_0 ymm6_0;
or ymm6_1@uint64 ymm12_1 ymm6_1;
or ymm6_2@uint64 ymm12_2 ymm6_2;
or ymm6_3@uint64 ymm12_3 ymm6_3;
(* vpandn %ymm6,%ymm3,%ymm5                        #! PC = 0x555555579e4a *)
not ymm3_0n@uint64 ymm3_0;
and ymm5_0@uint64 ymm3_0n ymm6_0;
not ymm3_1n@uint64 ymm3_1;
and ymm5_1@uint64 ymm3_1n ymm6_1;
not ymm3_2n@uint64 ymm3_2;
and ymm5_2@uint64 ymm3_2n ymm6_2;
not ymm3_3n@uint64 ymm3_3;
and ymm5_3@uint64 ymm3_3n ymm6_3;
(* vpxor  %ymm1,%ymm5,%ymm12                       #! PC = 0x555555579e4e *)
xor ymm12_0@uint64 ymm5_0 ymm1_0;
xor ymm12_1@uint64 ymm5_1 ymm1_1;
xor ymm12_2@uint64 ymm5_2 ymm1_2;
xor ymm12_3@uint64 ymm5_3 ymm1_3;
(* vpxor  -0x210(%rbp),%ymm11,%ymm5                #! EA = L0x7fffffffbd40; Value = 0xe9231008af083f0c; PC = 0x555555579e52 *)
xor ymm5_0@uint64 ymm11_0 L0x7fffffffbd40;
xor ymm5_1@uint64 ymm11_1 L0x7fffffffbd48;
xor ymm5_2@uint64 ymm11_2 L0x7fffffffbd50;
xor ymm5_3@uint64 ymm11_3 L0x7fffffffbd58;
(* vmovdqa %ymm12,-0xb0(%rbp)                      #! EA = L0x7fffffffbea0; PC = 0x555555579e5a *)
mov L0x7fffffffbea0 ymm12_0;
mov L0x7fffffffbea8 ymm12_1;
mov L0x7fffffffbeb0 ymm12_2;
mov L0x7fffffffbeb8 ymm12_3;
(* vpshufb 0x54095(%rip),%ymm5,%ymm5        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x555555579e62 *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm5_0, ymm5_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm5_2, ymm5_3, tmp_2, tmp_3);
mov ymm5_0 tmp_0;
mov ymm5_1 tmp_1;
mov ymm5_2 tmp_2;
mov ymm5_3 tmp_3;
(* vpandn %ymm4,%ymm5,%ymm12                       #! PC = 0x555555579e6b *)
not ymm5_0n@uint64 ymm5_0;
and ymm12_0@uint64 ymm5_0n ymm4_0;
not ymm5_1n@uint64 ymm5_1;
and ymm12_1@uint64 ymm5_1n ymm4_1;
not ymm5_2n@uint64 ymm5_2;
and ymm12_2@uint64 ymm5_2n ymm4_2;
not ymm5_3n@uint64 ymm5_3;
and ymm12_3@uint64 ymm5_3n ymm4_3;
(* vpandn %ymm5,%ymm6,%ymm9                        #! PC = 0x555555579e6f *)
not ymm6_0n@uint64 ymm6_0;
and ymm9_0@uint64 ymm6_0n ymm5_0;
not ymm6_1n@uint64 ymm6_1;
and ymm9_1@uint64 ymm6_1n ymm5_1;
not ymm6_2n@uint64 ymm6_2;
and ymm9_2@uint64 ymm6_2n ymm5_2;
not ymm6_3n@uint64 ymm6_3;
and ymm9_3@uint64 ymm6_3n ymm5_3;
(* vpxor  %ymm6,%ymm12,%ymm12                      #! PC = 0x555555579e73 *)
xor ymm12_0@uint64 ymm12_0 ymm6_0;
xor ymm12_1@uint64 ymm12_1 ymm6_1;
xor ymm12_2@uint64 ymm12_2 ymm6_2;
xor ymm12_3@uint64 ymm12_3 ymm6_3;
(* vpandn %ymm1,%ymm4,%ymm6                        #! PC = 0x555555579e77 *)
not ymm4_0n@uint64 ymm4_0;
and ymm6_0@uint64 ymm4_0n ymm1_0;
not ymm4_1n@uint64 ymm4_1;
and ymm6_1@uint64 ymm4_1n ymm1_1;
not ymm4_2n@uint64 ymm4_2;
and ymm6_2@uint64 ymm4_2n ymm1_2;
not ymm4_3n@uint64 ymm4_3;
and ymm6_3@uint64 ymm4_3n ymm1_3;
(* vpandn %ymm3,%ymm1,%ymm1                        #! PC = 0x555555579e7b *)
not ymm1_0n@uint64 ymm1_0;
and ymm1_0@uint64 ymm1_0n ymm3_0;
not ymm1_1n@uint64 ymm1_1;
and ymm1_1@uint64 ymm1_1n ymm3_1;
not ymm1_2n@uint64 ymm1_2;
and ymm1_2@uint64 ymm1_2n ymm3_2;
not ymm1_3n@uint64 ymm1_3;
and ymm1_3@uint64 ymm1_3n ymm3_3;
(* vpxor  %ymm5,%ymm6,%ymm6                        #! PC = 0x555555579e7f *)
xor ymm6_0@uint64 ymm6_0 ymm5_0;
xor ymm6_1@uint64 ymm6_1 ymm5_1;
xor ymm6_2@uint64 ymm6_2 ymm5_2;
xor ymm6_3@uint64 ymm6_3 ymm5_3;
(* vpxor  %ymm4,%ymm1,%ymm5                        #! PC = 0x555555579e83 *)
xor ymm5_0@uint64 ymm1_0 ymm4_0;
xor ymm5_1@uint64 ymm1_1 ymm4_1;
xor ymm5_2@uint64 ymm1_2 ymm4_2;
xor ymm5_3@uint64 ymm1_3 ymm4_3;
(* vpxor  -0x110(%rbp),%ymm11,%ymm1                #! EA = L0x7fffffffbe40; Value = 0x5efe9079fbd06374; PC = 0x555555579e87 *)
xor ymm1_0@uint64 ymm11_0 L0x7fffffffbe40;
xor ymm1_1@uint64 ymm11_1 L0x7fffffffbe48;
xor ymm1_2@uint64 ymm11_2 L0x7fffffffbe50;
xor ymm1_3@uint64 ymm11_3 L0x7fffffffbe58;
(* vmovdqa %ymm5,-0x210(%rbp)                      #! EA = L0x7fffffffbd40; PC = 0x555555579e8f *)
mov L0x7fffffffbd40 ymm5_0;
mov L0x7fffffffbd48 ymm5_1;
mov L0x7fffffffbd50 ymm5_2;
mov L0x7fffffffbd58 ymm5_3;
(* vpxor  %ymm3,%ymm9,%ymm15                       #! PC = 0x555555579e97 *)
xor ymm15_0@uint64 ymm9_0 ymm3_0;
xor ymm15_1@uint64 ymm9_1 ymm3_1;
xor ymm15_2@uint64 ymm9_2 ymm3_2;
xor ymm15_3@uint64 ymm9_3 ymm3_3;
(* vpxor  -0x1d0(%rbp),%ymm11,%ymm11               #! EA = L0x7fffffffbd80; Value = 0x525049f077812bc0; PC = 0x555555579e9b *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbd80;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbd88;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbd90;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbd98;
(* vpsrlq $0x25,%ymm1,%ymm5                        #! PC = 0x555555579ea3 *)
shr ymm5_0 ymm1_0 0x25@uint64;
shr ymm5_1 ymm1_1 0x25@uint64;
shr ymm5_2 ymm1_2 0x25@uint64;
shr ymm5_3 ymm1_3 0x25@uint64;
(* vpsllq $0x1b,%ymm1,%ymm1                        #! PC = 0x555555579ea8 *)
shl ymm1_0 ymm1_0 0x1b@uint64;
shl ymm1_1 ymm1_1 0x1b@uint64;
shl ymm1_2 ymm1_2 0x1b@uint64;
shl ymm1_3 ymm1_3 0x1b@uint64;
(* vmovdqa %ymm6,-0x1b0(%rbp)                      #! EA = L0x7fffffffbda0; PC = 0x555555579ead *)
mov L0x7fffffffbda0 ymm6_0;
mov L0x7fffffffbda8 ymm6_1;
mov L0x7fffffffbdb0 ymm6_2;
mov L0x7fffffffbdb8 ymm6_3;
(* vpor   %ymm1,%ymm5,%ymm5                        #! PC = 0x555555579eb5 *)
or ymm5_0@uint64 ymm5_0 ymm1_0;
or ymm5_1@uint64 ymm5_1 ymm1_1;
or ymm5_2@uint64 ymm5_2 ymm1_2;
or ymm5_3@uint64 ymm5_3 ymm1_3;
(* vpxor  -0x130(%rbp),%ymm2,%ymm1                 #! EA = L0x7fffffffbe20; Value = 0x8c004295c6f66cea; PC = 0x555555579eb9 *)
xor ymm1_0@uint64 ymm2_0 L0x7fffffffbe20;
xor ymm1_1@uint64 ymm2_1 L0x7fffffffbe28;
xor ymm1_2@uint64 ymm2_2 L0x7fffffffbe30;
xor ymm1_3@uint64 ymm2_3 L0x7fffffffbe38;
(* vmovdqa %ymm15,-0x150(%rbp)                     #! EA = L0x7fffffffbe00; PC = 0x555555579ec1 *)
mov L0x7fffffffbe00 ymm15_0;
mov L0x7fffffffbe08 ymm15_1;
mov L0x7fffffffbe10 ymm15_2;
mov L0x7fffffffbe18 ymm15_3;
(* vpxor  -0x1f0(%rbp),%ymm2,%ymm2                 #! EA = L0x7fffffffbd60; Value = 0x31a6e97ccae2238d; PC = 0x555555579ec9 *)
xor ymm2_0@uint64 ymm2_0 L0x7fffffffbd60;
xor ymm2_1@uint64 ymm2_1 L0x7fffffffbd68;
xor ymm2_2@uint64 ymm2_2 L0x7fffffffbd70;
xor ymm2_3@uint64 ymm2_3 L0x7fffffffbd78;
(* vpsrlq $0x1c,%ymm1,%ymm9                        #! PC = 0x555555579ed1 *)
shr ymm9_0 ymm1_0 0x1c@uint64;
shr ymm9_1 ymm1_1 0x1c@uint64;
shr ymm9_2 ymm1_2 0x1c@uint64;
shr ymm9_3 ymm1_3 0x1c@uint64;
(* vpsllq $0x24,%ymm1,%ymm1                        #! PC = 0x555555579ed6 *)
shl ymm1_0 ymm1_0 0x24@uint64;
shl ymm1_1 ymm1_1 0x24@uint64;
shl ymm1_2 ymm1_2 0x24@uint64;
shl ymm1_3 ymm1_3 0x24@uint64;
(* vpor   %ymm1,%ymm9,%ymm9                        #! PC = 0x555555579edb *)
or ymm9_0@uint64 ymm9_0 ymm1_0;
or ymm9_1@uint64 ymm9_1 ymm1_1;
or ymm9_2@uint64 ymm9_2 ymm1_2;
or ymm9_3@uint64 ymm9_3 ymm1_3;
(* vpxor  -0x70(%rbp),%ymm0,%ymm1                  #! EA = L0x7fffffffbee0; Value = 0x479dabb00fd1751d; PC = 0x555555579edf *)
xor ymm1_0@uint64 ymm0_0 L0x7fffffffbee0;
xor ymm1_1@uint64 ymm0_1 L0x7fffffffbee8;
xor ymm1_2@uint64 ymm0_2 L0x7fffffffbef0;
xor ymm1_3@uint64 ymm0_3 L0x7fffffffbef8;
(* vpsrlq $0x36,%ymm1,%ymm10                       #! PC = 0x555555579ee4 *)
shr ymm10_0 ymm1_0 0x36@uint64;
shr ymm10_1 ymm1_1 0x36@uint64;
shr ymm10_2 ymm1_2 0x36@uint64;
shr ymm10_3 ymm1_3 0x36@uint64;
(* vpsllq $0xa,%ymm1,%ymm1                         #! PC = 0x555555579ee9 *)
shl ymm1_0 ymm1_0 0xa@uint64;
shl ymm1_1 ymm1_1 0xa@uint64;
shl ymm1_2 ymm1_2 0xa@uint64;
shl ymm1_3 ymm1_3 0xa@uint64;
(* vpor   %ymm1,%ymm10,%ymm10                      #! PC = 0x555555579eee *)
or ymm10_0@uint64 ymm10_0 ymm1_0;
or ymm10_1@uint64 ymm10_1 ymm1_1;
or ymm10_2@uint64 ymm10_2 ymm1_2;
or ymm10_3@uint64 ymm10_3 ymm1_3;
(* vpxor  -0x50(%rbp),%ymm14,%ymm1                 #! EA = L0x7fffffffbf00; Value = 0x4a71a7f66478dd99; PC = 0x555555579ef2 *)
xor ymm1_0@uint64 ymm14_0 L0x7fffffffbf00;
xor ymm1_1@uint64 ymm14_1 L0x7fffffffbf08;
xor ymm1_2@uint64 ymm14_2 L0x7fffffffbf10;
xor ymm1_3@uint64 ymm14_3 L0x7fffffffbf18;
(* vpxor  -0xd0(%rbp),%ymm14,%ymm14                #! EA = L0x7fffffffbe80; Value = 0xb8b4102d555463a0; PC = 0x555555579ef7 *)
xor ymm14_0@uint64 ymm14_0 L0x7fffffffbe80;
xor ymm14_1@uint64 ymm14_1 L0x7fffffffbe88;
xor ymm14_2@uint64 ymm14_2 L0x7fffffffbe90;
xor ymm14_3@uint64 ymm14_3 L0x7fffffffbe98;
(* vpandn %ymm10,%ymm9,%ymm3                       #! PC = 0x555555579eff *)
not ymm9_0n@uint64 ymm9_0;
and ymm3_0@uint64 ymm9_0n ymm10_0;
not ymm9_1n@uint64 ymm9_1;
and ymm3_1@uint64 ymm9_1n ymm10_1;
not ymm9_2n@uint64 ymm9_2;
and ymm3_2@uint64 ymm9_2n ymm10_2;
not ymm9_3n@uint64 ymm9_3;
and ymm3_3@uint64 ymm9_3n ymm10_3;
(* vpsrlq $0x31,%ymm1,%ymm7                        #! PC = 0x555555579f04 *)
shr ymm7_0 ymm1_0 0x31@uint64;
shr ymm7_1 ymm1_1 0x31@uint64;
shr ymm7_2 ymm1_2 0x31@uint64;
shr ymm7_3 ymm1_3 0x31@uint64;
(* vpsllq $0xf,%ymm1,%ymm4                         #! PC = 0x555555579f09 *)
shl ymm4_0 ymm1_0 0xf@uint64;
shl ymm4_1 ymm1_1 0xf@uint64;
shl ymm4_2 ymm1_2 0xf@uint64;
shl ymm4_3 ymm1_3 0xf@uint64;
(* vpxor  %ymm5,%ymm3,%ymm6                        #! PC = 0x555555579f0e *)
xor ymm6_0@uint64 ymm3_0 ymm5_0;
xor ymm6_1@uint64 ymm3_1 ymm5_1;
xor ymm6_2@uint64 ymm3_2 ymm5_2;
xor ymm6_3@uint64 ymm3_3 ymm5_3;
(* vpxor  -0x2f0(%rbp),%ymm13,%ymm1                #! EA = L0x7fffffffbc60; Value = 0x73f3b6a47154402f; PC = 0x555555579f12 *)
xor ymm1_0@uint64 ymm13_0 L0x7fffffffbc60;
xor ymm1_1@uint64 ymm13_1 L0x7fffffffbc68;
xor ymm1_2@uint64 ymm13_2 L0x7fffffffbc70;
xor ymm1_3@uint64 ymm13_3 L0x7fffffffbc78;
(* vpor   %ymm4,%ymm7,%ymm4                        #! PC = 0x555555579f1a *)
or ymm4_0@uint64 ymm7_0 ymm4_0;
or ymm4_1@uint64 ymm7_1 ymm4_1;
or ymm4_2@uint64 ymm7_2 ymm4_2;
or ymm4_3@uint64 ymm7_3 ymm4_3;
(* vmovdqa %ymm6,%ymm15                            #! PC = 0x555555579f1e *)
mov ymm15_0 ymm6_0;
mov ymm15_1 ymm6_1;
mov ymm15_2 ymm6_2;
mov ymm15_3 ymm6_3;
(* vpxor  -0x170(%rbp),%ymm13,%ymm13               #! EA = L0x7fffffffbde0; Value = 0xde117f77f405d44b; PC = 0x555555579f22 *)
xor ymm13_0@uint64 ymm13_0 L0x7fffffffbde0;
xor ymm13_1@uint64 ymm13_1 L0x7fffffffbde8;
xor ymm13_2@uint64 ymm13_2 L0x7fffffffbdf0;
xor ymm13_3@uint64 ymm13_3 L0x7fffffffbdf8;
(* vpandn %ymm4,%ymm10,%ymm6                       #! PC = 0x555555579f2a *)
not ymm10_0n@uint64 ymm10_0;
and ymm6_0@uint64 ymm10_0n ymm4_0;
not ymm10_1n@uint64 ymm10_1;
and ymm6_1@uint64 ymm10_1n ymm4_1;
not ymm10_2n@uint64 ymm10_2;
and ymm6_2@uint64 ymm10_2n ymm4_2;
not ymm10_3n@uint64 ymm10_3;
and ymm6_3@uint64 ymm10_3n ymm4_3;
(* vmovdqa %ymm15,-0x110(%rbp)                     #! EA = L0x7fffffffbe40; PC = 0x555555579f2e *)
mov L0x7fffffffbe40 ymm15_0;
mov L0x7fffffffbe48 ymm15_1;
mov L0x7fffffffbe50 ymm15_2;
mov L0x7fffffffbe58 ymm15_3;
(* vpshufb 0x53fa1(%rip),%ymm1,%ymm1        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x555555579f36 *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm1_0, ymm1_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm1_2, ymm1_3, tmp_2, tmp_3);
mov ymm1_0 tmp_0;
mov ymm1_1 tmp_1;
mov ymm1_2 tmp_2;
mov ymm1_3 tmp_3;
(* vpxor  %ymm9,%ymm6,%ymm6                        #! PC = 0x555555579f3f *)
xor ymm6_0@uint64 ymm6_0 ymm9_0;
xor ymm6_1@uint64 ymm6_1 ymm9_1;
xor ymm6_2@uint64 ymm6_2 ymm9_2;
xor ymm6_3@uint64 ymm6_3 ymm9_3;
(* vpandn %ymm1,%ymm4,%ymm3                        #! PC = 0x555555579f44 *)
not ymm4_0n@uint64 ymm4_0;
and ymm3_0@uint64 ymm4_0n ymm1_0;
not ymm4_1n@uint64 ymm4_1;
and ymm3_1@uint64 ymm4_1n ymm1_1;
not ymm4_2n@uint64 ymm4_2;
and ymm3_2@uint64 ymm4_2n ymm1_2;
not ymm4_3n@uint64 ymm4_3;
and ymm3_3@uint64 ymm4_3n ymm1_3;
(* vpxor  %ymm10,%ymm3,%ymm7                       #! PC = 0x555555579f48 *)
xor ymm7_0@uint64 ymm3_0 ymm10_0;
xor ymm7_1@uint64 ymm3_1 ymm10_1;
xor ymm7_2@uint64 ymm3_2 ymm10_2;
xor ymm7_3@uint64 ymm3_3 ymm10_3;
(* vpsrlq $0x2,%ymm14,%ymm3                        #! PC = 0x555555579f4d *)
shr ymm3_0 ymm14_0 0x2@uint64;
shr ymm3_1 ymm14_1 0x2@uint64;
shr ymm3_2 ymm14_2 0x2@uint64;
shr ymm3_3 ymm14_3 0x2@uint64;
(* vmovdqa %ymm7,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x555555579f53 *)
mov L0x7fffffffbf00 ymm7_0;
mov L0x7fffffffbf08 ymm7_1;
mov L0x7fffffffbf10 ymm7_2;
mov L0x7fffffffbf18 ymm7_3;
(* vpandn %ymm5,%ymm1,%ymm7                        #! PC = 0x555555579f58 *)
not ymm1_0n@uint64 ymm1_0;
and ymm7_0@uint64 ymm1_0n ymm5_0;
not ymm1_1n@uint64 ymm1_1;
and ymm7_1@uint64 ymm1_1n ymm5_1;
not ymm1_2n@uint64 ymm1_2;
and ymm7_2@uint64 ymm1_2n ymm5_2;
not ymm1_3n@uint64 ymm1_3;
and ymm7_3@uint64 ymm1_3n ymm5_3;
(* vpandn %ymm9,%ymm5,%ymm5                        #! PC = 0x555555579f5c *)
not ymm5_0n@uint64 ymm5_0;
and ymm5_0@uint64 ymm5_0n ymm9_0;
not ymm5_1n@uint64 ymm5_1;
and ymm5_1@uint64 ymm5_1n ymm9_1;
not ymm5_2n@uint64 ymm5_2;
and ymm5_2@uint64 ymm5_2n ymm9_2;
not ymm5_3n@uint64 ymm5_3;
and ymm5_3@uint64 ymm5_3n ymm9_3;
(* vpxor  %ymm1,%ymm5,%ymm1                        #! PC = 0x555555579f61 *)
xor ymm1_0@uint64 ymm5_0 ymm1_0;
xor ymm1_1@uint64 ymm5_1 ymm1_1;
xor ymm1_2@uint64 ymm5_2 ymm1_2;
xor ymm1_3@uint64 ymm5_3 ymm1_3;
(* vpsllq $0x3e,%ymm14,%ymm14                      #! PC = 0x555555579f65 *)
shl ymm14_0 ymm14_0 0x3e@uint64;
shl ymm14_1 ymm14_1 0x3e@uint64;
shl ymm14_2 ymm14_2 0x3e@uint64;
shl ymm14_3 ymm14_3 0x3e@uint64;
(* vpxor  %ymm4,%ymm7,%ymm7                        #! PC = 0x555555579f6b *)
xor ymm7_0@uint64 ymm7_0 ymm4_0;
xor ymm7_1@uint64 ymm7_1 ymm4_1;
xor ymm7_2@uint64 ymm7_2 ymm4_2;
xor ymm7_3@uint64 ymm7_3 ymm4_3;
(* vmovdqa -0xb0(%rbp),%ymm4                       #! EA = L0x7fffffffbea0; Value = 0x1a235d324d3e8a10; PC = 0x555555579f6f *)
mov ymm4_0 L0x7fffffffbea0;
mov ymm4_1 L0x7fffffffbea8;
mov ymm4_2 L0x7fffffffbeb0;
mov ymm4_3 L0x7fffffffbeb8;
(* vmovdqa %ymm1,-0x70(%rbp)                       #! EA = L0x7fffffffbee0; PC = 0x555555579f77 *)
mov L0x7fffffffbee0 ymm1_0;
mov L0x7fffffffbee8 ymm1_1;
mov L0x7fffffffbef0 ymm1_2;
mov L0x7fffffffbef8 ymm1_3;
(* vpsrlq $0x9,%ymm13,%ymm1                        #! PC = 0x555555579f7c *)
shr ymm1_0 ymm13_0 0x9@uint64;
shr ymm1_1 ymm13_1 0x9@uint64;
shr ymm1_2 ymm13_2 0x9@uint64;
shr ymm1_3 ymm13_3 0x9@uint64;
(* vpsllq $0x37,%ymm13,%ymm13                      #! PC = 0x555555579f82 *)
shl ymm13_0 ymm13_0 0x37@uint64;
shl ymm13_1 ymm13_1 0x37@uint64;
shl ymm13_2 ymm13_2 0x37@uint64;
shl ymm13_3 ymm13_3 0x37@uint64;
(* vpor   %ymm14,%ymm3,%ymm3                       #! PC = 0x555555579f88 *)
or ymm3_0@uint64 ymm3_0 ymm14_0;
or ymm3_1@uint64 ymm3_1 ymm14_1;
or ymm3_2@uint64 ymm3_2 ymm14_2;
or ymm3_3@uint64 ymm3_3 ymm14_3;
(* vpor   %ymm13,%ymm1,%ymm13                      #! PC = 0x555555579f8d *)
or ymm13_0@uint64 ymm1_0 ymm13_0;
or ymm13_1@uint64 ymm1_1 ymm13_1;
or ymm13_2@uint64 ymm1_2 ymm13_2;
or ymm13_3@uint64 ymm1_3 ymm13_3;
(* vpsrlq $0x19,%ymm11,%ymm1                       #! PC = 0x555555579f92 *)
shr ymm1_0 ymm11_0 0x19@uint64;
shr ymm1_1 ymm11_1 0x19@uint64;
shr ymm1_2 ymm11_2 0x19@uint64;
shr ymm1_3 ymm11_3 0x19@uint64;
(* vpsllq $0x27,%ymm11,%ymm11                      #! PC = 0x555555579f98 *)
shl ymm11_0 ymm11_0 0x27@uint64;
shl ymm11_1 ymm11_1 0x27@uint64;
shl ymm11_2 ymm11_2 0x27@uint64;
shl ymm11_3 ymm11_3 0x27@uint64;
(* vpor   %ymm11,%ymm1,%ymm11                      #! PC = 0x555555579f9e *)
or ymm11_0@uint64 ymm1_0 ymm11_0;
or ymm11_1@uint64 ymm1_1 ymm11_1;
or ymm11_2@uint64 ymm1_2 ymm11_2;
or ymm11_3@uint64 ymm1_3 ymm11_3;
(* vpxor  -0xf0(%rbp),%ymm4,%ymm1                  #! EA = L0x7fffffffbe60; Value = 0x0c639e6d4dbb1c52; PC = 0x555555579fa3 *)
xor ymm1_0@uint64 ymm4_0 L0x7fffffffbe60;
xor ymm1_1@uint64 ymm4_1 L0x7fffffffbe68;
xor ymm1_2@uint64 ymm4_2 L0x7fffffffbe70;
xor ymm1_3@uint64 ymm4_3 L0x7fffffffbe78;
(* vpxor  -0x230(%rbp),%ymm8,%ymm4                 #! EA = L0x7fffffffbd20; Value = 0xe4b25c70ddc208d1; PC = 0x555555579fab *)
xor ymm4_0@uint64 ymm8_0 L0x7fffffffbd20;
xor ymm4_1@uint64 ymm8_1 L0x7fffffffbd28;
xor ymm4_2@uint64 ymm8_2 L0x7fffffffbd30;
xor ymm4_3@uint64 ymm8_3 L0x7fffffffbd38;
(* vpandn %ymm11,%ymm13,%ymm5                      #! PC = 0x555555579fb3 *)
not ymm13_0n@uint64 ymm13_0;
and ymm5_0@uint64 ymm13_0n ymm11_0;
not ymm13_1n@uint64 ymm13_1;
and ymm5_1@uint64 ymm13_1n ymm11_1;
not ymm13_2n@uint64 ymm13_2;
and ymm5_2@uint64 ymm13_2n ymm11_2;
not ymm13_3n@uint64 ymm13_3;
and ymm5_3@uint64 ymm13_3n ymm11_3;
(* vpxor  %ymm3,%ymm5,%ymm5                        #! PC = 0x555555579fb8 *)
xor ymm5_0@uint64 ymm5_0 ymm3_0;
xor ymm5_1@uint64 ymm5_1 ymm3_1;
xor ymm5_2@uint64 ymm5_2 ymm3_2;
xor ymm5_3@uint64 ymm5_3 ymm3_3;
(* vpxor  %ymm15,%ymm5,%ymm9                       #! PC = 0x555555579fbc *)
xor ymm9_0@uint64 ymm5_0 ymm15_0;
xor ymm9_1@uint64 ymm5_1 ymm15_1;
xor ymm9_2@uint64 ymm5_2 ymm15_2;
xor ymm9_3@uint64 ymm5_3 ymm15_3;
(* vpxor  %ymm1,%ymm9,%ymm9                        #! PC = 0x555555579fc1 *)
xor ymm9_0@uint64 ymm9_0 ymm1_0;
xor ymm9_1@uint64 ymm9_1 ymm1_1;
xor ymm9_2@uint64 ymm9_2 ymm1_2;
xor ymm9_3@uint64 ymm9_3 ymm1_3;
(* vpsrlq $0x17,%ymm2,%ymm1                        #! PC = 0x555555579fc5 *)
shr ymm1_0 ymm2_0 0x17@uint64;
shr ymm1_1 ymm2_1 0x17@uint64;
shr ymm1_2 ymm2_2 0x17@uint64;
shr ymm1_3 ymm2_3 0x17@uint64;
(* vpxor  -0x90(%rbp),%ymm9,%ymm9                  #! EA = L0x7fffffffbec0; Value = 0x31cbb7f055d0efc0; PC = 0x555555579fca *)
xor ymm9_0@uint64 ymm9_0 L0x7fffffffbec0;
xor ymm9_1@uint64 ymm9_1 L0x7fffffffbec8;
xor ymm9_2@uint64 ymm9_2 L0x7fffffffbed0;
xor ymm9_3@uint64 ymm9_3 L0x7fffffffbed8;
(* vpsllq $0x29,%ymm2,%ymm2                        #! PC = 0x555555579fd2 *)
shl ymm2_0 ymm2_0 0x29@uint64;
shl ymm2_1 ymm2_1 0x29@uint64;
shl ymm2_2 ymm2_2 0x29@uint64;
shl ymm2_3 ymm2_3 0x29@uint64;
(* vpor   %ymm2,%ymm1,%ymm2                        #! PC = 0x555555579fd7 *)
or ymm2_0@uint64 ymm1_0 ymm2_0;
or ymm2_1@uint64 ymm1_1 ymm2_1;
or ymm2_2@uint64 ymm1_2 ymm2_2;
or ymm2_3@uint64 ymm1_3 ymm2_3;
(* vpandn %ymm2,%ymm11,%ymm1                       #! PC = 0x555555579fdb *)
not ymm11_0n@uint64 ymm11_0;
and ymm1_0@uint64 ymm11_0n ymm2_0;
not ymm11_1n@uint64 ymm11_1;
and ymm1_1@uint64 ymm11_1n ymm2_1;
not ymm11_2n@uint64 ymm11_2;
and ymm1_2@uint64 ymm11_2n ymm2_2;
not ymm11_3n@uint64 ymm11_3;
and ymm1_3@uint64 ymm11_3n ymm2_3;
(* vpxor  %ymm13,%ymm1,%ymm14                      #! PC = 0x555555579fdf *)
xor ymm14_0@uint64 ymm1_0 ymm13_0;
xor ymm14_1@uint64 ymm1_1 ymm13_1;
xor ymm14_2@uint64 ymm1_2 ymm13_2;
xor ymm14_3@uint64 ymm1_3 ymm13_3;
(* vmovdqa %ymm14,%ymm10                           #! PC = 0x555555579fe4 *)
mov ymm10_0 ymm14_0;
mov ymm10_1 ymm14_1;
mov ymm10_2 ymm14_2;
mov ymm10_3 ymm14_3;
(* vpxor  -0x150(%rbp),%ymm6,%ymm14                #! EA = L0x7fffffffbe00; Value = 0x23b9113dc331409e; PC = 0x555555579fe9 *)
xor ymm14_0@uint64 ymm6_0 L0x7fffffffbe00;
xor ymm14_1@uint64 ymm6_1 L0x7fffffffbe08;
xor ymm14_2@uint64 ymm6_2 L0x7fffffffbe10;
xor ymm14_3@uint64 ymm6_3 L0x7fffffffbe18;
(* vmovdqa %ymm10,-0x130(%rbp)                     #! EA = L0x7fffffffbe20; PC = 0x555555579ff1 *)
mov L0x7fffffffbe20 ymm10_0;
mov L0x7fffffffbe28 ymm10_1;
mov L0x7fffffffbe30 ymm10_2;
mov L0x7fffffffbe38 ymm10_3;
(* vpxor  -0x310(%rbp),%ymm0,%ymm1                 #! EA = L0x7fffffffbc40; Value = 0xf7cc92e1c5618e18; PC = 0x555555579ff9 *)
xor ymm1_0@uint64 ymm0_0 L0x7fffffffbc40;
xor ymm1_1@uint64 ymm0_1 L0x7fffffffbc48;
xor ymm1_2@uint64 ymm0_2 L0x7fffffffbc50;
xor ymm1_3@uint64 ymm0_3 L0x7fffffffbc58;
(* vpxor  -0x50(%rbp),%ymm12,%ymm15                #! EA = L0x7fffffffbf00; Value = 0x968da580db523741; PC = 0x55555557a001 *)
xor ymm15_0@uint64 ymm12_0 L0x7fffffffbf00;
xor ymm15_1@uint64 ymm12_1 L0x7fffffffbf08;
xor ymm15_2@uint64 ymm12_2 L0x7fffffffbf10;
xor ymm15_3@uint64 ymm12_3 L0x7fffffffbf18;
(* vpxor  %ymm4,%ymm14,%ymm14                      #! PC = 0x55555557a006 *)
xor ymm14_0@uint64 ymm14_0 ymm4_0;
xor ymm14_1@uint64 ymm14_1 ymm4_1;
xor ymm14_2@uint64 ymm14_2 ymm4_2;
xor ymm14_3@uint64 ymm14_3 ymm4_3;
(* vpsllq $0x2,%ymm1,%ymm0                         #! PC = 0x55555557a00a *)
shl ymm0_0 ymm1_0 0x2@uint64;
shl ymm0_1 ymm1_1 0x2@uint64;
shl ymm0_2 ymm1_2 0x2@uint64;
shl ymm0_3 ymm1_3 0x2@uint64;
(* vpsrlq $0x3e,%ymm1,%ymm4                        #! PC = 0x55555557a00f *)
shr ymm4_0 ymm1_0 0x3e@uint64;
shr ymm4_1 ymm1_1 0x3e@uint64;
shr ymm4_2 ymm1_2 0x3e@uint64;
shr ymm4_3 ymm1_3 0x3e@uint64;
(* vpxor  %ymm10,%ymm14,%ymm14                     #! PC = 0x55555557a014 *)
xor ymm14_0@uint64 ymm14_0 ymm10_0;
xor ymm14_1@uint64 ymm14_1 ymm10_1;
xor ymm14_2@uint64 ymm14_2 ymm10_2;
xor ymm14_3@uint64 ymm14_3 ymm10_3;
(* vmovdqa -0x2b0(%rbp),%ymm10                     #! EA = L0x7fffffffbca0; Value = 0x8fb3a1c9623230f1; PC = 0x55555557a019 *)
mov ymm10_0 L0x7fffffffbca0;
mov ymm10_1 L0x7fffffffbca8;
mov ymm10_2 L0x7fffffffbcb0;
mov ymm10_3 L0x7fffffffbcb8;
(* vpor   %ymm0,%ymm4,%ymm4                        #! PC = 0x55555557a021 *)
or ymm4_0@uint64 ymm4_0 ymm0_0;
or ymm4_1@uint64 ymm4_1 ymm0_1;
or ymm4_2@uint64 ymm4_2 ymm0_2;
or ymm4_3@uint64 ymm4_3 ymm0_3;
(* vpandn %ymm4,%ymm2,%ymm1                        #! PC = 0x55555557a025 *)
not ymm2_0n@uint64 ymm2_0;
and ymm1_0@uint64 ymm2_0n ymm4_0;
not ymm2_1n@uint64 ymm2_1;
and ymm1_1@uint64 ymm2_1n ymm4_1;
not ymm2_2n@uint64 ymm2_2;
and ymm1_2@uint64 ymm2_2n ymm4_2;
not ymm2_3n@uint64 ymm2_3;
and ymm1_3@uint64 ymm2_3n ymm4_3;
(* vpxor  %ymm11,%ymm1,%ymm1                       #! PC = 0x55555557a029 *)
xor ymm1_0@uint64 ymm1_0 ymm11_0;
xor ymm1_1@uint64 ymm1_1 ymm11_1;
xor ymm1_2@uint64 ymm1_2 ymm11_2;
xor ymm1_3@uint64 ymm1_3 ymm11_3;
(* vmovdqa -0x190(%rbp),%ymm11                     #! EA = L0x7fffffffbdc0; Value = 0xd14705c13c413b26; PC = 0x55555557a02e *)
mov ymm11_0 L0x7fffffffbdc0;
mov ymm11_1 L0x7fffffffbdc8;
mov ymm11_2 L0x7fffffffbdd0;
mov ymm11_3 L0x7fffffffbdd8;
(* vpxor  -0x250(%rbp),%ymm11,%ymm0                #! EA = L0x7fffffffbd00; Value = 0x9da2f29054ce16ee; PC = 0x55555557a036 *)
xor ymm0_0@uint64 ymm11_0 L0x7fffffffbd00;
xor ymm0_1@uint64 ymm11_1 L0x7fffffffbd08;
xor ymm0_2@uint64 ymm11_2 L0x7fffffffbd10;
xor ymm0_3@uint64 ymm11_3 L0x7fffffffbd18;
(* vpxor  %ymm0,%ymm15,%ymm15                      #! PC = 0x55555557a03e *)
xor ymm15_0@uint64 ymm15_0 ymm0_0;
xor ymm15_1@uint64 ymm15_1 ymm0_1;
xor ymm15_2@uint64 ymm15_2 ymm0_2;
xor ymm15_3@uint64 ymm15_3 ymm0_3;
(* vpandn %ymm3,%ymm4,%ymm0                        #! PC = 0x55555557a042 *)
not ymm4_0n@uint64 ymm4_0;
and ymm0_0@uint64 ymm4_0n ymm3_0;
not ymm4_1n@uint64 ymm4_1;
and ymm0_1@uint64 ymm4_1n ymm3_1;
not ymm4_2n@uint64 ymm4_2;
and ymm0_2@uint64 ymm4_2n ymm3_2;
not ymm4_3n@uint64 ymm4_3;
and ymm0_3@uint64 ymm4_3n ymm3_3;
(* vpxor  %ymm2,%ymm0,%ymm2                        #! PC = 0x55555557a046 *)
xor ymm2_0@uint64 ymm0_0 ymm2_0;
xor ymm2_1@uint64 ymm0_1 ymm2_1;
xor ymm2_2@uint64 ymm0_2 ymm2_2;
xor ymm2_3@uint64 ymm0_3 ymm2_3;
(* vpxor  -0x270(%rbp),%ymm10,%ymm0                #! EA = L0x7fffffffbce0; Value = 0x7e8db0b9132d1f74; PC = 0x55555557a04a *)
xor ymm0_0@uint64 ymm10_0 L0x7fffffffbce0;
xor ymm0_1@uint64 ymm10_1 L0x7fffffffbce8;
xor ymm0_2@uint64 ymm10_2 L0x7fffffffbcf0;
xor ymm0_3@uint64 ymm10_3 L0x7fffffffbcf8;
(* vpxor  %ymm1,%ymm15,%ymm15                      #! PC = 0x55555557a052 *)
xor ymm15_0@uint64 ymm15_0 ymm1_0;
xor ymm15_1@uint64 ymm15_1 ymm1_1;
xor ymm15_2@uint64 ymm15_2 ymm1_2;
xor ymm15_3@uint64 ymm15_3 ymm1_3;
(* vpxor  %ymm2,%ymm7,%ymm11                       #! PC = 0x55555557a056 *)
xor ymm11_0@uint64 ymm7_0 ymm2_0;
xor ymm11_1@uint64 ymm7_1 ymm2_1;
xor ymm11_2@uint64 ymm7_2 ymm2_2;
xor ymm11_3@uint64 ymm7_3 ymm2_3;
(* vmovdqa %ymm2,-0x1f0(%rbp)                      #! EA = L0x7fffffffbd60; PC = 0x55555557a05a *)
mov L0x7fffffffbd60 ymm2_0;
mov L0x7fffffffbd68 ymm2_1;
mov L0x7fffffffbd70 ymm2_2;
mov L0x7fffffffbd78 ymm2_3;
(* vpxor  %ymm0,%ymm11,%ymm11                      #! PC = 0x55555557a062 *)
xor ymm11_0@uint64 ymm11_0 ymm0_0;
xor ymm11_1@uint64 ymm11_1 ymm0_1;
xor ymm11_2@uint64 ymm11_2 ymm0_2;
xor ymm11_3@uint64 ymm11_3 ymm0_3;
(* vpandn %ymm13,%ymm3,%ymm0                       #! PC = 0x55555557a066 *)
not ymm3_0n@uint64 ymm3_0;
and ymm0_0@uint64 ymm3_0n ymm13_0;
not ymm3_1n@uint64 ymm3_1;
and ymm0_1@uint64 ymm3_1n ymm13_1;
not ymm3_2n@uint64 ymm3_2;
and ymm0_2@uint64 ymm3_2n ymm13_2;
not ymm3_3n@uint64 ymm3_3;
and ymm0_3@uint64 ymm3_3n ymm13_3;
(* vmovdqa -0x2d0(%rbp),%ymm13                     #! EA = L0x7fffffffbc80; Value = 0x928ac03bb01eb0a4; PC = 0x55555557a06b *)
mov ymm13_0 L0x7fffffffbc80;
mov ymm13_1 L0x7fffffffbc88;
mov ymm13_2 L0x7fffffffbc90;
mov ymm13_3 L0x7fffffffbc98;
(* vpxor  -0x290(%rbp),%ymm13,%ymm2                #! EA = L0x7fffffffbcc0; Value = 0xb563450ecb53cae8; PC = 0x55555557a073 *)
xor ymm2_0@uint64 ymm13_0 L0x7fffffffbcc0;
xor ymm2_1@uint64 ymm13_1 L0x7fffffffbcc8;
xor ymm2_2@uint64 ymm13_2 L0x7fffffffbcd0;
xor ymm2_3@uint64 ymm13_3 L0x7fffffffbcd8;
(* vpxor  %ymm4,%ymm0,%ymm0                        #! PC = 0x55555557a07b *)
xor ymm0_0@uint64 ymm0_0 ymm4_0;
xor ymm0_1@uint64 ymm0_1 ymm4_1;
xor ymm0_2@uint64 ymm0_2 ymm4_2;
xor ymm0_3@uint64 ymm0_3 ymm4_3;
(* vpxor  -0x210(%rbp),%ymm0,%ymm10                #! EA = L0x7fffffffbd40; Value = 0x5b22dd2d64fec38f; PC = 0x55555557a07f *)
xor ymm10_0@uint64 ymm0_0 L0x7fffffffbd40;
xor ymm10_1@uint64 ymm0_1 L0x7fffffffbd48;
xor ymm10_2@uint64 ymm0_2 L0x7fffffffbd50;
xor ymm10_3@uint64 ymm0_3 L0x7fffffffbd58;
(* vpxor  -0x1b0(%rbp),%ymm11,%ymm11               #! EA = L0x7fffffffbda0; Value = 0xf068bde5cf48f1b5; PC = 0x55555557a087 *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbda0;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbda8;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbdb0;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbdb8;
(* vpsrlq $0x3f,%ymm14,%ymm3                       #! PC = 0x55555557a08f *)
shr ymm3_0 ymm14_0 0x3f@uint64;
shr ymm3_1 ymm14_1 0x3f@uint64;
shr ymm3_2 ymm14_2 0x3f@uint64;
shr ymm3_3 ymm14_3 0x3f@uint64;
(* vpsllq $0x1,%ymm15,%ymm4                        #! PC = 0x55555557a095 *)
shl ymm4_0 ymm15_0 0x1@uint64;
shl ymm4_1 ymm15_1 0x1@uint64;
shl ymm4_2 ymm15_2 0x1@uint64;
shl ymm4_3 ymm15_3 0x1@uint64;
(* vpxor  %ymm2,%ymm10,%ymm10                      #! PC = 0x55555557a09b *)
xor ymm10_0@uint64 ymm10_0 ymm2_0;
xor ymm10_1@uint64 ymm10_1 ymm2_1;
xor ymm10_2@uint64 ymm10_2 ymm2_2;
xor ymm10_3@uint64 ymm10_3 ymm2_3;
(* vpsllq $0x1,%ymm14,%ymm2                        #! PC = 0x55555557a09f *)
shl ymm2_0 ymm14_0 0x1@uint64;
shl ymm2_1 ymm14_1 0x1@uint64;
shl ymm2_2 ymm14_2 0x1@uint64;
shl ymm2_3 ymm14_3 0x1@uint64;
(* vpxor  -0x70(%rbp),%ymm10,%ymm10                #! EA = L0x7fffffffbee0; Value = 0xb534d89df4ee2434; PC = 0x55555557a0a5 *)
xor ymm10_0@uint64 ymm10_0 L0x7fffffffbee0;
xor ymm10_1@uint64 ymm10_1 L0x7fffffffbee8;
xor ymm10_2@uint64 ymm10_2 L0x7fffffffbef0;
xor ymm10_3@uint64 ymm10_3 L0x7fffffffbef8;
(* vpor   %ymm2,%ymm3,%ymm3                        #! PC = 0x55555557a0aa *)
or ymm3_0@uint64 ymm3_0 ymm2_0;
or ymm3_1@uint64 ymm3_1 ymm2_1;
or ymm3_2@uint64 ymm3_2 ymm2_2;
or ymm3_3@uint64 ymm3_3 ymm2_3;
(* vpsrlq $0x3f,%ymm15,%ymm2                       #! PC = 0x55555557a0ae *)
shr ymm2_0 ymm15_0 0x3f@uint64;
shr ymm2_1 ymm15_1 0x3f@uint64;
shr ymm2_2 ymm15_2 0x3f@uint64;
shr ymm2_3 ymm15_3 0x3f@uint64;
(* vpsllq $0x1,%ymm11,%ymm13                       #! PC = 0x55555557a0b4 *)
shl ymm13_0 ymm11_0 0x1@uint64;
shl ymm13_1 ymm11_1 0x1@uint64;
shl ymm13_2 ymm11_2 0x1@uint64;
shl ymm13_3 ymm11_3 0x1@uint64;
(* vpor   %ymm4,%ymm2,%ymm2                        #! PC = 0x55555557a0ba *)
or ymm2_0@uint64 ymm2_0 ymm4_0;
or ymm2_1@uint64 ymm2_1 ymm4_1;
or ymm2_2@uint64 ymm2_2 ymm4_2;
or ymm2_3@uint64 ymm2_3 ymm4_3;
(* vpxor  %ymm10,%ymm3,%ymm3                       #! PC = 0x55555557a0be *)
xor ymm3_0@uint64 ymm3_0 ymm10_0;
xor ymm3_1@uint64 ymm3_1 ymm10_1;
xor ymm3_2@uint64 ymm3_2 ymm10_2;
xor ymm3_3@uint64 ymm3_3 ymm10_3;
(* vpsrlq $0x3f,%ymm11,%ymm4                       #! PC = 0x55555557a0c3 *)
shr ymm4_0 ymm11_0 0x3f@uint64;
shr ymm4_1 ymm11_1 0x3f@uint64;
shr ymm4_2 ymm11_2 0x3f@uint64;
shr ymm4_3 ymm11_3 0x3f@uint64;
(* vpxor  %ymm9,%ymm2,%ymm2                        #! PC = 0x55555557a0c9 *)
xor ymm2_0@uint64 ymm2_0 ymm9_0;
xor ymm2_1@uint64 ymm2_1 ymm9_1;
xor ymm2_2@uint64 ymm2_2 ymm9_2;
xor ymm2_3@uint64 ymm2_3 ymm9_3;
(* vpxor  %ymm3,%ymm5,%ymm5                        #! PC = 0x55555557a0ce *)
xor ymm5_0@uint64 ymm5_0 ymm3_0;
xor ymm5_1@uint64 ymm5_1 ymm3_1;
xor ymm5_2@uint64 ymm5_2 ymm3_2;
xor ymm5_3@uint64 ymm5_3 ymm3_3;
(* vpor   %ymm13,%ymm4,%ymm4                       #! PC = 0x55555557a0d2 *)
or ymm4_0@uint64 ymm4_0 ymm13_0;
or ymm4_1@uint64 ymm4_1 ymm13_1;
or ymm4_2@uint64 ymm4_2 ymm13_2;
or ymm4_3@uint64 ymm4_3 ymm13_3;
(* vpxor  %ymm2,%ymm8,%ymm8                        #! PC = 0x55555557a0d7 *)
xor ymm8_0@uint64 ymm8_0 ymm2_0;
xor ymm8_1@uint64 ymm8_1 ymm2_1;
xor ymm8_2@uint64 ymm8_2 ymm2_2;
xor ymm8_3@uint64 ymm8_3 ymm2_3;
(* vmovq  %r13,%xmm13                              #! PC = 0x55555557a0db *)
mov xmm13_0 r13;
mov xmm13_1 0@uint64;
(* vpxor  %ymm14,%ymm4,%ymm14                      #! PC = 0x55555557a0e0 *)
xor ymm14_0@uint64 ymm4_0 ymm14_0;
xor ymm14_1@uint64 ymm4_1 ymm14_1;
xor ymm14_2@uint64 ymm4_2 ymm14_2;
xor ymm14_3@uint64 ymm4_3 ymm14_3;
(* vpsrlq $0x3f,%ymm10,%ymm4                       #! PC = 0x55555557a0e5 *)
shr ymm4_0 ymm10_0 0x3f@uint64;
shr ymm4_1 ymm10_1 0x3f@uint64;
shr ymm4_2 ymm10_2 0x3f@uint64;
shr ymm4_3 ymm10_3 0x3f@uint64;
(* vpxor  %ymm2,%ymm6,%ymm6                        #! PC = 0x55555557a0eb *)
xor ymm6_0@uint64 ymm6_0 ymm2_0;
xor ymm6_1@uint64 ymm6_1 ymm2_1;
xor ymm6_2@uint64 ymm6_2 ymm2_2;
xor ymm6_3@uint64 ymm6_3 ymm2_3;
(* vpsllq $0x1,%ymm10,%ymm10                       #! PC = 0x55555557a0ef *)
shl ymm10_0 ymm10_0 0x1@uint64;
shl ymm10_1 ymm10_1 0x1@uint64;
shl ymm10_2 ymm10_2 0x1@uint64;
shl ymm10_3 ymm10_3 0x1@uint64;
(* vpxor  %ymm14,%ymm12,%ymm12                     #! PC = 0x55555557a0f5 *)
xor ymm12_0@uint64 ymm12_0 ymm14_0;
xor ymm12_1@uint64 ymm12_1 ymm14_1;
xor ymm12_2@uint64 ymm12_2 ymm14_2;
xor ymm12_3@uint64 ymm12_3 ymm14_3;
(* vpxor  %ymm14,%ymm1,%ymm1                       #! PC = 0x55555557a0fa *)
xor ymm1_0@uint64 ymm1_0 ymm14_0;
xor ymm1_1@uint64 ymm1_1 ymm14_1;
xor ymm1_2@uint64 ymm1_2 ymm14_2;
xor ymm1_3@uint64 ymm1_3 ymm14_3;
(* vpor   %ymm10,%ymm4,%ymm10                      #! PC = 0x55555557a0ff *)
or ymm10_0@uint64 ymm4_0 ymm10_0;
or ymm10_1@uint64 ymm4_1 ymm10_1;
or ymm10_2@uint64 ymm4_2 ymm10_2;
or ymm10_3@uint64 ymm4_3 ymm10_3;
(* vpsrlq $0x3f,%ymm9,%ymm4                        #! PC = 0x55555557a104 *)
shr ymm4_0 ymm9_0 0x3f@uint64;
shr ymm4_1 ymm9_1 0x3f@uint64;
shr ymm4_2 ymm9_2 0x3f@uint64;
shr ymm4_3 ymm9_3 0x3f@uint64;
(* vpsllq $0x1,%ymm9,%ymm9                         #! PC = 0x55555557a10a *)
shl ymm9_0 ymm9_0 0x1@uint64;
shl ymm9_1 ymm9_1 0x1@uint64;
shl ymm9_2 ymm9_2 0x1@uint64;
shl ymm9_3 ymm9_3 0x1@uint64;
(* vpxor  %ymm15,%ymm10,%ymm15                     #! PC = 0x55555557a110 *)
xor ymm15_0@uint64 ymm10_0 ymm15_0;
xor ymm15_1@uint64 ymm10_1 ymm15_1;
xor ymm15_2@uint64 ymm10_2 ymm15_2;
xor ymm15_3@uint64 ymm10_3 ymm15_3;
(* vpbroadcastq %xmm13,%ymm10                      #! PC = 0x55555557a115 *)
mov ymm10_0 xmm13_0;
mov ymm10_1 xmm13_0;
mov ymm10_2 xmm13_0;
mov ymm10_3 xmm13_0;
(* vpor   %ymm9,%ymm4,%ymm9                        #! PC = 0x55555557a11a *)
or ymm9_0@uint64 ymm4_0 ymm9_0;
or ymm9_1@uint64 ymm4_1 ymm9_1;
or ymm9_2@uint64 ymm4_2 ymm9_2;
or ymm9_3@uint64 ymm4_3 ymm9_3;
(* vpsrlq $0x14,%ymm8,%ymm4                        #! PC = 0x55555557a11f *)
shr ymm4_0 ymm8_0 0x14@uint64;
shr ymm4_1 ymm8_1 0x14@uint64;
shr ymm4_2 ymm8_2 0x14@uint64;
shr ymm4_3 ymm8_3 0x14@uint64;
(* vpxor  %ymm15,%ymm7,%ymm7                       #! PC = 0x55555557a125 *)
xor ymm7_0@uint64 ymm7_0 ymm15_0;
xor ymm7_1@uint64 ymm7_1 ymm15_1;
xor ymm7_2@uint64 ymm7_2 ymm15_2;
xor ymm7_3@uint64 ymm7_3 ymm15_3;
(* vpsllq $0x2c,%ymm8,%ymm8                        #! PC = 0x55555557a12a *)
shl ymm8_0 ymm8_0 0x2c@uint64;
shl ymm8_1 ymm8_1 0x2c@uint64;
shl ymm8_2 ymm8_2 0x2c@uint64;
shl ymm8_3 ymm8_3 0x2c@uint64;
(* vpxor  %ymm11,%ymm9,%ymm11                      #! PC = 0x55555557a130 *)
xor ymm11_0@uint64 ymm9_0 ymm11_0;
xor ymm11_1@uint64 ymm9_1 ymm11_1;
xor ymm11_2@uint64 ymm9_2 ymm11_2;
xor ymm11_3@uint64 ymm9_3 ymm11_3;
(* vpxor  -0x90(%rbp),%ymm3,%ymm9                  #! EA = L0x7fffffffbec0; Value = 0x31cbb7f055d0efc0; PC = 0x55555557a135 *)
xor ymm9_0@uint64 ymm3_0 L0x7fffffffbec0;
xor ymm9_1@uint64 ymm3_1 L0x7fffffffbec8;
xor ymm9_2@uint64 ymm3_2 L0x7fffffffbed0;
xor ymm9_3@uint64 ymm3_3 L0x7fffffffbed8;
(* vpor   %ymm8,%ymm4,%ymm8                        #! PC = 0x55555557a13d *)
or ymm8_0@uint64 ymm4_0 ymm8_0;
or ymm8_1@uint64 ymm4_1 ymm8_1;
or ymm8_2@uint64 ymm4_2 ymm8_2;
or ymm8_3@uint64 ymm4_3 ymm8_3;
(* vpsrlq $0x15,%ymm12,%ymm4                       #! PC = 0x55555557a142 *)
shr ymm4_0 ymm12_0 0x15@uint64;
shr ymm4_1 ymm12_1 0x15@uint64;
shr ymm4_2 ymm12_2 0x15@uint64;
shr ymm4_3 ymm12_3 0x15@uint64;
(* vpxor  %ymm11,%ymm0,%ymm0                       #! PC = 0x55555557a148 *)
xor ymm0_0@uint64 ymm0_0 ymm11_0;
xor ymm0_1@uint64 ymm0_1 ymm11_1;
xor ymm0_2@uint64 ymm0_2 ymm11_2;
xor ymm0_3@uint64 ymm0_3 ymm11_3;
(* vpsllq $0x2b,%ymm12,%ymm12                      #! PC = 0x55555557a14d *)
shl ymm12_0 ymm12_0 0x2b@uint64;
shl ymm12_1 ymm12_1 0x2b@uint64;
shl ymm12_2 ymm12_2 0x2b@uint64;
shl ymm12_3 ymm12_3 0x2b@uint64;
(* vpor   %ymm12,%ymm4,%ymm12                      #! PC = 0x55555557a153 *)
or ymm12_0@uint64 ymm4_0 ymm12_0;
or ymm12_1@uint64 ymm4_1 ymm12_1;
or ymm12_2@uint64 ymm4_2 ymm12_2;
or ymm12_3@uint64 ymm4_3 ymm12_3;
(* vpandn %ymm12,%ymm8,%ymm4                       #! PC = 0x55555557a158 *)
not ymm8_0n@uint64 ymm8_0;
and ymm4_0@uint64 ymm8_0n ymm12_0;
not ymm8_1n@uint64 ymm8_1;
and ymm4_1@uint64 ymm8_1n ymm12_1;
not ymm8_2n@uint64 ymm8_2;
and ymm4_2@uint64 ymm8_2n ymm12_2;
not ymm8_3n@uint64 ymm8_3;
and ymm4_3@uint64 ymm8_3n ymm12_3;
(* vpxor  %ymm10,%ymm4,%ymm4                       #! PC = 0x55555557a15d *)
xor ymm4_0@uint64 ymm4_0 ymm10_0;
xor ymm4_1@uint64 ymm4_1 ymm10_1;
xor ymm4_2@uint64 ymm4_2 ymm10_2;
xor ymm4_3@uint64 ymm4_3 ymm10_3;
(* vpsrlq $0x13,%ymm6,%ymm10                       #! PC = 0x55555557a162 *)
shr ymm10_0 ymm6_0 0x13@uint64;
shr ymm10_1 ymm6_1 0x13@uint64;
shr ymm10_2 ymm6_2 0x13@uint64;
shr ymm10_3 ymm6_3 0x13@uint64;
(* vpxor  %ymm9,%ymm4,%ymm13                       #! PC = 0x55555557a167 *)
xor ymm13_0@uint64 ymm4_0 ymm9_0;
xor ymm13_1@uint64 ymm4_1 ymm9_1;
xor ymm13_2@uint64 ymm4_2 ymm9_2;
xor ymm13_3@uint64 ymm4_3 ymm9_3;
(* vpsrlq $0x2b,%ymm7,%ymm4                        #! PC = 0x55555557a16c *)
shr ymm4_0 ymm7_0 0x2b@uint64;
shr ymm4_1 ymm7_1 0x2b@uint64;
shr ymm4_2 ymm7_2 0x2b@uint64;
shr ymm4_3 ymm7_3 0x2b@uint64;
(* vpsllq $0x15,%ymm7,%ymm7                        #! PC = 0x55555557a171 *)
shl ymm7_0 ymm7_0 0x15@uint64;
shl ymm7_1 ymm7_1 0x15@uint64;
shl ymm7_2 ymm7_2 0x15@uint64;
shl ymm7_3 ymm7_3 0x15@uint64;
(* vmovdqa %ymm13,-0x90(%rbp)                      #! EA = L0x7fffffffbec0; PC = 0x55555557a176 *)
mov L0x7fffffffbec0 ymm13_0;
mov L0x7fffffffbec8 ymm13_1;
mov L0x7fffffffbed0 ymm13_2;
mov L0x7fffffffbed8 ymm13_3;
(* vpsllq $0x2d,%ymm6,%ymm6                        #! PC = 0x55555557a17e *)
shl ymm6_0 ymm6_0 0x2d@uint64;
shl ymm6_1 ymm6_1 0x2d@uint64;
shl ymm6_2 ymm6_2 0x2d@uint64;
shl ymm6_3 ymm6_3 0x2d@uint64;
(* vpor   %ymm7,%ymm4,%ymm7                        #! PC = 0x55555557a183 *)
or ymm7_0@uint64 ymm4_0 ymm7_0;
or ymm7_1@uint64 ymm4_1 ymm7_1;
or ymm7_2@uint64 ymm4_2 ymm7_2;
or ymm7_3@uint64 ymm4_3 ymm7_3;
(* vpor   %ymm6,%ymm10,%ymm6                       #! PC = 0x55555557a187 *)
or ymm6_0@uint64 ymm10_0 ymm6_0;
or ymm6_1@uint64 ymm10_1 ymm6_1;
or ymm6_2@uint64 ymm10_2 ymm6_2;
or ymm6_3@uint64 ymm10_3 ymm6_3;
(* vpandn %ymm7,%ymm12,%ymm4                       #! PC = 0x55555557a18b *)
not ymm12_0n@uint64 ymm12_0;
and ymm4_0@uint64 ymm12_0n ymm7_0;
not ymm12_1n@uint64 ymm12_1;
and ymm4_1@uint64 ymm12_1n ymm7_1;
not ymm12_2n@uint64 ymm12_2;
and ymm4_2@uint64 ymm12_2n ymm7_2;
not ymm12_3n@uint64 ymm12_3;
and ymm4_3@uint64 ymm12_3n ymm7_3;
(* vpxor  %ymm8,%ymm4,%ymm13                       #! PC = 0x55555557a18f *)
xor ymm13_0@uint64 ymm4_0 ymm8_0;
xor ymm13_1@uint64 ymm4_1 ymm8_1;
xor ymm13_2@uint64 ymm4_2 ymm8_2;
xor ymm13_3@uint64 ymm4_3 ymm8_3;
(* vpsrlq $0x32,%ymm0,%ymm4                        #! PC = 0x55555557a194 *)
shr ymm4_0 ymm0_0 0x32@uint64;
shr ymm4_1 ymm0_1 0x32@uint64;
shr ymm4_2 ymm0_2 0x32@uint64;
shr ymm4_3 ymm0_3 0x32@uint64;
(* vpsllq $0xe,%ymm0,%ymm0                         #! PC = 0x55555557a199 *)
shl ymm0_0 ymm0_0 0xe@uint64;
shl ymm0_1 ymm0_1 0xe@uint64;
shl ymm0_2 ymm0_2 0xe@uint64;
shl ymm0_3 ymm0_3 0xe@uint64;
(* vmovdqa %ymm13,-0xd0(%rbp)                      #! EA = L0x7fffffffbe80; PC = 0x55555557a19e *)
mov L0x7fffffffbe80 ymm13_0;
mov L0x7fffffffbe88 ymm13_1;
mov L0x7fffffffbe90 ymm13_2;
mov L0x7fffffffbe98 ymm13_3;
(* vpor   %ymm0,%ymm4,%ymm0                        #! PC = 0x55555557a1a6 *)
or ymm0_0@uint64 ymm4_0 ymm0_0;
or ymm0_1@uint64 ymm4_1 ymm0_1;
or ymm0_2@uint64 ymm4_2 ymm0_2;
or ymm0_3@uint64 ymm4_3 ymm0_3;
(* vpandn %ymm0,%ymm7,%ymm4                        #! PC = 0x55555557a1aa *)
not ymm7_0n@uint64 ymm7_0;
and ymm4_0@uint64 ymm7_0n ymm0_0;
not ymm7_1n@uint64 ymm7_1;
and ymm4_1@uint64 ymm7_1n ymm0_1;
not ymm7_2n@uint64 ymm7_2;
and ymm4_2@uint64 ymm7_2n ymm0_2;
not ymm7_3n@uint64 ymm7_3;
and ymm4_3@uint64 ymm7_3n ymm0_3;
(* vpxor  %ymm12,%ymm4,%ymm12                      #! PC = 0x55555557a1ae *)
xor ymm12_0@uint64 ymm4_0 ymm12_0;
xor ymm12_1@uint64 ymm4_1 ymm12_1;
xor ymm12_2@uint64 ymm4_2 ymm12_2;
xor ymm12_3@uint64 ymm4_3 ymm12_3;
(* vpandn %ymm9,%ymm0,%ymm4                        #! PC = 0x55555557a1b3 *)
not ymm0_0n@uint64 ymm0_0;
and ymm4_0@uint64 ymm0_0n ymm9_0;
not ymm0_1n@uint64 ymm0_1;
and ymm4_1@uint64 ymm0_1n ymm9_1;
not ymm0_2n@uint64 ymm0_2;
and ymm4_2@uint64 ymm0_2n ymm9_2;
not ymm0_3n@uint64 ymm0_3;
and ymm4_3@uint64 ymm0_3n ymm9_3;
(* vpandn %ymm8,%ymm9,%ymm9                        #! PC = 0x55555557a1b8 *)
not ymm9_0n@uint64 ymm9_0;
and ymm9_0@uint64 ymm9_0n ymm8_0;
not ymm9_1n@uint64 ymm9_1;
and ymm9_1@uint64 ymm9_1n ymm8_1;
not ymm9_2n@uint64 ymm9_2;
and ymm9_2@uint64 ymm9_2n ymm8_2;
not ymm9_3n@uint64 ymm9_3;
and ymm9_3@uint64 ymm9_3n ymm8_3;
(* vpxor  %ymm0,%ymm9,%ymm8                        #! PC = 0x55555557a1bd *)
xor ymm8_0@uint64 ymm9_0 ymm0_0;
xor ymm8_1@uint64 ymm9_1 ymm0_1;
xor ymm8_2@uint64 ymm9_2 ymm0_2;
xor ymm8_3@uint64 ymm9_3 ymm0_3;
(* vmovdqa %ymm12,-0x2f0(%rbp)                     #! EA = L0x7fffffffbc60; PC = 0x55555557a1c1 *)
mov L0x7fffffffbc60 ymm12_0;
mov L0x7fffffffbc68 ymm12_1;
mov L0x7fffffffbc70 ymm12_2;
mov L0x7fffffffbc78 ymm12_3;
(* vpxor  %ymm7,%ymm4,%ymm12                       #! PC = 0x55555557a1c9 *)
xor ymm12_0@uint64 ymm4_0 ymm7_0;
xor ymm12_1@uint64 ymm4_1 ymm7_1;
xor ymm12_2@uint64 ymm4_2 ymm7_2;
xor ymm12_3@uint64 ymm4_3 ymm7_3;
(* vpxor  -0x270(%rbp),%ymm15,%ymm4                #! EA = L0x7fffffffbce0; Value = 0x7e8db0b9132d1f74; PC = 0x55555557a1cd *)
xor ymm4_0@uint64 ymm15_0 L0x7fffffffbce0;
xor ymm4_1@uint64 ymm15_1 L0x7fffffffbce8;
xor ymm4_2@uint64 ymm15_2 L0x7fffffffbcf0;
xor ymm4_3@uint64 ymm15_3 L0x7fffffffbcf8;
(* vmovdqa %ymm12,-0x170(%rbp)                     #! EA = L0x7fffffffbde0; PC = 0x55555557a1d5 *)
mov L0x7fffffffbde0 ymm12_0;
mov L0x7fffffffbde8 ymm12_1;
mov L0x7fffffffbdf0 ymm12_2;
mov L0x7fffffffbdf8 ymm12_3;
(* vpxor  -0x2d0(%rbp),%ymm11,%ymm12               #! EA = L0x7fffffffbc80; Value = 0x928ac03bb01eb0a4; PC = 0x55555557a1dd *)
xor ymm12_0@uint64 ymm11_0 L0x7fffffffbc80;
xor ymm12_1@uint64 ymm11_1 L0x7fffffffbc88;
xor ymm12_2@uint64 ymm11_2 L0x7fffffffbc90;
xor ymm12_3@uint64 ymm11_3 L0x7fffffffbc98;
(* vmovdqa %ymm8,-0x1d0(%rbp)                      #! EA = L0x7fffffffbd80; PC = 0x55555557a1e5 *)
mov L0x7fffffffbd80 ymm8_0;
mov L0x7fffffffbd88 ymm8_1;
mov L0x7fffffffbd90 ymm8_2;
mov L0x7fffffffbd98 ymm8_3;
(* vpxor  -0xb0(%rbp),%ymm3,%ymm8                  #! EA = L0x7fffffffbea0; Value = 0x1a235d324d3e8a10; PC = 0x55555557a1ed *)
xor ymm8_0@uint64 ymm3_0 L0x7fffffffbea0;
xor ymm8_1@uint64 ymm3_1 L0x7fffffffbea8;
xor ymm8_2@uint64 ymm3_2 L0x7fffffffbeb0;
xor ymm8_3@uint64 ymm3_3 L0x7fffffffbeb8;
(* vpsrlq $0x24,%ymm4,%ymm0                        #! PC = 0x55555557a1f5 *)
shr ymm0_0 ymm4_0 0x24@uint64;
shr ymm0_1 ymm4_1 0x24@uint64;
shr ymm0_2 ymm4_2 0x24@uint64;
shr ymm0_3 ymm4_3 0x24@uint64;
(* vpsllq $0x1c,%ymm4,%ymm4                        #! PC = 0x55555557a1fa *)
shl ymm4_0 ymm4_0 0x1c@uint64;
shl ymm4_1 ymm4_1 0x1c@uint64;
shl ymm4_2 ymm4_2 0x1c@uint64;
shl ymm4_3 ymm4_3 0x1c@uint64;
(* vpor   %ymm4,%ymm0,%ymm0                        #! PC = 0x55555557a1ff *)
or ymm0_0@uint64 ymm0_0 ymm4_0;
or ymm0_1@uint64 ymm0_1 ymm4_1;
or ymm0_2@uint64 ymm0_2 ymm4_2;
or ymm0_3@uint64 ymm0_3 ymm4_3;
(* vpsrlq $0x3d,%ymm8,%ymm7                        #! PC = 0x55555557a203 *)
shr ymm7_0 ymm8_0 0x3d@uint64;
shr ymm7_1 ymm8_1 0x3d@uint64;
shr ymm7_2 ymm8_2 0x3d@uint64;
shr ymm7_3 ymm8_3 0x3d@uint64;
(* vpsrlq $0x2c,%ymm12,%ymm4                       #! PC = 0x55555557a209 *)
shr ymm4_0 ymm12_0 0x2c@uint64;
shr ymm4_1 ymm12_1 0x2c@uint64;
shr ymm4_2 ymm12_2 0x2c@uint64;
shr ymm4_3 ymm12_3 0x2c@uint64;
(* vpsllq $0x3,%ymm8,%ymm8                         #! PC = 0x55555557a20f *)
shl ymm8_0 ymm8_0 0x3@uint64;
shl ymm8_1 ymm8_1 0x3@uint64;
shl ymm8_2 ymm8_2 0x3@uint64;
shl ymm8_3 ymm8_3 0x3@uint64;
(* vpsllq $0x14,%ymm12,%ymm12                      #! PC = 0x55555557a215 *)
shl ymm12_0 ymm12_0 0x14@uint64;
shl ymm12_1 ymm12_1 0x14@uint64;
shl ymm12_2 ymm12_2 0x14@uint64;
shl ymm12_3 ymm12_3 0x14@uint64;
(* vpor   %ymm8,%ymm7,%ymm7                        #! PC = 0x55555557a21b *)
or ymm7_0@uint64 ymm7_0 ymm8_0;
or ymm7_1@uint64 ymm7_1 ymm8_1;
or ymm7_2@uint64 ymm7_2 ymm8_2;
or ymm7_3@uint64 ymm7_3 ymm8_3;
(* vpor   %ymm12,%ymm4,%ymm4                       #! PC = 0x55555557a220 *)
or ymm4_0@uint64 ymm4_0 ymm12_0;
or ymm4_1@uint64 ymm4_1 ymm12_1;
or ymm4_2@uint64 ymm4_2 ymm12_2;
or ymm4_3@uint64 ymm4_3 ymm12_3;
(* vpandn %ymm7,%ymm4,%ymm8                        #! PC = 0x55555557a225 *)
not ymm4_0n@uint64 ymm4_0;
and ymm8_0@uint64 ymm4_0n ymm7_0;
not ymm4_1n@uint64 ymm4_1;
and ymm8_1@uint64 ymm4_1n ymm7_1;
not ymm4_2n@uint64 ymm4_2;
and ymm8_2@uint64 ymm4_2n ymm7_2;
not ymm4_3n@uint64 ymm4_3;
and ymm8_3@uint64 ymm4_3n ymm7_3;
(* vpxor  %ymm0,%ymm8,%ymm9                        #! PC = 0x55555557a229 *)
xor ymm9_0@uint64 ymm8_0 ymm0_0;
xor ymm9_1@uint64 ymm8_1 ymm0_1;
xor ymm9_2@uint64 ymm8_2 ymm0_2;
xor ymm9_3@uint64 ymm8_3 ymm0_3;
(* vpandn %ymm6,%ymm7,%ymm8                        #! PC = 0x55555557a22d *)
not ymm7_0n@uint64 ymm7_0;
and ymm8_0@uint64 ymm7_0n ymm6_0;
not ymm7_1n@uint64 ymm7_1;
and ymm8_1@uint64 ymm7_1n ymm6_1;
not ymm7_2n@uint64 ymm7_2;
and ymm8_2@uint64 ymm7_2n ymm6_2;
not ymm7_3n@uint64 ymm7_3;
and ymm8_3@uint64 ymm7_3n ymm6_3;
(* vmovdqa %ymm9,%ymm13                            #! PC = 0x55555557a231 *)
mov ymm13_0 ymm9_0;
mov ymm13_1 ymm9_1;
mov ymm13_2 ymm9_2;
mov ymm13_3 ymm9_3;
(* vpsrlq $0x3,%ymm1,%ymm9                         #! PC = 0x55555557a236 *)
shr ymm9_0 ymm1_0 0x3@uint64;
shr ymm9_1 ymm1_1 0x3@uint64;
shr ymm9_2 ymm1_2 0x3@uint64;
shr ymm9_3 ymm1_3 0x3@uint64;
(* vpxor  %ymm4,%ymm8,%ymm8                        #! PC = 0x55555557a23b *)
xor ymm8_0@uint64 ymm8_0 ymm4_0;
xor ymm8_1@uint64 ymm8_1 ymm4_1;
xor ymm8_2@uint64 ymm8_2 ymm4_2;
xor ymm8_3@uint64 ymm8_3 ymm4_3;
(* vpsllq $0x3d,%ymm1,%ymm1                        #! PC = 0x55555557a23f *)
shl ymm1_0 ymm1_0 0x3d@uint64;
shl ymm1_1 ymm1_1 0x3d@uint64;
shl ymm1_2 ymm1_2 0x3d@uint64;
shl ymm1_3 ymm1_3 0x3d@uint64;
(* vpor   %ymm1,%ymm9,%ymm1                        #! PC = 0x55555557a244 *)
or ymm1_0@uint64 ymm9_0 ymm1_0;
or ymm1_1@uint64 ymm9_1 ymm1_1;
or ymm1_2@uint64 ymm9_2 ymm1_2;
or ymm1_3@uint64 ymm9_3 ymm1_3;
(* vpandn %ymm1,%ymm6,%ymm9                        #! PC = 0x55555557a248 *)
not ymm6_0n@uint64 ymm6_0;
and ymm9_0@uint64 ymm6_0n ymm1_0;
not ymm6_1n@uint64 ymm6_1;
and ymm9_1@uint64 ymm6_1n ymm1_1;
not ymm6_2n@uint64 ymm6_2;
and ymm9_2@uint64 ymm6_2n ymm1_2;
not ymm6_3n@uint64 ymm6_3;
and ymm9_3@uint64 ymm6_3n ymm1_3;
(* vpandn %ymm0,%ymm1,%ymm10                       #! PC = 0x55555557a24c *)
not ymm1_0n@uint64 ymm1_0;
and ymm10_0@uint64 ymm1_0n ymm0_0;
not ymm1_1n@uint64 ymm1_1;
and ymm10_1@uint64 ymm1_1n ymm0_1;
not ymm1_2n@uint64 ymm1_2;
and ymm10_2@uint64 ymm1_2n ymm0_2;
not ymm1_3n@uint64 ymm1_3;
and ymm10_3@uint64 ymm1_3n ymm0_3;
(* vpandn %ymm4,%ymm0,%ymm0                        #! PC = 0x55555557a250 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm4_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm4_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm4_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm4_3;
(* vpxor  %ymm7,%ymm9,%ymm9                        #! PC = 0x55555557a254 *)
xor ymm9_0@uint64 ymm9_0 ymm7_0;
xor ymm9_1@uint64 ymm9_1 ymm7_1;
xor ymm9_2@uint64 ymm9_2 ymm7_2;
xor ymm9_3@uint64 ymm9_3 ymm7_3;
(* vpxor  -0x190(%rbp),%ymm14,%ymm4                #! EA = L0x7fffffffbdc0; Value = 0xd14705c13c413b26; PC = 0x55555557a258 *)
xor ymm4_0@uint64 ymm14_0 L0x7fffffffbdc0;
xor ymm4_1@uint64 ymm14_1 L0x7fffffffbdc8;
xor ymm4_2@uint64 ymm14_2 L0x7fffffffbdd0;
xor ymm4_3@uint64 ymm14_3 L0x7fffffffbdd8;
(* vmovdqa %ymm9,-0xb0(%rbp)                       #! EA = L0x7fffffffbea0; PC = 0x55555557a260 *)
mov L0x7fffffffbea0 ymm9_0;
mov L0x7fffffffbea8 ymm9_1;
mov L0x7fffffffbeb0 ymm9_2;
mov L0x7fffffffbeb8 ymm9_3;
(* vpxor  %ymm6,%ymm10,%ymm9                       #! PC = 0x55555557a268 *)
xor ymm9_0@uint64 ymm10_0 ymm6_0;
xor ymm9_1@uint64 ymm10_1 ymm6_1;
xor ymm9_2@uint64 ymm10_2 ymm6_2;
xor ymm9_3@uint64 ymm10_3 ymm6_3;
(* vpxor  %ymm1,%ymm0,%ymm6                        #! PC = 0x55555557a26c *)
xor ymm6_0@uint64 ymm0_0 ymm1_0;
xor ymm6_1@uint64 ymm0_1 ymm1_1;
xor ymm6_2@uint64 ymm0_2 ymm1_2;
xor ymm6_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x230(%rbp),%ymm2,%ymm1                 #! EA = L0x7fffffffbd20; Value = 0xe4b25c70ddc208d1; PC = 0x55555557a270 *)
xor ymm1_0@uint64 ymm2_0 L0x7fffffffbd20;
xor ymm1_1@uint64 ymm2_1 L0x7fffffffbd28;
xor ymm1_2@uint64 ymm2_2 L0x7fffffffbd30;
xor ymm1_3@uint64 ymm2_3 L0x7fffffffbd38;
(* vmovdqa %ymm6,-0x2d0(%rbp)                      #! EA = L0x7fffffffbc80; PC = 0x55555557a278 *)
mov L0x7fffffffbc80 ymm6_0;
mov L0x7fffffffbc88 ymm6_1;
mov L0x7fffffffbc90 ymm6_2;
mov L0x7fffffffbc98 ymm6_3;
(* vpsrlq $0x3f,%ymm1,%ymm0                        #! PC = 0x55555557a280 *)
shr ymm0_0 ymm1_0 0x3f@uint64;
shr ymm0_1 ymm1_1 0x3f@uint64;
shr ymm0_2 ymm1_2 0x3f@uint64;
shr ymm0_3 ymm1_3 0x3f@uint64;
(* vpsllq $0x1,%ymm1,%ymm1                         #! PC = 0x55555557a285 *)
shl ymm1_0 ymm1_0 0x1@uint64;
shl ymm1_1 ymm1_1 0x1@uint64;
shl ymm1_2 ymm1_2 0x1@uint64;
shl ymm1_3 ymm1_3 0x1@uint64;
(* vmovdqa %ymm9,-0x270(%rbp)                      #! EA = L0x7fffffffbce0; PC = 0x55555557a28a *)
mov L0x7fffffffbce0 ymm9_0;
mov L0x7fffffffbce8 ymm9_1;
mov L0x7fffffffbcf0 ymm9_2;
mov L0x7fffffffbcf8 ymm9_3;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x55555557a292 *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpsrlq $0x3a,%ymm4,%ymm1                        #! PC = 0x55555557a296 *)
shr ymm1_0 ymm4_0 0x3a@uint64;
shr ymm1_1 ymm4_1 0x3a@uint64;
shr ymm1_2 ymm4_2 0x3a@uint64;
shr ymm1_3 ymm4_3 0x3a@uint64;
(* vpsllq $0x6,%ymm4,%ymm4                         #! PC = 0x55555557a29b *)
shl ymm4_0 ymm4_0 0x6@uint64;
shl ymm4_1 ymm4_1 0x6@uint64;
shl ymm4_2 ymm4_2 0x6@uint64;
shl ymm4_3 ymm4_3 0x6@uint64;
(* vpor   %ymm4,%ymm1,%ymm1                        #! PC = 0x55555557a2a0 *)
or ymm1_0@uint64 ymm1_0 ymm4_0;
or ymm1_1@uint64 ymm1_1 ymm4_1;
or ymm1_2@uint64 ymm1_2 ymm4_2;
or ymm1_3@uint64 ymm1_3 ymm4_3;
(* vpxor  -0x1b0(%rbp),%ymm15,%ymm4                #! EA = L0x7fffffffbda0; Value = 0xf068bde5cf48f1b5; PC = 0x55555557a2a4 *)
xor ymm4_0@uint64 ymm15_0 L0x7fffffffbda0;
xor ymm4_1@uint64 ymm15_1 L0x7fffffffbda8;
xor ymm4_2@uint64 ymm15_2 L0x7fffffffbdb0;
xor ymm4_3@uint64 ymm15_3 L0x7fffffffbdb8;
(* vpsrlq $0x27,%ymm4,%ymm12                       #! PC = 0x55555557a2ac *)
shr ymm12_0 ymm4_0 0x27@uint64;
shr ymm12_1 ymm4_1 0x27@uint64;
shr ymm12_2 ymm4_2 0x27@uint64;
shr ymm12_3 ymm4_3 0x27@uint64;
(* vpsllq $0x19,%ymm4,%ymm6                        #! PC = 0x55555557a2b1 *)
shl ymm6_0 ymm4_0 0x19@uint64;
shl ymm6_1 ymm4_1 0x19@uint64;
shl ymm6_2 ymm4_2 0x19@uint64;
shl ymm6_3 ymm4_3 0x19@uint64;
(* vpor   %ymm6,%ymm12,%ymm6                       #! PC = 0x55555557a2b6 *)
or ymm6_0@uint64 ymm12_0 ymm6_0;
or ymm6_1@uint64 ymm12_1 ymm6_1;
or ymm6_2@uint64 ymm12_2 ymm6_2;
or ymm6_3@uint64 ymm12_3 ymm6_3;
(* vpandn %ymm6,%ymm1,%ymm4                        #! PC = 0x55555557a2ba *)
not ymm1_0n@uint64 ymm1_0;
and ymm4_0@uint64 ymm1_0n ymm6_0;
not ymm1_1n@uint64 ymm1_1;
and ymm4_1@uint64 ymm1_1n ymm6_1;
not ymm1_2n@uint64 ymm1_2;
and ymm4_2@uint64 ymm1_2n ymm6_2;
not ymm1_3n@uint64 ymm1_3;
and ymm4_3@uint64 ymm1_3n ymm6_3;
(* vpxor  %ymm0,%ymm4,%ymm4                        #! PC = 0x55555557a2be *)
xor ymm4_0@uint64 ymm4_0 ymm0_0;
xor ymm4_1@uint64 ymm4_1 ymm0_1;
xor ymm4_2@uint64 ymm4_2 ymm0_2;
xor ymm4_3@uint64 ymm4_3 ymm0_3;
(* vmovdqa %ymm4,-0x190(%rbp)                      #! EA = L0x7fffffffbdc0; PC = 0x55555557a2c2 *)
mov L0x7fffffffbdc0 ymm4_0;
mov L0x7fffffffbdc8 ymm4_1;
mov L0x7fffffffbdd0 ymm4_2;
mov L0x7fffffffbdd8 ymm4_3;
(* vpxor  -0x70(%rbp),%ymm11,%ymm4                 #! EA = L0x7fffffffbee0; Value = 0xb534d89df4ee2434; PC = 0x55555557a2ca *)
xor ymm4_0@uint64 ymm11_0 L0x7fffffffbee0;
xor ymm4_1@uint64 ymm11_1 L0x7fffffffbee8;
xor ymm4_2@uint64 ymm11_2 L0x7fffffffbef0;
xor ymm4_3@uint64 ymm11_3 L0x7fffffffbef8;
(* vpshufb 0x53c28(%rip),%ymm4,%ymm4        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x55555557a2cf *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm4_0, ymm4_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm4_2, ymm4_3, tmp_2, tmp_3);
mov ymm4_0 tmp_0;
mov ymm4_1 tmp_1;
mov ymm4_2 tmp_2;
mov ymm4_3 tmp_3;
(* vpandn %ymm4,%ymm6,%ymm7                        #! PC = 0x55555557a2d8 *)
not ymm6_0n@uint64 ymm6_0;
and ymm7_0@uint64 ymm6_0n ymm4_0;
not ymm6_1n@uint64 ymm6_1;
and ymm7_1@uint64 ymm6_1n ymm4_1;
not ymm6_2n@uint64 ymm6_2;
and ymm7_2@uint64 ymm6_2n ymm4_2;
not ymm6_3n@uint64 ymm6_3;
and ymm7_3@uint64 ymm6_3n ymm4_3;
(* vpxor  %ymm1,%ymm7,%ymm12                       #! PC = 0x55555557a2dc *)
xor ymm12_0@uint64 ymm7_0 ymm1_0;
xor ymm12_1@uint64 ymm7_1 ymm1_1;
xor ymm12_2@uint64 ymm7_2 ymm1_2;
xor ymm12_3@uint64 ymm7_3 ymm1_3;
(* vpsrlq $0x2e,%ymm5,%ymm7                        #! PC = 0x55555557a2e0 *)
shr ymm7_0 ymm5_0 0x2e@uint64;
shr ymm7_1 ymm5_1 0x2e@uint64;
shr ymm7_2 ymm5_2 0x2e@uint64;
shr ymm7_3 ymm5_3 0x2e@uint64;
(* vpsllq $0x12,%ymm5,%ymm5                        #! PC = 0x55555557a2e5 *)
shl ymm5_0 ymm5_0 0x12@uint64;
shl ymm5_1 ymm5_1 0x12@uint64;
shl ymm5_2 ymm5_2 0x12@uint64;
shl ymm5_3 ymm5_3 0x12@uint64;
(* vmovdqa %ymm12,-0x230(%rbp)                     #! EA = L0x7fffffffbd20; PC = 0x55555557a2ea *)
mov L0x7fffffffbd20 ymm12_0;
mov L0x7fffffffbd28 ymm12_1;
mov L0x7fffffffbd30 ymm12_2;
mov L0x7fffffffbd38 ymm12_3;
(* vpor   %ymm5,%ymm7,%ymm5                        #! PC = 0x55555557a2f2 *)
or ymm5_0@uint64 ymm7_0 ymm5_0;
or ymm5_1@uint64 ymm7_1 ymm5_1;
or ymm5_2@uint64 ymm7_2 ymm5_2;
or ymm5_3@uint64 ymm7_3 ymm5_3;
(* vpandn %ymm5,%ymm4,%ymm12                       #! PC = 0x55555557a2f6 *)
not ymm4_0n@uint64 ymm4_0;
and ymm12_0@uint64 ymm4_0n ymm5_0;
not ymm4_1n@uint64 ymm4_1;
and ymm12_1@uint64 ymm4_1n ymm5_1;
not ymm4_2n@uint64 ymm4_2;
and ymm12_2@uint64 ymm4_2n ymm5_2;
not ymm4_3n@uint64 ymm4_3;
and ymm12_3@uint64 ymm4_3n ymm5_3;
(* vpxor  %ymm6,%ymm12,%ymm12                      #! PC = 0x55555557a2fa *)
xor ymm12_0@uint64 ymm12_0 ymm6_0;
xor ymm12_1@uint64 ymm12_1 ymm6_1;
xor ymm12_2@uint64 ymm12_2 ymm6_2;
xor ymm12_3@uint64 ymm12_3 ymm6_3;
(* vpandn %ymm0,%ymm5,%ymm6                        #! PC = 0x55555557a2fe *)
not ymm5_0n@uint64 ymm5_0;
and ymm6_0@uint64 ymm5_0n ymm0_0;
not ymm5_1n@uint64 ymm5_1;
and ymm6_1@uint64 ymm5_1n ymm0_1;
not ymm5_2n@uint64 ymm5_2;
and ymm6_2@uint64 ymm5_2n ymm0_2;
not ymm5_3n@uint64 ymm5_3;
and ymm6_3@uint64 ymm5_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x55555557a302 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm4,%ymm6,%ymm4                        #! PC = 0x55555557a306 *)
xor ymm4_0@uint64 ymm6_0 ymm4_0;
xor ymm4_1@uint64 ymm6_1 ymm4_1;
xor ymm4_2@uint64 ymm6_2 ymm4_2;
xor ymm4_3@uint64 ymm6_3 ymm4_3;
(* vpxor  %ymm5,%ymm0,%ymm0                        #! PC = 0x55555557a30a *)
xor ymm0_0@uint64 ymm0_0 ymm5_0;
xor ymm0_1@uint64 ymm0_1 ymm5_1;
xor ymm0_2@uint64 ymm0_2 ymm5_2;
xor ymm0_3@uint64 ymm0_3 ymm5_3;
(* vmovdqa %ymm4,-0x1b0(%rbp)                      #! EA = L0x7fffffffbda0; PC = 0x55555557a30e *)
mov L0x7fffffffbda0 ymm4_0;
mov L0x7fffffffbda8 ymm4_1;
mov L0x7fffffffbdb0 ymm4_2;
mov L0x7fffffffbdb8 ymm4_3;
(* vmovdqa %ymm0,-0x310(%rbp)                      #! EA = L0x7fffffffbc40; PC = 0x55555557a316 *)
mov L0x7fffffffbc40 ymm0_0;
mov L0x7fffffffbc48 ymm0_1;
mov L0x7fffffffbc50 ymm0_2;
mov L0x7fffffffbc58 ymm0_3;
(* vpxor  -0x150(%rbp),%ymm2,%ymm0                 #! EA = L0x7fffffffbe00; Value = 0x23b9113dc331409e; PC = 0x55555557a31e *)
xor ymm0_0@uint64 ymm2_0 L0x7fffffffbe00;
xor ymm0_1@uint64 ymm2_1 L0x7fffffffbe08;
xor ymm0_2@uint64 ymm2_2 L0x7fffffffbe10;
xor ymm0_3@uint64 ymm2_3 L0x7fffffffbe18;
(* vpxor  -0x290(%rbp),%ymm11,%ymm10               #! EA = L0x7fffffffbcc0; Value = 0xb563450ecb53cae8; PC = 0x55555557a326 *)
xor ymm10_0@uint64 ymm11_0 L0x7fffffffbcc0;
xor ymm10_1@uint64 ymm11_1 L0x7fffffffbcc8;
xor ymm10_2@uint64 ymm11_2 L0x7fffffffbcd0;
xor ymm10_3@uint64 ymm11_3 L0x7fffffffbcd8;
(* vpxor  -0xf0(%rbp),%ymm3,%ymm6                  #! EA = L0x7fffffffbe60; Value = 0x0c639e6d4dbb1c52; PC = 0x55555557a32e *)
xor ymm6_0@uint64 ymm3_0 L0x7fffffffbe60;
xor ymm6_1@uint64 ymm3_1 L0x7fffffffbe68;
xor ymm6_2@uint64 ymm3_2 L0x7fffffffbe70;
xor ymm6_3@uint64 ymm3_3 L0x7fffffffbe78;
(* vpxor  -0x210(%rbp),%ymm11,%ymm11               #! EA = L0x7fffffffbd40; Value = 0x5b22dd2d64fec38f; PC = 0x55555557a336 *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbd40;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbd48;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbd50;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbd58;
(* vmovdqa %ymm13,-0x210(%rbp)                     #! EA = L0x7fffffffbd40; PC = 0x55555557a33e *)
mov L0x7fffffffbd40 ymm13_0;
mov L0x7fffffffbd48 ymm13_1;
mov L0x7fffffffbd50 ymm13_2;
mov L0x7fffffffbd58 ymm13_3;
(* vpsrlq $0x36,%ymm0,%ymm9                        #! PC = 0x55555557a346 *)
shr ymm9_0 ymm0_0 0x36@uint64;
shr ymm9_1 ymm0_1 0x36@uint64;
shr ymm9_2 ymm0_2 0x36@uint64;
shr ymm9_3 ymm0_3 0x36@uint64;
(* vpsllq $0xa,%ymm0,%ymm0                         #! PC = 0x55555557a34b *)
shl ymm0_0 ymm0_0 0xa@uint64;
shl ymm0_1 ymm0_1 0xa@uint64;
shl ymm0_2 ymm0_2 0xa@uint64;
shl ymm0_3 ymm0_3 0xa@uint64;
(* vpxor  -0x110(%rbp),%ymm3,%ymm3                 #! EA = L0x7fffffffbe40; Value = 0x759959b043b15e3e; PC = 0x55555557a350 *)
xor ymm3_0@uint64 ymm3_0 L0x7fffffffbe40;
xor ymm3_1@uint64 ymm3_1 L0x7fffffffbe48;
xor ymm3_2@uint64 ymm3_2 L0x7fffffffbe50;
xor ymm3_3@uint64 ymm3_3 L0x7fffffffbe58;
(* vpsrlq $0x25,%ymm10,%ymm1                       #! PC = 0x55555557a358 *)
shr ymm1_0 ymm10_0 0x25@uint64;
shr ymm1_1 ymm10_1 0x25@uint64;
shr ymm1_2 ymm10_2 0x25@uint64;
shr ymm1_3 ymm10_3 0x25@uint64;
(* vpsllq $0x1b,%ymm10,%ymm10                      #! PC = 0x55555557a35e *)
shl ymm10_0 ymm10_0 0x1b@uint64;
shl ymm10_1 ymm10_1 0x1b@uint64;
shl ymm10_2 ymm10_2 0x1b@uint64;
shl ymm10_3 ymm10_3 0x1b@uint64;
(* vpor   %ymm0,%ymm9,%ymm9                        #! PC = 0x55555557a364 *)
or ymm9_0@uint64 ymm9_0 ymm0_0;
or ymm9_1@uint64 ymm9_1 ymm0_1;
or ymm9_2@uint64 ymm9_2 ymm0_2;
or ymm9_3@uint64 ymm9_3 ymm0_3;
(* vpxor  -0x50(%rbp),%ymm14,%ymm0                 #! EA = L0x7fffffffbf00; Value = 0x968da580db523741; PC = 0x55555557a368 *)
xor ymm0_0@uint64 ymm14_0 L0x7fffffffbf00;
xor ymm0_1@uint64 ymm14_1 L0x7fffffffbf08;
xor ymm0_2@uint64 ymm14_2 L0x7fffffffbf10;
xor ymm0_3@uint64 ymm14_3 L0x7fffffffbf18;
(* vpsrlq $0x1c,%ymm6,%ymm5                        #! PC = 0x55555557a36d *)
shr ymm5_0 ymm6_0 0x1c@uint64;
shr ymm5_1 ymm6_1 0x1c@uint64;
shr ymm5_2 ymm6_2 0x1c@uint64;
shr ymm5_3 ymm6_3 0x1c@uint64;
(* vpor   %ymm10,%ymm1,%ymm1                       #! PC = 0x55555557a372 *)
or ymm1_0@uint64 ymm1_0 ymm10_0;
or ymm1_1@uint64 ymm1_1 ymm10_1;
or ymm1_2@uint64 ymm1_2 ymm10_2;
or ymm1_3@uint64 ymm1_3 ymm10_3;
(* vpsllq $0x24,%ymm6,%ymm6                        #! PC = 0x55555557a377 *)
shl ymm6_0 ymm6_0 0x24@uint64;
shl ymm6_1 ymm6_1 0x24@uint64;
shl ymm6_2 ymm6_2 0x24@uint64;
shl ymm6_3 ymm6_3 0x24@uint64;
(* vpxor  -0x250(%rbp),%ymm14,%ymm14               #! EA = L0x7fffffffbd00; Value = 0x9da2f29054ce16ee; PC = 0x55555557a37c *)
xor ymm14_0@uint64 ymm14_0 L0x7fffffffbd00;
xor ymm14_1@uint64 ymm14_1 L0x7fffffffbd08;
xor ymm14_2@uint64 ymm14_2 L0x7fffffffbd10;
xor ymm14_3@uint64 ymm14_3 L0x7fffffffbd18;
(* vpsrlq $0x31,%ymm0,%ymm7                        #! PC = 0x55555557a384 *)
shr ymm7_0 ymm0_0 0x31@uint64;
shr ymm7_1 ymm0_1 0x31@uint64;
shr ymm7_2 ymm0_2 0x31@uint64;
shr ymm7_3 ymm0_3 0x31@uint64;
(* vpsllq $0xf,%ymm0,%ymm10                        #! PC = 0x55555557a389 *)
shl ymm10_0 ymm0_0 0xf@uint64;
shl ymm10_1 ymm0_1 0xf@uint64;
shl ymm10_2 ymm0_2 0xf@uint64;
shl ymm10_3 ymm0_3 0xf@uint64;
(* vpor   %ymm6,%ymm5,%ymm5                        #! PC = 0x55555557a38e *)
or ymm5_0@uint64 ymm5_0 ymm6_0;
or ymm5_1@uint64 ymm5_1 ymm6_1;
or ymm5_2@uint64 ymm5_2 ymm6_2;
or ymm5_3@uint64 ymm5_3 ymm6_3;
(* vpxor  -0x1f0(%rbp),%ymm15,%ymm0                #! EA = L0x7fffffffbd60; Value = 0x61089150769da6cf; PC = 0x55555557a392 *)
xor ymm0_0@uint64 ymm15_0 L0x7fffffffbd60;
xor ymm0_1@uint64 ymm15_1 L0x7fffffffbd68;
xor ymm0_2@uint64 ymm15_2 L0x7fffffffbd70;
xor ymm0_3@uint64 ymm15_3 L0x7fffffffbd78;
(* vpor   %ymm10,%ymm7,%ymm10                      #! PC = 0x55555557a39a *)
or ymm10_0@uint64 ymm7_0 ymm10_0;
or ymm10_1@uint64 ymm7_1 ymm10_1;
or ymm10_2@uint64 ymm7_2 ymm10_2;
or ymm10_3@uint64 ymm7_3 ymm10_3;
(* vpandn %ymm9,%ymm5,%ymm4                        #! PC = 0x55555557a39f *)
not ymm5_0n@uint64 ymm5_0;
and ymm4_0@uint64 ymm5_0n ymm9_0;
not ymm5_1n@uint64 ymm5_1;
and ymm4_1@uint64 ymm5_1n ymm9_1;
not ymm5_2n@uint64 ymm5_2;
and ymm4_2@uint64 ymm5_2n ymm9_2;
not ymm5_3n@uint64 ymm5_3;
and ymm4_3@uint64 ymm5_3n ymm9_3;
(* vpxor  %ymm1,%ymm4,%ymm6                        #! PC = 0x55555557a3a4 *)
xor ymm6_0@uint64 ymm4_0 ymm1_0;
xor ymm6_1@uint64 ymm4_1 ymm1_1;
xor ymm6_2@uint64 ymm4_2 ymm1_2;
xor ymm6_3@uint64 ymm4_3 ymm1_3;
(* vpxor  -0x2b0(%rbp),%ymm15,%ymm15               #! EA = L0x7fffffffbca0; Value = 0x8fb3a1c9623230f1; PC = 0x55555557a3a8 *)
xor ymm15_0@uint64 ymm15_0 L0x7fffffffbca0;
xor ymm15_1@uint64 ymm15_1 L0x7fffffffbca8;
xor ymm15_2@uint64 ymm15_2 L0x7fffffffbcb0;
xor ymm15_3@uint64 ymm15_3 L0x7fffffffbcb8;
(* vpshufb 0x53b27(%rip),%ymm0,%ymm0        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x55555557a3b0 *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm0_0, ymm0_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm0_2, ymm0_3, tmp_2, tmp_3);
mov ymm0_0 tmp_0;
mov ymm0_1 tmp_1;
mov ymm0_2 tmp_2;
mov ymm0_3 tmp_3;
(* vmovdqa %ymm6,-0xf0(%rbp)                       #! EA = L0x7fffffffbe60; PC = 0x55555557a3b9 *)
mov L0x7fffffffbe60 ymm6_0;
mov L0x7fffffffbe68 ymm6_1;
mov L0x7fffffffbe70 ymm6_2;
mov L0x7fffffffbe78 ymm6_3;
(* vpandn %ymm10,%ymm9,%ymm6                       #! PC = 0x55555557a3c1 *)
not ymm9_0n@uint64 ymm9_0;
and ymm6_0@uint64 ymm9_0n ymm10_0;
not ymm9_1n@uint64 ymm9_1;
and ymm6_1@uint64 ymm9_1n ymm10_1;
not ymm9_2n@uint64 ymm9_2;
and ymm6_2@uint64 ymm9_2n ymm10_2;
not ymm9_3n@uint64 ymm9_3;
and ymm6_3@uint64 ymm9_3n ymm10_3;
(* vpandn %ymm0,%ymm10,%ymm4                       #! PC = 0x55555557a3c6 *)
not ymm10_0n@uint64 ymm10_0;
and ymm4_0@uint64 ymm10_0n ymm0_0;
not ymm10_1n@uint64 ymm10_1;
and ymm4_1@uint64 ymm10_1n ymm0_1;
not ymm10_2n@uint64 ymm10_2;
and ymm4_2@uint64 ymm10_2n ymm0_2;
not ymm10_3n@uint64 ymm10_3;
and ymm4_3@uint64 ymm10_3n ymm0_3;
(* vpxor  %ymm5,%ymm6,%ymm6                        #! PC = 0x55555557a3ca *)
xor ymm6_0@uint64 ymm6_0 ymm5_0;
xor ymm6_1@uint64 ymm6_1 ymm5_1;
xor ymm6_2@uint64 ymm6_2 ymm5_2;
xor ymm6_3@uint64 ymm6_3 ymm5_3;
(* vpxor  %ymm9,%ymm4,%ymm7                        #! PC = 0x55555557a3ce *)
xor ymm7_0@uint64 ymm4_0 ymm9_0;
xor ymm7_1@uint64 ymm4_1 ymm9_1;
xor ymm7_2@uint64 ymm4_2 ymm9_2;
xor ymm7_3@uint64 ymm4_3 ymm9_3;
(* vpsrlq $0x2,%ymm14,%ymm4                        #! PC = 0x55555557a3d3 *)
shr ymm4_0 ymm14_0 0x2@uint64;
shr ymm4_1 ymm14_1 0x2@uint64;
shr ymm4_2 ymm14_2 0x2@uint64;
shr ymm4_3 ymm14_3 0x2@uint64;
(* vmovdqa %ymm7,-0x290(%rbp)                      #! EA = L0x7fffffffbcc0; PC = 0x55555557a3d9 *)
mov L0x7fffffffbcc0 ymm7_0;
mov L0x7fffffffbcc8 ymm7_1;
mov L0x7fffffffbcd0 ymm7_2;
mov L0x7fffffffbcd8 ymm7_3;
(* vpandn %ymm1,%ymm0,%ymm7                        #! PC = 0x55555557a3e1 *)
not ymm0_0n@uint64 ymm0_0;
and ymm7_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm7_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm7_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm7_3@uint64 ymm0_3n ymm1_3;
(* vpandn %ymm5,%ymm1,%ymm1                        #! PC = 0x55555557a3e5 *)
not ymm1_0n@uint64 ymm1_0;
and ymm1_0@uint64 ymm1_0n ymm5_0;
not ymm1_1n@uint64 ymm1_1;
and ymm1_1@uint64 ymm1_1n ymm5_1;
not ymm1_2n@uint64 ymm1_2;
and ymm1_2@uint64 ymm1_2n ymm5_2;
not ymm1_3n@uint64 ymm1_3;
and ymm1_3@uint64 ymm1_3n ymm5_3;
(* vpxor  %ymm0,%ymm1,%ymm5                        #! PC = 0x55555557a3e9 *)
xor ymm5_0@uint64 ymm1_0 ymm0_0;
xor ymm5_1@uint64 ymm1_1 ymm0_1;
xor ymm5_2@uint64 ymm1_2 ymm0_2;
xor ymm5_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x9,%ymm15,%ymm0                        #! PC = 0x55555557a3ed *)
shr ymm0_0 ymm15_0 0x9@uint64;
shr ymm0_1 ymm15_1 0x9@uint64;
shr ymm0_2 ymm15_2 0x9@uint64;
shr ymm0_3 ymm15_3 0x9@uint64;
(* vpxor  %ymm10,%ymm7,%ymm7                       #! PC = 0x55555557a3f3 *)
xor ymm7_0@uint64 ymm7_0 ymm10_0;
xor ymm7_1@uint64 ymm7_1 ymm10_1;
xor ymm7_2@uint64 ymm7_2 ymm10_2;
xor ymm7_3@uint64 ymm7_3 ymm10_3;
(* vmovdqa -0x270(%rbp),%ymm10                     #! EA = L0x7fffffffbce0; Value = 0x5a19d348ebbbb880; PC = 0x55555557a3f8 *)
mov ymm10_0 L0x7fffffffbce0;
mov ymm10_1 L0x7fffffffbce8;
mov ymm10_2 L0x7fffffffbcf0;
mov ymm10_3 L0x7fffffffbcf8;
(* vpsrlq $0x19,%ymm11,%ymm1                       #! PC = 0x55555557a400 *)
shr ymm1_0 ymm11_0 0x19@uint64;
shr ymm1_1 ymm11_1 0x19@uint64;
shr ymm1_2 ymm11_2 0x19@uint64;
shr ymm1_3 ymm11_3 0x19@uint64;
(* vpsllq $0x37,%ymm15,%ymm15                      #! PC = 0x55555557a406 *)
shl ymm15_0 ymm15_0 0x37@uint64;
shl ymm15_1 ymm15_1 0x37@uint64;
shl ymm15_2 ymm15_2 0x37@uint64;
shl ymm15_3 ymm15_3 0x37@uint64;
(* vmovdqa %ymm5,-0x330(%rbp)                      #! EA = L0x7fffffffbc20; PC = 0x55555557a40c *)
mov L0x7fffffffbc20 ymm5_0;
mov L0x7fffffffbc28 ymm5_1;
mov L0x7fffffffbc30 ymm5_2;
mov L0x7fffffffbc38 ymm5_3;
(* vmovdqa -0x190(%rbp),%ymm5                      #! EA = L0x7fffffffbdc0; Value = 0xe14da812846248af; PC = 0x55555557a414 *)
mov ymm5_0 L0x7fffffffbdc0;
mov ymm5_1 L0x7fffffffbdc8;
mov ymm5_2 L0x7fffffffbdd0;
mov ymm5_3 L0x7fffffffbdd8;
(* vpsllq $0x27,%ymm11,%ymm11                      #! PC = 0x55555557a41c *)
shl ymm11_0 ymm11_0 0x27@uint64;
shl ymm11_1 ymm11_1 0x27@uint64;
shl ymm11_2 ymm11_2 0x27@uint64;
shl ymm11_3 ymm11_3 0x27@uint64;
(* vpsllq $0x3e,%ymm14,%ymm14                      #! PC = 0x55555557a422 *)
shl ymm14_0 ymm14_0 0x3e@uint64;
shl ymm14_1 ymm14_1 0x3e@uint64;
shl ymm14_2 ymm14_2 0x3e@uint64;
shl ymm14_3 ymm14_3 0x3e@uint64;
(* vpor   %ymm15,%ymm0,%ymm15                      #! PC = 0x55555557a428 *)
or ymm15_0@uint64 ymm0_0 ymm15_0;
or ymm15_1@uint64 ymm0_1 ymm15_1;
or ymm15_2@uint64 ymm0_2 ymm15_2;
or ymm15_3@uint64 ymm0_3 ymm15_3;
(* vpor   %ymm11,%ymm1,%ymm11                      #! PC = 0x55555557a42d *)
or ymm11_0@uint64 ymm1_0 ymm11_0;
or ymm11_1@uint64 ymm1_1 ymm11_1;
or ymm11_2@uint64 ymm1_2 ymm11_2;
or ymm11_3@uint64 ymm1_3 ymm11_3;
(* vpor   %ymm14,%ymm4,%ymm4                       #! PC = 0x55555557a432 *)
or ymm4_0@uint64 ymm4_0 ymm14_0;
or ymm4_1@uint64 ymm4_1 ymm14_1;
or ymm4_2@uint64 ymm4_2 ymm14_2;
or ymm4_3@uint64 ymm4_3 ymm14_3;
(* vpxor  %ymm13,%ymm5,%ymm1                       #! PC = 0x55555557a437 *)
xor ymm1_0@uint64 ymm5_0 ymm13_0;
xor ymm1_1@uint64 ymm5_1 ymm13_1;
xor ymm1_2@uint64 ymm5_2 ymm13_2;
xor ymm1_3@uint64 ymm5_3 ymm13_3;
(* vpandn %ymm11,%ymm15,%ymm0                      #! PC = 0x55555557a43c *)
not ymm15_0n@uint64 ymm15_0;
and ymm0_0@uint64 ymm15_0n ymm11_0;
not ymm15_1n@uint64 ymm15_1;
and ymm0_1@uint64 ymm15_1n ymm11_1;
not ymm15_2n@uint64 ymm15_2;
and ymm0_2@uint64 ymm15_2n ymm11_2;
not ymm15_3n@uint64 ymm15_3;
and ymm0_3@uint64 ymm15_3n ymm11_3;
(* vpxor  -0xd0(%rbp),%ymm8,%ymm5                  #! EA = L0x7fffffffbe80; Value = 0x2e734b02ce704378; PC = 0x55555557a441 *)
xor ymm5_0@uint64 ymm8_0 L0x7fffffffbe80;
xor ymm5_1@uint64 ymm8_1 L0x7fffffffbe88;
xor ymm5_2@uint64 ymm8_2 L0x7fffffffbe90;
xor ymm5_3@uint64 ymm8_3 L0x7fffffffbe98;
(* vpxor  -0x230(%rbp),%ymm6,%ymm14                #! EA = L0x7fffffffbd20; Value = 0xd3376cb3ecec921b; PC = 0x55555557a449 *)
xor ymm14_0@uint64 ymm6_0 L0x7fffffffbd20;
xor ymm14_1@uint64 ymm6_1 L0x7fffffffbd28;
xor ymm14_2@uint64 ymm6_2 L0x7fffffffbd30;
xor ymm14_3@uint64 ymm6_3 L0x7fffffffbd38;
(* vpxor  %ymm4,%ymm0,%ymm0                        #! PC = 0x55555557a451 *)
xor ymm0_0@uint64 ymm0_0 ymm4_0;
xor ymm0_1@uint64 ymm0_1 ymm4_1;
xor ymm0_2@uint64 ymm0_2 ymm4_2;
xor ymm0_3@uint64 ymm0_3 ymm4_3;
(* vpxor  -0xf0(%rbp),%ymm0,%ymm9                  #! EA = L0x7fffffffbe60; Value = 0xa164d352b18499c4; PC = 0x55555557a455 *)
xor ymm9_0@uint64 ymm0_0 L0x7fffffffbe60;
xor ymm9_1@uint64 ymm0_1 L0x7fffffffbe68;
xor ymm9_2@uint64 ymm0_2 L0x7fffffffbe70;
xor ymm9_3@uint64 ymm0_3 L0x7fffffffbe78;
(* vpxor  -0x290(%rbp),%ymm12,%ymm13               #! EA = L0x7fffffffbcc0; Value = 0x3660c07bad4b60af; PC = 0x55555557a45d *)
xor ymm13_0@uint64 ymm12_0 L0x7fffffffbcc0;
xor ymm13_1@uint64 ymm12_1 L0x7fffffffbcc8;
xor ymm13_2@uint64 ymm12_2 L0x7fffffffbcd0;
xor ymm13_3@uint64 ymm12_3 L0x7fffffffbcd8;
(* vpxor  %ymm5,%ymm14,%ymm14                      #! PC = 0x55555557a465 *)
xor ymm14_0@uint64 ymm14_0 ymm5_0;
xor ymm14_1@uint64 ymm14_1 ymm5_1;
xor ymm14_2@uint64 ymm14_2 ymm5_2;
xor ymm14_3@uint64 ymm14_3 ymm5_3;
(* vpxor  %ymm1,%ymm9,%ymm9                        #! PC = 0x55555557a469 *)
xor ymm9_0@uint64 ymm9_0 ymm1_0;
xor ymm9_1@uint64 ymm9_1 ymm1_1;
xor ymm9_2@uint64 ymm9_2 ymm1_2;
xor ymm9_3@uint64 ymm9_3 ymm1_3;
(* vpsrlq $0x17,%ymm3,%ymm1                        #! PC = 0x55555557a46d *)
shr ymm1_0 ymm3_0 0x17@uint64;
shr ymm1_1 ymm3_1 0x17@uint64;
shr ymm1_2 ymm3_2 0x17@uint64;
shr ymm1_3 ymm3_3 0x17@uint64;
(* vpxor  -0x90(%rbp),%ymm9,%ymm9                  #! EA = L0x7fffffffbec0; Value = 0x736d2c1d55c4ef73; PC = 0x55555557a472 *)
xor ymm9_0@uint64 ymm9_0 L0x7fffffffbec0;
xor ymm9_1@uint64 ymm9_1 L0x7fffffffbec8;
xor ymm9_2@uint64 ymm9_2 L0x7fffffffbed0;
xor ymm9_3@uint64 ymm9_3 L0x7fffffffbed8;
(* vpsllq $0x29,%ymm3,%ymm3                        #! PC = 0x55555557a47a *)
shl ymm3_0 ymm3_0 0x29@uint64;
shl ymm3_1 ymm3_1 0x29@uint64;
shl ymm3_2 ymm3_2 0x29@uint64;
shl ymm3_3 ymm3_3 0x29@uint64;
(* vpor   %ymm3,%ymm1,%ymm3                        #! PC = 0x55555557a47f *)
or ymm3_0@uint64 ymm1_0 ymm3_0;
or ymm3_1@uint64 ymm1_1 ymm3_1;
or ymm3_2@uint64 ymm1_2 ymm3_2;
or ymm3_3@uint64 ymm1_3 ymm3_3;
(* vpandn %ymm3,%ymm11,%ymm1                       #! PC = 0x55555557a483 *)
not ymm11_0n@uint64 ymm11_0;
and ymm1_0@uint64 ymm11_0n ymm3_0;
not ymm11_1n@uint64 ymm11_1;
and ymm1_1@uint64 ymm11_1n ymm3_1;
not ymm11_2n@uint64 ymm11_2;
and ymm1_2@uint64 ymm11_2n ymm3_2;
not ymm11_3n@uint64 ymm11_3;
and ymm1_3@uint64 ymm11_3n ymm3_3;
(* vpxor  %ymm15,%ymm1,%ymm1                       #! PC = 0x55555557a487 *)
xor ymm1_0@uint64 ymm1_0 ymm15_0;
xor ymm1_1@uint64 ymm1_1 ymm15_1;
xor ymm1_2@uint64 ymm1_2 ymm15_2;
xor ymm1_3@uint64 ymm1_3 ymm15_3;
(* vpxor  %ymm1,%ymm14,%ymm14                      #! PC = 0x55555557a48c *)
xor ymm14_0@uint64 ymm14_0 ymm1_0;
xor ymm14_1@uint64 ymm14_1 ymm1_1;
xor ymm14_2@uint64 ymm14_2 ymm1_2;
xor ymm14_3@uint64 ymm14_3 ymm1_3;
(* vmovdqa %ymm1,-0x2b0(%rbp)                      #! EA = L0x7fffffffbca0; PC = 0x55555557a490 *)
mov L0x7fffffffbca0 ymm1_0;
mov L0x7fffffffbca8 ymm1_1;
mov L0x7fffffffbcb0 ymm1_2;
mov L0x7fffffffbcb8 ymm1_3;
(* vpxor  -0x130(%rbp),%ymm2,%ymm1                 #! EA = L0x7fffffffbe20; Value = 0x28fedc4ead93fed0; PC = 0x55555557a498 *)
xor ymm1_0@uint64 ymm2_0 L0x7fffffffbe20;
xor ymm1_1@uint64 ymm2_1 L0x7fffffffbe28;
xor ymm1_2@uint64 ymm2_2 L0x7fffffffbe30;
xor ymm1_3@uint64 ymm2_3 L0x7fffffffbe38;
(* vpsllq $0x2,%ymm1,%ymm2                         #! PC = 0x55555557a4a0 *)
shl ymm2_0 ymm1_0 0x2@uint64;
shl ymm2_1 ymm1_1 0x2@uint64;
shl ymm2_2 ymm1_2 0x2@uint64;
shl ymm2_3 ymm1_3 0x2@uint64;
(* vpsrlq $0x3e,%ymm1,%ymm5                        #! PC = 0x55555557a4a5 *)
shr ymm5_0 ymm1_0 0x3e@uint64;
shr ymm5_1 ymm1_1 0x3e@uint64;
shr ymm5_2 ymm1_2 0x3e@uint64;
shr ymm5_3 ymm1_3 0x3e@uint64;
(* vpor   %ymm2,%ymm5,%ymm5                        #! PC = 0x55555557a4aa *)
or ymm5_0@uint64 ymm5_0 ymm2_0;
or ymm5_1@uint64 ymm5_1 ymm2_1;
or ymm5_2@uint64 ymm5_2 ymm2_2;
or ymm5_3@uint64 ymm5_3 ymm2_3;
(* vpandn %ymm5,%ymm3,%ymm1                        #! PC = 0x55555557a4ae *)
not ymm3_0n@uint64 ymm3_0;
and ymm1_0@uint64 ymm3_0n ymm5_0;
not ymm3_1n@uint64 ymm3_1;
and ymm1_1@uint64 ymm3_1n ymm5_1;
not ymm3_2n@uint64 ymm3_2;
and ymm1_2@uint64 ymm3_2n ymm5_2;
not ymm3_3n@uint64 ymm3_3;
and ymm1_3@uint64 ymm3_3n ymm5_3;
(* vpxor  %ymm11,%ymm1,%ymm1                       #! PC = 0x55555557a4b2 *)
xor ymm1_0@uint64 ymm1_0 ymm11_0;
xor ymm1_1@uint64 ymm1_1 ymm11_1;
xor ymm1_2@uint64 ymm1_2 ymm11_2;
xor ymm1_3@uint64 ymm1_3 ymm11_3;
(* vmovdqa -0xb0(%rbp),%ymm11                      #! EA = L0x7fffffffbea0; Value = 0xe4af9f9f755c1079; PC = 0x55555557a4b7 *)
mov ymm11_0 L0x7fffffffbea0;
mov ymm11_1 L0x7fffffffbea8;
mov ymm11_2 L0x7fffffffbeb0;
mov ymm11_3 L0x7fffffffbeb8;
(* vpxor  -0x2f0(%rbp),%ymm11,%ymm2                #! EA = L0x7fffffffbc60; Value = 0xbfaf17a701e92e36; PC = 0x55555557a4bf *)
xor ymm2_0@uint64 ymm11_0 L0x7fffffffbc60;
xor ymm2_1@uint64 ymm11_1 L0x7fffffffbc68;
xor ymm2_2@uint64 ymm11_2 L0x7fffffffbc70;
xor ymm2_3@uint64 ymm11_3 L0x7fffffffbc78;
(* vpxor  %ymm2,%ymm13,%ymm13                      #! PC = 0x55555557a4c7 *)
xor ymm13_0@uint64 ymm13_0 ymm2_0;
xor ymm13_1@uint64 ymm13_1 ymm2_1;
xor ymm13_2@uint64 ymm13_2 ymm2_2;
xor ymm13_3@uint64 ymm13_3 ymm2_3;
(* vpandn %ymm4,%ymm5,%ymm2                        #! PC = 0x55555557a4cb *)
not ymm5_0n@uint64 ymm5_0;
and ymm2_0@uint64 ymm5_0n ymm4_0;
not ymm5_1n@uint64 ymm5_1;
and ymm2_1@uint64 ymm5_1n ymm4_1;
not ymm5_2n@uint64 ymm5_2;
and ymm2_2@uint64 ymm5_2n ymm4_2;
not ymm5_3n@uint64 ymm5_3;
and ymm2_3@uint64 ymm5_3n ymm4_3;
(* vpxor  %ymm3,%ymm2,%ymm3                        #! PC = 0x55555557a4cf *)
xor ymm3_0@uint64 ymm2_0 ymm3_0;
xor ymm3_1@uint64 ymm2_1 ymm3_1;
xor ymm3_2@uint64 ymm2_2 ymm3_2;
xor ymm3_3@uint64 ymm2_3 ymm3_3;
(* vpxor  -0x170(%rbp),%ymm10,%ymm2                #! EA = L0x7fffffffbde0; Value = 0x5c9b10c4952fd875; PC = 0x55555557a4d3 *)
xor ymm2_0@uint64 ymm10_0 L0x7fffffffbde0;
xor ymm2_1@uint64 ymm10_1 L0x7fffffffbde8;
xor ymm2_2@uint64 ymm10_2 L0x7fffffffbdf0;
xor ymm2_3@uint64 ymm10_3 L0x7fffffffbdf8;
(* vpxor  %ymm1,%ymm13,%ymm13                      #! PC = 0x55555557a4db *)
xor ymm13_0@uint64 ymm13_0 ymm1_0;
xor ymm13_1@uint64 ymm13_1 ymm1_1;
xor ymm13_2@uint64 ymm13_2 ymm1_2;
xor ymm13_3@uint64 ymm13_3 ymm1_3;
(* vpxor  %ymm3,%ymm7,%ymm11                       #! PC = 0x55555557a4df *)
xor ymm11_0@uint64 ymm7_0 ymm3_0;
xor ymm11_1@uint64 ymm7_1 ymm3_1;
xor ymm11_2@uint64 ymm7_2 ymm3_2;
xor ymm11_3@uint64 ymm7_3 ymm3_3;
(* vmovdqa %ymm3,-0x250(%rbp)                      #! EA = L0x7fffffffbd00; PC = 0x55555557a4e3 *)
mov L0x7fffffffbd00 ymm3_0;
mov L0x7fffffffbd08 ymm3_1;
mov L0x7fffffffbd10 ymm3_2;
mov L0x7fffffffbd18 ymm3_3;
(* vpxor  %ymm2,%ymm11,%ymm11                      #! PC = 0x55555557a4eb *)
xor ymm11_0@uint64 ymm11_0 ymm2_0;
xor ymm11_1@uint64 ymm11_1 ymm2_1;
xor ymm11_2@uint64 ymm11_2 ymm2_2;
xor ymm11_3@uint64 ymm11_3 ymm2_3;
(* vpandn %ymm15,%ymm4,%ymm2                       #! PC = 0x55555557a4ef *)
not ymm4_0n@uint64 ymm4_0;
and ymm2_0@uint64 ymm4_0n ymm15_0;
not ymm4_1n@uint64 ymm4_1;
and ymm2_1@uint64 ymm4_1n ymm15_1;
not ymm4_2n@uint64 ymm4_2;
and ymm2_2@uint64 ymm4_2n ymm15_2;
not ymm4_3n@uint64 ymm4_3;
and ymm2_3@uint64 ymm4_3n ymm15_3;
(* vmovdqa -0x2d0(%rbp),%ymm15                     #! EA = L0x7fffffffbc80; Value = 0xa4e4fc5a17f63732; PC = 0x55555557a4f4 *)
mov ymm15_0 L0x7fffffffbc80;
mov ymm15_1 L0x7fffffffbc88;
mov ymm15_2 L0x7fffffffbc90;
mov ymm15_3 L0x7fffffffbc98;
(* vpxor  -0x1d0(%rbp),%ymm15,%ymm10               #! EA = L0x7fffffffbd80; Value = 0xcdc8e10f6ba13c50; PC = 0x55555557a4fc *)
xor ymm10_0@uint64 ymm15_0 L0x7fffffffbd80;
xor ymm10_1@uint64 ymm15_1 L0x7fffffffbd88;
xor ymm10_2@uint64 ymm15_2 L0x7fffffffbd90;
xor ymm10_3@uint64 ymm15_3 L0x7fffffffbd98;
(* vpxor  %ymm5,%ymm2,%ymm2                        #! PC = 0x55555557a504 *)
xor ymm2_0@uint64 ymm2_0 ymm5_0;
xor ymm2_1@uint64 ymm2_1 ymm5_1;
xor ymm2_2@uint64 ymm2_2 ymm5_2;
xor ymm2_3@uint64 ymm2_3 ymm5_3;
(* vpxor  -0x310(%rbp),%ymm2,%ymm3                 #! EA = L0x7fffffffbc40; Value = 0xd31cfcdcd9c4cdb2; PC = 0x55555557a508 *)
xor ymm3_0@uint64 ymm2_0 L0x7fffffffbc40;
xor ymm3_1@uint64 ymm2_1 L0x7fffffffbc48;
xor ymm3_2@uint64 ymm2_2 L0x7fffffffbc50;
xor ymm3_3@uint64 ymm2_3 L0x7fffffffbc58;
(* vpxor  -0x1b0(%rbp),%ymm11,%ymm11               #! EA = L0x7fffffffbda0; Value = 0xc4e603913394eb11; PC = 0x55555557a510 *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbda0;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbda8;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbdb0;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbdb8;
(* vpsrlq $0x3f,%ymm14,%ymm4                       #! PC = 0x55555557a518 *)
shr ymm4_0 ymm14_0 0x3f@uint64;
shr ymm4_1 ymm14_1 0x3f@uint64;
shr ymm4_2 ymm14_2 0x3f@uint64;
shr ymm4_3 ymm14_3 0x3f@uint64;
(* vpsllq $0x1,%ymm13,%ymm5                        #! PC = 0x55555557a51e *)
shl ymm5_0 ymm13_0 0x1@uint64;
shl ymm5_1 ymm13_1 0x1@uint64;
shl ymm5_2 ymm13_2 0x1@uint64;
shl ymm5_3 ymm13_3 0x1@uint64;
(* vpxor  %ymm3,%ymm10,%ymm10                      #! PC = 0x55555557a524 *)
xor ymm10_0@uint64 ymm10_0 ymm3_0;
xor ymm10_1@uint64 ymm10_1 ymm3_1;
xor ymm10_2@uint64 ymm10_2 ymm3_2;
xor ymm10_3@uint64 ymm10_3 ymm3_3;
(* vpsllq $0x1,%ymm14,%ymm3                        #! PC = 0x55555557a528 *)
shl ymm3_0 ymm14_0 0x1@uint64;
shl ymm3_1 ymm14_1 0x1@uint64;
shl ymm3_2 ymm14_2 0x1@uint64;
shl ymm3_3 ymm14_3 0x1@uint64;
(* vpxor  -0x330(%rbp),%ymm10,%ymm10               #! EA = L0x7fffffffbc20; Value = 0xaad25dd9c221c271; PC = 0x55555557a52e *)
xor ymm10_0@uint64 ymm10_0 L0x7fffffffbc20;
xor ymm10_1@uint64 ymm10_1 L0x7fffffffbc28;
xor ymm10_2@uint64 ymm10_2 L0x7fffffffbc30;
xor ymm10_3@uint64 ymm10_3 L0x7fffffffbc38;
(* vpor   %ymm3,%ymm4,%ymm4                        #! PC = 0x55555557a536 *)
or ymm4_0@uint64 ymm4_0 ymm3_0;
or ymm4_1@uint64 ymm4_1 ymm3_1;
or ymm4_2@uint64 ymm4_2 ymm3_2;
or ymm4_3@uint64 ymm4_3 ymm3_3;
(* vpsrlq $0x3f,%ymm13,%ymm3                       #! PC = 0x55555557a53a *)
shr ymm3_0 ymm13_0 0x3f@uint64;
shr ymm3_1 ymm13_1 0x3f@uint64;
shr ymm3_2 ymm13_2 0x3f@uint64;
shr ymm3_3 ymm13_3 0x3f@uint64;
(* vpsllq $0x1,%ymm11,%ymm15                       #! PC = 0x55555557a540 *)
shl ymm15_0 ymm11_0 0x1@uint64;
shl ymm15_1 ymm11_1 0x1@uint64;
shl ymm15_2 ymm11_2 0x1@uint64;
shl ymm15_3 ymm11_3 0x1@uint64;
(* vpor   %ymm5,%ymm3,%ymm3                        #! PC = 0x55555557a546 *)
or ymm3_0@uint64 ymm3_0 ymm5_0;
or ymm3_1@uint64 ymm3_1 ymm5_1;
or ymm3_2@uint64 ymm3_2 ymm5_2;
or ymm3_3@uint64 ymm3_3 ymm5_3;
(* vpxor  %ymm10,%ymm4,%ymm4                       #! PC = 0x55555557a54a *)
xor ymm4_0@uint64 ymm4_0 ymm10_0;
xor ymm4_1@uint64 ymm4_1 ymm10_1;
xor ymm4_2@uint64 ymm4_2 ymm10_2;
xor ymm4_3@uint64 ymm4_3 ymm10_3;
(* vpsrlq $0x3f,%ymm11,%ymm5                       #! PC = 0x55555557a54f *)
shr ymm5_0 ymm11_0 0x3f@uint64;
shr ymm5_1 ymm11_1 0x3f@uint64;
shr ymm5_2 ymm11_2 0x3f@uint64;
shr ymm5_3 ymm11_3 0x3f@uint64;
(* vpxor  %ymm9,%ymm3,%ymm3                        #! PC = 0x55555557a555 *)
xor ymm3_0@uint64 ymm3_0 ymm9_0;
xor ymm3_1@uint64 ymm3_1 ymm9_1;
xor ymm3_2@uint64 ymm3_2 ymm9_2;
xor ymm3_3@uint64 ymm3_3 ymm9_3;
(* vpxor  %ymm4,%ymm0,%ymm0                        #! PC = 0x55555557a55a *)
xor ymm0_0@uint64 ymm0_0 ymm4_0;
xor ymm0_1@uint64 ymm0_1 ymm4_1;
xor ymm0_2@uint64 ymm0_2 ymm4_2;
xor ymm0_3@uint64 ymm0_3 ymm4_3;
(* vpor   %ymm15,%ymm5,%ymm5                       #! PC = 0x55555557a55e *)
or ymm5_0@uint64 ymm5_0 ymm15_0;
or ymm5_1@uint64 ymm5_1 ymm15_1;
or ymm5_2@uint64 ymm5_2 ymm15_2;
or ymm5_3@uint64 ymm5_3 ymm15_3;
(* vpxor  %ymm3,%ymm8,%ymm8                        #! PC = 0x55555557a563 *)
xor ymm8_0@uint64 ymm8_0 ymm3_0;
xor ymm8_1@uint64 ymm8_1 ymm3_1;
xor ymm8_2@uint64 ymm8_2 ymm3_2;
xor ymm8_3@uint64 ymm8_3 ymm3_3;
(* vpxor  %ymm14,%ymm5,%ymm5                       #! PC = 0x55555557a567 *)
xor ymm5_0@uint64 ymm5_0 ymm14_0;
xor ymm5_1@uint64 ymm5_1 ymm14_1;
xor ymm5_2@uint64 ymm5_2 ymm14_2;
xor ymm5_3@uint64 ymm5_3 ymm14_3;
(* vpsrlq $0x3f,%ymm10,%ymm14                      #! PC = 0x55555557a56c *)
shr ymm14_0 ymm10_0 0x3f@uint64;
shr ymm14_1 ymm10_1 0x3f@uint64;
shr ymm14_2 ymm10_2 0x3f@uint64;
shr ymm14_3 ymm10_3 0x3f@uint64;
(* vpsllq $0x1,%ymm10,%ymm10                       #! PC = 0x55555557a572 *)
shl ymm10_0 ymm10_0 0x1@uint64;
shl ymm10_1 ymm10_1 0x1@uint64;
shl ymm10_2 ymm10_2 0x1@uint64;
shl ymm10_3 ymm10_3 0x1@uint64;
(* vpxor  %ymm5,%ymm12,%ymm12                      #! PC = 0x55555557a578 *)
xor ymm12_0@uint64 ymm12_0 ymm5_0;
xor ymm12_1@uint64 ymm12_1 ymm5_1;
xor ymm12_2@uint64 ymm12_2 ymm5_2;
xor ymm12_3@uint64 ymm12_3 ymm5_3;
(* vpxor  %ymm5,%ymm1,%ymm1                        #! PC = 0x55555557a57c *)
xor ymm1_0@uint64 ymm1_0 ymm5_0;
xor ymm1_1@uint64 ymm1_1 ymm5_1;
xor ymm1_2@uint64 ymm1_2 ymm5_2;
xor ymm1_3@uint64 ymm1_3 ymm5_3;
(* vpor   %ymm10,%ymm14,%ymm10                     #! PC = 0x55555557a580 *)
or ymm10_0@uint64 ymm14_0 ymm10_0;
or ymm10_1@uint64 ymm14_1 ymm10_1;
or ymm10_2@uint64 ymm14_2 ymm10_2;
or ymm10_3@uint64 ymm14_3 ymm10_3;
(* vmovq  %r14,%xmm14                              #! PC = 0x55555557a585 *)
mov xmm14_0 r14;
mov xmm14_1 0@uint64;
(* vpxor  %ymm13,%ymm10,%ymm10                     #! PC = 0x55555557a58a *)
xor ymm10_0@uint64 ymm10_0 ymm13_0;
xor ymm10_1@uint64 ymm10_1 ymm13_1;
xor ymm10_2@uint64 ymm10_2 ymm13_2;
xor ymm10_3@uint64 ymm10_3 ymm13_3;
(* vpsrlq $0x3f,%ymm9,%ymm13                       #! PC = 0x55555557a58f *)
shr ymm13_0 ymm9_0 0x3f@uint64;
shr ymm13_1 ymm9_1 0x3f@uint64;
shr ymm13_2 ymm9_2 0x3f@uint64;
shr ymm13_3 ymm9_3 0x3f@uint64;
(* vpbroadcastq %xmm14,%ymm14                      #! PC = 0x55555557a595 *)
mov ymm14_0 xmm14_0;
mov ymm14_1 xmm14_0;
mov ymm14_2 xmm14_0;
mov ymm14_3 xmm14_0;
(* vpsllq $0x1,%ymm9,%ymm9                         #! PC = 0x55555557a59a *)
shl ymm9_0 ymm9_0 0x1@uint64;
shl ymm9_1 ymm9_1 0x1@uint64;
shl ymm9_2 ymm9_2 0x1@uint64;
shl ymm9_3 ymm9_3 0x1@uint64;
(* vpxor  %ymm10,%ymm7,%ymm7                       #! PC = 0x55555557a5a0 *)
xor ymm7_0@uint64 ymm7_0 ymm10_0;
xor ymm7_1@uint64 ymm7_1 ymm10_1;
xor ymm7_2@uint64 ymm7_2 ymm10_2;
xor ymm7_3@uint64 ymm7_3 ymm10_3;
(* vpor   %ymm9,%ymm13,%ymm9                       #! PC = 0x55555557a5a5 *)
or ymm9_0@uint64 ymm13_0 ymm9_0;
or ymm9_1@uint64 ymm13_1 ymm9_1;
or ymm9_2@uint64 ymm13_2 ymm9_2;
or ymm9_3@uint64 ymm13_3 ymm9_3;
(* vpsrlq $0x14,%ymm8,%ymm13                       #! PC = 0x55555557a5aa *)
shr ymm13_0 ymm8_0 0x14@uint64;
shr ymm13_1 ymm8_1 0x14@uint64;
shr ymm13_2 ymm8_2 0x14@uint64;
shr ymm13_3 ymm8_3 0x14@uint64;
(* vpsllq $0x2c,%ymm8,%ymm8                        #! PC = 0x55555557a5b0 *)
shl ymm8_0 ymm8_0 0x2c@uint64;
shl ymm8_1 ymm8_1 0x2c@uint64;
shl ymm8_2 ymm8_2 0x2c@uint64;
shl ymm8_3 ymm8_3 0x2c@uint64;
(* vpxor  %ymm11,%ymm9,%ymm11                      #! PC = 0x55555557a5b6 *)
xor ymm11_0@uint64 ymm9_0 ymm11_0;
xor ymm11_1@uint64 ymm9_1 ymm11_1;
xor ymm11_2@uint64 ymm9_2 ymm11_2;
xor ymm11_3@uint64 ymm9_3 ymm11_3;
(* vpxor  -0x90(%rbp),%ymm4,%ymm9                  #! EA = L0x7fffffffbec0; Value = 0x736d2c1d55c4ef73; PC = 0x55555557a5bb *)
xor ymm9_0@uint64 ymm4_0 L0x7fffffffbec0;
xor ymm9_1@uint64 ymm4_1 L0x7fffffffbec8;
xor ymm9_2@uint64 ymm4_2 L0x7fffffffbed0;
xor ymm9_3@uint64 ymm4_3 L0x7fffffffbed8;
(* vpor   %ymm8,%ymm13,%ymm8                       #! PC = 0x55555557a5c3 *)
or ymm8_0@uint64 ymm13_0 ymm8_0;
or ymm8_1@uint64 ymm13_1 ymm8_1;
or ymm8_2@uint64 ymm13_2 ymm8_2;
or ymm8_3@uint64 ymm13_3 ymm8_3;
(* vpsrlq $0x15,%ymm12,%ymm13                      #! PC = 0x55555557a5c8 *)
shr ymm13_0 ymm12_0 0x15@uint64;
shr ymm13_1 ymm12_1 0x15@uint64;
shr ymm13_2 ymm12_2 0x15@uint64;
shr ymm13_3 ymm12_3 0x15@uint64;
(* vpxor  %ymm11,%ymm2,%ymm2                       #! PC = 0x55555557a5ce *)
xor ymm2_0@uint64 ymm2_0 ymm11_0;
xor ymm2_1@uint64 ymm2_1 ymm11_1;
xor ymm2_2@uint64 ymm2_2 ymm11_2;
xor ymm2_3@uint64 ymm2_3 ymm11_3;
(* vpsllq $0x2b,%ymm12,%ymm12                      #! PC = 0x55555557a5d3 *)
shl ymm12_0 ymm12_0 0x2b@uint64;
shl ymm12_1 ymm12_1 0x2b@uint64;
shl ymm12_2 ymm12_2 0x2b@uint64;
shl ymm12_3 ymm12_3 0x2b@uint64;
(* vpor   %ymm12,%ymm13,%ymm12                     #! PC = 0x55555557a5d9 *)
or ymm12_0@uint64 ymm13_0 ymm12_0;
or ymm12_1@uint64 ymm13_1 ymm12_1;
or ymm12_2@uint64 ymm13_2 ymm12_2;
or ymm12_3@uint64 ymm13_3 ymm12_3;
(* vpandn %ymm12,%ymm8,%ymm13                      #! PC = 0x55555557a5de *)
not ymm8_0n@uint64 ymm8_0;
and ymm13_0@uint64 ymm8_0n ymm12_0;
not ymm8_1n@uint64 ymm8_1;
and ymm13_1@uint64 ymm8_1n ymm12_1;
not ymm8_2n@uint64 ymm8_2;
and ymm13_2@uint64 ymm8_2n ymm12_2;
not ymm8_3n@uint64 ymm8_3;
and ymm13_3@uint64 ymm8_3n ymm12_3;
(* vpxor  %ymm14,%ymm13,%ymm13                     #! PC = 0x55555557a5e3 *)
xor ymm13_0@uint64 ymm13_0 ymm14_0;
xor ymm13_1@uint64 ymm13_1 ymm14_1;
xor ymm13_2@uint64 ymm13_2 ymm14_2;
xor ymm13_3@uint64 ymm13_3 ymm14_3;
(* vpxor  %ymm9,%ymm13,%ymm13                      #! PC = 0x55555557a5e8 *)
xor ymm13_0@uint64 ymm13_0 ymm9_0;
xor ymm13_1@uint64 ymm13_1 ymm9_1;
xor ymm13_2@uint64 ymm13_2 ymm9_2;
xor ymm13_3@uint64 ymm13_3 ymm9_3;
(* vmovdqa %ymm13,-0x1f0(%rbp)                     #! EA = L0x7fffffffbd60; PC = 0x55555557a5ed *)
mov L0x7fffffffbd60 ymm13_0;
mov L0x7fffffffbd68 ymm13_1;
mov L0x7fffffffbd70 ymm13_2;
mov L0x7fffffffbd78 ymm13_3;
(* vpsrlq $0x2b,%ymm7,%ymm13                       #! PC = 0x55555557a5f5 *)
shr ymm13_0 ymm7_0 0x2b@uint64;
shr ymm13_1 ymm7_1 0x2b@uint64;
shr ymm13_2 ymm7_2 0x2b@uint64;
shr ymm13_3 ymm7_3 0x2b@uint64;
(* vpsllq $0x15,%ymm7,%ymm7                        #! PC = 0x55555557a5fa *)
shl ymm7_0 ymm7_0 0x15@uint64;
shl ymm7_1 ymm7_1 0x15@uint64;
shl ymm7_2 ymm7_2 0x15@uint64;
shl ymm7_3 ymm7_3 0x15@uint64;
(* vpor   %ymm7,%ymm13,%ymm7                       #! PC = 0x55555557a5ff *)
or ymm7_0@uint64 ymm13_0 ymm7_0;
or ymm7_1@uint64 ymm13_1 ymm7_1;
or ymm7_2@uint64 ymm13_2 ymm7_2;
or ymm7_3@uint64 ymm13_3 ymm7_3;
(* vpandn %ymm7,%ymm12,%ymm13                      #! PC = 0x55555557a603 *)
not ymm12_0n@uint64 ymm12_0;
and ymm13_0@uint64 ymm12_0n ymm7_0;
not ymm12_1n@uint64 ymm12_1;
and ymm13_1@uint64 ymm12_1n ymm7_1;
not ymm12_2n@uint64 ymm12_2;
and ymm13_2@uint64 ymm12_2n ymm7_2;
not ymm12_3n@uint64 ymm12_3;
and ymm13_3@uint64 ymm12_3n ymm7_3;
(* vpxor  %ymm8,%ymm13,%ymm13                      #! PC = 0x55555557a607 *)
xor ymm13_0@uint64 ymm13_0 ymm8_0;
xor ymm13_1@uint64 ymm13_1 ymm8_1;
xor ymm13_2@uint64 ymm13_2 ymm8_2;
xor ymm13_3@uint64 ymm13_3 ymm8_3;
(* vmovdqa %ymm13,%ymm14                           #! PC = 0x55555557a60c *)
mov ymm14_0 ymm13_0;
mov ymm14_1 ymm13_1;
mov ymm14_2 ymm13_2;
mov ymm14_3 ymm13_3;
(* vmovdqa %ymm13,-0x3b0(%rbp)                     #! EA = L0x7fffffffbba0; PC = 0x55555557a611 *)
mov L0x7fffffffbba0 ymm13_0;
mov L0x7fffffffbba8 ymm13_1;
mov L0x7fffffffbbb0 ymm13_2;
mov L0x7fffffffbbb8 ymm13_3;
(* vpsrlq $0x32,%ymm2,%ymm13                       #! PC = 0x55555557a619 *)
shr ymm13_0 ymm2_0 0x32@uint64;
shr ymm13_1 ymm2_1 0x32@uint64;
shr ymm13_2 ymm2_2 0x32@uint64;
shr ymm13_3 ymm2_3 0x32@uint64;
(* vpsllq $0xe,%ymm2,%ymm2                         #! PC = 0x55555557a61e *)
shl ymm2_0 ymm2_0 0xe@uint64;
shl ymm2_1 ymm2_1 0xe@uint64;
shl ymm2_2 ymm2_2 0xe@uint64;
shl ymm2_3 ymm2_3 0xe@uint64;
(* vpor   %ymm2,%ymm13,%ymm2                       #! PC = 0x55555557a623 *)
or ymm2_0@uint64 ymm13_0 ymm2_0;
or ymm2_1@uint64 ymm13_1 ymm2_1;
or ymm2_2@uint64 ymm13_2 ymm2_2;
or ymm2_3@uint64 ymm13_3 ymm2_3;
(* vpandn %ymm2,%ymm7,%ymm15                       #! PC = 0x55555557a627 *)
not ymm7_0n@uint64 ymm7_0;
and ymm15_0@uint64 ymm7_0n ymm2_0;
not ymm7_1n@uint64 ymm7_1;
and ymm15_1@uint64 ymm7_1n ymm2_1;
not ymm7_2n@uint64 ymm7_2;
and ymm15_2@uint64 ymm7_2n ymm2_2;
not ymm7_3n@uint64 ymm7_3;
and ymm15_3@uint64 ymm7_3n ymm2_3;
(* vpxor  %ymm12,%ymm15,%ymm12                     #! PC = 0x55555557a62b *)
xor ymm12_0@uint64 ymm15_0 ymm12_0;
xor ymm12_1@uint64 ymm15_1 ymm12_1;
xor ymm12_2@uint64 ymm15_2 ymm12_2;
xor ymm12_3@uint64 ymm15_3 ymm12_3;
(* vmovdqa %ymm12,-0x110(%rbp)                     #! EA = L0x7fffffffbe40; PC = 0x55555557a630 *)
mov L0x7fffffffbe40 ymm12_0;
mov L0x7fffffffbe48 ymm12_1;
mov L0x7fffffffbe50 ymm12_2;
mov L0x7fffffffbe58 ymm12_3;
(* vpandn %ymm9,%ymm2,%ymm12                       #! PC = 0x55555557a638 *)
not ymm2_0n@uint64 ymm2_0;
and ymm12_0@uint64 ymm2_0n ymm9_0;
not ymm2_1n@uint64 ymm2_1;
and ymm12_1@uint64 ymm2_1n ymm9_1;
not ymm2_2n@uint64 ymm2_2;
and ymm12_2@uint64 ymm2_2n ymm9_2;
not ymm2_3n@uint64 ymm2_3;
and ymm12_3@uint64 ymm2_3n ymm9_3;
(* vpandn %ymm8,%ymm9,%ymm9                        #! PC = 0x55555557a63d *)
not ymm9_0n@uint64 ymm9_0;
and ymm9_0@uint64 ymm9_0n ymm8_0;
not ymm9_1n@uint64 ymm9_1;
and ymm9_1@uint64 ymm9_1n ymm8_1;
not ymm9_2n@uint64 ymm9_2;
and ymm9_2@uint64 ymm9_2n ymm8_2;
not ymm9_3n@uint64 ymm9_3;
and ymm9_3@uint64 ymm9_3n ymm8_3;
(* vpxor  %ymm2,%ymm9,%ymm8                        #! PC = 0x55555557a642 *)
xor ymm8_0@uint64 ymm9_0 ymm2_0;
xor ymm8_1@uint64 ymm9_1 ymm2_1;
xor ymm8_2@uint64 ymm9_2 ymm2_2;
xor ymm8_3@uint64 ymm9_3 ymm2_3;
(* vpxor  %ymm7,%ymm12,%ymm12                      #! PC = 0x55555557a646 *)
xor ymm12_0@uint64 ymm12_0 ymm7_0;
xor ymm12_1@uint64 ymm12_1 ymm7_1;
xor ymm12_2@uint64 ymm12_2 ymm7_2;
xor ymm12_3@uint64 ymm12_3 ymm7_3;
(* vpxor  -0x170(%rbp),%ymm10,%ymm7                #! EA = L0x7fffffffbde0; Value = 0x5c9b10c4952fd875; PC = 0x55555557a64a *)
xor ymm7_0@uint64 ymm10_0 L0x7fffffffbde0;
xor ymm7_1@uint64 ymm10_1 L0x7fffffffbde8;
xor ymm7_2@uint64 ymm10_2 L0x7fffffffbdf0;
xor ymm7_3@uint64 ymm10_3 L0x7fffffffbdf8;
(* vmovdqa %ymm8,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x55555557a652 *)
mov L0x7fffffffbf00 ymm8_0;
mov L0x7fffffffbf08 ymm8_1;
mov L0x7fffffffbf10 ymm8_2;
mov L0x7fffffffbf18 ymm8_3;
(* vpxor  -0x2d0(%rbp),%ymm11,%ymm8                #! EA = L0x7fffffffbc80; Value = 0xa4e4fc5a17f63732; PC = 0x55555557a657 *)
xor ymm8_0@uint64 ymm11_0 L0x7fffffffbc80;
xor ymm8_1@uint64 ymm11_1 L0x7fffffffbc88;
xor ymm8_2@uint64 ymm11_2 L0x7fffffffbc90;
xor ymm8_3@uint64 ymm11_3 L0x7fffffffbc98;
(* vpxor  -0x190(%rbp),%ymm4,%ymm9                 #! EA = L0x7fffffffbdc0; Value = 0xe14da812846248af; PC = 0x55555557a65f *)
xor ymm9_0@uint64 ymm4_0 L0x7fffffffbdc0;
xor ymm9_1@uint64 ymm4_1 L0x7fffffffbdc8;
xor ymm9_2@uint64 ymm4_2 L0x7fffffffbdd0;
xor ymm9_3@uint64 ymm4_3 L0x7fffffffbdd8;
(* vpsrlq $0x24,%ymm7,%ymm2                        #! PC = 0x55555557a667 *)
shr ymm2_0 ymm7_0 0x24@uint64;
shr ymm2_1 ymm7_1 0x24@uint64;
shr ymm2_2 ymm7_2 0x24@uint64;
shr ymm2_3 ymm7_3 0x24@uint64;
(* vpsllq $0x1c,%ymm7,%ymm7                        #! PC = 0x55555557a66c *)
shl ymm7_0 ymm7_0 0x1c@uint64;
shl ymm7_1 ymm7_1 0x1c@uint64;
shl ymm7_2 ymm7_2 0x1c@uint64;
shl ymm7_3 ymm7_3 0x1c@uint64;
(* vmovdqa %ymm12,-0x130(%rbp)                     #! EA = L0x7fffffffbe20; PC = 0x55555557a671 *)
mov L0x7fffffffbe20 ymm12_0;
mov L0x7fffffffbe28 ymm12_1;
mov L0x7fffffffbe30 ymm12_2;
mov L0x7fffffffbe38 ymm12_3;
(* vpor   %ymm7,%ymm2,%ymm7                        #! PC = 0x55555557a679 *)
or ymm7_0@uint64 ymm2_0 ymm7_0;
or ymm7_1@uint64 ymm2_1 ymm7_1;
or ymm7_2@uint64 ymm2_2 ymm7_2;
or ymm7_3@uint64 ymm2_3 ymm7_3;
(* vpsrlq $0x2c,%ymm8,%ymm2                        #! PC = 0x55555557a67d *)
shr ymm2_0 ymm8_0 0x2c@uint64;
shr ymm2_1 ymm8_1 0x2c@uint64;
shr ymm2_2 ymm8_2 0x2c@uint64;
shr ymm2_3 ymm8_3 0x2c@uint64;
(* vpsllq $0x14,%ymm8,%ymm8                        #! PC = 0x55555557a683 *)
shl ymm8_0 ymm8_0 0x14@uint64;
shl ymm8_1 ymm8_1 0x14@uint64;
shl ymm8_2 ymm8_2 0x14@uint64;
shl ymm8_3 ymm8_3 0x14@uint64;
(* vpor   %ymm8,%ymm2,%ymm2                        #! PC = 0x55555557a689 *)
or ymm2_0@uint64 ymm2_0 ymm8_0;
or ymm2_1@uint64 ymm2_1 ymm8_1;
or ymm2_2@uint64 ymm2_2 ymm8_2;
or ymm2_3@uint64 ymm2_3 ymm8_3;
(* vpsrlq $0x3d,%ymm9,%ymm8                        #! PC = 0x55555557a68e *)
shr ymm8_0 ymm9_0 0x3d@uint64;
shr ymm8_1 ymm9_1 0x3d@uint64;
shr ymm8_2 ymm9_2 0x3d@uint64;
shr ymm8_3 ymm9_3 0x3d@uint64;
(* vpsllq $0x3,%ymm9,%ymm9                         #! PC = 0x55555557a694 *)
shl ymm9_0 ymm9_0 0x3@uint64;
shl ymm9_1 ymm9_1 0x3@uint64;
shl ymm9_2 ymm9_2 0x3@uint64;
shl ymm9_3 ymm9_3 0x3@uint64;
(* vpor   %ymm9,%ymm8,%ymm8                        #! PC = 0x55555557a69a *)
or ymm8_0@uint64 ymm8_0 ymm9_0;
or ymm8_1@uint64 ymm8_1 ymm9_1;
or ymm8_2@uint64 ymm8_2 ymm9_2;
or ymm8_3@uint64 ymm8_3 ymm9_3;
(* vpandn %ymm8,%ymm2,%ymm9                        #! PC = 0x55555557a69f *)
not ymm2_0n@uint64 ymm2_0;
and ymm9_0@uint64 ymm2_0n ymm8_0;
not ymm2_1n@uint64 ymm2_1;
and ymm9_1@uint64 ymm2_1n ymm8_1;
not ymm2_2n@uint64 ymm2_2;
and ymm9_2@uint64 ymm2_2n ymm8_2;
not ymm2_3n@uint64 ymm2_3;
and ymm9_3@uint64 ymm2_3n ymm8_3;
(* vpxor  %ymm7,%ymm9,%ymm9                        #! PC = 0x55555557a6a4 *)
xor ymm9_0@uint64 ymm9_0 ymm7_0;
xor ymm9_1@uint64 ymm9_1 ymm7_1;
xor ymm9_2@uint64 ymm9_2 ymm7_2;
xor ymm9_3@uint64 ymm9_3 ymm7_3;
(* vmovdqa %ymm9,-0x150(%rbp)                      #! EA = L0x7fffffffbe00; PC = 0x55555557a6a8 *)
mov L0x7fffffffbe00 ymm9_0;
mov L0x7fffffffbe08 ymm9_1;
mov L0x7fffffffbe10 ymm9_2;
mov L0x7fffffffbe18 ymm9_3;
(* vpxor  %ymm3,%ymm6,%ymm9                        #! PC = 0x55555557a6b0 *)
xor ymm9_0@uint64 ymm6_0 ymm3_0;
xor ymm9_1@uint64 ymm6_1 ymm3_1;
xor ymm9_2@uint64 ymm6_2 ymm3_2;
xor ymm9_3@uint64 ymm6_3 ymm3_3;
(* vpsrlq $0x13,%ymm9,%ymm12                       #! PC = 0x55555557a6b4 *)
shr ymm12_0 ymm9_0 0x13@uint64;
shr ymm12_1 ymm9_1 0x13@uint64;
shr ymm12_2 ymm9_2 0x13@uint64;
shr ymm12_3 ymm9_3 0x13@uint64;
(* vpsllq $0x2d,%ymm9,%ymm6                        #! PC = 0x55555557a6ba *)
shl ymm6_0 ymm9_0 0x2d@uint64;
shl ymm6_1 ymm9_1 0x2d@uint64;
shl ymm6_2 ymm9_2 0x2d@uint64;
shl ymm6_3 ymm9_3 0x2d@uint64;
(* vpor   %ymm6,%ymm12,%ymm6                       #! PC = 0x55555557a6c0 *)
or ymm6_0@uint64 ymm12_0 ymm6_0;
or ymm6_1@uint64 ymm12_1 ymm6_1;
or ymm6_2@uint64 ymm12_2 ymm6_2;
or ymm6_3@uint64 ymm12_3 ymm6_3;
(* vpandn %ymm6,%ymm8,%ymm9                        #! PC = 0x55555557a6c4 *)
not ymm8_0n@uint64 ymm8_0;
and ymm9_0@uint64 ymm8_0n ymm6_0;
not ymm8_1n@uint64 ymm8_1;
and ymm9_1@uint64 ymm8_1n ymm6_1;
not ymm8_2n@uint64 ymm8_2;
and ymm9_2@uint64 ymm8_2n ymm6_2;
not ymm8_3n@uint64 ymm8_3;
and ymm9_3@uint64 ymm8_3n ymm6_3;
(* vpxor  %ymm2,%ymm9,%ymm9                        #! PC = 0x55555557a6c8 *)
xor ymm9_0@uint64 ymm9_0 ymm2_0;
xor ymm9_1@uint64 ymm9_1 ymm2_1;
xor ymm9_2@uint64 ymm9_2 ymm2_2;
xor ymm9_3@uint64 ymm9_3 ymm2_3;
(* vmovdqa %ymm9,-0x170(%rbp)                      #! EA = L0x7fffffffbde0; PC = 0x55555557a6cc *)
mov L0x7fffffffbde0 ymm9_0;
mov L0x7fffffffbde8 ymm9_1;
mov L0x7fffffffbdf0 ymm9_2;
mov L0x7fffffffbdf8 ymm9_3;
(* vpsrlq $0x3,%ymm1,%ymm9                         #! PC = 0x55555557a6d4 *)
shr ymm9_0 ymm1_0 0x3@uint64;
shr ymm9_1 ymm1_1 0x3@uint64;
shr ymm9_2 ymm1_2 0x3@uint64;
shr ymm9_3 ymm1_3 0x3@uint64;
(* vpsllq $0x3d,%ymm1,%ymm1                        #! PC = 0x55555557a6d9 *)
shl ymm1_0 ymm1_0 0x3d@uint64;
shl ymm1_1 ymm1_1 0x3d@uint64;
shl ymm1_2 ymm1_2 0x3d@uint64;
shl ymm1_3 ymm1_3 0x3d@uint64;
(* vpor   %ymm1,%ymm9,%ymm1                        #! PC = 0x55555557a6de *)
or ymm1_0@uint64 ymm9_0 ymm1_0;
or ymm1_1@uint64 ymm9_1 ymm1_1;
or ymm1_2@uint64 ymm9_2 ymm1_2;
or ymm1_3@uint64 ymm9_3 ymm1_3;
(* vpandn %ymm1,%ymm6,%ymm9                        #! PC = 0x55555557a6e2 *)
not ymm6_0n@uint64 ymm6_0;
and ymm9_0@uint64 ymm6_0n ymm1_0;
not ymm6_1n@uint64 ymm6_1;
and ymm9_1@uint64 ymm6_1n ymm1_1;
not ymm6_2n@uint64 ymm6_2;
and ymm9_2@uint64 ymm6_2n ymm1_2;
not ymm6_3n@uint64 ymm6_3;
and ymm9_3@uint64 ymm6_3n ymm1_3;
(* vpxor  %ymm8,%ymm9,%ymm8                        #! PC = 0x55555557a6e6 *)
xor ymm8_0@uint64 ymm9_0 ymm8_0;
xor ymm8_1@uint64 ymm9_1 ymm8_1;
xor ymm8_2@uint64 ymm9_2 ymm8_2;
xor ymm8_3@uint64 ymm9_3 ymm8_3;
(* vmovdqa %ymm8,-0x190(%rbp)                      #! EA = L0x7fffffffbdc0; PC = 0x55555557a6eb *)
mov L0x7fffffffbdc0 ymm8_0;
mov L0x7fffffffbdc8 ymm8_1;
mov L0x7fffffffbdd0 ymm8_2;
mov L0x7fffffffbdd8 ymm8_3;
(* vpandn %ymm7,%ymm1,%ymm8                        #! PC = 0x55555557a6f3 *)
not ymm1_0n@uint64 ymm1_0;
and ymm8_0@uint64 ymm1_0n ymm7_0;
not ymm1_1n@uint64 ymm1_1;
and ymm8_1@uint64 ymm1_1n ymm7_1;
not ymm1_2n@uint64 ymm1_2;
and ymm8_2@uint64 ymm1_2n ymm7_2;
not ymm1_3n@uint64 ymm1_3;
and ymm8_3@uint64 ymm1_3n ymm7_3;
(* vpandn %ymm2,%ymm7,%ymm7                        #! PC = 0x55555557a6f7 *)
not ymm7_0n@uint64 ymm7_0;
and ymm7_0@uint64 ymm7_0n ymm2_0;
not ymm7_1n@uint64 ymm7_1;
and ymm7_1@uint64 ymm7_1n ymm2_1;
not ymm7_2n@uint64 ymm7_2;
and ymm7_2@uint64 ymm7_2n ymm2_2;
not ymm7_3n@uint64 ymm7_3;
and ymm7_3@uint64 ymm7_3n ymm2_3;
(* vpxor  %ymm1,%ymm7,%ymm2                        #! PC = 0x55555557a6fb *)
xor ymm2_0@uint64 ymm7_0 ymm1_0;
xor ymm2_1@uint64 ymm7_1 ymm1_1;
xor ymm2_2@uint64 ymm7_2 ymm1_2;
xor ymm2_3@uint64 ymm7_3 ymm1_3;
(* vpxor  -0xb0(%rbp),%ymm5,%ymm7                  #! EA = L0x7fffffffbea0; Value = 0xe4af9f9f755c1079; PC = 0x55555557a6ff *)
xor ymm7_0@uint64 ymm5_0 L0x7fffffffbea0;
xor ymm7_1@uint64 ymm5_1 L0x7fffffffbea8;
xor ymm7_2@uint64 ymm5_2 L0x7fffffffbeb0;
xor ymm7_3@uint64 ymm5_3 L0x7fffffffbeb8;
(* vpxor  %ymm6,%ymm8,%ymm12                       #! PC = 0x55555557a707 *)
xor ymm12_0@uint64 ymm8_0 ymm6_0;
xor ymm12_1@uint64 ymm8_1 ymm6_1;
xor ymm12_2@uint64 ymm8_2 ymm6_2;
xor ymm12_3@uint64 ymm8_3 ymm6_3;
(* vmovdqa %ymm2,-0x90(%rbp)                       #! EA = L0x7fffffffbec0; PC = 0x55555557a70b *)
mov L0x7fffffffbec0 ymm2_0;
mov L0x7fffffffbec8 ymm2_1;
mov L0x7fffffffbed0 ymm2_2;
mov L0x7fffffffbed8 ymm2_3;
(* vpxor  -0xd0(%rbp),%ymm3,%ymm2                  #! EA = L0x7fffffffbe80; Value = 0x2e734b02ce704378; PC = 0x55555557a713 *)
xor ymm2_0@uint64 ymm3_0 L0x7fffffffbe80;
xor ymm2_1@uint64 ymm3_1 L0x7fffffffbe88;
xor ymm2_2@uint64 ymm3_2 L0x7fffffffbe90;
xor ymm2_3@uint64 ymm3_3 L0x7fffffffbe98;
(* vpxor  -0x1b0(%rbp),%ymm10,%ymm6                #! EA = L0x7fffffffbda0; Value = 0xc4e603913394eb11; PC = 0x55555557a71b *)
xor ymm6_0@uint64 ymm10_0 L0x7fffffffbda0;
xor ymm6_1@uint64 ymm10_1 L0x7fffffffbda8;
xor ymm6_2@uint64 ymm10_2 L0x7fffffffbdb0;
xor ymm6_3@uint64 ymm10_3 L0x7fffffffbdb8;
(* vpsllq $0x6,%ymm7,%ymm9                         #! PC = 0x55555557a723 *)
shl ymm9_0 ymm7_0 0x6@uint64;
shl ymm9_1 ymm7_1 0x6@uint64;
shl ymm9_2 ymm7_2 0x6@uint64;
shl ymm9_3 ymm7_3 0x6@uint64;
(* vmovdqa %ymm12,-0x70(%rbp)                      #! EA = L0x7fffffffbee0; PC = 0x55555557a728 *)
mov L0x7fffffffbee0 ymm12_0;
mov L0x7fffffffbee8 ymm12_1;
mov L0x7fffffffbef0 ymm12_2;
mov L0x7fffffffbef8 ymm12_3;
(* vpxor  -0x330(%rbp),%ymm11,%ymm12               #! EA = L0x7fffffffbc20; Value = 0xaad25dd9c221c271; PC = 0x55555557a72d *)
xor ymm12_0@uint64 ymm11_0 L0x7fffffffbc20;
xor ymm12_1@uint64 ymm11_1 L0x7fffffffbc28;
xor ymm12_2@uint64 ymm11_2 L0x7fffffffbc30;
xor ymm12_3@uint64 ymm11_3 L0x7fffffffbc38;
(* vpsrlq $0x3f,%ymm2,%ymm1                        #! PC = 0x55555557a735 *)
shr ymm1_0 ymm2_0 0x3f@uint64;
shr ymm1_1 ymm2_1 0x3f@uint64;
shr ymm1_2 ymm2_2 0x3f@uint64;
shr ymm1_3 ymm2_3 0x3f@uint64;
(* vpsllq $0x1,%ymm2,%ymm2                         #! PC = 0x55555557a73a *)
shl ymm2_0 ymm2_0 0x1@uint64;
shl ymm2_1 ymm2_1 0x1@uint64;
shl ymm2_2 ymm2_2 0x1@uint64;
shl ymm2_3 ymm2_3 0x1@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x55555557a73f *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpsrlq $0x3a,%ymm7,%ymm2                        #! PC = 0x55555557a743 *)
shr ymm2_0 ymm7_0 0x3a@uint64;
shr ymm2_1 ymm7_1 0x3a@uint64;
shr ymm2_2 ymm7_2 0x3a@uint64;
shr ymm2_3 ymm7_3 0x3a@uint64;
(* vpshufb 0x537af(%rip),%ymm12,%ymm12        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x55555557a748 *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm12_0, ymm12_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm12_2, ymm12_3, tmp_2, tmp_3);
mov ymm12_0 tmp_0;
mov ymm12_1 tmp_1;
mov ymm12_2 tmp_2;
mov ymm12_3 tmp_3;
(* vpor   %ymm9,%ymm2,%ymm9                        #! PC = 0x55555557a751 *)
or ymm9_0@uint64 ymm2_0 ymm9_0;
or ymm9_1@uint64 ymm2_1 ymm9_1;
or ymm9_2@uint64 ymm2_2 ymm9_2;
or ymm9_3@uint64 ymm2_3 ymm9_3;
(* vpsrlq $0x27,%ymm6,%ymm13                       #! PC = 0x55555557a756 *)
shr ymm13_0 ymm6_0 0x27@uint64;
shr ymm13_1 ymm6_1 0x27@uint64;
shr ymm13_2 ymm6_2 0x27@uint64;
shr ymm13_3 ymm6_3 0x27@uint64;
(* vpxor  -0x230(%rbp),%ymm3,%ymm7                 #! EA = L0x7fffffffbd20; Value = 0xd3376cb3ecec921b; PC = 0x55555557a75b *)
xor ymm7_0@uint64 ymm3_0 L0x7fffffffbd20;
xor ymm7_1@uint64 ymm3_1 L0x7fffffffbd28;
xor ymm7_2@uint64 ymm3_2 L0x7fffffffbd30;
xor ymm7_3@uint64 ymm3_3 L0x7fffffffbd38;
(* vpsllq $0x19,%ymm6,%ymm2                        #! PC = 0x55555557a763 *)
shl ymm2_0 ymm6_0 0x19@uint64;
shl ymm2_1 ymm6_1 0x19@uint64;
shl ymm2_2 ymm6_2 0x19@uint64;
shl ymm2_3 ymm6_3 0x19@uint64;
(* vpor   %ymm2,%ymm13,%ymm2                       #! PC = 0x55555557a768 *)
or ymm2_0@uint64 ymm13_0 ymm2_0;
or ymm2_1@uint64 ymm13_1 ymm2_1;
or ymm2_2@uint64 ymm13_2 ymm2_2;
or ymm2_3@uint64 ymm13_3 ymm2_3;
(* vpandn %ymm2,%ymm9,%ymm6                        #! PC = 0x55555557a76c *)
not ymm9_0n@uint64 ymm9_0;
and ymm6_0@uint64 ymm9_0n ymm2_0;
not ymm9_1n@uint64 ymm9_1;
and ymm6_1@uint64 ymm9_1n ymm2_1;
not ymm9_2n@uint64 ymm9_2;
and ymm6_2@uint64 ymm9_2n ymm2_2;
not ymm9_3n@uint64 ymm9_3;
and ymm6_3@uint64 ymm9_3n ymm2_3;
(* vpandn %ymm12,%ymm2,%ymm8                       #! PC = 0x55555557a770 *)
not ymm2_0n@uint64 ymm2_0;
and ymm8_0@uint64 ymm2_0n ymm12_0;
not ymm2_1n@uint64 ymm2_1;
and ymm8_1@uint64 ymm2_1n ymm12_1;
not ymm2_2n@uint64 ymm2_2;
and ymm8_2@uint64 ymm2_2n ymm12_2;
not ymm2_3n@uint64 ymm2_3;
and ymm8_3@uint64 ymm2_3n ymm12_3;
(* vpxor  %ymm1,%ymm6,%ymm13                       #! PC = 0x55555557a775 *)
xor ymm13_0@uint64 ymm6_0 ymm1_0;
xor ymm13_1@uint64 ymm6_1 ymm1_1;
xor ymm13_2@uint64 ymm6_2 ymm1_2;
xor ymm13_3@uint64 ymm6_3 ymm1_3;
(* vpsrlq $0x2e,%ymm0,%ymm6                        #! PC = 0x55555557a779 *)
shr ymm6_0 ymm0_0 0x2e@uint64;
shr ymm6_1 ymm0_1 0x2e@uint64;
shr ymm6_2 ymm0_2 0x2e@uint64;
shr ymm6_3 ymm0_3 0x2e@uint64;
(* vpxor  %ymm9,%ymm8,%ymm15                       #! PC = 0x55555557a77e *)
xor ymm15_0@uint64 ymm8_0 ymm9_0;
xor ymm15_1@uint64 ymm8_1 ymm9_1;
xor ymm15_2@uint64 ymm8_2 ymm9_2;
xor ymm15_3@uint64 ymm8_3 ymm9_3;
(* vpsllq $0x12,%ymm0,%ymm0                        #! PC = 0x55555557a783 *)
shl ymm0_0 ymm0_0 0x12@uint64;
shl ymm0_1 ymm0_1 0x12@uint64;
shl ymm0_2 ymm0_2 0x12@uint64;
shl ymm0_3 ymm0_3 0x12@uint64;
(* vmovdqa %ymm13,-0x1b0(%rbp)                     #! EA = L0x7fffffffbda0; PC = 0x55555557a788 *)
mov L0x7fffffffbda0 ymm13_0;
mov L0x7fffffffbda8 ymm13_1;
mov L0x7fffffffbdb0 ymm13_2;
mov L0x7fffffffbdb8 ymm13_3;
(* vpxor  -0x210(%rbp),%ymm4,%ymm8                 #! EA = L0x7fffffffbd40; Value = 0xf5f58fa9fc4545d6; PC = 0x55555557a790 *)
xor ymm8_0@uint64 ymm4_0 L0x7fffffffbd40;
xor ymm8_1@uint64 ymm4_1 L0x7fffffffbd48;
xor ymm8_2@uint64 ymm4_2 L0x7fffffffbd50;
xor ymm8_3@uint64 ymm4_3 L0x7fffffffbd58;
(* vpor   %ymm0,%ymm6,%ymm0                        #! PC = 0x55555557a798 *)
or ymm0_0@uint64 ymm6_0 ymm0_0;
or ymm0_1@uint64 ymm6_1 ymm0_1;
or ymm0_2@uint64 ymm6_2 ymm0_2;
or ymm0_3@uint64 ymm6_3 ymm0_3;
(* vmovdqa %ymm15,-0x3d0(%rbp)                     #! EA = L0x7fffffffbb80; PC = 0x55555557a79c *)
mov L0x7fffffffbb80 ymm15_0;
mov L0x7fffffffbb88 ymm15_1;
mov L0x7fffffffbb90 ymm15_2;
mov L0x7fffffffbb98 ymm15_3;
(* vpandn %ymm0,%ymm12,%ymm13                      #! PC = 0x55555557a7a4 *)
not ymm12_0n@uint64 ymm12_0;
and ymm13_0@uint64 ymm12_0n ymm0_0;
not ymm12_1n@uint64 ymm12_1;
and ymm13_1@uint64 ymm12_1n ymm0_1;
not ymm12_2n@uint64 ymm12_2;
and ymm13_2@uint64 ymm12_2n ymm0_2;
not ymm12_3n@uint64 ymm12_3;
and ymm13_3@uint64 ymm12_3n ymm0_3;
(* vpxor  %ymm2,%ymm13,%ymm13                      #! PC = 0x55555557a7a8 *)
xor ymm13_0@uint64 ymm13_0 ymm2_0;
xor ymm13_1@uint64 ymm13_1 ymm2_1;
xor ymm13_2@uint64 ymm13_2 ymm2_2;
xor ymm13_3@uint64 ymm13_3 ymm2_3;
(* vpandn %ymm1,%ymm0,%ymm2                        #! PC = 0x55555557a7ac *)
not ymm0_0n@uint64 ymm0_0;
and ymm2_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm2_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm2_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm2_3@uint64 ymm0_3n ymm1_3;
(* vpandn %ymm9,%ymm1,%ymm1                        #! PC = 0x55555557a7b0 *)
not ymm1_0n@uint64 ymm1_0;
and ymm1_0@uint64 ymm1_0n ymm9_0;
not ymm1_1n@uint64 ymm1_1;
and ymm1_1@uint64 ymm1_1n ymm9_1;
not ymm1_2n@uint64 ymm1_2;
and ymm1_2@uint64 ymm1_2n ymm9_2;
not ymm1_3n@uint64 ymm1_3;
and ymm1_3@uint64 ymm1_3n ymm9_3;
(* vpxor  %ymm0,%ymm1,%ymm9                        #! PC = 0x55555557a7b5 *)
xor ymm9_0@uint64 ymm1_0 ymm0_0;
xor ymm9_1@uint64 ymm1_1 ymm0_1;
xor ymm9_2@uint64 ymm1_2 ymm0_2;
xor ymm9_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x36,%ymm7,%ymm0                        #! PC = 0x55555557a7b9 *)
shr ymm0_0 ymm7_0 0x36@uint64;
shr ymm0_1 ymm7_1 0x36@uint64;
shr ymm0_2 ymm7_2 0x36@uint64;
shr ymm0_3 ymm7_3 0x36@uint64;
(* vpxor  %ymm12,%ymm2,%ymm6                       #! PC = 0x55555557a7be *)
xor ymm6_0@uint64 ymm2_0 ymm12_0;
xor ymm6_1@uint64 ymm2_1 ymm12_1;
xor ymm6_2@uint64 ymm2_2 ymm12_2;
xor ymm6_3@uint64 ymm2_3 ymm12_3;
(* vmovdqa %ymm9,-0xb0(%rbp)                       #! EA = L0x7fffffffbea0; PC = 0x55555557a7c3 *)
mov L0x7fffffffbea0 ymm9_0;
mov L0x7fffffffbea8 ymm9_1;
mov L0x7fffffffbeb0 ymm9_2;
mov L0x7fffffffbeb8 ymm9_3;
(* vpxor  -0x1d0(%rbp),%ymm11,%ymm9                #! EA = L0x7fffffffbd80; Value = 0xcdc8e10f6ba13c50; PC = 0x55555557a7cb *)
xor ymm9_0@uint64 ymm11_0 L0x7fffffffbd80;
xor ymm9_1@uint64 ymm11_1 L0x7fffffffbd88;
xor ymm9_2@uint64 ymm11_2 L0x7fffffffbd90;
xor ymm9_3@uint64 ymm11_3 L0x7fffffffbd98;
(* vpsrlq $0x1c,%ymm8,%ymm12                       #! PC = 0x55555557a7d3 *)
shr ymm12_0 ymm8_0 0x1c@uint64;
shr ymm12_1 ymm8_1 0x1c@uint64;
shr ymm12_2 ymm8_2 0x1c@uint64;
shr ymm12_3 ymm8_3 0x1c@uint64;
(* vpsllq $0xa,%ymm7,%ymm7                         #! PC = 0x55555557a7d9 *)
shl ymm7_0 ymm7_0 0xa@uint64;
shl ymm7_1 ymm7_1 0xa@uint64;
shl ymm7_2 ymm7_2 0xa@uint64;
shl ymm7_3 ymm7_3 0xa@uint64;
(* vpsllq $0x24,%ymm8,%ymm8                        #! PC = 0x55555557a7de *)
shl ymm8_0 ymm8_0 0x24@uint64;
shl ymm8_1 ymm8_1 0x24@uint64;
shl ymm8_2 ymm8_2 0x24@uint64;
shl ymm8_3 ymm8_3 0x24@uint64;
(* vmovdqa %ymm6,-0xd0(%rbp)                       #! EA = L0x7fffffffbe80; PC = 0x55555557a7e4 *)
mov L0x7fffffffbe80 ymm6_0;
mov L0x7fffffffbe88 ymm6_1;
mov L0x7fffffffbe90 ymm6_2;
mov L0x7fffffffbe98 ymm6_3;
(* vmovdqa -0x170(%rbp),%ymm6                      #! EA = L0x7fffffffbde0; Value = 0x3671348a3fbd6487; PC = 0x55555557a7ec *)
mov ymm6_0 L0x7fffffffbde0;
mov ymm6_1 L0x7fffffffbde8;
mov ymm6_2 L0x7fffffffbdf0;
mov ymm6_3 L0x7fffffffbdf8;
(* vpor   %ymm8,%ymm12,%ymm12                      #! PC = 0x55555557a7f4 *)
or ymm12_0@uint64 ymm12_0 ymm8_0;
or ymm12_1@uint64 ymm12_1 ymm8_1;
or ymm12_2@uint64 ymm12_2 ymm8_2;
or ymm12_3@uint64 ymm12_3 ymm8_3;
(* vpsrlq $0x25,%ymm9,%ymm2                        #! PC = 0x55555557a7f9 *)
shr ymm2_0 ymm9_0 0x25@uint64;
shr ymm2_1 ymm9_1 0x25@uint64;
shr ymm2_2 ymm9_2 0x25@uint64;
shr ymm2_3 ymm9_3 0x25@uint64;
(* vpor   %ymm7,%ymm0,%ymm7                        #! PC = 0x55555557a7ff *)
or ymm7_0@uint64 ymm0_0 ymm7_0;
or ymm7_1@uint64 ymm0_1 ymm7_1;
or ymm7_2@uint64 ymm0_2 ymm7_2;
or ymm7_3@uint64 ymm0_3 ymm7_3;
(* vpsllq $0x1b,%ymm9,%ymm9                        #! PC = 0x55555557a803 *)
shl ymm9_0 ymm9_0 0x1b@uint64;
shl ymm9_1 ymm9_1 0x1b@uint64;
shl ymm9_2 ymm9_2 0x1b@uint64;
shl ymm9_3 ymm9_3 0x1b@uint64;
(* vpandn %ymm7,%ymm12,%ymm0                       #! PC = 0x55555557a809 *)
not ymm12_0n@uint64 ymm12_0;
and ymm0_0@uint64 ymm12_0n ymm7_0;
not ymm12_1n@uint64 ymm12_1;
and ymm0_1@uint64 ymm12_1n ymm7_1;
not ymm12_2n@uint64 ymm12_2;
and ymm0_2@uint64 ymm12_2n ymm7_2;
not ymm12_3n@uint64 ymm12_3;
and ymm0_3@uint64 ymm12_3n ymm7_3;
(* vpor   %ymm9,%ymm2,%ymm2                        #! PC = 0x55555557a80d *)
or ymm2_0@uint64 ymm2_0 ymm9_0;
or ymm2_1@uint64 ymm2_1 ymm9_1;
or ymm2_2@uint64 ymm2_2 ymm9_2;
or ymm2_3@uint64 ymm2_3 ymm9_3;
(* vpxor  %ymm2,%ymm0,%ymm1                        #! PC = 0x55555557a812 *)
xor ymm1_0@uint64 ymm0_0 ymm2_0;
xor ymm1_1@uint64 ymm0_1 ymm2_1;
xor ymm1_2@uint64 ymm0_2 ymm2_2;
xor ymm1_3@uint64 ymm0_3 ymm2_3;
(* vpxor  -0x290(%rbp),%ymm5,%ymm0                 #! EA = L0x7fffffffbcc0; Value = 0x3660c07bad4b60af; PC = 0x55555557a816 *)
xor ymm0_0@uint64 ymm5_0 L0x7fffffffbcc0;
xor ymm0_1@uint64 ymm5_1 L0x7fffffffbcc8;
xor ymm0_2@uint64 ymm5_2 L0x7fffffffbcd0;
xor ymm0_3@uint64 ymm5_3 L0x7fffffffbcd8;
(* vmovdqa %ymm1,-0x1d0(%rbp)                      #! EA = L0x7fffffffbd80; PC = 0x55555557a81e *)
mov L0x7fffffffbd80 ymm1_0;
mov L0x7fffffffbd88 ymm1_1;
mov L0x7fffffffbd90 ymm1_2;
mov L0x7fffffffbd98 ymm1_3;
(* vpxor  %ymm14,%ymm6,%ymm1                       #! PC = 0x55555557a826 *)
xor ymm1_0@uint64 ymm6_0 ymm14_0;
xor ymm1_1@uint64 ymm6_1 ymm14_1;
xor ymm1_2@uint64 ymm6_2 ymm14_2;
xor ymm1_3@uint64 ymm6_3 ymm14_3;
(* vpxor  -0x250(%rbp),%ymm10,%ymm14               #! EA = L0x7fffffffbd00; Value = 0x5defaffe64a1e6a4; PC = 0x55555557a82b *)
xor ymm14_0@uint64 ymm10_0 L0x7fffffffbd00;
xor ymm14_1@uint64 ymm10_1 L0x7fffffffbd08;
xor ymm14_2@uint64 ymm10_2 L0x7fffffffbd10;
xor ymm14_3@uint64 ymm10_3 L0x7fffffffbd18;
(* vpsrlq $0x31,%ymm0,%ymm9                        #! PC = 0x55555557a833 *)
shr ymm9_0 ymm0_0 0x31@uint64;
shr ymm9_1 ymm0_1 0x31@uint64;
shr ymm9_2 ymm0_2 0x31@uint64;
shr ymm9_3 ymm0_3 0x31@uint64;
(* vpsllq $0xf,%ymm0,%ymm0                         #! PC = 0x55555557a838 *)
shl ymm0_0 ymm0_0 0xf@uint64;
shl ymm0_1 ymm0_1 0xf@uint64;
shl ymm0_2 ymm0_2 0xf@uint64;
shl ymm0_3 ymm0_3 0xf@uint64;
(* vpor   %ymm0,%ymm9,%ymm9                        #! PC = 0x55555557a83d *)
or ymm9_0@uint64 ymm9_0 ymm0_0;
or ymm9_1@uint64 ymm9_1 ymm0_1;
or ymm9_2@uint64 ymm9_2 ymm0_2;
or ymm9_3@uint64 ymm9_3 ymm0_3;
(* vpandn %ymm9,%ymm7,%ymm0                        #! PC = 0x55555557a841 *)
not ymm7_0n@uint64 ymm7_0;
and ymm0_0@uint64 ymm7_0n ymm9_0;
not ymm7_1n@uint64 ymm7_1;
and ymm0_1@uint64 ymm7_1n ymm9_1;
not ymm7_2n@uint64 ymm7_2;
and ymm0_2@uint64 ymm7_2n ymm9_2;
not ymm7_3n@uint64 ymm7_3;
and ymm0_3@uint64 ymm7_3n ymm9_3;
(* vpxor  %ymm12,%ymm0,%ymm0                       #! PC = 0x55555557a846 *)
xor ymm0_0@uint64 ymm0_0 ymm12_0;
xor ymm0_1@uint64 ymm0_1 ymm12_1;
xor ymm0_2@uint64 ymm0_2 ymm12_2;
xor ymm0_3@uint64 ymm0_3 ymm12_3;
(* vpxor  %ymm15,%ymm0,%ymm8                       #! PC = 0x55555557a84b *)
xor ymm8_0@uint64 ymm0_0 ymm15_0;
xor ymm8_1@uint64 ymm0_1 ymm15_1;
xor ymm8_2@uint64 ymm0_2 ymm15_2;
xor ymm8_3@uint64 ymm0_3 ymm15_3;
(* vmovdqa %ymm0,-0x210(%rbp)                      #! EA = L0x7fffffffbd40; PC = 0x55555557a850 *)
mov L0x7fffffffbd40 ymm0_0;
mov L0x7fffffffbd48 ymm0_1;
mov L0x7fffffffbd50 ymm0_2;
mov L0x7fffffffbd58 ymm0_3;
(* vpshufb 0x5367f(%rip),%ymm14,%ymm0        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x55555557a858 *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm14_0, ymm14_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm14_2, ymm14_3, tmp_2, tmp_3);
mov ymm0_0 tmp_0;
mov ymm0_1 tmp_1;
mov ymm0_2 tmp_2;
mov ymm0_3 tmp_3;
(* vmovdqa -0x190(%rbp),%ymm14                     #! EA = L0x7fffffffbdc0; Value = 0x81e9f633f01dcf03; PC = 0x55555557a861 *)
mov ymm14_0 L0x7fffffffbdc0;
mov ymm14_1 L0x7fffffffbdc8;
mov ymm14_2 L0x7fffffffbdd0;
mov ymm14_3 L0x7fffffffbdd8;
(* vpxor  %ymm1,%ymm8,%ymm8                        #! PC = 0x55555557a869 *)
xor ymm8_0@uint64 ymm8_0 ymm1_0;
xor ymm8_1@uint64 ymm8_1 ymm1_1;
xor ymm8_2@uint64 ymm8_2 ymm1_2;
xor ymm8_3@uint64 ymm8_3 ymm1_3;
(* vpandn %ymm0,%ymm9,%ymm1                        #! PC = 0x55555557a86d *)
not ymm9_0n@uint64 ymm9_0;
and ymm1_0@uint64 ymm9_0n ymm0_0;
not ymm9_1n@uint64 ymm9_1;
and ymm1_1@uint64 ymm9_1n ymm0_1;
not ymm9_2n@uint64 ymm9_2;
and ymm1_2@uint64 ymm9_2n ymm0_2;
not ymm9_3n@uint64 ymm9_3;
and ymm1_3@uint64 ymm9_3n ymm0_3;
(* vpxor  -0x270(%rbp),%ymm10,%ymm10               #! EA = L0x7fffffffbce0; Value = 0x5a19d348ebbbb880; PC = 0x55555557a871 *)
xor ymm10_0@uint64 ymm10_0 L0x7fffffffbce0;
xor ymm10_1@uint64 ymm10_1 L0x7fffffffbce8;
xor ymm10_2@uint64 ymm10_2 L0x7fffffffbcf0;
xor ymm10_3@uint64 ymm10_3 L0x7fffffffbcf8;
(* vpxor  %ymm7,%ymm1,%ymm7                        #! PC = 0x55555557a879 *)
xor ymm7_0@uint64 ymm1_0 ymm7_0;
xor ymm7_1@uint64 ymm1_1 ymm7_1;
xor ymm7_2@uint64 ymm1_2 ymm7_2;
xor ymm7_3@uint64 ymm1_3 ymm7_3;
(* vpxor  -0x310(%rbp),%ymm11,%ymm11               #! EA = L0x7fffffffbc40; Value = 0xd31cfcdcd9c4cdb2; PC = 0x55555557a87d *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbc40;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbc48;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbc50;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbc58;
(* vpxor  -0x110(%rbp),%ymm14,%ymm1                #! EA = L0x7fffffffbe40; Value = 0x12657d5a4568ead5; PC = 0x55555557a885 *)
xor ymm1_0@uint64 ymm14_0 L0x7fffffffbe40;
xor ymm1_1@uint64 ymm14_1 L0x7fffffffbe48;
xor ymm1_2@uint64 ymm14_2 L0x7fffffffbe50;
xor ymm1_3@uint64 ymm14_3 L0x7fffffffbe58;
(* vpxor  %ymm7,%ymm13,%ymm6                       #! PC = 0x55555557a88d *)
xor ymm6_0@uint64 ymm13_0 ymm7_0;
xor ymm6_1@uint64 ymm13_1 ymm7_1;
xor ymm6_2@uint64 ymm13_2 ymm7_2;
xor ymm6_3@uint64 ymm13_3 ymm7_3;
(* vpandn %ymm12,%ymm2,%ymm14                      #! PC = 0x55555557a891 *)
not ymm2_0n@uint64 ymm2_0;
and ymm14_0@uint64 ymm2_0n ymm12_0;
not ymm2_1n@uint64 ymm2_1;
and ymm14_1@uint64 ymm2_1n ymm12_1;
not ymm2_2n@uint64 ymm2_2;
and ymm14_2@uint64 ymm2_2n ymm12_2;
not ymm2_3n@uint64 ymm2_3;
and ymm14_3@uint64 ymm2_3n ymm12_3;
(* vpxor  -0x2f0(%rbp),%ymm5,%ymm5                 #! EA = L0x7fffffffbc60; Value = 0xbfaf17a701e92e36; PC = 0x55555557a896 *)
xor ymm5_0@uint64 ymm5_0 L0x7fffffffbc60;
xor ymm5_1@uint64 ymm5_1 L0x7fffffffbc68;
xor ymm5_2@uint64 ymm5_2 L0x7fffffffbc70;
xor ymm5_3@uint64 ymm5_3 L0x7fffffffbc78;
(* vmovdqa %ymm7,-0x230(%rbp)                      #! EA = L0x7fffffffbd20; PC = 0x55555557a89e *)
mov L0x7fffffffbd20 ymm7_0;
mov L0x7fffffffbd28 ymm7_1;
mov L0x7fffffffbd30 ymm7_2;
mov L0x7fffffffbd38 ymm7_3;
(* vpsrlq $0x19,%ymm11,%ymm15                      #! PC = 0x55555557a8a6 *)
shr ymm15_0 ymm11_0 0x19@uint64;
shr ymm15_1 ymm11_1 0x19@uint64;
shr ymm15_2 ymm11_2 0x19@uint64;
shr ymm15_3 ymm11_3 0x19@uint64;
(* vpxor  %ymm1,%ymm6,%ymm6                        #! PC = 0x55555557a8ac *)
xor ymm6_0@uint64 ymm6_0 ymm1_0;
xor ymm6_1@uint64 ymm6_1 ymm1_1;
xor ymm6_2@uint64 ymm6_2 ymm1_2;
xor ymm6_3@uint64 ymm6_3 ymm1_3;
(* vpxor  %ymm0,%ymm14,%ymm14                      #! PC = 0x55555557a8b0 *)
xor ymm14_0@uint64 ymm14_0 ymm0_0;
xor ymm14_1@uint64 ymm14_1 ymm0_1;
xor ymm14_2@uint64 ymm14_2 ymm0_2;
xor ymm14_3@uint64 ymm14_3 ymm0_3;
(* vpsllq $0x27,%ymm11,%ymm11                      #! PC = 0x55555557a8b4 *)
shl ymm11_0 ymm11_0 0x27@uint64;
shl ymm11_1 ymm11_1 0x27@uint64;
shl ymm11_2 ymm11_2 0x27@uint64;
shl ymm11_3 ymm11_3 0x27@uint64;
(* vpandn %ymm2,%ymm0,%ymm1                        #! PC = 0x55555557a8ba *)
not ymm0_0n@uint64 ymm0_0;
and ymm1_0@uint64 ymm0_0n ymm2_0;
not ymm0_1n@uint64 ymm0_1;
and ymm1_1@uint64 ymm0_1n ymm2_1;
not ymm0_2n@uint64 ymm0_2;
and ymm1_2@uint64 ymm0_2n ymm2_2;
not ymm0_3n@uint64 ymm0_3;
and ymm1_3@uint64 ymm0_3n ymm2_3;
(* vpxor  -0xf0(%rbp),%ymm4,%ymm4                  #! EA = L0x7fffffffbe60; Value = 0xa164d352b18499c4; PC = 0x55555557a8be *)
xor ymm4_0@uint64 ymm4_0 L0x7fffffffbe60;
xor ymm4_1@uint64 ymm4_1 L0x7fffffffbe68;
xor ymm4_2@uint64 ymm4_2 L0x7fffffffbe70;
xor ymm4_3@uint64 ymm4_3 L0x7fffffffbe78;
(* vpsrlq $0x9,%ymm10,%ymm0                        #! PC = 0x55555557a8c6 *)
shr ymm0_0 ymm10_0 0x9@uint64;
shr ymm0_1 ymm10_1 0x9@uint64;
shr ymm0_2 ymm10_2 0x9@uint64;
shr ymm0_3 ymm10_3 0x9@uint64;
(* vpsllq $0x37,%ymm10,%ymm10                      #! PC = 0x55555557a8cc *)
shl ymm10_0 ymm10_0 0x37@uint64;
shl ymm10_1 ymm10_1 0x37@uint64;
shl ymm10_2 ymm10_2 0x37@uint64;
shl ymm10_3 ymm10_3 0x37@uint64;
(* vpor   %ymm11,%ymm15,%ymm11                     #! PC = 0x55555557a8d2 *)
or ymm11_0@uint64 ymm15_0 ymm11_0;
or ymm11_1@uint64 ymm15_1 ymm11_1;
or ymm11_2@uint64 ymm15_2 ymm11_2;
or ymm11_3@uint64 ymm15_3 ymm11_3;
(* vpor   %ymm10,%ymm0,%ymm10                      #! PC = 0x55555557a8d7 *)
or ymm10_0@uint64 ymm0_0 ymm10_0;
or ymm10_1@uint64 ymm0_1 ymm10_1;
or ymm10_2@uint64 ymm0_2 ymm10_2;
or ymm10_3@uint64 ymm0_3 ymm10_3;
(* vpsrlq $0x2,%ymm5,%ymm12                        #! PC = 0x55555557a8dc *)
shr ymm12_0 ymm5_0 0x2@uint64;
shr ymm12_1 ymm5_1 0x2@uint64;
shr ymm12_2 ymm5_2 0x2@uint64;
shr ymm12_3 ymm5_3 0x2@uint64;
(* vpxor  %ymm9,%ymm1,%ymm9                        #! PC = 0x55555557a8e1 *)
xor ymm9_0@uint64 ymm1_0 ymm9_0;
xor ymm9_1@uint64 ymm1_1 ymm9_1;
xor ymm9_2@uint64 ymm1_2 ymm9_2;
xor ymm9_3@uint64 ymm1_3 ymm9_3;
(* vmovdqa -0x1b0(%rbp),%ymm0                      #! EA = L0x7fffffffbda0; Value = 0x3229e84a8e1704c2; PC = 0x55555557a8e6 *)
mov ymm0_0 L0x7fffffffbda0;
mov ymm0_1 L0x7fffffffbda8;
mov ymm0_2 L0x7fffffffbdb0;
mov ymm0_3 L0x7fffffffbdb8;
(* vpsllq $0x3e,%ymm5,%ymm5                        #! PC = 0x55555557a8ee *)
shl ymm5_0 ymm5_0 0x3e@uint64;
shl ymm5_1 ymm5_1 0x3e@uint64;
shl ymm5_2 ymm5_2 0x3e@uint64;
shl ymm5_3 ymm5_3 0x3e@uint64;
(* vpandn %ymm11,%ymm10,%ymm7                      #! PC = 0x55555557a8f3 *)
not ymm10_0n@uint64 ymm10_0;
and ymm7_0@uint64 ymm10_0n ymm11_0;
not ymm10_1n@uint64 ymm10_1;
and ymm7_1@uint64 ymm10_1n ymm11_1;
not ymm10_2n@uint64 ymm10_2;
and ymm7_2@uint64 ymm10_2n ymm11_2;
not ymm10_3n@uint64 ymm10_3;
and ymm7_3@uint64 ymm10_3n ymm11_3;
(* vpxor  -0x150(%rbp),%ymm0,%ymm0                 #! EA = L0x7fffffffbe00; Value = 0x61ed7fb0ce6ca6af; PC = 0x55555557a8f8 *)
xor ymm0_0@uint64 ymm0_0 L0x7fffffffbe00;
xor ymm0_1@uint64 ymm0_1 L0x7fffffffbe08;
xor ymm0_2@uint64 ymm0_2 L0x7fffffffbe10;
xor ymm0_3@uint64 ymm0_3 L0x7fffffffbe18;
(* vpor   %ymm5,%ymm12,%ymm12                      #! PC = 0x55555557a900 *)
or ymm12_0@uint64 ymm12_0 ymm5_0;
or ymm12_1@uint64 ymm12_1 ymm5_1;
or ymm12_2@uint64 ymm12_2 ymm5_2;
or ymm12_3@uint64 ymm12_3 ymm5_3;
(* vpxor  -0x2b0(%rbp),%ymm3,%ymm3                 #! EA = L0x7fffffffbca0; Value = 0xa717f1200b3b94b2; PC = 0x55555557a904 *)
xor ymm3_0@uint64 ymm3_0 L0x7fffffffbca0;
xor ymm3_1@uint64 ymm3_1 L0x7fffffffbca8;
xor ymm3_2@uint64 ymm3_2 L0x7fffffffbcb0;
xor ymm3_3@uint64 ymm3_3 L0x7fffffffbcb8;
(* vmovdqa -0x70(%rbp),%ymm5                       #! EA = L0x7fffffffbee0; Value = 0xab5eefa38e5546d9; PC = 0x55555557a90c *)
mov ymm5_0 L0x7fffffffbee0;
mov ymm5_1 L0x7fffffffbee8;
mov ymm5_2 L0x7fffffffbef0;
mov ymm5_3 L0x7fffffffbef8;
(* vpxor  %ymm12,%ymm7,%ymm7                       #! PC = 0x55555557a911 *)
xor ymm7_0@uint64 ymm7_0 ymm12_0;
xor ymm7_1@uint64 ymm7_1 ymm12_1;
xor ymm7_2@uint64 ymm7_2 ymm12_2;
xor ymm7_3@uint64 ymm7_3 ymm12_3;
(* vpxor  -0x1d0(%rbp),%ymm7,%ymm1                 #! EA = L0x7fffffffbd80; Value = 0x9af85d4549fb53c7; PC = 0x55555557a916 *)
xor ymm1_0@uint64 ymm7_0 L0x7fffffffbd80;
xor ymm1_1@uint64 ymm7_1 L0x7fffffffbd88;
xor ymm1_2@uint64 ymm7_2 L0x7fffffffbd90;
xor ymm1_3@uint64 ymm7_3 L0x7fffffffbd98;
(* vmovdqa %ymm7,-0x250(%rbp)                      #! EA = L0x7fffffffbd00; PC = 0x55555557a91e *)
mov L0x7fffffffbd00 ymm7_0;
mov L0x7fffffffbd08 ymm7_1;
mov L0x7fffffffbd10 ymm7_2;
mov L0x7fffffffbd18 ymm7_3;
(* vpxor  %ymm0,%ymm1,%ymm0                        #! PC = 0x55555557a926 *)
xor ymm0_0@uint64 ymm1_0 ymm0_0;
xor ymm0_1@uint64 ymm1_1 ymm0_1;
xor ymm0_2@uint64 ymm1_2 ymm0_2;
xor ymm0_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x17,%ymm4,%ymm1                        #! PC = 0x55555557a92a *)
shr ymm1_0 ymm4_0 0x17@uint64;
shr ymm1_1 ymm4_1 0x17@uint64;
shr ymm1_2 ymm4_2 0x17@uint64;
shr ymm1_3 ymm4_3 0x17@uint64;
(* vpsllq $0x29,%ymm4,%ymm4                        #! PC = 0x55555557a92f *)
shl ymm4_0 ymm4_0 0x29@uint64;
shl ymm4_1 ymm4_1 0x29@uint64;
shl ymm4_2 ymm4_2 0x29@uint64;
shl ymm4_3 ymm4_3 0x29@uint64;
(* vpor   %ymm4,%ymm1,%ymm4                        #! PC = 0x55555557a934 *)
or ymm4_0@uint64 ymm1_0 ymm4_0;
or ymm4_1@uint64 ymm1_1 ymm4_1;
or ymm4_2@uint64 ymm1_2 ymm4_2;
or ymm4_3@uint64 ymm1_3 ymm4_3;
(* vpandn %ymm4,%ymm11,%ymm2                       #! PC = 0x55555557a938 *)
not ymm11_0n@uint64 ymm11_0;
and ymm2_0@uint64 ymm11_0n ymm4_0;
not ymm11_1n@uint64 ymm11_1;
and ymm2_1@uint64 ymm11_1n ymm4_1;
not ymm11_2n@uint64 ymm11_2;
and ymm2_2@uint64 ymm11_2n ymm4_2;
not ymm11_3n@uint64 ymm11_3;
and ymm2_3@uint64 ymm11_3n ymm4_3;
(* vpxor  %ymm10,%ymm2,%ymm15                      #! PC = 0x55555557a93c *)
xor ymm15_0@uint64 ymm2_0 ymm10_0;
xor ymm15_1@uint64 ymm2_1 ymm10_1;
xor ymm15_2@uint64 ymm2_2 ymm10_2;
xor ymm15_3@uint64 ymm2_3 ymm10_3;
(* vpsrlq $0x3e,%ymm3,%ymm2                        #! PC = 0x55555557a941 *)
shr ymm2_0 ymm3_0 0x3e@uint64;
shr ymm2_1 ymm3_1 0x3e@uint64;
shr ymm2_2 ymm3_2 0x3e@uint64;
shr ymm2_3 ymm3_3 0x3e@uint64;
(* vpandn %ymm10,%ymm12,%ymm10                     #! PC = 0x55555557a946 *)
not ymm12_0n@uint64 ymm12_0;
and ymm10_0@uint64 ymm12_0n ymm10_0;
not ymm12_1n@uint64 ymm12_1;
and ymm10_1@uint64 ymm12_1n ymm10_1;
not ymm12_2n@uint64 ymm12_2;
and ymm10_2@uint64 ymm12_2n ymm10_2;
not ymm12_3n@uint64 ymm12_3;
and ymm10_3@uint64 ymm12_3n ymm10_3;
(* vpsllq $0x2,%ymm3,%ymm3                         #! PC = 0x55555557a94b *)
shl ymm3_0 ymm3_0 0x2@uint64;
shl ymm3_1 ymm3_1 0x2@uint64;
shl ymm3_2 ymm3_2 0x2@uint64;
shl ymm3_3 ymm3_3 0x2@uint64;
(* vmovdqa %ymm15,-0xf0(%rbp)                      #! EA = L0x7fffffffbe60; PC = 0x55555557a950 *)
mov L0x7fffffffbe60 ymm15_0;
mov L0x7fffffffbe68 ymm15_1;
mov L0x7fffffffbe70 ymm15_2;
mov L0x7fffffffbe78 ymm15_3;
(* vpor   %ymm3,%ymm2,%ymm1                        #! PC = 0x55555557a958 *)
or ymm1_0@uint64 ymm2_0 ymm3_0;
or ymm1_1@uint64 ymm2_1 ymm3_1;
or ymm1_2@uint64 ymm2_2 ymm3_2;
or ymm1_3@uint64 ymm2_3 ymm3_3;
(* vpxor  -0x130(%rbp),%ymm5,%ymm3                 #! EA = L0x7fffffffbe20; Value = 0x4d9f3e32387e66d3; PC = 0x55555557a95c *)
xor ymm3_0@uint64 ymm5_0 L0x7fffffffbe20;
xor ymm3_1@uint64 ymm5_1 L0x7fffffffbe28;
xor ymm3_2@uint64 ymm5_2 L0x7fffffffbe30;
xor ymm3_3@uint64 ymm5_3 L0x7fffffffbe38;
(* vmovdqa -0x90(%rbp),%ymm5                       #! EA = L0x7fffffffbec0; Value = 0x8298d4e430062494; PC = 0x55555557a964 *)
mov ymm5_0 L0x7fffffffbec0;
mov ymm5_1 L0x7fffffffbec8;
mov ymm5_2 L0x7fffffffbed0;
mov ymm5_3 L0x7fffffffbed8;
(* vpandn %ymm1,%ymm4,%ymm15                       #! PC = 0x55555557a96c *)
not ymm4_0n@uint64 ymm4_0;
and ymm15_0@uint64 ymm4_0n ymm1_0;
not ymm4_1n@uint64 ymm4_1;
and ymm15_1@uint64 ymm4_1n ymm1_1;
not ymm4_2n@uint64 ymm4_2;
and ymm15_2@uint64 ymm4_2n ymm1_2;
not ymm4_3n@uint64 ymm4_3;
and ymm15_3@uint64 ymm4_3n ymm1_3;
(* vpandn %ymm12,%ymm1,%ymm2                       #! PC = 0x55555557a970 *)
not ymm1_0n@uint64 ymm1_0;
and ymm2_0@uint64 ymm1_0n ymm12_0;
not ymm1_1n@uint64 ymm1_1;
and ymm2_1@uint64 ymm1_1n ymm12_1;
not ymm1_2n@uint64 ymm1_2;
and ymm2_2@uint64 ymm1_2n ymm12_2;
not ymm1_3n@uint64 ymm1_3;
and ymm2_3@uint64 ymm1_3n ymm12_3;
(* vpxor  %ymm1,%ymm10,%ymm10                      #! PC = 0x55555557a975 *)
xor ymm10_0@uint64 ymm10_0 ymm1_0;
xor ymm10_1@uint64 ymm10_1 ymm1_1;
xor ymm10_2@uint64 ymm10_2 ymm1_2;
xor ymm10_3@uint64 ymm10_3 ymm1_3;
(* vpxor  %ymm4,%ymm2,%ymm2                        #! PC = 0x55555557a979 *)
xor ymm2_0@uint64 ymm2_0 ymm4_0;
xor ymm2_1@uint64 ymm2_1 ymm4_1;
xor ymm2_2@uint64 ymm2_2 ymm4_2;
xor ymm2_3@uint64 ymm2_3 ymm4_3;
(* vpxor  %ymm11,%ymm15,%ymm15                     #! PC = 0x55555557a97d *)
xor ymm15_0@uint64 ymm15_0 ymm11_0;
xor ymm15_1@uint64 ymm15_1 ymm11_1;
xor ymm15_2@uint64 ymm15_2 ymm11_2;
xor ymm15_3@uint64 ymm15_3 ymm11_3;
(* vpxor  -0x50(%rbp),%ymm5,%ymm1                  #! EA = L0x7fffffffbf00; Value = 0x3be7f6680bb1c847; PC = 0x55555557a982 *)
xor ymm1_0@uint64 ymm5_0 L0x7fffffffbf00;
xor ymm1_1@uint64 ymm5_1 L0x7fffffffbf08;
xor ymm1_2@uint64 ymm5_2 L0x7fffffffbf10;
xor ymm1_3@uint64 ymm5_3 L0x7fffffffbf18;
(* vpxor  -0xb0(%rbp),%ymm10,%ymm11                #! EA = L0x7fffffffbea0; Value = 0x98a1c2d38194eda1; PC = 0x55555557a987 *)
xor ymm11_0@uint64 ymm10_0 L0x7fffffffbea0;
xor ymm11_1@uint64 ymm10_1 L0x7fffffffbea8;
xor ymm11_2@uint64 ymm10_2 L0x7fffffffbeb0;
xor ymm11_3@uint64 ymm10_3 L0x7fffffffbeb8;
(* vpxor  %ymm2,%ymm9,%ymm4                        #! PC = 0x55555557a98f *)
xor ymm4_0@uint64 ymm9_0 ymm2_0;
xor ymm4_1@uint64 ymm9_1 ymm2_1;
xor ymm4_2@uint64 ymm9_2 ymm2_2;
xor ymm4_3@uint64 ymm9_3 ymm2_3;
(* vmovdqa %ymm2,-0x270(%rbp)                      #! EA = L0x7fffffffbce0; PC = 0x55555557a993 *)
mov L0x7fffffffbce0 ymm2_0;
mov L0x7fffffffbce8 ymm2_1;
mov L0x7fffffffbcf0 ymm2_2;
mov L0x7fffffffbcf8 ymm2_3;
(* vpxor  %ymm3,%ymm4,%ymm4                        #! PC = 0x55555557a99b *)
xor ymm4_0@uint64 ymm4_0 ymm3_0;
xor ymm4_1@uint64 ymm4_1 ymm3_1;
xor ymm4_2@uint64 ymm4_2 ymm3_2;
xor ymm4_3@uint64 ymm4_3 ymm3_3;
(* vpxor  %ymm1,%ymm11,%ymm11                      #! PC = 0x55555557a99f *)
xor ymm11_0@uint64 ymm11_0 ymm1_0;
xor ymm11_1@uint64 ymm11_1 ymm1_1;
xor ymm11_2@uint64 ymm11_2 ymm1_2;
xor ymm11_3@uint64 ymm11_3 ymm1_3;
(* #je     0x55555557aa08 <KeccakP1600times4_PermuteAll_24rounds+14488>#! PC = 0x55555557a9a6 *)
#je     0x55555557aa08 <KeccakP1600times4_PermuteAll_24rounds+14488>#! 0x55555557a9a6 = 0x55555557a9a6;
(* mov    (%rax),%rdx                              #! EA = L0x5555555cde80; Value = 0x000000008000808b; PC = 0x55555557a9a8 *)
mov rdx L0x5555555cde80;
(* mov    0x10(%rax),%rcx                          #! EA = L0x5555555cde90; Value = 0x8000000000008089; PC = 0x55555557a9ab *)
mov rcx L0x5555555cde90;
(* vpxor  %ymm6,%ymm15,%ymm5                       #! PC = 0x55555557a9af *)
xor ymm5_0@uint64 ymm15_0 ymm6_0;
xor ymm5_1@uint64 ymm15_1 ymm6_1;
xor ymm5_2@uint64 ymm15_2 ymm6_2;
xor ymm5_3@uint64 ymm15_3 ymm6_3;
(* vpxor  %ymm11,%ymm14,%ymm11                     #! PC = 0x55555557a9b3 *)
xor ymm11_0@uint64 ymm14_0 ymm11_0;
xor ymm11_1@uint64 ymm14_1 ymm11_1;
xor ymm11_2@uint64 ymm14_2 ymm11_2;
xor ymm11_3@uint64 ymm14_3 ymm11_3;
(* vpxor  -0xf0(%rbp),%ymm8,%ymm7                  #! EA = L0x7fffffffbe60; Value = 0x50a13329b93c785f; PC = 0x55555557a9b8 *)
xor ymm7_0@uint64 ymm8_0 L0x7fffffffbe60;
xor ymm7_1@uint64 ymm8_1 L0x7fffffffbe68;
xor ymm7_2@uint64 ymm8_2 L0x7fffffffbe70;
xor ymm7_3@uint64 ymm8_3 L0x7fffffffbe78;
(* mov    0x18(%rax),%rsi                          #! EA = L0x5555555cde98; Value = 0x8000000000008003; PC = 0x55555557a9c0 *)
mov rsi L0x5555555cde98;
(* mov    0x20(%rax),%r8                           #! EA = L0x5555555cdea0; Value = 0x8000000000008002; PC = 0x55555557a9c4 *)
mov r8 L0x5555555cdea0;
(* mov    0x28(%rax),%r9                           #! EA = L0x5555555cdea8; Value = 0x8000000000000080; PC = 0x55555557a9c8 *)
mov r9 L0x5555555cdea8;
(* mov    %rdx,-0x290(%rbp)                        #! EA = L0x7fffffffbcc0; PC = 0x55555557a9cc *)
mov L0x7fffffffbcc0 rdx;
(* mov    0x30(%rax),%r10                          #! EA = L0x5555555cdeb0; Value = 0x000000000000800a; PC = 0x55555557a9d3 *)
mov r10 L0x5555555cdeb0;
(* mov    0x8(%rax),%rdx                           #! EA = L0x5555555cde88; Value = 0x800000000000008b; PC = 0x55555557a9d7 *)
mov rdx L0x5555555cde88;
(* mov    0x38(%rax),%r11                          #! EA = L0x5555555cdeb8; Value = 0x800000008000000a; PC = 0x55555557a9db *)
mov r11 L0x5555555cdeb8;
(* mov    0x40(%rax),%rbx                          #! EA = L0x5555555cdec0; Value = 0x8000000080008081; PC = 0x55555557a9df *)
mov rbx L0x5555555cdec0;
(* mov    0x48(%rax),%r12                          #! EA = L0x5555555cdec8; Value = 0x8000000000008080; PC = 0x55555557a9e3 *)
mov r12 L0x5555555cdec8;
(* mov    0x50(%rax),%r13                          #! EA = L0x5555555cded0; Value = 0x0000000080000001; PC = 0x55555557a9e7 *)
mov r13 L0x5555555cded0;
(* mov    0x58(%rax),%r14                          #! EA = L0x5555555cded8; Value = 0x8000000080008008; PC = 0x55555557a9eb *)
mov r14 L0x5555555cded8;
(* vpxor  -0x1f0(%rbp),%ymm0,%ymm12                #! EA = L0x7fffffffbd60; Value = 0x4089b1d2ef5d3a26; PC = 0x55555557a9ef *)
xor ymm12_0@uint64 ymm0_0 L0x7fffffffbd60;
xor ymm12_1@uint64 ymm0_1 L0x7fffffffbd68;
xor ymm12_2@uint64 ymm0_2 L0x7fffffffbd70;
xor ymm12_3@uint64 ymm0_3 L0x7fffffffbd78;
(* vpxor  -0xd0(%rbp),%ymm4,%ymm8                  #! EA = L0x7fffffffbe80; Value = 0x52de6de69935b888; PC = 0x55555557a9f7 *)
xor ymm8_0@uint64 ymm4_0 L0x7fffffffbe80;
xor ymm8_1@uint64 ymm4_1 L0x7fffffffbe88;
xor ymm8_2@uint64 ymm4_2 L0x7fffffffbe90;
xor ymm8_3@uint64 ymm4_3 L0x7fffffffbe98;
(* #jmpq   0x5555555773d9 <KeccakP1600times4_PermuteAll_24rounds+617>#! PC = 0x55555557a9ff *)
#jmpq   0x5555555773d9 <KeccakP1600times4_PermuteAll_24rounds+617>#! 0x55555557a9ff = 0x55555557a9ff;
(* vpsrlq $0x3f,%ymm7,%ymm1                        #! PC = 0x5555555773d9 *)
shr ymm1_0 ymm7_0 0x3f@uint64;
shr ymm1_1 ymm7_1 0x3f@uint64;
shr ymm1_2 ymm7_2 0x3f@uint64;
shr ymm1_3 ymm7_3 0x3f@uint64;
(* vpsllq $0x1,%ymm7,%ymm0                         #! PC = 0x5555555773de *)
shl ymm0_0 ymm7_0 0x1@uint64;
shl ymm0_1 ymm7_1 0x1@uint64;
shl ymm0_2 ymm7_2 0x1@uint64;
shl ymm0_3 ymm7_3 0x1@uint64;
(* add    $0x60,%rax                               #! PC = 0x5555555773e3 *)
adds carry rax rax 0x60@uint64;
(* vpsllq $0x1,%ymm5,%ymm2                         #! PC = 0x5555555773e7 *)
shl ymm2_0 ymm5_0 0x1@uint64;
shl ymm2_1 ymm5_1 0x1@uint64;
shl ymm2_2 ymm5_2 0x1@uint64;
shl ymm2_3 ymm5_3 0x1@uint64;
(* vpsllq $0x1,%ymm8,%ymm3                         #! PC = 0x5555555773ec *)
shl ymm3_0 ymm8_0 0x1@uint64;
shl ymm3_1 ymm8_1 0x1@uint64;
shl ymm3_2 ymm8_2 0x1@uint64;
shl ymm3_3 ymm8_3 0x1@uint64;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x5555555773f2 *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpsrlq $0x3f,%ymm5,%ymm1                        #! PC = 0x5555555773f6 *)
shr ymm1_0 ymm5_0 0x3f@uint64;
shr ymm1_1 ymm5_1 0x3f@uint64;
shr ymm1_2 ymm5_2 0x3f@uint64;
shr ymm1_3 ymm5_3 0x3f@uint64;
(* vpxor  %ymm11,%ymm0,%ymm0                       #! PC = 0x5555555773fb *)
xor ymm0_0@uint64 ymm0_0 ymm11_0;
xor ymm0_1@uint64 ymm0_1 ymm11_1;
xor ymm0_2@uint64 ymm0_2 ymm11_2;
xor ymm0_3@uint64 ymm0_3 ymm11_3;
(* vpor   %ymm1,%ymm2,%ymm2                        #! PC = 0x555555577400 *)
or ymm2_0@uint64 ymm2_0 ymm1_0;
or ymm2_1@uint64 ymm2_1 ymm1_1;
or ymm2_2@uint64 ymm2_2 ymm1_2;
or ymm2_3@uint64 ymm2_3 ymm1_3;
(* vpsrlq $0x3f,%ymm8,%ymm1                        #! PC = 0x555555577404 *)
shr ymm1_0 ymm8_0 0x3f@uint64;
shr ymm1_1 ymm8_1 0x3f@uint64;
shr ymm1_2 ymm8_2 0x3f@uint64;
shr ymm1_3 ymm8_3 0x3f@uint64;
(* vpor   %ymm1,%ymm3,%ymm1                        #! PC = 0x55555557740a *)
or ymm1_0@uint64 ymm3_0 ymm1_0;
or ymm1_1@uint64 ymm3_1 ymm1_1;
or ymm1_2@uint64 ymm3_2 ymm1_2;
or ymm1_3@uint64 ymm3_3 ymm1_3;
(* vpsrlq $0x3f,%ymm11,%ymm3                       #! PC = 0x55555557740e *)
shr ymm3_0 ymm11_0 0x3f@uint64;
shr ymm3_1 ymm11_1 0x3f@uint64;
shr ymm3_2 ymm11_2 0x3f@uint64;
shr ymm3_3 ymm11_3 0x3f@uint64;
(* vpxor  %ymm12,%ymm2,%ymm2                       #! PC = 0x555555577414 *)
xor ymm2_0@uint64 ymm2_0 ymm12_0;
xor ymm2_1@uint64 ymm2_1 ymm12_1;
xor ymm2_2@uint64 ymm2_2 ymm12_2;
xor ymm2_3@uint64 ymm2_3 ymm12_3;
(* vpsllq $0x1,%ymm11,%ymm11                       #! PC = 0x555555577419 *)
shl ymm11_0 ymm11_0 0x1@uint64;
shl ymm11_1 ymm11_1 0x1@uint64;
shl ymm11_2 ymm11_2 0x1@uint64;
shl ymm11_3 ymm11_3 0x1@uint64;
(* vpxor  %ymm7,%ymm1,%ymm1                        #! PC = 0x55555557741f *)
xor ymm1_0@uint64 ymm1_0 ymm7_0;
xor ymm1_1@uint64 ymm1_1 ymm7_1;
xor ymm1_2@uint64 ymm1_2 ymm7_2;
xor ymm1_3@uint64 ymm1_3 ymm7_3;
(* vpor   %ymm3,%ymm11,%ymm11                      #! PC = 0x555555577423 *)
or ymm11_0@uint64 ymm11_0 ymm3_0;
or ymm11_1@uint64 ymm11_1 ymm3_1;
or ymm11_2@uint64 ymm11_2 ymm3_2;
or ymm11_3@uint64 ymm11_3 ymm3_3;
(* vpxor  %ymm1,%ymm13,%ymm13                      #! PC = 0x555555577427 *)
xor ymm13_0@uint64 ymm13_0 ymm1_0;
xor ymm13_1@uint64 ymm13_1 ymm1_1;
xor ymm13_2@uint64 ymm13_2 ymm1_2;
xor ymm13_3@uint64 ymm13_3 ymm1_3;
(* vpxor  %ymm1,%ymm15,%ymm15                      #! PC = 0x55555557742b *)
xor ymm15_0@uint64 ymm15_0 ymm1_0;
xor ymm15_1@uint64 ymm15_1 ymm1_1;
xor ymm15_2@uint64 ymm15_2 ymm1_2;
xor ymm15_3@uint64 ymm15_3 ymm1_3;
(* vpxor  %ymm5,%ymm11,%ymm6                       #! PC = 0x55555557742f *)
xor ymm6_0@uint64 ymm11_0 ymm5_0;
xor ymm6_1@uint64 ymm11_1 ymm5_1;
xor ymm6_2@uint64 ymm11_2 ymm5_2;
xor ymm6_3@uint64 ymm11_3 ymm5_3;
(* vpxor  -0x170(%rbp),%ymm2,%ymm5                 #! EA = L0x7fffffffbde0; Value = 0x3671348a3fbd6487; PC = 0x555555577433 *)
xor ymm5_0@uint64 ymm2_0 L0x7fffffffbde0;
xor ymm5_1@uint64 ymm2_1 L0x7fffffffbde8;
xor ymm5_2@uint64 ymm2_2 L0x7fffffffbdf0;
xor ymm5_3@uint64 ymm2_3 L0x7fffffffbdf8;
(* vpsrlq $0x3f,%ymm12,%ymm3                       #! PC = 0x55555557743b *)
shr ymm3_0 ymm12_0 0x3f@uint64;
shr ymm3_1 ymm12_1 0x3f@uint64;
shr ymm3_2 ymm12_2 0x3f@uint64;
shr ymm3_3 ymm12_3 0x3f@uint64;
(* vpsllq $0x1,%ymm12,%ymm12                       #! PC = 0x555555577441 *)
shl ymm12_0 ymm12_0 0x1@uint64;
shl ymm12_1 ymm12_1 0x1@uint64;
shl ymm12_2 ymm12_2 0x1@uint64;
shl ymm12_3 ymm12_3 0x1@uint64;
(* vpxor  %ymm6,%ymm9,%ymm9                        #! PC = 0x555555577447 *)
xor ymm9_0@uint64 ymm9_0 ymm6_0;
xor ymm9_1@uint64 ymm9_1 ymm6_1;
xor ymm9_2@uint64 ymm9_2 ymm6_2;
xor ymm9_3@uint64 ymm9_3 ymm6_3;
(* vpsrlq $0x14,%ymm5,%ymm7                        #! PC = 0x55555557744b *)
shr ymm7_0 ymm5_0 0x14@uint64;
shr ymm7_1 ymm5_1 0x14@uint64;
shr ymm7_2 ymm5_2 0x14@uint64;
shr ymm7_3 ymm5_3 0x14@uint64;
(* vpsllq $0x2c,%ymm5,%ymm5                        #! PC = 0x555555577450 *)
shl ymm5_0 ymm5_0 0x2c@uint64;
shl ymm5_1 ymm5_1 0x2c@uint64;
shl ymm5_2 ymm5_2 0x2c@uint64;
shl ymm5_3 ymm5_3 0x2c@uint64;
(* vpor   %ymm3,%ymm12,%ymm12                      #! PC = 0x555555577455 *)
or ymm12_0@uint64 ymm12_0 ymm3_0;
or ymm12_1@uint64 ymm12_1 ymm3_1;
or ymm12_2@uint64 ymm12_2 ymm3_2;
or ymm12_3@uint64 ymm12_3 ymm3_3;
(* vpor   %ymm7,%ymm5,%ymm5                        #! PC = 0x555555577459 *)
or ymm5_0@uint64 ymm5_0 ymm7_0;
or ymm5_1@uint64 ymm5_1 ymm7_1;
or ymm5_2@uint64 ymm5_2 ymm7_2;
or ymm5_3@uint64 ymm5_3 ymm7_3;
(* vpsrlq $0x15,%ymm13,%ymm7                       #! PC = 0x55555557745d *)
shr ymm7_0 ymm13_0 0x15@uint64;
shr ymm7_1 ymm13_1 0x15@uint64;
shr ymm7_2 ymm13_2 0x15@uint64;
shr ymm7_3 ymm13_3 0x15@uint64;
(* vpxor  %ymm8,%ymm12,%ymm4                       #! PC = 0x555555577463 *)
xor ymm4_0@uint64 ymm12_0 ymm8_0;
xor ymm4_1@uint64 ymm12_1 ymm8_1;
xor ymm4_2@uint64 ymm12_2 ymm8_2;
xor ymm4_3@uint64 ymm12_3 ymm8_3;
(* vpsllq $0x2b,%ymm13,%ymm13                      #! PC = 0x555555577468 *)
shl ymm13_0 ymm13_0 0x2b@uint64;
shl ymm13_1 ymm13_1 0x2b@uint64;
shl ymm13_2 ymm13_2 0x2b@uint64;
shl ymm13_3 ymm13_3 0x2b@uint64;
(* vpxor  -0x1f0(%rbp),%ymm0,%ymm3                 #! EA = L0x7fffffffbd60; Value = 0x4089b1d2ef5d3a26; PC = 0x55555557746e *)
xor ymm3_0@uint64 ymm0_0 L0x7fffffffbd60;
xor ymm3_1@uint64 ymm0_1 L0x7fffffffbd68;
xor ymm3_2@uint64 ymm0_2 L0x7fffffffbd70;
xor ymm3_3@uint64 ymm0_3 L0x7fffffffbd78;
(* vpxor  %ymm4,%ymm10,%ymm10                      #! PC = 0x555555577476 *)
xor ymm10_0@uint64 ymm10_0 ymm4_0;
xor ymm10_1@uint64 ymm10_1 ymm4_1;
xor ymm10_2@uint64 ymm10_2 ymm4_2;
xor ymm10_3@uint64 ymm10_3 ymm4_3;
(* vpor   %ymm7,%ymm13,%ymm13                      #! PC = 0x55555557747a *)
or ymm13_0@uint64 ymm13_0 ymm7_0;
or ymm13_1@uint64 ymm13_1 ymm7_1;
or ymm13_2@uint64 ymm13_2 ymm7_2;
or ymm13_3@uint64 ymm13_3 ymm7_3;
(* vpxor  %ymm4,%ymm14,%ymm14                      #! PC = 0x55555557747e *)
xor ymm14_0@uint64 ymm14_0 ymm4_0;
xor ymm14_1@uint64 ymm14_1 ymm4_1;
xor ymm14_2@uint64 ymm14_2 ymm4_2;
xor ymm14_3@uint64 ymm14_3 ymm4_3;
(* vpbroadcastq -0x290(%rbp),%ymm7                 #! EA = L0x7fffffffbcc0; Value = 0x000000008000808b; PC = 0x555555577482 *)
mov ymm7_0 L0x7fffffffbcc0;
mov ymm7_1 L0x7fffffffbcc0;
mov ymm7_2 L0x7fffffffbcc0;
mov ymm7_3 L0x7fffffffbcc0;
(* vpshufb 0x56a6c(%rip),%ymm14,%ymm14        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x55555557748b *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm14_0, ymm14_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm14_2, ymm14_3, tmp_2, tmp_3);
mov ymm14_0 tmp_0;
mov ymm14_1 tmp_1;
mov ymm14_2 tmp_2;
mov ymm14_3 tmp_3;
(* vpandn %ymm13,%ymm5,%ymm8                       #! PC = 0x555555577494 *)
not ymm5_0n@uint64 ymm5_0;
and ymm8_0@uint64 ymm5_0n ymm13_0;
not ymm5_1n@uint64 ymm5_1;
and ymm8_1@uint64 ymm5_1n ymm13_1;
not ymm5_2n@uint64 ymm5_2;
and ymm8_2@uint64 ymm5_2n ymm13_2;
not ymm5_3n@uint64 ymm5_3;
and ymm8_3@uint64 ymm5_3n ymm13_3;
(* vpxor  %ymm8,%ymm7,%ymm7                        #! PC = 0x555555577499 *)
xor ymm7_0@uint64 ymm7_0 ymm8_0;
xor ymm7_1@uint64 ymm7_1 ymm8_1;
xor ymm7_2@uint64 ymm7_2 ymm8_2;
xor ymm7_3@uint64 ymm7_3 ymm8_3;
(* vpxor  %ymm3,%ymm7,%ymm7                        #! PC = 0x55555557749e *)
xor ymm7_0@uint64 ymm7_0 ymm3_0;
xor ymm7_1@uint64 ymm7_1 ymm3_1;
xor ymm7_2@uint64 ymm7_2 ymm3_2;
xor ymm7_3@uint64 ymm7_3 ymm3_3;
(* vmovdqa %ymm7,-0x390(%rbp)                      #! EA = L0x7fffffffbbc0; PC = 0x5555555774a2 *)
mov L0x7fffffffbbc0 ymm7_0;
mov L0x7fffffffbbc8 ymm7_1;
mov L0x7fffffffbbd0 ymm7_2;
mov L0x7fffffffbbd8 ymm7_3;
(* vpsrlq $0x2b,%ymm9,%ymm7                        #! PC = 0x5555555774aa *)
shr ymm7_0 ymm9_0 0x2b@uint64;
shr ymm7_1 ymm9_1 0x2b@uint64;
shr ymm7_2 ymm9_2 0x2b@uint64;
shr ymm7_3 ymm9_3 0x2b@uint64;
(* vpsllq $0x15,%ymm9,%ymm9                        #! PC = 0x5555555774b0 *)
shl ymm9_0 ymm9_0 0x15@uint64;
shl ymm9_1 ymm9_1 0x15@uint64;
shl ymm9_2 ymm9_2 0x15@uint64;
shl ymm9_3 ymm9_3 0x15@uint64;
(* vpor   %ymm7,%ymm9,%ymm9                        #! PC = 0x5555555774b6 *)
or ymm9_0@uint64 ymm9_0 ymm7_0;
or ymm9_1@uint64 ymm9_1 ymm7_1;
or ymm9_2@uint64 ymm9_2 ymm7_2;
or ymm9_3@uint64 ymm9_3 ymm7_3;
(* vpandn %ymm9,%ymm13,%ymm7                       #! PC = 0x5555555774ba *)
not ymm13_0n@uint64 ymm13_0;
and ymm7_0@uint64 ymm13_0n ymm9_0;
not ymm13_1n@uint64 ymm13_1;
and ymm7_1@uint64 ymm13_1n ymm9_1;
not ymm13_2n@uint64 ymm13_2;
and ymm7_2@uint64 ymm13_2n ymm9_2;
not ymm13_3n@uint64 ymm13_3;
and ymm7_3@uint64 ymm13_3n ymm9_3;
(* vpxor  %ymm5,%ymm7,%ymm8                        #! PC = 0x5555555774bf *)
xor ymm8_0@uint64 ymm7_0 ymm5_0;
xor ymm8_1@uint64 ymm7_1 ymm5_1;
xor ymm8_2@uint64 ymm7_2 ymm5_2;
xor ymm8_3@uint64 ymm7_3 ymm5_3;
(* vpsrlq $0x32,%ymm10,%ymm7                       #! PC = 0x5555555774c3 *)
shr ymm7_0 ymm10_0 0x32@uint64;
shr ymm7_1 ymm10_1 0x32@uint64;
shr ymm7_2 ymm10_2 0x32@uint64;
shr ymm7_3 ymm10_3 0x32@uint64;
(* vpsllq $0xe,%ymm10,%ymm10                       #! PC = 0x5555555774c9 *)
shl ymm10_0 ymm10_0 0xe@uint64;
shl ymm10_1 ymm10_1 0xe@uint64;
shl ymm10_2 ymm10_2 0xe@uint64;
shl ymm10_3 ymm10_3 0xe@uint64;
(* vmovdqa %ymm8,-0x370(%rbp)                      #! EA = L0x7fffffffbbe0; PC = 0x5555555774cf *)
mov L0x7fffffffbbe0 ymm8_0;
mov L0x7fffffffbbe8 ymm8_1;
mov L0x7fffffffbbf0 ymm8_2;
mov L0x7fffffffbbf8 ymm8_3;
(* vpor   %ymm7,%ymm10,%ymm10                      #! PC = 0x5555555774d7 *)
or ymm10_0@uint64 ymm10_0 ymm7_0;
or ymm10_1@uint64 ymm10_1 ymm7_1;
or ymm10_2@uint64 ymm10_2 ymm7_2;
or ymm10_3@uint64 ymm10_3 ymm7_3;
(* vpandn %ymm10,%ymm9,%ymm7                       #! PC = 0x5555555774db *)
not ymm9_0n@uint64 ymm9_0;
and ymm7_0@uint64 ymm9_0n ymm10_0;
not ymm9_1n@uint64 ymm9_1;
and ymm7_1@uint64 ymm9_1n ymm10_1;
not ymm9_2n@uint64 ymm9_2;
and ymm7_2@uint64 ymm9_2n ymm10_2;
not ymm9_3n@uint64 ymm9_3;
and ymm7_3@uint64 ymm9_3n ymm10_3;
(* vpxor  %ymm13,%ymm7,%ymm13                      #! PC = 0x5555555774e0 *)
xor ymm13_0@uint64 ymm7_0 ymm13_0;
xor ymm13_1@uint64 ymm7_1 ymm13_1;
xor ymm13_2@uint64 ymm7_2 ymm13_2;
xor ymm13_3@uint64 ymm7_3 ymm13_3;
(* vpandn %ymm3,%ymm10,%ymm7                       #! PC = 0x5555555774e5 *)
not ymm10_0n@uint64 ymm10_0;
and ymm7_0@uint64 ymm10_0n ymm3_0;
not ymm10_1n@uint64 ymm10_1;
and ymm7_1@uint64 ymm10_1n ymm3_1;
not ymm10_2n@uint64 ymm10_2;
and ymm7_2@uint64 ymm10_2n ymm3_2;
not ymm10_3n@uint64 ymm10_3;
and ymm7_3@uint64 ymm10_3n ymm3_3;
(* vpandn %ymm5,%ymm3,%ymm3                        #! PC = 0x5555555774e9 *)
not ymm3_0n@uint64 ymm3_0;
and ymm3_0@uint64 ymm3_0n ymm5_0;
not ymm3_1n@uint64 ymm3_1;
and ymm3_1@uint64 ymm3_1n ymm5_1;
not ymm3_2n@uint64 ymm3_2;
and ymm3_2@uint64 ymm3_2n ymm5_2;
not ymm3_3n@uint64 ymm3_3;
and ymm3_3@uint64 ymm3_3n ymm5_3;
(* vpxor  %ymm9,%ymm7,%ymm12                       #! PC = 0x5555555774ed *)
xor ymm12_0@uint64 ymm7_0 ymm9_0;
xor ymm12_1@uint64 ymm7_1 ymm9_1;
xor ymm12_2@uint64 ymm7_2 ymm9_2;
xor ymm12_3@uint64 ymm7_3 ymm9_3;
(* vpxor  -0x130(%rbp),%ymm6,%ymm7                 #! EA = L0x7fffffffbe20; Value = 0x4d9f3e32387e66d3; PC = 0x5555555774f2 *)
xor ymm7_0@uint64 ymm6_0 L0x7fffffffbe20;
xor ymm7_1@uint64 ymm6_1 L0x7fffffffbe28;
xor ymm7_2@uint64 ymm6_2 L0x7fffffffbe30;
xor ymm7_3@uint64 ymm6_3 L0x7fffffffbe38;
(* vpxor  %ymm10,%ymm3,%ymm5                       #! PC = 0x5555555774fa *)
xor ymm5_0@uint64 ymm3_0 ymm10_0;
xor ymm5_1@uint64 ymm3_1 ymm10_1;
xor ymm5_2@uint64 ymm3_2 ymm10_2;
xor ymm5_3@uint64 ymm3_3 ymm10_3;
(* vmovdqa %ymm13,-0x350(%rbp)                     #! EA = L0x7fffffffbc00; PC = 0x5555555774ff *)
mov L0x7fffffffbc00 ymm13_0;
mov L0x7fffffffbc08 ymm13_1;
mov L0x7fffffffbc10 ymm13_2;
mov L0x7fffffffbc18 ymm13_3;
(* vmovdqa %ymm12,-0x330(%rbp)                     #! EA = L0x7fffffffbc20; PC = 0x555555577507 *)
mov L0x7fffffffbc20 ymm12_0;
mov L0x7fffffffbc28 ymm12_1;
mov L0x7fffffffbc30 ymm12_2;
mov L0x7fffffffbc38 ymm12_3;
(* vpsrlq $0x24,%ymm7,%ymm3                        #! PC = 0x55555557750f *)
shr ymm3_0 ymm7_0 0x24@uint64;
shr ymm3_1 ymm7_1 0x24@uint64;
shr ymm3_2 ymm7_2 0x24@uint64;
shr ymm3_3 ymm7_3 0x24@uint64;
(* vpsllq $0x1c,%ymm7,%ymm12                       #! PC = 0x555555577514 *)
shl ymm12_0 ymm7_0 0x1c@uint64;
shl ymm12_1 ymm7_1 0x1c@uint64;
shl ymm12_2 ymm7_2 0x1c@uint64;
shl ymm12_3 ymm7_3 0x1c@uint64;
(* vpxor  -0x90(%rbp),%ymm4,%ymm7                  #! EA = L0x7fffffffbec0; Value = 0x8298d4e430062494; PC = 0x555555577519 *)
xor ymm7_0@uint64 ymm4_0 L0x7fffffffbec0;
xor ymm7_1@uint64 ymm4_1 L0x7fffffffbec8;
xor ymm7_2@uint64 ymm4_2 L0x7fffffffbed0;
xor ymm7_3@uint64 ymm4_3 L0x7fffffffbed8;
(* vmovdqa %ymm5,-0x310(%rbp)                      #! EA = L0x7fffffffbc40; PC = 0x555555577521 *)
mov L0x7fffffffbc40 ymm5_0;
mov L0x7fffffffbc48 ymm5_1;
mov L0x7fffffffbc50 ymm5_2;
mov L0x7fffffffbc58 ymm5_3;
(* vpxor  -0x1b0(%rbp),%ymm0,%ymm5                 #! EA = L0x7fffffffbda0; Value = 0x3229e84a8e1704c2; PC = 0x555555577529 *)
xor ymm5_0@uint64 ymm0_0 L0x7fffffffbda0;
xor ymm5_1@uint64 ymm0_1 L0x7fffffffbda8;
xor ymm5_2@uint64 ymm0_2 L0x7fffffffbdb0;
xor ymm5_3@uint64 ymm0_3 L0x7fffffffbdb8;
(* vpor   %ymm3,%ymm12,%ymm12                      #! PC = 0x555555577531 *)
or ymm12_0@uint64 ymm12_0 ymm3_0;
or ymm12_1@uint64 ymm12_1 ymm3_1;
or ymm12_2@uint64 ymm12_2 ymm3_2;
or ymm12_3@uint64 ymm12_3 ymm3_3;
(* vpsrlq $0x2c,%ymm7,%ymm3                        #! PC = 0x555555577535 *)
shr ymm3_0 ymm7_0 0x2c@uint64;
shr ymm3_1 ymm7_1 0x2c@uint64;
shr ymm3_2 ymm7_2 0x2c@uint64;
shr ymm3_3 ymm7_3 0x2c@uint64;
(* vpsllq $0x14,%ymm7,%ymm7                        #! PC = 0x55555557753a *)
shl ymm7_0 ymm7_0 0x14@uint64;
shl ymm7_1 ymm7_1 0x14@uint64;
shl ymm7_2 ymm7_2 0x14@uint64;
shl ymm7_3 ymm7_3 0x14@uint64;
(* vpor   %ymm3,%ymm7,%ymm7                        #! PC = 0x55555557753f *)
or ymm7_0@uint64 ymm7_0 ymm3_0;
or ymm7_1@uint64 ymm7_1 ymm3_1;
or ymm7_2@uint64 ymm7_2 ymm3_2;
or ymm7_3@uint64 ymm7_3 ymm3_3;
(* vpsrlq $0x3d,%ymm5,%ymm3                        #! PC = 0x555555577543 *)
shr ymm3_0 ymm5_0 0x3d@uint64;
shr ymm3_1 ymm5_1 0x3d@uint64;
shr ymm3_2 ymm5_2 0x3d@uint64;
shr ymm3_3 ymm5_3 0x3d@uint64;
(* vpsllq $0x3,%ymm5,%ymm5                         #! PC = 0x555555577548 *)
shl ymm5_0 ymm5_0 0x3@uint64;
shl ymm5_1 ymm5_1 0x3@uint64;
shl ymm5_2 ymm5_2 0x3@uint64;
shl ymm5_3 ymm5_3 0x3@uint64;
(* vpor   %ymm3,%ymm5,%ymm5                        #! PC = 0x55555557754d *)
or ymm5_0@uint64 ymm5_0 ymm3_0;
or ymm5_1@uint64 ymm5_1 ymm3_1;
or ymm5_2@uint64 ymm5_2 ymm3_2;
or ymm5_3@uint64 ymm5_3 ymm3_3;
(* vpandn %ymm5,%ymm7,%ymm3                        #! PC = 0x555555577551 *)
not ymm7_0n@uint64 ymm7_0;
and ymm3_0@uint64 ymm7_0n ymm5_0;
not ymm7_1n@uint64 ymm7_1;
and ymm3_1@uint64 ymm7_1n ymm5_1;
not ymm7_2n@uint64 ymm7_2;
and ymm3_2@uint64 ymm7_2n ymm5_2;
not ymm7_3n@uint64 ymm7_3;
and ymm3_3@uint64 ymm7_3n ymm5_3;
(* vpxor  %ymm12,%ymm3,%ymm11                      #! PC = 0x555555577555 *)
xor ymm11_0@uint64 ymm3_0 ymm12_0;
xor ymm11_1@uint64 ymm3_1 ymm12_1;
xor ymm11_2@uint64 ymm3_2 ymm12_2;
xor ymm11_3@uint64 ymm3_3 ymm12_3;
(* vpxor  -0x210(%rbp),%ymm2,%ymm3                 #! EA = L0x7fffffffbd40; Value = 0x667e09de6b59e740; PC = 0x55555557755a *)
xor ymm3_0@uint64 ymm2_0 L0x7fffffffbd40;
xor ymm3_1@uint64 ymm2_1 L0x7fffffffbd48;
xor ymm3_2@uint64 ymm2_2 L0x7fffffffbd50;
xor ymm3_3@uint64 ymm2_3 L0x7fffffffbd58;
(* vmovdqa %ymm11,-0x2f0(%rbp)                     #! EA = L0x7fffffffbc60; PC = 0x555555577562 *)
mov L0x7fffffffbc60 ymm11_0;
mov L0x7fffffffbc68 ymm11_1;
mov L0x7fffffffbc70 ymm11_2;
mov L0x7fffffffbc78 ymm11_3;
(* vpxor  -0x230(%rbp),%ymm1,%ymm11                #! EA = L0x7fffffffbd20; Value = 0xc08877b3d1a7a9f3; PC = 0x55555557756a *)
xor ymm11_0@uint64 ymm1_0 L0x7fffffffbd20;
xor ymm11_1@uint64 ymm1_1 L0x7fffffffbd28;
xor ymm11_2@uint64 ymm1_2 L0x7fffffffbd30;
xor ymm11_3@uint64 ymm1_3 L0x7fffffffbd38;
(* vpsrlq $0x13,%ymm3,%ymm8                        #! PC = 0x555555577572 *)
shr ymm8_0 ymm3_0 0x13@uint64;
shr ymm8_1 ymm3_1 0x13@uint64;
shr ymm8_2 ymm3_2 0x13@uint64;
shr ymm8_3 ymm3_3 0x13@uint64;
(* vpsllq $0x2d,%ymm3,%ymm13                       #! PC = 0x555555577577 *)
shl ymm13_0 ymm3_0 0x2d@uint64;
shl ymm13_1 ymm3_1 0x2d@uint64;
shl ymm13_2 ymm3_2 0x2d@uint64;
shl ymm13_3 ymm3_3 0x2d@uint64;
(* vpor   %ymm8,%ymm13,%ymm13                      #! PC = 0x55555557757c *)
or ymm13_0@uint64 ymm13_0 ymm8_0;
or ymm13_1@uint64 ymm13_1 ymm8_1;
or ymm13_2@uint64 ymm13_2 ymm8_2;
or ymm13_3@uint64 ymm13_3 ymm8_3;
(* vpsrlq $0x3,%ymm15,%ymm8                        #! PC = 0x555555577581 *)
shr ymm8_0 ymm15_0 0x3@uint64;
shr ymm8_1 ymm15_1 0x3@uint64;
shr ymm8_2 ymm15_2 0x3@uint64;
shr ymm8_3 ymm15_3 0x3@uint64;
(* vpsllq $0x3d,%ymm15,%ymm15                      #! PC = 0x555555577587 *)
shl ymm15_0 ymm15_0 0x3d@uint64;
shl ymm15_1 ymm15_1 0x3d@uint64;
shl ymm15_2 ymm15_2 0x3d@uint64;
shl ymm15_3 ymm15_3 0x3d@uint64;
(* vpandn %ymm13,%ymm5,%ymm10                      #! PC = 0x55555557758d *)
not ymm5_0n@uint64 ymm5_0;
and ymm10_0@uint64 ymm5_0n ymm13_0;
not ymm5_1n@uint64 ymm5_1;
and ymm10_1@uint64 ymm5_1n ymm13_1;
not ymm5_2n@uint64 ymm5_2;
and ymm10_2@uint64 ymm5_2n ymm13_2;
not ymm5_3n@uint64 ymm5_3;
and ymm10_3@uint64 ymm5_3n ymm13_3;
(* vpor   %ymm8,%ymm15,%ymm3                       #! PC = 0x555555577592 *)
or ymm3_0@uint64 ymm15_0 ymm8_0;
or ymm3_1@uint64 ymm15_1 ymm8_1;
or ymm3_2@uint64 ymm15_2 ymm8_2;
or ymm3_3@uint64 ymm15_3 ymm8_3;
(* vpxor  %ymm7,%ymm10,%ymm10                      #! PC = 0x555555577597 *)
xor ymm10_0@uint64 ymm10_0 ymm7_0;
xor ymm10_1@uint64 ymm10_1 ymm7_1;
xor ymm10_2@uint64 ymm10_2 ymm7_2;
xor ymm10_3@uint64 ymm10_3 ymm7_3;
(* vpandn %ymm3,%ymm13,%ymm8                       #! PC = 0x55555557759b *)
not ymm13_0n@uint64 ymm13_0;
and ymm8_0@uint64 ymm13_0n ymm3_0;
not ymm13_1n@uint64 ymm13_1;
and ymm8_1@uint64 ymm13_1n ymm3_1;
not ymm13_2n@uint64 ymm13_2;
and ymm8_2@uint64 ymm13_2n ymm3_2;
not ymm13_3n@uint64 ymm13_3;
and ymm8_3@uint64 ymm13_3n ymm3_3;
(* vpxor  %ymm5,%ymm8,%ymm15                       #! PC = 0x55555557759f *)
xor ymm15_0@uint64 ymm8_0 ymm5_0;
xor ymm15_1@uint64 ymm8_1 ymm5_1;
xor ymm15_2@uint64 ymm8_2 ymm5_2;
xor ymm15_3@uint64 ymm8_3 ymm5_3;
(* vpandn %ymm12,%ymm3,%ymm5                       #! PC = 0x5555555775a3 *)
not ymm3_0n@uint64 ymm3_0;
and ymm5_0@uint64 ymm3_0n ymm12_0;
not ymm3_1n@uint64 ymm3_1;
and ymm5_1@uint64 ymm3_1n ymm12_1;
not ymm3_2n@uint64 ymm3_2;
and ymm5_2@uint64 ymm3_2n ymm12_2;
not ymm3_3n@uint64 ymm3_3;
and ymm5_3@uint64 ymm3_3n ymm12_3;
(* vpandn %ymm7,%ymm12,%ymm12                      #! PC = 0x5555555775a8 *)
not ymm12_0n@uint64 ymm12_0;
and ymm12_0@uint64 ymm12_0n ymm7_0;
not ymm12_1n@uint64 ymm12_1;
and ymm12_1@uint64 ymm12_1n ymm7_1;
not ymm12_2n@uint64 ymm12_2;
and ymm12_2@uint64 ymm12_2n ymm7_2;
not ymm12_3n@uint64 ymm12_3;
and ymm12_3@uint64 ymm12_3n ymm7_3;
(* vpxor  %ymm13,%ymm5,%ymm9                       #! PC = 0x5555555775ac *)
xor ymm9_0@uint64 ymm5_0 ymm13_0;
xor ymm9_1@uint64 ymm5_1 ymm13_1;
xor ymm9_2@uint64 ymm5_2 ymm13_2;
xor ymm9_3@uint64 ymm5_3 ymm13_3;
(* vpxor  -0x190(%rbp),%ymm1,%ymm8                 #! EA = L0x7fffffffbdc0; Value = 0x81e9f633f01dcf03; PC = 0x5555555775b1 *)
xor ymm8_0@uint64 ymm1_0 L0x7fffffffbdc0;
xor ymm8_1@uint64 ymm1_1 L0x7fffffffbdc8;
xor ymm8_2@uint64 ymm1_2 L0x7fffffffbdd0;
xor ymm8_3@uint64 ymm1_3 L0x7fffffffbdd8;
(* vpxor  -0x3b0(%rbp),%ymm2,%ymm13                #! EA = L0x7fffffffbba0; Value = 0x8409f6c9be239ecb; PC = 0x5555555775b9 *)
xor ymm13_0@uint64 ymm2_0 L0x7fffffffbba0;
xor ymm13_1@uint64 ymm2_1 L0x7fffffffbba8;
xor ymm13_2@uint64 ymm2_2 L0x7fffffffbbb0;
xor ymm13_3@uint64 ymm2_3 L0x7fffffffbbb8;
(* vmovdqa %ymm15,-0x2d0(%rbp)                     #! EA = L0x7fffffffbc80; PC = 0x5555555775c1 *)
mov L0x7fffffffbc80 ymm15_0;
mov L0x7fffffffbc88 ymm15_1;
mov L0x7fffffffbc90 ymm15_2;
mov L0x7fffffffbc98 ymm15_3;
(* vpxor  %ymm3,%ymm12,%ymm3                       #! PC = 0x5555555775c9 *)
xor ymm3_0@uint64 ymm12_0 ymm3_0;
xor ymm3_1@uint64 ymm12_1 ymm3_1;
xor ymm3_2@uint64 ymm12_2 ymm3_2;
xor ymm3_3@uint64 ymm12_3 ymm3_3;
(* vpxor  -0xd0(%rbp),%ymm6,%ymm12                 #! EA = L0x7fffffffbe80; Value = 0x52de6de69935b888; PC = 0x5555555775cd *)
xor ymm12_0@uint64 ymm6_0 L0x7fffffffbe80;
xor ymm12_1@uint64 ymm6_1 L0x7fffffffbe88;
xor ymm12_2@uint64 ymm6_2 L0x7fffffffbe90;
xor ymm12_3@uint64 ymm6_3 L0x7fffffffbe98;
(* vmovdqa %ymm9,-0x2b0(%rbp)                      #! EA = L0x7fffffffbca0; PC = 0x5555555775d5 *)
mov L0x7fffffffbca0 ymm9_0;
mov L0x7fffffffbca8 ymm9_1;
mov L0x7fffffffbcb0 ymm9_2;
mov L0x7fffffffbcb8 ymm9_3;
(* vpsrlq $0x3a,%ymm8,%ymm5                        #! PC = 0x5555555775dd *)
shr ymm5_0 ymm8_0 0x3a@uint64;
shr ymm5_1 ymm8_1 0x3a@uint64;
shr ymm5_2 ymm8_2 0x3a@uint64;
shr ymm5_3 ymm8_3 0x3a@uint64;
(* vmovdqa %ymm3,-0x210(%rbp)                      #! EA = L0x7fffffffbd40; PC = 0x5555555775e3 *)
mov L0x7fffffffbd40 ymm3_0;
mov L0x7fffffffbd48 ymm3_1;
mov L0x7fffffffbd50 ymm3_2;
mov L0x7fffffffbd58 ymm3_3;
(* vpsllq $0x6,%ymm8,%ymm8                         #! PC = 0x5555555775eb *)
shl ymm8_0 ymm8_0 0x6@uint64;
shl ymm8_1 ymm8_1 0x6@uint64;
shl ymm8_2 ymm8_2 0x6@uint64;
shl ymm8_3 ymm8_3 0x6@uint64;
(* vpsrlq $0x3f,%ymm13,%ymm3                       #! PC = 0x5555555775f1 *)
shr ymm3_0 ymm13_0 0x3f@uint64;
shr ymm3_1 ymm13_1 0x3f@uint64;
shr ymm3_2 ymm13_2 0x3f@uint64;
shr ymm3_3 ymm13_3 0x3f@uint64;
(* vpsrlq $0x27,%ymm12,%ymm7                       #! PC = 0x5555555775f7 *)
shr ymm7_0 ymm12_0 0x27@uint64;
shr ymm7_1 ymm12_1 0x27@uint64;
shr ymm7_2 ymm12_2 0x27@uint64;
shr ymm7_3 ymm12_3 0x27@uint64;
(* vpsllq $0x1,%ymm13,%ymm13                       #! PC = 0x5555555775fd *)
shl ymm13_0 ymm13_0 0x1@uint64;
shl ymm13_1 ymm13_1 0x1@uint64;
shl ymm13_2 ymm13_2 0x1@uint64;
shl ymm13_3 ymm13_3 0x1@uint64;
(* vpsllq $0x19,%ymm12,%ymm12                      #! PC = 0x555555577603 *)
shl ymm12_0 ymm12_0 0x19@uint64;
shl ymm12_1 ymm12_1 0x19@uint64;
shl ymm12_2 ymm12_2 0x19@uint64;
shl ymm12_3 ymm12_3 0x19@uint64;
(* vpor   %ymm3,%ymm13,%ymm13                      #! PC = 0x555555577609 *)
or ymm13_0@uint64 ymm13_0 ymm3_0;
or ymm13_1@uint64 ymm13_1 ymm3_1;
or ymm13_2@uint64 ymm13_2 ymm3_2;
or ymm13_3@uint64 ymm13_3 ymm3_3;
(* vpor   %ymm5,%ymm8,%ymm3                        #! PC = 0x55555557760d *)
or ymm3_0@uint64 ymm8_0 ymm5_0;
or ymm3_1@uint64 ymm8_1 ymm5_1;
or ymm3_2@uint64 ymm8_2 ymm5_2;
or ymm3_3@uint64 ymm8_3 ymm5_3;
(* vpor   %ymm7,%ymm12,%ymm5                       #! PC = 0x555555577611 *)
or ymm5_0@uint64 ymm12_0 ymm7_0;
or ymm5_1@uint64 ymm12_1 ymm7_1;
or ymm5_2@uint64 ymm12_2 ymm7_2;
or ymm5_3@uint64 ymm12_3 ymm7_3;
(* vpandn %ymm5,%ymm3,%ymm7                        #! PC = 0x555555577615 *)
not ymm3_0n@uint64 ymm3_0;
and ymm7_0@uint64 ymm3_0n ymm5_0;
not ymm3_1n@uint64 ymm3_1;
and ymm7_1@uint64 ymm3_1n ymm5_1;
not ymm3_2n@uint64 ymm3_2;
and ymm7_2@uint64 ymm3_2n ymm5_2;
not ymm3_3n@uint64 ymm3_3;
and ymm7_3@uint64 ymm3_3n ymm5_3;
(* vpxor  %ymm13,%ymm7,%ymm15                      #! PC = 0x555555577619 *)
xor ymm15_0@uint64 ymm7_0 ymm13_0;
xor ymm15_1@uint64 ymm7_1 ymm13_1;
xor ymm15_2@uint64 ymm7_2 ymm13_2;
xor ymm15_3@uint64 ymm7_3 ymm13_3;
(* vpandn %ymm14,%ymm5,%ymm7                       #! PC = 0x55555557761e *)
not ymm5_0n@uint64 ymm5_0;
and ymm7_0@uint64 ymm5_0n ymm14_0;
not ymm5_1n@uint64 ymm5_1;
and ymm7_1@uint64 ymm5_1n ymm14_1;
not ymm5_2n@uint64 ymm5_2;
and ymm7_2@uint64 ymm5_2n ymm14_2;
not ymm5_3n@uint64 ymm5_3;
and ymm7_3@uint64 ymm5_3n ymm14_3;
(* vpxor  %ymm3,%ymm7,%ymm8                        #! PC = 0x555555577623 *)
xor ymm8_0@uint64 ymm7_0 ymm3_0;
xor ymm8_1@uint64 ymm7_1 ymm3_1;
xor ymm8_2@uint64 ymm7_2 ymm3_2;
xor ymm8_3@uint64 ymm7_3 ymm3_3;
(* vpxor  -0x250(%rbp),%ymm0,%ymm7                 #! EA = L0x7fffffffbd00; Value = 0x77badafbfd43d602; PC = 0x555555577627 *)
xor ymm7_0@uint64 ymm0_0 L0x7fffffffbd00;
xor ymm7_1@uint64 ymm0_1 L0x7fffffffbd08;
xor ymm7_2@uint64 ymm0_2 L0x7fffffffbd10;
xor ymm7_3@uint64 ymm0_3 L0x7fffffffbd18;
(* vmovdqa %ymm15,-0xd0(%rbp)                      #! EA = L0x7fffffffbe80; PC = 0x55555557762f *)
mov L0x7fffffffbe80 ymm15_0;
mov L0x7fffffffbe88 ymm15_1;
mov L0x7fffffffbe90 ymm15_2;
mov L0x7fffffffbe98 ymm15_3;
(* vmovdqa %ymm8,-0x290(%rbp)                      #! EA = L0x7fffffffbcc0; PC = 0x555555577637 *)
mov L0x7fffffffbcc0 ymm8_0;
mov L0x7fffffffbcc8 ymm8_1;
mov L0x7fffffffbcd0 ymm8_2;
mov L0x7fffffffbcd8 ymm8_3;
(* vpsrlq $0x2e,%ymm7,%ymm9                        #! PC = 0x55555557763f *)
shr ymm9_0 ymm7_0 0x2e@uint64;
shr ymm9_1 ymm7_1 0x2e@uint64;
shr ymm9_2 ymm7_2 0x2e@uint64;
shr ymm9_3 ymm7_3 0x2e@uint64;
(* vpsllq $0x12,%ymm7,%ymm8                        #! PC = 0x555555577644 *)
shl ymm8_0 ymm7_0 0x12@uint64;
shl ymm8_1 ymm7_1 0x12@uint64;
shl ymm8_2 ymm7_2 0x12@uint64;
shl ymm8_3 ymm7_3 0x12@uint64;
(* vpor   %ymm9,%ymm8,%ymm8                        #! PC = 0x555555577649 *)
or ymm8_0@uint64 ymm8_0 ymm9_0;
or ymm8_1@uint64 ymm8_1 ymm9_1;
or ymm8_2@uint64 ymm8_2 ymm9_2;
or ymm8_3@uint64 ymm8_3 ymm9_3;
(* vpandn %ymm13,%ymm8,%ymm9                       #! PC = 0x55555557764e *)
not ymm8_0n@uint64 ymm8_0;
and ymm9_0@uint64 ymm8_0n ymm13_0;
not ymm8_1n@uint64 ymm8_1;
and ymm9_1@uint64 ymm8_1n ymm13_1;
not ymm8_2n@uint64 ymm8_2;
and ymm9_2@uint64 ymm8_2n ymm13_2;
not ymm8_3n@uint64 ymm8_3;
and ymm9_3@uint64 ymm8_3n ymm13_3;
(* vpandn %ymm8,%ymm14,%ymm12                      #! PC = 0x555555577653 *)
not ymm14_0n@uint64 ymm14_0;
and ymm12_0@uint64 ymm14_0n ymm8_0;
not ymm14_1n@uint64 ymm14_1;
and ymm12_1@uint64 ymm14_1n ymm8_1;
not ymm14_2n@uint64 ymm14_2;
and ymm12_2@uint64 ymm14_2n ymm8_2;
not ymm14_3n@uint64 ymm14_3;
and ymm12_3@uint64 ymm14_3n ymm8_3;
(* vpandn %ymm3,%ymm13,%ymm13                      #! PC = 0x555555577658 *)
not ymm13_0n@uint64 ymm13_0;
and ymm13_0@uint64 ymm13_0n ymm3_0;
not ymm13_1n@uint64 ymm13_1;
and ymm13_1@uint64 ymm13_1n ymm3_1;
not ymm13_2n@uint64 ymm13_2;
and ymm13_2@uint64 ymm13_2n ymm3_2;
not ymm13_3n@uint64 ymm13_3;
and ymm13_3@uint64 ymm13_3n ymm3_3;
(* vpxor  %ymm14,%ymm9,%ymm14                      #! PC = 0x55555557765c *)
xor ymm14_0@uint64 ymm9_0 ymm14_0;
xor ymm14_1@uint64 ymm9_1 ymm14_1;
xor ymm14_2@uint64 ymm9_2 ymm14_2;
xor ymm14_3@uint64 ymm9_3 ymm14_3;
(* vpxor  -0x50(%rbp),%ymm4,%ymm9                  #! EA = L0x7fffffffbf00; Value = 0x3be7f6680bb1c847; PC = 0x555555577661 *)
xor ymm9_0@uint64 ymm4_0 L0x7fffffffbf00;
xor ymm9_1@uint64 ymm4_1 L0x7fffffffbf08;
xor ymm9_2@uint64 ymm4_2 L0x7fffffffbf10;
xor ymm9_3@uint64 ymm4_3 L0x7fffffffbf18;
(* vpxor  %ymm5,%ymm12,%ymm12                      #! PC = 0x555555577666 *)
xor ymm12_0@uint64 ymm12_0 ymm5_0;
xor ymm12_1@uint64 ymm12_1 ymm5_1;
xor ymm12_2@uint64 ymm12_2 ymm5_2;
xor ymm12_3@uint64 ymm12_3 ymm5_3;
(* vpxor  %ymm8,%ymm13,%ymm13                      #! PC = 0x55555557766a *)
xor ymm13_0@uint64 ymm13_0 ymm8_0;
xor ymm13_1@uint64 ymm13_1 ymm8_1;
xor ymm13_2@uint64 ymm13_2 ymm8_2;
xor ymm13_3@uint64 ymm13_3 ymm8_3;
(* vpxor  -0x3d0(%rbp),%ymm2,%ymm8                 #! EA = L0x7fffffffbb80; Value = 0x56f221867a1b3140; PC = 0x55555557766f *)
xor ymm8_0@uint64 ymm2_0 L0x7fffffffbb80;
xor ymm8_1@uint64 ymm2_1 L0x7fffffffbb88;
xor ymm8_2@uint64 ymm2_2 L0x7fffffffbb90;
xor ymm8_3@uint64 ymm2_3 L0x7fffffffbb98;
(* vmovdqa %ymm14,-0x170(%rbp)                     #! EA = L0x7fffffffbde0; PC = 0x555555577677 *)
mov L0x7fffffffbde0 ymm14_0;
mov L0x7fffffffbde8 ymm14_1;
mov L0x7fffffffbdf0 ymm14_2;
mov L0x7fffffffbdf8 ymm14_3;
(* vpsrlq $0x25,%ymm9,%ymm5                        #! PC = 0x55555557767f *)
shr ymm5_0 ymm9_0 0x25@uint64;
shr ymm5_1 ymm9_1 0x25@uint64;
shr ymm5_2 ymm9_2 0x25@uint64;
shr ymm5_3 ymm9_3 0x25@uint64;
(* vpsllq $0x1b,%ymm9,%ymm9                        #! PC = 0x555555577685 *)
shl ymm9_0 ymm9_0 0x1b@uint64;
shl ymm9_1 ymm9_1 0x1b@uint64;
shl ymm9_2 ymm9_2 0x1b@uint64;
shl ymm9_3 ymm9_3 0x1b@uint64;
(* vmovdqa %ymm13,-0x1f0(%rbp)                     #! EA = L0x7fffffffbd60; PC = 0x55555557768b *)
mov L0x7fffffffbd60 ymm13_0;
mov L0x7fffffffbd68 ymm13_1;
mov L0x7fffffffbd70 ymm13_2;
mov L0x7fffffffbd78 ymm13_3;
(* vpxor  -0x270(%rbp),%ymm6,%ymm13                #! EA = L0x7fffffffbce0; Value = 0x366d5164bb8f0bb5; PC = 0x555555577693 *)
xor ymm13_0@uint64 ymm6_0 L0x7fffffffbce0;
xor ymm13_1@uint64 ymm6_1 L0x7fffffffbce8;
xor ymm13_2@uint64 ymm6_2 L0x7fffffffbcf0;
xor ymm13_3@uint64 ymm6_3 L0x7fffffffbcf8;
(* vpor   %ymm5,%ymm9,%ymm3                        #! PC = 0x55555557769b *)
or ymm3_0@uint64 ymm9_0 ymm5_0;
or ymm3_1@uint64 ymm9_1 ymm5_1;
or ymm3_2@uint64 ymm9_2 ymm5_2;
or ymm3_3@uint64 ymm9_3 ymm5_3;
(* vpxor  -0x150(%rbp),%ymm0,%ymm9                 #! EA = L0x7fffffffbe00; Value = 0x61ed7fb0ce6ca6af; PC = 0x55555557769f *)
xor ymm9_0@uint64 ymm0_0 L0x7fffffffbe00;
xor ymm9_1@uint64 ymm0_1 L0x7fffffffbe08;
xor ymm9_2@uint64 ymm0_2 L0x7fffffffbe10;
xor ymm9_3@uint64 ymm0_3 L0x7fffffffbe18;
(* vpshufb 0x56830(%rip),%ymm13,%ymm13        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x5555555776a7 *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm13_0, ymm13_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm13_2, ymm13_3, tmp_2, tmp_3);
mov ymm13_0 tmp_0;
mov ymm13_1 tmp_1;
mov ymm13_2 tmp_2;
mov ymm13_3 tmp_3;
(* vpsrlq $0x1c,%ymm9,%ymm5                        #! PC = 0x5555555776b0 *)
shr ymm5_0 ymm9_0 0x1c@uint64;
shr ymm5_1 ymm9_1 0x1c@uint64;
shr ymm5_2 ymm9_2 0x1c@uint64;
shr ymm5_3 ymm9_3 0x1c@uint64;
(* vpsllq $0x24,%ymm9,%ymm9                        #! PC = 0x5555555776b6 *)
shl ymm9_0 ymm9_0 0x24@uint64;
shl ymm9_1 ymm9_1 0x24@uint64;
shl ymm9_2 ymm9_2 0x24@uint64;
shl ymm9_3 ymm9_3 0x24@uint64;
(* vpor   %ymm5,%ymm9,%ymm7                        #! PC = 0x5555555776bc *)
or ymm7_0@uint64 ymm9_0 ymm5_0;
or ymm7_1@uint64 ymm9_1 ymm5_1;
or ymm7_2@uint64 ymm9_2 ymm5_2;
or ymm7_3@uint64 ymm9_3 ymm5_3;
(* vpsrlq $0x36,%ymm8,%ymm5                        #! PC = 0x5555555776c0 *)
shr ymm5_0 ymm8_0 0x36@uint64;
shr ymm5_1 ymm8_1 0x36@uint64;
shr ymm5_2 ymm8_2 0x36@uint64;
shr ymm5_3 ymm8_3 0x36@uint64;
(* vpsllq $0xa,%ymm8,%ymm8                         #! PC = 0x5555555776c6 *)
shl ymm8_0 ymm8_0 0xa@uint64;
shl ymm8_1 ymm8_1 0xa@uint64;
shl ymm8_2 ymm8_2 0xa@uint64;
shl ymm8_3 ymm8_3 0xa@uint64;
(* vpsrlq $0x31,%ymm11,%ymm9                       #! PC = 0x5555555776cc *)
shr ymm9_0 ymm11_0 0x31@uint64;
shr ymm9_1 ymm11_1 0x31@uint64;
shr ymm9_2 ymm11_2 0x31@uint64;
shr ymm9_3 ymm11_3 0x31@uint64;
(* vpor   %ymm5,%ymm8,%ymm8                        #! PC = 0x5555555776d2 *)
or ymm8_0@uint64 ymm8_0 ymm5_0;
or ymm8_1@uint64 ymm8_1 ymm5_1;
or ymm8_2@uint64 ymm8_2 ymm5_2;
or ymm8_3@uint64 ymm8_3 ymm5_3;
(* vpsllq $0xf,%ymm11,%ymm11                       #! PC = 0x5555555776d6 *)
shl ymm11_0 ymm11_0 0xf@uint64;
shl ymm11_1 ymm11_1 0xf@uint64;
shl ymm11_2 ymm11_2 0xf@uint64;
shl ymm11_3 ymm11_3 0xf@uint64;
(* vpandn %ymm8,%ymm7,%ymm5                        #! PC = 0x5555555776dc *)
not ymm7_0n@uint64 ymm7_0;
and ymm5_0@uint64 ymm7_0n ymm8_0;
not ymm7_1n@uint64 ymm7_1;
and ymm5_1@uint64 ymm7_1n ymm8_1;
not ymm7_2n@uint64 ymm7_2;
and ymm5_2@uint64 ymm7_2n ymm8_2;
not ymm7_3n@uint64 ymm7_3;
and ymm5_3@uint64 ymm7_3n ymm8_3;
(* vpxor  %ymm3,%ymm5,%ymm14                       #! PC = 0x5555555776e1 *)
xor ymm14_0@uint64 ymm5_0 ymm3_0;
xor ymm14_1@uint64 ymm5_1 ymm3_1;
xor ymm14_2@uint64 ymm5_2 ymm3_2;
xor ymm14_3@uint64 ymm5_3 ymm3_3;
(* vpor   %ymm9,%ymm11,%ymm5                       #! PC = 0x5555555776e5 *)
or ymm5_0@uint64 ymm11_0 ymm9_0;
or ymm5_1@uint64 ymm11_1 ymm9_1;
or ymm5_2@uint64 ymm11_2 ymm9_2;
or ymm5_3@uint64 ymm11_3 ymm9_3;
(* vpandn %ymm13,%ymm5,%ymm11                      #! PC = 0x5555555776ea *)
not ymm5_0n@uint64 ymm5_0;
and ymm11_0@uint64 ymm5_0n ymm13_0;
not ymm5_1n@uint64 ymm5_1;
and ymm11_1@uint64 ymm5_1n ymm13_1;
not ymm5_2n@uint64 ymm5_2;
and ymm11_2@uint64 ymm5_2n ymm13_2;
not ymm5_3n@uint64 ymm5_3;
and ymm11_3@uint64 ymm5_3n ymm13_3;
(* vpandn %ymm5,%ymm8,%ymm9                        #! PC = 0x5555555776ef *)
not ymm8_0n@uint64 ymm8_0;
and ymm9_0@uint64 ymm8_0n ymm5_0;
not ymm8_1n@uint64 ymm8_1;
and ymm9_1@uint64 ymm8_1n ymm5_1;
not ymm8_2n@uint64 ymm8_2;
and ymm9_2@uint64 ymm8_2n ymm5_2;
not ymm8_3n@uint64 ymm8_3;
and ymm9_3@uint64 ymm8_3n ymm5_3;
(* vpxor  %ymm8,%ymm11,%ymm11                      #! PC = 0x5555555776f3 *)
xor ymm11_0@uint64 ymm11_0 ymm8_0;
xor ymm11_1@uint64 ymm11_1 ymm8_1;
xor ymm11_2@uint64 ymm11_2 ymm8_2;
xor ymm11_3@uint64 ymm11_3 ymm8_3;
(* vpxor  %ymm7,%ymm9,%ymm9                        #! PC = 0x5555555776f8 *)
xor ymm9_0@uint64 ymm9_0 ymm7_0;
xor ymm9_1@uint64 ymm9_1 ymm7_1;
xor ymm9_2@uint64 ymm9_2 ymm7_2;
xor ymm9_3@uint64 ymm9_3 ymm7_3;
(* vmovdqa %ymm11,-0x130(%rbp)                     #! EA = L0x7fffffffbe20; PC = 0x5555555776fc *)
mov L0x7fffffffbe20 ymm11_0;
mov L0x7fffffffbe28 ymm11_1;
mov L0x7fffffffbe30 ymm11_2;
mov L0x7fffffffbe38 ymm11_3;
(* vpandn %ymm3,%ymm13,%ymm11                      #! PC = 0x555555577704 *)
not ymm13_0n@uint64 ymm13_0;
and ymm11_0@uint64 ymm13_0n ymm3_0;
not ymm13_1n@uint64 ymm13_1;
and ymm11_1@uint64 ymm13_1n ymm3_1;
not ymm13_2n@uint64 ymm13_2;
and ymm11_2@uint64 ymm13_2n ymm3_2;
not ymm13_3n@uint64 ymm13_3;
and ymm11_3@uint64 ymm13_3n ymm3_3;
(* vpandn %ymm7,%ymm3,%ymm3                        #! PC = 0x555555577708 *)
not ymm3_0n@uint64 ymm3_0;
and ymm3_0@uint64 ymm3_0n ymm7_0;
not ymm3_1n@uint64 ymm3_1;
and ymm3_1@uint64 ymm3_1n ymm7_1;
not ymm3_2n@uint64 ymm3_2;
and ymm3_2@uint64 ymm3_2n ymm7_2;
not ymm3_3n@uint64 ymm3_3;
and ymm3_3@uint64 ymm3_3n ymm7_3;
(* vpxor  %ymm5,%ymm11,%ymm11                      #! PC = 0x55555557770c *)
xor ymm11_0@uint64 ymm11_0 ymm5_0;
xor ymm11_1@uint64 ymm11_1 ymm5_1;
xor ymm11_2@uint64 ymm11_2 ymm5_2;
xor ymm11_3@uint64 ymm11_3 ymm5_3;
(* vpxor  %ymm13,%ymm3,%ymm5                       #! PC = 0x555555577710 *)
xor ymm5_0@uint64 ymm3_0 ymm13_0;
xor ymm5_1@uint64 ymm3_1 ymm13_1;
xor ymm5_2@uint64 ymm3_2 ymm13_2;
xor ymm5_3@uint64 ymm3_3 ymm13_3;
(* vmovdqa %ymm5,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x555555577715 *)
mov L0x7fffffffbf00 ymm5_0;
mov L0x7fffffffbf08 ymm5_1;
mov L0x7fffffffbf10 ymm5_2;
mov L0x7fffffffbf18 ymm5_3;
(* vpxor  -0x110(%rbp),%ymm1,%ymm1                 #! EA = L0x7fffffffbe40; Value = 0x12657d5a4568ead5; PC = 0x55555557771a *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbe40;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbe48;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbe50;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbe58;
(* vpxor  -0x70(%rbp),%ymm6,%ymm6                  #! EA = L0x7fffffffbee0; Value = 0xab5eefa38e5546d9; PC = 0x555555577722 *)
xor ymm6_0@uint64 ymm6_0 L0x7fffffffbee0;
xor ymm6_1@uint64 ymm6_1 L0x7fffffffbee8;
xor ymm6_2@uint64 ymm6_2 L0x7fffffffbef0;
xor ymm6_3@uint64 ymm6_3 L0x7fffffffbef8;
(* vpxor  -0xb0(%rbp),%ymm4,%ymm4                  #! EA = L0x7fffffffbea0; Value = 0x98a1c2d38194eda1; PC = 0x555555577727 *)
xor ymm4_0@uint64 ymm4_0 L0x7fffffffbea0;
xor ymm4_1@uint64 ymm4_1 L0x7fffffffbea8;
xor ymm4_2@uint64 ymm4_2 L0x7fffffffbeb0;
xor ymm4_3@uint64 ymm4_3 L0x7fffffffbeb8;
(* vpxor  -0x1d0(%rbp),%ymm0,%ymm0                 #! EA = L0x7fffffffbd80; Value = 0x9af85d4549fb53c7; PC = 0x55555557772f *)
xor ymm0_0@uint64 ymm0_0 L0x7fffffffbd80;
xor ymm0_1@uint64 ymm0_1 L0x7fffffffbd88;
xor ymm0_2@uint64 ymm0_2 L0x7fffffffbd90;
xor ymm0_3@uint64 ymm0_3 L0x7fffffffbd98;
(* vmovdqa %ymm14,-0x3b0(%rbp)                     #! EA = L0x7fffffffbba0; PC = 0x555555577737 *)
mov L0x7fffffffbba0 ymm14_0;
mov L0x7fffffffbba8 ymm14_1;
mov L0x7fffffffbbb0 ymm14_2;
mov L0x7fffffffbbb8 ymm14_3;
(* vpsrlq $0x2,%ymm1,%ymm3                         #! PC = 0x55555557773f *)
shr ymm3_0 ymm1_0 0x2@uint64;
shr ymm3_1 ymm1_1 0x2@uint64;
shr ymm3_2 ymm1_2 0x2@uint64;
shr ymm3_3 ymm1_3 0x2@uint64;
(* vpsllq $0x3e,%ymm1,%ymm1                        #! PC = 0x555555577744 *)
shl ymm1_0 ymm1_0 0x3e@uint64;
shl ymm1_1 ymm1_1 0x3e@uint64;
shl ymm1_2 ymm1_2 0x3e@uint64;
shl ymm1_3 ymm1_3 0x3e@uint64;
(* vpxor  -0xf0(%rbp),%ymm2,%ymm2                  #! EA = L0x7fffffffbe60; Value = 0x50a13329b93c785f; PC = 0x555555577749 *)
xor ymm2_0@uint64 ymm2_0 L0x7fffffffbe60;
xor ymm2_1@uint64 ymm2_1 L0x7fffffffbe68;
xor ymm2_2@uint64 ymm2_2 L0x7fffffffbe70;
xor ymm2_3@uint64 ymm2_3 L0x7fffffffbe78;
(* vpor   %ymm3,%ymm1,%ymm15                       #! PC = 0x555555577751 *)
or ymm15_0@uint64 ymm1_0 ymm3_0;
or ymm15_1@uint64 ymm1_1 ymm3_1;
or ymm15_2@uint64 ymm1_2 ymm3_2;
or ymm15_3@uint64 ymm1_3 ymm3_3;
(* vpsrlq $0x9,%ymm6,%ymm3                         #! PC = 0x555555577755 *)
shr ymm3_0 ymm6_0 0x9@uint64;
shr ymm3_1 ymm6_1 0x9@uint64;
shr ymm3_2 ymm6_2 0x9@uint64;
shr ymm3_3 ymm6_3 0x9@uint64;
(* vpsrlq $0x19,%ymm4,%ymm1                        #! PC = 0x55555557775a *)
shr ymm1_0 ymm4_0 0x19@uint64;
shr ymm1_1 ymm4_1 0x19@uint64;
shr ymm1_2 ymm4_2 0x19@uint64;
shr ymm1_3 ymm4_3 0x19@uint64;
(* vpsllq $0x37,%ymm6,%ymm6                        #! PC = 0x55555557775f *)
shl ymm6_0 ymm6_0 0x37@uint64;
shl ymm6_1 ymm6_1 0x37@uint64;
shl ymm6_2 ymm6_2 0x37@uint64;
shl ymm6_3 ymm6_3 0x37@uint64;
(* vpsllq $0x27,%ymm4,%ymm4                        #! PC = 0x555555577764 *)
shl ymm4_0 ymm4_0 0x27@uint64;
shl ymm4_1 ymm4_1 0x27@uint64;
shl ymm4_2 ymm4_2 0x27@uint64;
shl ymm4_3 ymm4_3 0x27@uint64;
(* vpsrlq $0x3e,%ymm2,%ymm7                        #! PC = 0x555555577769 *)
shr ymm7_0 ymm2_0 0x3e@uint64;
shr ymm7_1 ymm2_1 0x3e@uint64;
shr ymm7_2 ymm2_2 0x3e@uint64;
shr ymm7_3 ymm2_3 0x3e@uint64;
(* vpor   %ymm3,%ymm6,%ymm5                        #! PC = 0x55555557776e *)
or ymm5_0@uint64 ymm6_0 ymm3_0;
or ymm5_1@uint64 ymm6_1 ymm3_1;
or ymm5_2@uint64 ymm6_2 ymm3_2;
or ymm5_3@uint64 ymm6_3 ymm3_3;
(* vpor   %ymm1,%ymm4,%ymm4                        #! PC = 0x555555577772 *)
or ymm4_0@uint64 ymm4_0 ymm1_0;
or ymm4_1@uint64 ymm4_1 ymm1_1;
or ymm4_2@uint64 ymm4_2 ymm1_2;
or ymm4_3@uint64 ymm4_3 ymm1_3;
(* vmovdqa -0xd0(%rbp),%ymm1                       #! EA = L0x7fffffffbe80; Value = 0x4e6245814643330b; PC = 0x555555577776 *)
mov ymm1_0 L0x7fffffffbe80;
mov ymm1_1 L0x7fffffffbe88;
mov ymm1_2 L0x7fffffffbe90;
mov ymm1_3 L0x7fffffffbe98;
(* vpsllq $0x2,%ymm2,%ymm2                         #! PC = 0x55555557777e *)
shl ymm2_0 ymm2_0 0x2@uint64;
shl ymm2_1 ymm2_1 0x2@uint64;
shl ymm2_2 ymm2_2 0x2@uint64;
shl ymm2_3 ymm2_3 0x2@uint64;
(* vpxor  -0x2f0(%rbp),%ymm1,%ymm1                 #! EA = L0x7fffffffbc60; Value = 0x1322254abe46649e; PC = 0x555555577783 *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbc60;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbc68;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbc70;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbc78;
(* vpandn %ymm4,%ymm5,%ymm6                        #! PC = 0x55555557778b *)
not ymm5_0n@uint64 ymm5_0;
and ymm6_0@uint64 ymm5_0n ymm4_0;
not ymm5_1n@uint64 ymm5_1;
and ymm6_1@uint64 ymm5_1n ymm4_1;
not ymm5_2n@uint64 ymm5_2;
and ymm6_2@uint64 ymm5_2n ymm4_2;
not ymm5_3n@uint64 ymm5_3;
and ymm6_3@uint64 ymm5_3n ymm4_3;
(* vpxor  %ymm15,%ymm6,%ymm6                       #! PC = 0x55555557778f *)
xor ymm6_0@uint64 ymm6_0 ymm15_0;
xor ymm6_1@uint64 ymm6_1 ymm15_1;
xor ymm6_2@uint64 ymm6_2 ymm15_2;
xor ymm6_3@uint64 ymm6_3 ymm15_3;
(* vpxor  %ymm14,%ymm6,%ymm3                       #! PC = 0x555555577794 *)
xor ymm3_0@uint64 ymm6_0 ymm14_0;
xor ymm3_1@uint64 ymm6_1 ymm14_1;
xor ymm3_2@uint64 ymm6_2 ymm14_2;
xor ymm3_3@uint64 ymm6_3 ymm14_3;
(* vpxor  -0x290(%rbp),%ymm9,%ymm14                #! EA = L0x7fffffffbcc0; Value = 0x02c931b528b695b9; PC = 0x555555577799 *)
xor ymm14_0@uint64 ymm9_0 L0x7fffffffbcc0;
xor ymm14_1@uint64 ymm9_1 L0x7fffffffbcc8;
xor ymm14_2@uint64 ymm9_2 L0x7fffffffbcd0;
xor ymm14_3@uint64 ymm9_3 L0x7fffffffbcd8;
(* vpxor  %ymm1,%ymm3,%ymm3                        #! PC = 0x5555555777a1 *)
xor ymm3_0@uint64 ymm3_0 ymm1_0;
xor ymm3_1@uint64 ymm3_1 ymm1_1;
xor ymm3_2@uint64 ymm3_2 ymm1_2;
xor ymm3_3@uint64 ymm3_3 ymm1_3;
(* vpsrlq $0x17,%ymm0,%ymm1                        #! PC = 0x5555555777a5 *)
shr ymm1_0 ymm0_0 0x17@uint64;
shr ymm1_1 ymm0_1 0x17@uint64;
shr ymm1_2 ymm0_2 0x17@uint64;
shr ymm1_3 ymm0_3 0x17@uint64;
(* vpxor  -0x390(%rbp),%ymm3,%ymm3                 #! EA = L0x7fffffffbbc0; Value = 0x648de89c6833c102; PC = 0x5555555777aa *)
xor ymm3_0@uint64 ymm3_0 L0x7fffffffbbc0;
xor ymm3_1@uint64 ymm3_1 L0x7fffffffbbc8;
xor ymm3_2@uint64 ymm3_2 L0x7fffffffbbd0;
xor ymm3_3@uint64 ymm3_3 L0x7fffffffbbd8;
(* vpsllq $0x29,%ymm0,%ymm0                        #! PC = 0x5555555777b2 *)
shl ymm0_0 ymm0_0 0x29@uint64;
shl ymm0_1 ymm0_1 0x29@uint64;
shl ymm0_2 ymm0_2 0x29@uint64;
shl ymm0_3 ymm0_3 0x29@uint64;
(* vpor   %ymm1,%ymm0,%ymm8                        #! PC = 0x5555555777b7 *)
or ymm8_0@uint64 ymm0_0 ymm1_0;
or ymm8_1@uint64 ymm0_1 ymm1_1;
or ymm8_2@uint64 ymm0_2 ymm1_2;
or ymm8_3@uint64 ymm0_3 ymm1_3;
(* vpandn %ymm8,%ymm4,%ymm0                        #! PC = 0x5555555777bb *)
not ymm4_0n@uint64 ymm4_0;
and ymm0_0@uint64 ymm4_0n ymm8_0;
not ymm4_1n@uint64 ymm4_1;
and ymm0_1@uint64 ymm4_1n ymm8_1;
not ymm4_2n@uint64 ymm4_2;
and ymm0_2@uint64 ymm4_2n ymm8_2;
not ymm4_3n@uint64 ymm4_3;
and ymm0_3@uint64 ymm4_3n ymm8_3;
(* vpxor  %ymm5,%ymm0,%ymm0                        #! PC = 0x5555555777c0 *)
xor ymm0_0@uint64 ymm0_0 ymm5_0;
xor ymm0_1@uint64 ymm0_1 ymm5_1;
xor ymm0_2@uint64 ymm0_2 ymm5_2;
xor ymm0_3@uint64 ymm0_3 ymm5_3;
(* vmovdqa %ymm0,%ymm13                            #! PC = 0x5555555777c4 *)
mov ymm13_0 ymm0_0;
mov ymm13_1 ymm0_1;
mov ymm13_2 ymm0_2;
mov ymm13_3 ymm0_3;
(* vpxor  -0x370(%rbp),%ymm10,%ymm0                #! EA = L0x7fffffffbbe0; Value = 0x656c92d6905c3423; PC = 0x5555555777c8 *)
xor ymm0_0@uint64 ymm10_0 L0x7fffffffbbe0;
xor ymm0_1@uint64 ymm10_1 L0x7fffffffbbe8;
xor ymm0_2@uint64 ymm10_2 L0x7fffffffbbf0;
xor ymm0_3@uint64 ymm10_3 L0x7fffffffbbf8;
(* vmovdqa %ymm13,-0x270(%rbp)                     #! EA = L0x7fffffffbce0; PC = 0x5555555777d0 *)
mov L0x7fffffffbce0 ymm13_0;
mov L0x7fffffffbce8 ymm13_1;
mov L0x7fffffffbcf0 ymm13_2;
mov L0x7fffffffbcf8 ymm13_3;
(* vpxor  %ymm0,%ymm14,%ymm14                      #! PC = 0x5555555777d8 *)
xor ymm14_0@uint64 ymm14_0 ymm0_0;
xor ymm14_1@uint64 ymm14_1 ymm0_1;
xor ymm14_2@uint64 ymm14_2 ymm0_2;
xor ymm14_3@uint64 ymm14_3 ymm0_3;
(* vpor   %ymm7,%ymm2,%ymm0                        #! PC = 0x5555555777dc *)
or ymm0_0@uint64 ymm2_0 ymm7_0;
or ymm0_1@uint64 ymm2_1 ymm7_1;
or ymm0_2@uint64 ymm2_2 ymm7_2;
or ymm0_3@uint64 ymm2_3 ymm7_3;
(* vmovdqa -0x2d0(%rbp),%ymm2                      #! EA = L0x7fffffffbc80; Value = 0xa1ffa1e15ff0bd68; PC = 0x5555555777e0 *)
mov ymm2_0 L0x7fffffffbc80;
mov ymm2_1 L0x7fffffffbc88;
mov ymm2_2 L0x7fffffffbc90;
mov ymm2_3 L0x7fffffffbc98;
(* vpxor  -0x350(%rbp),%ymm2,%ymm1                 #! EA = L0x7fffffffbc00; Value = 0x29f78ba152939175; PC = 0x5555555777e8 *)
xor ymm1_0@uint64 ymm2_0 L0x7fffffffbc00;
xor ymm1_1@uint64 ymm2_1 L0x7fffffffbc08;
xor ymm1_2@uint64 ymm2_2 L0x7fffffffbc10;
xor ymm1_3@uint64 ymm2_3 L0x7fffffffbc18;
(* vpxor  %ymm13,%ymm14,%ymm14                     #! PC = 0x5555555777f0 *)
xor ymm14_0@uint64 ymm14_0 ymm13_0;
xor ymm14_1@uint64 ymm14_1 ymm13_1;
xor ymm14_2@uint64 ymm14_2 ymm13_2;
xor ymm14_3@uint64 ymm14_3 ymm13_3;
(* vpxor  -0x130(%rbp),%ymm12,%ymm13               #! EA = L0x7fffffffbe20; Value = 0xee1b3818d5a83825; PC = 0x5555555777f5 *)
xor ymm13_0@uint64 ymm12_0 L0x7fffffffbe20;
xor ymm13_1@uint64 ymm12_1 L0x7fffffffbe28;
xor ymm13_2@uint64 ymm12_2 L0x7fffffffbe30;
xor ymm13_3@uint64 ymm12_3 L0x7fffffffbe38;
(* vpandn %ymm0,%ymm8,%ymm7                        #! PC = 0x5555555777fd *)
not ymm8_0n@uint64 ymm8_0;
and ymm7_0@uint64 ymm8_0n ymm0_0;
not ymm8_1n@uint64 ymm8_1;
and ymm7_1@uint64 ymm8_1n ymm0_1;
not ymm8_2n@uint64 ymm8_2;
and ymm7_2@uint64 ymm8_2n ymm0_2;
not ymm8_3n@uint64 ymm8_3;
and ymm7_3@uint64 ymm8_3n ymm0_3;
(* vpxor  %ymm4,%ymm7,%ymm7                        #! PC = 0x555555577801 *)
xor ymm7_0@uint64 ymm7_0 ymm4_0;
xor ymm7_1@uint64 ymm7_1 ymm4_1;
xor ymm7_2@uint64 ymm7_2 ymm4_2;
xor ymm7_3@uint64 ymm7_3 ymm4_3;
(* vmovdqa -0x2b0(%rbp),%ymm4                      #! EA = L0x7fffffffbca0; Value = 0xfaf58b3abf5862df; PC = 0x555555577805 *)
mov ymm4_0 L0x7fffffffbca0;
mov ymm4_1 L0x7fffffffbca8;
mov ymm4_2 L0x7fffffffbcb0;
mov ymm4_3 L0x7fffffffbcb8;
(* vpxor  %ymm1,%ymm13,%ymm13                      #! PC = 0x55555557780d *)
xor ymm13_0@uint64 ymm13_0 ymm1_0;
xor ymm13_1@uint64 ymm13_1 ymm1_1;
xor ymm13_2@uint64 ymm13_2 ymm1_2;
xor ymm13_3@uint64 ymm13_3 ymm1_3;
(* vpandn %ymm15,%ymm0,%ymm1                       #! PC = 0x555555577811 *)
not ymm0_0n@uint64 ymm0_0;
and ymm1_0@uint64 ymm0_0n ymm15_0;
not ymm0_1n@uint64 ymm0_1;
and ymm1_1@uint64 ymm0_1n ymm15_1;
not ymm0_2n@uint64 ymm0_2;
and ymm1_2@uint64 ymm0_2n ymm15_2;
not ymm0_3n@uint64 ymm0_3;
and ymm1_3@uint64 ymm0_3n ymm15_3;
(* vpxor  %ymm8,%ymm1,%ymm8                        #! PC = 0x555555577816 *)
xor ymm8_0@uint64 ymm1_0 ymm8_0;
xor ymm8_1@uint64 ymm1_1 ymm8_1;
xor ymm8_2@uint64 ymm1_2 ymm8_2;
xor ymm8_3@uint64 ymm1_3 ymm8_3;
(* vpxor  -0x330(%rbp),%ymm4,%ymm1                 #! EA = L0x7fffffffbc20; Value = 0x915f4042be658083; PC = 0x55555557781b *)
xor ymm1_0@uint64 ymm4_0 L0x7fffffffbc20;
xor ymm1_1@uint64 ymm4_1 L0x7fffffffbc28;
xor ymm1_2@uint64 ymm4_2 L0x7fffffffbc30;
xor ymm1_3@uint64 ymm4_3 L0x7fffffffbc38;
(* vpxor  %ymm7,%ymm13,%ymm13                      #! PC = 0x555555577823 *)
xor ymm13_0@uint64 ymm13_0 ymm7_0;
xor ymm13_1@uint64 ymm13_1 ymm7_1;
xor ymm13_2@uint64 ymm13_2 ymm7_2;
xor ymm13_3@uint64 ymm13_3 ymm7_3;
(* vmovdqa %ymm8,-0x70(%rbp)                       #! EA = L0x7fffffffbee0; PC = 0x555555577827 *)
mov L0x7fffffffbee0 ymm8_0;
mov L0x7fffffffbee8 ymm8_1;
mov L0x7fffffffbef0 ymm8_2;
mov L0x7fffffffbef8 ymm8_3;
(* vpxor  %ymm8,%ymm11,%ymm8                       #! PC = 0x55555557782c *)
xor ymm8_0@uint64 ymm11_0 ymm8_0;
xor ymm8_1@uint64 ymm11_1 ymm8_1;
xor ymm8_2@uint64 ymm11_2 ymm8_2;
xor ymm8_3@uint64 ymm11_3 ymm8_3;
(* vpsllq $0x1,%ymm13,%ymm4                        #! PC = 0x555555577831 *)
shl ymm4_0 ymm13_0 0x1@uint64;
shl ymm4_1 ymm13_1 0x1@uint64;
shl ymm4_2 ymm13_2 0x1@uint64;
shl ymm4_3 ymm13_3 0x1@uint64;
(* vpxor  %ymm1,%ymm8,%ymm8                        #! PC = 0x555555577837 *)
xor ymm8_0@uint64 ymm8_0 ymm1_0;
xor ymm8_1@uint64 ymm8_1 ymm1_1;
xor ymm8_2@uint64 ymm8_2 ymm1_2;
xor ymm8_3@uint64 ymm8_3 ymm1_3;
(* vpandn %ymm5,%ymm15,%ymm1                       #! PC = 0x55555557783b *)
not ymm15_0n@uint64 ymm15_0;
and ymm1_0@uint64 ymm15_0n ymm5_0;
not ymm15_1n@uint64 ymm15_1;
and ymm1_1@uint64 ymm15_1n ymm5_1;
not ymm15_2n@uint64 ymm15_2;
and ymm1_2@uint64 ymm15_2n ymm5_2;
not ymm15_3n@uint64 ymm15_3;
and ymm1_3@uint64 ymm15_3n ymm5_3;
(* vpxor  -0x170(%rbp),%ymm8,%ymm8                 #! EA = L0x7fffffffbde0; Value = 0x091010d55fa046ca; PC = 0x55555557783f *)
xor ymm8_0@uint64 ymm8_0 L0x7fffffffbde0;
xor ymm8_1@uint64 ymm8_1 L0x7fffffffbde8;
xor ymm8_2@uint64 ymm8_2 L0x7fffffffbdf0;
xor ymm8_3@uint64 ymm8_3 L0x7fffffffbdf8;
(* vpxor  %ymm0,%ymm1,%ymm1                        #! PC = 0x555555577847 *)
xor ymm1_0@uint64 ymm1_0 ymm0_0;
xor ymm1_1@uint64 ymm1_1 ymm0_1;
xor ymm1_2@uint64 ymm1_2 ymm0_2;
xor ymm1_3@uint64 ymm1_3 ymm0_3;
(* vmovdqa -0x210(%rbp),%ymm0                      #! EA = L0x7fffffffbd40; Value = 0xb22b9af8df7d5fe7; PC = 0x55555557784b *)
mov ymm0_0 L0x7fffffffbd40;
mov ymm0_1 L0x7fffffffbd48;
mov ymm0_2 L0x7fffffffbd50;
mov ymm0_3 L0x7fffffffbd58;
(* vpsllq $0x1,%ymm14,%ymm5                        #! PC = 0x555555577853 *)
shl ymm5_0 ymm14_0 0x1@uint64;
shl ymm5_1 ymm14_1 0x1@uint64;
shl ymm5_2 ymm14_2 0x1@uint64;
shl ymm5_3 ymm14_3 0x1@uint64;
(* vpxor  -0x1f0(%rbp),%ymm1,%ymm2                 #! EA = L0x7fffffffbd60; Value = 0x1acec2cda2b1c6c6; PC = 0x555555577859 *)
xor ymm2_0@uint64 ymm1_0 L0x7fffffffbd60;
xor ymm2_1@uint64 ymm1_1 L0x7fffffffbd68;
xor ymm2_2@uint64 ymm1_2 L0x7fffffffbd70;
xor ymm2_3@uint64 ymm1_3 L0x7fffffffbd78;
(* vpxor  -0x310(%rbp),%ymm0,%ymm0                 #! EA = L0x7fffffffbc40; Value = 0x1ef11c5578706c33; PC = 0x555555577861 *)
xor ymm0_0@uint64 ymm0_0 L0x7fffffffbc40;
xor ymm0_1@uint64 ymm0_1 L0x7fffffffbc48;
xor ymm0_2@uint64 ymm0_2 L0x7fffffffbc50;
xor ymm0_3@uint64 ymm0_3 L0x7fffffffbc58;
(* vpsrlq $0x3f,%ymm8,%ymm15                       #! PC = 0x555555577869 *)
shr ymm15_0 ymm8_0 0x3f@uint64;
shr ymm15_1 ymm8_1 0x3f@uint64;
shr ymm15_2 ymm8_2 0x3f@uint64;
shr ymm15_3 ymm8_3 0x3f@uint64;
(* vpxor  %ymm0,%ymm2,%ymm2                        #! PC = 0x55555557786f *)
xor ymm2_0@uint64 ymm2_0 ymm0_0;
xor ymm2_1@uint64 ymm2_1 ymm0_1;
xor ymm2_2@uint64 ymm2_2 ymm0_2;
xor ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x3f,%ymm14,%ymm0                       #! PC = 0x555555577873 *)
shr ymm0_0 ymm14_0 0x3f@uint64;
shr ymm0_1 ymm14_1 0x3f@uint64;
shr ymm0_2 ymm14_2 0x3f@uint64;
shr ymm0_3 ymm14_3 0x3f@uint64;
(* vpxor  -0x50(%rbp),%ymm2,%ymm2                  #! EA = L0x7fffffffbf00; Value = 0x45428a665086ac41; PC = 0x555555577879 *)
xor ymm2_0@uint64 ymm2_0 L0x7fffffffbf00;
xor ymm2_1@uint64 ymm2_1 L0x7fffffffbf08;
xor ymm2_2@uint64 ymm2_2 L0x7fffffffbf10;
xor ymm2_3@uint64 ymm2_3 L0x7fffffffbf18;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x55555557787e *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpsrlq $0x3f,%ymm13,%ymm0                       #! PC = 0x555555577882 *)
shr ymm0_0 ymm13_0 0x3f@uint64;
shr ymm0_1 ymm13_1 0x3f@uint64;
shr ymm0_2 ymm13_2 0x3f@uint64;
shr ymm0_3 ymm13_3 0x3f@uint64;
(* vpor   %ymm0,%ymm4,%ymm4                        #! PC = 0x555555577888 *)
or ymm4_0@uint64 ymm4_0 ymm0_0;
or ymm4_1@uint64 ymm4_1 ymm0_1;
or ymm4_2@uint64 ymm4_2 ymm0_2;
or ymm4_3@uint64 ymm4_3 ymm0_3;
(* vpsllq $0x1,%ymm8,%ymm0                         #! PC = 0x55555557788c *)
shl ymm0_0 ymm8_0 0x1@uint64;
shl ymm0_1 ymm8_1 0x1@uint64;
shl ymm0_2 ymm8_2 0x1@uint64;
shl ymm0_3 ymm8_3 0x1@uint64;
(* vpxor  %ymm2,%ymm5,%ymm5                        #! PC = 0x555555577892 *)
xor ymm5_0@uint64 ymm5_0 ymm2_0;
xor ymm5_1@uint64 ymm5_1 ymm2_1;
xor ymm5_2@uint64 ymm5_2 ymm2_2;
xor ymm5_3@uint64 ymm5_3 ymm2_3;
(* vpor   %ymm15,%ymm0,%ymm0                       #! PC = 0x555555577896 *)
or ymm0_0@uint64 ymm0_0 ymm15_0;
or ymm0_1@uint64 ymm0_1 ymm15_1;
or ymm0_2@uint64 ymm0_2 ymm15_2;
or ymm0_3@uint64 ymm0_3 ymm15_3;
(* vpxor  %ymm3,%ymm4,%ymm4                        #! PC = 0x55555557789b *)
xor ymm4_0@uint64 ymm4_0 ymm3_0;
xor ymm4_1@uint64 ymm4_1 ymm3_1;
xor ymm4_2@uint64 ymm4_2 ymm3_2;
xor ymm4_3@uint64 ymm4_3 ymm3_3;
(* vmovq  %rdx,%xmm15                              #! PC = 0x55555557789f *)
mov xmm15_0 rdx;
mov xmm15_1 0@uint64;
(* vpxor  %ymm14,%ymm0,%ymm14                      #! PC = 0x5555555778a4 *)
xor ymm14_0@uint64 ymm0_0 ymm14_0;
xor ymm14_1@uint64 ymm0_1 ymm14_1;
xor ymm14_2@uint64 ymm0_2 ymm14_2;
xor ymm14_3@uint64 ymm0_3 ymm14_3;
(* vpsrlq $0x3f,%ymm2,%ymm0                        #! PC = 0x5555555778a9 *)
shr ymm0_0 ymm2_0 0x3f@uint64;
shr ymm0_1 ymm2_1 0x3f@uint64;
shr ymm0_2 ymm2_2 0x3f@uint64;
shr ymm0_3 ymm2_3 0x3f@uint64;
(* vpxor  %ymm10,%ymm4,%ymm10                      #! PC = 0x5555555778ae *)
xor ymm10_0@uint64 ymm4_0 ymm10_0;
xor ymm10_1@uint64 ymm4_1 ymm10_1;
xor ymm10_2@uint64 ymm4_2 ymm10_2;
xor ymm10_3@uint64 ymm4_3 ymm10_3;
(* vpsllq $0x1,%ymm2,%ymm2                         #! PC = 0x5555555778b3 *)
shl ymm2_0 ymm2_0 0x1@uint64;
shl ymm2_1 ymm2_1 0x1@uint64;
shl ymm2_2 ymm2_2 0x1@uint64;
shl ymm2_3 ymm2_3 0x1@uint64;
(* vpxor  %ymm12,%ymm14,%ymm12                     #! PC = 0x5555555778b8 *)
xor ymm12_0@uint64 ymm14_0 ymm12_0;
xor ymm12_1@uint64 ymm14_1 ymm12_1;
xor ymm12_2@uint64 ymm14_2 ymm12_2;
xor ymm12_3@uint64 ymm14_3 ymm12_3;
(* vpxor  %ymm9,%ymm4,%ymm9                        #! PC = 0x5555555778bd *)
xor ymm9_0@uint64 ymm4_0 ymm9_0;
xor ymm9_1@uint64 ymm4_1 ymm9_1;
xor ymm9_2@uint64 ymm4_2 ymm9_2;
xor ymm9_3@uint64 ymm4_3 ymm9_3;
(* vpor   %ymm0,%ymm2,%ymm2                        #! PC = 0x5555555778c2 *)
or ymm2_0@uint64 ymm2_0 ymm0_0;
or ymm2_1@uint64 ymm2_1 ymm0_1;
or ymm2_2@uint64 ymm2_2 ymm0_2;
or ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x3f,%ymm3,%ymm0                        #! PC = 0x5555555778c6 *)
shr ymm0_0 ymm3_0 0x3f@uint64;
shr ymm0_1 ymm3_1 0x3f@uint64;
shr ymm0_2 ymm3_2 0x3f@uint64;
shr ymm0_3 ymm3_3 0x3f@uint64;
(* vpxor  %ymm7,%ymm14,%ymm7                       #! PC = 0x5555555778cb *)
xor ymm7_0@uint64 ymm14_0 ymm7_0;
xor ymm7_1@uint64 ymm14_1 ymm7_1;
xor ymm7_2@uint64 ymm14_2 ymm7_2;
xor ymm7_3@uint64 ymm14_3 ymm7_3;
(* vpxor  %ymm13,%ymm2,%ymm13                      #! PC = 0x5555555778cf *)
xor ymm13_0@uint64 ymm2_0 ymm13_0;
xor ymm13_1@uint64 ymm2_1 ymm13_1;
xor ymm13_2@uint64 ymm2_2 ymm13_2;
xor ymm13_3@uint64 ymm2_3 ymm13_3;
(* vpsrlq $0x14,%ymm10,%ymm2                       #! PC = 0x5555555778d4 *)
shr ymm2_0 ymm10_0 0x14@uint64;
shr ymm2_1 ymm10_1 0x14@uint64;
shr ymm2_2 ymm10_2 0x14@uint64;
shr ymm2_3 ymm10_3 0x14@uint64;
(* vpxor  %ymm6,%ymm5,%ymm6                        #! PC = 0x5555555778da *)
xor ymm6_0@uint64 ymm5_0 ymm6_0;
xor ymm6_1@uint64 ymm5_1 ymm6_1;
xor ymm6_2@uint64 ymm5_2 ymm6_2;
xor ymm6_3@uint64 ymm5_3 ymm6_3;
(* vpsllq $0x2c,%ymm10,%ymm10                      #! PC = 0x5555555778de *)
shl ymm10_0 ymm10_0 0x2c@uint64;
shl ymm10_1 ymm10_1 0x2c@uint64;
shl ymm10_2 ymm10_2 0x2c@uint64;
shl ymm10_3 ymm10_3 0x2c@uint64;
(* vpsllq $0x1,%ymm3,%ymm3                         #! PC = 0x5555555778e4 *)
shl ymm3_0 ymm3_0 0x1@uint64;
shl ymm3_1 ymm3_1 0x1@uint64;
shl ymm3_2 ymm3_2 0x1@uint64;
shl ymm3_3 ymm3_3 0x1@uint64;
(* vpxor  %ymm11,%ymm13,%ymm11                     #! PC = 0x5555555778e9 *)
xor ymm11_0@uint64 ymm13_0 ymm11_0;
xor ymm11_1@uint64 ymm13_1 ymm11_1;
xor ymm11_2@uint64 ymm13_2 ymm11_2;
xor ymm11_3@uint64 ymm13_3 ymm11_3;
(* vpor   %ymm2,%ymm10,%ymm10                      #! PC = 0x5555555778ee *)
or ymm10_0@uint64 ymm10_0 ymm2_0;
or ymm10_1@uint64 ymm10_1 ymm2_1;
or ymm10_2@uint64 ymm10_2 ymm2_2;
or ymm10_3@uint64 ymm10_3 ymm2_3;
(* vpsrlq $0x15,%ymm12,%ymm2                       #! PC = 0x5555555778f2 *)
shr ymm2_0 ymm12_0 0x15@uint64;
shr ymm2_1 ymm12_1 0x15@uint64;
shr ymm2_2 ymm12_2 0x15@uint64;
shr ymm2_3 ymm12_3 0x15@uint64;
(* vpor   %ymm0,%ymm3,%ymm3                        #! PC = 0x5555555778f8 *)
or ymm3_0@uint64 ymm3_0 ymm0_0;
or ymm3_1@uint64 ymm3_1 ymm0_1;
or ymm3_2@uint64 ymm3_2 ymm0_2;
or ymm3_3@uint64 ymm3_3 ymm0_3;
(* vpsllq $0x2b,%ymm12,%ymm12                      #! PC = 0x5555555778fc *)
shl ymm12_0 ymm12_0 0x2b@uint64;
shl ymm12_1 ymm12_1 0x2b@uint64;
shl ymm12_2 ymm12_2 0x2b@uint64;
shl ymm12_3 ymm12_3 0x2b@uint64;
(* vpxor  %ymm8,%ymm3,%ymm8                        #! PC = 0x555555577902 *)
xor ymm8_0@uint64 ymm3_0 ymm8_0;
xor ymm8_1@uint64 ymm3_1 ymm8_1;
xor ymm8_2@uint64 ymm3_2 ymm8_2;
xor ymm8_3@uint64 ymm3_3 ymm8_3;
(* vpxor  -0x390(%rbp),%ymm5,%ymm0                 #! EA = L0x7fffffffbbc0; Value = 0x648de89c6833c102; PC = 0x555555577907 *)
xor ymm0_0@uint64 ymm5_0 L0x7fffffffbbc0;
xor ymm0_1@uint64 ymm5_1 L0x7fffffffbbc8;
xor ymm0_2@uint64 ymm5_2 L0x7fffffffbbd0;
xor ymm0_3@uint64 ymm5_3 L0x7fffffffbbd8;
(* vpor   %ymm2,%ymm12,%ymm12                      #! PC = 0x55555557790f *)
or ymm12_0@uint64 ymm12_0 ymm2_0;
or ymm12_1@uint64 ymm12_1 ymm2_1;
or ymm12_2@uint64 ymm12_2 ymm2_2;
or ymm12_3@uint64 ymm12_3 ymm2_3;
(* vpbroadcastq %xmm15,%ymm2                       #! PC = 0x555555577913 *)
mov ymm2_0 xmm15_0;
mov ymm2_1 xmm15_0;
mov ymm2_2 xmm15_0;
mov ymm2_3 xmm15_0;
(* vpxor  %ymm1,%ymm8,%ymm1                        #! PC = 0x555555577918 *)
xor ymm1_0@uint64 ymm8_0 ymm1_0;
xor ymm1_1@uint64 ymm8_1 ymm1_1;
xor ymm1_2@uint64 ymm8_2 ymm1_2;
xor ymm1_3@uint64 ymm8_3 ymm1_3;
(* vpandn %ymm12,%ymm10,%ymm3                      #! PC = 0x55555557791c *)
not ymm10_0n@uint64 ymm10_0;
and ymm3_0@uint64 ymm10_0n ymm12_0;
not ymm10_1n@uint64 ymm10_1;
and ymm3_1@uint64 ymm10_1n ymm12_1;
not ymm10_2n@uint64 ymm10_2;
and ymm3_2@uint64 ymm10_2n ymm12_2;
not ymm10_3n@uint64 ymm10_3;
and ymm3_3@uint64 ymm10_3n ymm12_3;
(* vpxor  %ymm3,%ymm2,%ymm2                        #! PC = 0x555555577921 *)
xor ymm2_0@uint64 ymm2_0 ymm3_0;
xor ymm2_1@uint64 ymm2_1 ymm3_1;
xor ymm2_2@uint64 ymm2_2 ymm3_2;
xor ymm2_3@uint64 ymm2_3 ymm3_3;
(* vpxor  %ymm0,%ymm2,%ymm15                       #! PC = 0x555555577925 *)
xor ymm15_0@uint64 ymm2_0 ymm0_0;
xor ymm15_1@uint64 ymm2_1 ymm0_1;
xor ymm15_2@uint64 ymm2_2 ymm0_2;
xor ymm15_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x2b,%ymm11,%ymm2                       #! PC = 0x555555577929 *)
shr ymm2_0 ymm11_0 0x2b@uint64;
shr ymm2_1 ymm11_1 0x2b@uint64;
shr ymm2_2 ymm11_2 0x2b@uint64;
shr ymm2_3 ymm11_3 0x2b@uint64;
(* vpsllq $0x15,%ymm11,%ymm11                      #! PC = 0x55555557792f *)
shl ymm11_0 ymm11_0 0x15@uint64;
shl ymm11_1 ymm11_1 0x15@uint64;
shl ymm11_2 ymm11_2 0x15@uint64;
shl ymm11_3 ymm11_3 0x15@uint64;
(* vmovdqa %ymm15,-0x90(%rbp)                      #! EA = L0x7fffffffbec0; PC = 0x555555577935 *)
mov L0x7fffffffbec0 ymm15_0;
mov L0x7fffffffbec8 ymm15_1;
mov L0x7fffffffbed0 ymm15_2;
mov L0x7fffffffbed8 ymm15_3;
(* vpor   %ymm2,%ymm11,%ymm11                      #! PC = 0x55555557793d *)
or ymm11_0@uint64 ymm11_0 ymm2_0;
or ymm11_1@uint64 ymm11_1 ymm2_1;
or ymm11_2@uint64 ymm11_2 ymm2_2;
or ymm11_3@uint64 ymm11_3 ymm2_3;
(* vpandn %ymm11,%ymm12,%ymm2                      #! PC = 0x555555577941 *)
not ymm12_0n@uint64 ymm12_0;
and ymm2_0@uint64 ymm12_0n ymm11_0;
not ymm12_1n@uint64 ymm12_1;
and ymm2_1@uint64 ymm12_1n ymm11_1;
not ymm12_2n@uint64 ymm12_2;
and ymm2_2@uint64 ymm12_2n ymm11_2;
not ymm12_3n@uint64 ymm12_3;
and ymm2_3@uint64 ymm12_3n ymm11_3;
(* vpxor  %ymm10,%ymm2,%ymm15                      #! PC = 0x555555577946 *)
xor ymm15_0@uint64 ymm2_0 ymm10_0;
xor ymm15_1@uint64 ymm2_1 ymm10_1;
xor ymm15_2@uint64 ymm2_2 ymm10_2;
xor ymm15_3@uint64 ymm2_3 ymm10_3;
(* vpsrlq $0x32,%ymm1,%ymm2                        #! PC = 0x55555557794b *)
shr ymm2_0 ymm1_0 0x32@uint64;
shr ymm2_1 ymm1_1 0x32@uint64;
shr ymm2_2 ymm1_2 0x32@uint64;
shr ymm2_3 ymm1_3 0x32@uint64;
(* vpandn %ymm10,%ymm0,%ymm10                      #! PC = 0x555555577950 *)
not ymm0_0n@uint64 ymm0_0;
and ymm10_0@uint64 ymm0_0n ymm10_0;
not ymm0_1n@uint64 ymm0_1;
and ymm10_1@uint64 ymm0_1n ymm10_1;
not ymm0_2n@uint64 ymm0_2;
and ymm10_2@uint64 ymm0_2n ymm10_2;
not ymm0_3n@uint64 ymm0_3;
and ymm10_3@uint64 ymm0_3n ymm10_3;
(* vpsllq $0xe,%ymm1,%ymm1                         #! PC = 0x555555577955 *)
shl ymm1_0 ymm1_0 0xe@uint64;
shl ymm1_1 ymm1_1 0xe@uint64;
shl ymm1_2 ymm1_2 0xe@uint64;
shl ymm1_3 ymm1_3 0xe@uint64;
(* vmovdqa %ymm15,-0x1b0(%rbp)                     #! EA = L0x7fffffffbda0; PC = 0x55555557795a *)
mov L0x7fffffffbda0 ymm15_0;
mov L0x7fffffffbda8 ymm15_1;
mov L0x7fffffffbdb0 ymm15_2;
mov L0x7fffffffbdb8 ymm15_3;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x555555577962 *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpandn %ymm1,%ymm11,%ymm2                       #! PC = 0x555555577966 *)
not ymm11_0n@uint64 ymm11_0;
and ymm2_0@uint64 ymm11_0n ymm1_0;
not ymm11_1n@uint64 ymm11_1;
and ymm2_1@uint64 ymm11_1n ymm1_1;
not ymm11_2n@uint64 ymm11_2;
and ymm2_2@uint64 ymm11_2n ymm1_2;
not ymm11_3n@uint64 ymm11_3;
and ymm2_3@uint64 ymm11_3n ymm1_3;
(* vpxor  %ymm1,%ymm10,%ymm10                      #! PC = 0x55555557796a *)
xor ymm10_0@uint64 ymm10_0 ymm1_0;
xor ymm10_1@uint64 ymm10_1 ymm1_1;
xor ymm10_2@uint64 ymm10_2 ymm1_2;
xor ymm10_3@uint64 ymm10_3 ymm1_3;
(* vpxor  %ymm12,%ymm2,%ymm12                      #! PC = 0x55555557796e *)
xor ymm12_0@uint64 ymm2_0 ymm12_0;
xor ymm12_1@uint64 ymm2_1 ymm12_1;
xor ymm12_2@uint64 ymm2_2 ymm12_2;
xor ymm12_3@uint64 ymm2_3 ymm12_3;
(* vpandn %ymm0,%ymm1,%ymm2                        #! PC = 0x555555577973 *)
not ymm1_0n@uint64 ymm1_0;
and ymm2_0@uint64 ymm1_0n ymm0_0;
not ymm1_1n@uint64 ymm1_1;
and ymm2_1@uint64 ymm1_1n ymm0_1;
not ymm1_2n@uint64 ymm1_2;
and ymm2_2@uint64 ymm1_2n ymm0_2;
not ymm1_3n@uint64 ymm1_3;
and ymm2_3@uint64 ymm1_3n ymm0_3;
(* vpxor  -0x330(%rbp),%ymm13,%ymm0                #! EA = L0x7fffffffbc20; Value = 0x915f4042be658083; PC = 0x555555577977 *)
xor ymm0_0@uint64 ymm13_0 L0x7fffffffbc20;
xor ymm0_1@uint64 ymm13_1 L0x7fffffffbc28;
xor ymm0_2@uint64 ymm13_2 L0x7fffffffbc30;
xor ymm0_3@uint64 ymm13_3 L0x7fffffffbc38;
(* vmovdqa %ymm10,-0x230(%rbp)                     #! EA = L0x7fffffffbd20; PC = 0x55555557797f *)
mov L0x7fffffffbd20 ymm10_0;
mov L0x7fffffffbd28 ymm10_1;
mov L0x7fffffffbd30 ymm10_2;
mov L0x7fffffffbd38 ymm10_3;
(* vmovdqa %ymm12,-0x250(%rbp)                     #! EA = L0x7fffffffbd00; PC = 0x555555577987 *)
mov L0x7fffffffbd00 ymm12_0;
mov L0x7fffffffbd08 ymm12_1;
mov L0x7fffffffbd10 ymm12_2;
mov L0x7fffffffbd18 ymm12_3;
(* vpxor  %ymm11,%ymm2,%ymm12                      #! PC = 0x55555557798f *)
xor ymm12_0@uint64 ymm2_0 ymm11_0;
xor ymm12_1@uint64 ymm2_1 ymm11_1;
xor ymm12_2@uint64 ymm2_2 ymm11_2;
xor ymm12_3@uint64 ymm2_3 ymm11_3;
(* vpsrlq $0x24,%ymm0,%ymm1                        #! PC = 0x555555577994 *)
shr ymm1_0 ymm0_0 0x24@uint64;
shr ymm1_1 ymm0_1 0x24@uint64;
shr ymm1_2 ymm0_2 0x24@uint64;
shr ymm1_3 ymm0_3 0x24@uint64;
(* vpsllq $0x1c,%ymm0,%ymm0                        #! PC = 0x555555577999 *)
shl ymm0_0 ymm0_0 0x1c@uint64;
shl ymm0_1 ymm0_1 0x1c@uint64;
shl ymm0_2 ymm0_2 0x1c@uint64;
shl ymm0_3 ymm0_3 0x1c@uint64;
(* vmovdqa %ymm12,-0x110(%rbp)                     #! EA = L0x7fffffffbe40; PC = 0x55555557799e *)
mov L0x7fffffffbe40 ymm12_0;
mov L0x7fffffffbe48 ymm12_1;
mov L0x7fffffffbe50 ymm12_2;
mov L0x7fffffffbe58 ymm12_3;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x5555555779a6 *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x210(%rbp),%ymm8,%ymm1                 #! EA = L0x7fffffffbd40; Value = 0xb22b9af8df7d5fe7; PC = 0x5555555779aa *)
xor ymm1_0@uint64 ymm8_0 L0x7fffffffbd40;
xor ymm1_1@uint64 ymm8_1 L0x7fffffffbd48;
xor ymm1_2@uint64 ymm8_2 L0x7fffffffbd50;
xor ymm1_3@uint64 ymm8_3 L0x7fffffffbd58;
(* vpsrlq $0x2c,%ymm1,%ymm2                        #! PC = 0x5555555779b2 *)
shr ymm2_0 ymm1_0 0x2c@uint64;
shr ymm2_1 ymm1_1 0x2c@uint64;
shr ymm2_2 ymm1_2 0x2c@uint64;
shr ymm2_3 ymm1_3 0x2c@uint64;
(* vpsllq $0x14,%ymm1,%ymm1                        #! PC = 0x5555555779b7 *)
shl ymm1_0 ymm1_0 0x14@uint64;
shl ymm1_1 ymm1_1 0x14@uint64;
shl ymm1_2 ymm1_2 0x14@uint64;
shl ymm1_3 ymm1_3 0x14@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x5555555779bc *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpxor  -0xd0(%rbp),%ymm5,%ymm2                  #! EA = L0x7fffffffbe80; Value = 0x4e6245814643330b; PC = 0x5555555779c0 *)
xor ymm2_0@uint64 ymm5_0 L0x7fffffffbe80;
xor ymm2_1@uint64 ymm5_1 L0x7fffffffbe88;
xor ymm2_2@uint64 ymm5_2 L0x7fffffffbe90;
xor ymm2_3@uint64 ymm5_3 L0x7fffffffbe98;
(* vpsrlq $0x3d,%ymm2,%ymm3                        #! PC = 0x5555555779c8 *)
shr ymm3_0 ymm2_0 0x3d@uint64;
shr ymm3_1 ymm2_1 0x3d@uint64;
shr ymm3_2 ymm2_2 0x3d@uint64;
shr ymm3_3 ymm2_3 0x3d@uint64;
(* vpsllq $0x3,%ymm2,%ymm2                         #! PC = 0x5555555779cd *)
shl ymm2_0 ymm2_0 0x3@uint64;
shl ymm2_1 ymm2_1 0x3@uint64;
shl ymm2_2 ymm2_2 0x3@uint64;
shl ymm2_3 ymm2_3 0x3@uint64;
(* vpor   %ymm3,%ymm2,%ymm2                        #! PC = 0x5555555779d2 *)
or ymm2_0@uint64 ymm2_0 ymm3_0;
or ymm2_1@uint64 ymm2_1 ymm3_1;
or ymm2_2@uint64 ymm2_2 ymm3_2;
or ymm2_3@uint64 ymm2_3 ymm3_3;
(* vpandn %ymm2,%ymm1,%ymm3                        #! PC = 0x5555555779d6 *)
not ymm1_0n@uint64 ymm1_0;
and ymm3_0@uint64 ymm1_0n ymm2_0;
not ymm1_1n@uint64 ymm1_1;
and ymm3_1@uint64 ymm1_1n ymm2_1;
not ymm1_2n@uint64 ymm1_2;
and ymm3_2@uint64 ymm1_2n ymm2_2;
not ymm1_3n@uint64 ymm1_3;
and ymm3_3@uint64 ymm1_3n ymm2_3;
(* vpxor  %ymm0,%ymm3,%ymm10                       #! PC = 0x5555555779da *)
xor ymm10_0@uint64 ymm3_0 ymm0_0;
xor ymm10_1@uint64 ymm3_1 ymm0_1;
xor ymm10_2@uint64 ymm3_2 ymm0_2;
xor ymm10_3@uint64 ymm3_3 ymm0_3;
(* vpsrlq $0x13,%ymm9,%ymm3                        #! PC = 0x5555555779de *)
shr ymm3_0 ymm9_0 0x13@uint64;
shr ymm3_1 ymm9_1 0x13@uint64;
shr ymm3_2 ymm9_2 0x13@uint64;
shr ymm3_3 ymm9_3 0x13@uint64;
(* vpsllq $0x2d,%ymm9,%ymm9                        #! PC = 0x5555555779e4 *)
shl ymm9_0 ymm9_0 0x2d@uint64;
shl ymm9_1 ymm9_1 0x2d@uint64;
shl ymm9_2 ymm9_2 0x2d@uint64;
shl ymm9_3 ymm9_3 0x2d@uint64;
(* vmovdqa %ymm10,-0x210(%rbp)                     #! EA = L0x7fffffffbd40; PC = 0x5555555779ea *)
mov L0x7fffffffbd40 ymm10_0;
mov L0x7fffffffbd48 ymm10_1;
mov L0x7fffffffbd50 ymm10_2;
mov L0x7fffffffbd58 ymm10_3;
(* vpxor  -0x130(%rbp),%ymm14,%ymm11               #! EA = L0x7fffffffbe20; Value = 0xee1b3818d5a83825; PC = 0x5555555779f2 *)
xor ymm11_0@uint64 ymm14_0 L0x7fffffffbe20;
xor ymm11_1@uint64 ymm14_1 L0x7fffffffbe28;
xor ymm11_2@uint64 ymm14_2 L0x7fffffffbe30;
xor ymm11_3@uint64 ymm14_3 L0x7fffffffbe38;
(* vpor   %ymm3,%ymm9,%ymm15                       #! PC = 0x5555555779fa *)
or ymm15_0@uint64 ymm9_0 ymm3_0;
or ymm15_1@uint64 ymm9_1 ymm3_1;
or ymm15_2@uint64 ymm9_2 ymm3_2;
or ymm15_3@uint64 ymm9_3 ymm3_3;
(* vpsrlq $0x3,%ymm7,%ymm3                         #! PC = 0x5555555779fe *)
shr ymm3_0 ymm7_0 0x3@uint64;
shr ymm3_1 ymm7_1 0x3@uint64;
shr ymm3_2 ymm7_2 0x3@uint64;
shr ymm3_3 ymm7_3 0x3@uint64;
(* vpsllq $0x3d,%ymm7,%ymm7                        #! PC = 0x555555577a03 *)
shl ymm7_0 ymm7_0 0x3d@uint64;
shl ymm7_1 ymm7_1 0x3d@uint64;
shl ymm7_2 ymm7_2 0x3d@uint64;
shl ymm7_3 ymm7_3 0x3d@uint64;
(* vpandn %ymm15,%ymm2,%ymm10                      #! PC = 0x555555577a08 *)
not ymm2_0n@uint64 ymm2_0;
and ymm10_0@uint64 ymm2_0n ymm15_0;
not ymm2_1n@uint64 ymm2_1;
and ymm10_1@uint64 ymm2_1n ymm15_1;
not ymm2_2n@uint64 ymm2_2;
and ymm10_2@uint64 ymm2_2n ymm15_2;
not ymm2_3n@uint64 ymm2_3;
and ymm10_3@uint64 ymm2_3n ymm15_3;
(* vpor   %ymm3,%ymm7,%ymm7                        #! PC = 0x555555577a0d *)
or ymm7_0@uint64 ymm7_0 ymm3_0;
or ymm7_1@uint64 ymm7_1 ymm3_1;
or ymm7_2@uint64 ymm7_2 ymm3_2;
or ymm7_3@uint64 ymm7_3 ymm3_3;
(* vpxor  %ymm1,%ymm10,%ymm10                      #! PC = 0x555555577a11 *)
xor ymm10_0@uint64 ymm10_0 ymm1_0;
xor ymm10_1@uint64 ymm10_1 ymm1_1;
xor ymm10_2@uint64 ymm10_2 ymm1_2;
xor ymm10_3@uint64 ymm10_3 ymm1_3;
(* vpandn %ymm7,%ymm15,%ymm3                       #! PC = 0x555555577a15 *)
not ymm15_0n@uint64 ymm15_0;
and ymm3_0@uint64 ymm15_0n ymm7_0;
not ymm15_1n@uint64 ymm15_1;
and ymm3_1@uint64 ymm15_1n ymm7_1;
not ymm15_2n@uint64 ymm15_2;
and ymm3_2@uint64 ymm15_2n ymm7_2;
not ymm15_3n@uint64 ymm15_3;
and ymm3_3@uint64 ymm15_3n ymm7_3;
(* vpxor  %ymm2,%ymm3,%ymm9                        #! PC = 0x555555577a19 *)
xor ymm9_0@uint64 ymm3_0 ymm2_0;
xor ymm9_1@uint64 ymm3_1 ymm2_1;
xor ymm9_2@uint64 ymm3_2 ymm2_2;
xor ymm9_3@uint64 ymm3_3 ymm2_3;
(* vpandn %ymm0,%ymm7,%ymm2                        #! PC = 0x555555577a1d *)
not ymm7_0n@uint64 ymm7_0;
and ymm2_0@uint64 ymm7_0n ymm0_0;
not ymm7_1n@uint64 ymm7_1;
and ymm2_1@uint64 ymm7_1n ymm0_1;
not ymm7_2n@uint64 ymm7_2;
and ymm2_2@uint64 ymm7_2n ymm0_2;
not ymm7_3n@uint64 ymm7_3;
and ymm2_3@uint64 ymm7_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x555555577a21 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm7,%ymm0,%ymm7                        #! PC = 0x555555577a25 *)
xor ymm7_0@uint64 ymm0_0 ymm7_0;
xor ymm7_1@uint64 ymm0_1 ymm7_1;
xor ymm7_2@uint64 ymm0_2 ymm7_2;
xor ymm7_3@uint64 ymm0_3 ymm7_3;
(* vpxor  -0x370(%rbp),%ymm4,%ymm0                 #! EA = L0x7fffffffbbe0; Value = 0x656c92d6905c3423; PC = 0x555555577a29 *)
xor ymm0_0@uint64 ymm4_0 L0x7fffffffbbe0;
xor ymm0_1@uint64 ymm4_1 L0x7fffffffbbe8;
xor ymm0_2@uint64 ymm4_2 L0x7fffffffbbf0;
xor ymm0_3@uint64 ymm4_3 L0x7fffffffbbf8;
(* vmovdqa %ymm9,-0xf0(%rbp)                       #! EA = L0x7fffffffbe60; PC = 0x555555577a31 *)
mov L0x7fffffffbe60 ymm9_0;
mov L0x7fffffffbe68 ymm9_1;
mov L0x7fffffffbe70 ymm9_2;
mov L0x7fffffffbe78 ymm9_3;
(* vpxor  %ymm15,%ymm2,%ymm9                       #! PC = 0x555555577a39 *)
xor ymm9_0@uint64 ymm2_0 ymm15_0;
xor ymm9_1@uint64 ymm2_1 ymm15_1;
xor ymm9_2@uint64 ymm2_2 ymm15_2;
xor ymm9_3@uint64 ymm2_3 ymm15_3;
(* vmovdqa %ymm9,-0x190(%rbp)                      #! EA = L0x7fffffffbdc0; PC = 0x555555577a3e *)
mov L0x7fffffffbdc0 ymm9_0;
mov L0x7fffffffbdc8 ymm9_1;
mov L0x7fffffffbdd0 ymm9_2;
mov L0x7fffffffbdd8 ymm9_3;
(* vpxor  -0x170(%rbp),%ymm13,%ymm9                #! EA = L0x7fffffffbde0; Value = 0x091010d55fa046ca; PC = 0x555555577a46 *)
xor ymm9_0@uint64 ymm13_0 L0x7fffffffbde0;
xor ymm9_1@uint64 ymm13_1 L0x7fffffffbde8;
xor ymm9_2@uint64 ymm13_2 L0x7fffffffbdf0;
xor ymm9_3@uint64 ymm13_3 L0x7fffffffbdf8;
(* vpsrlq $0x3f,%ymm0,%ymm1                        #! PC = 0x555555577a4e *)
shr ymm1_0 ymm0_0 0x3f@uint64;
shr ymm1_1 ymm0_1 0x3f@uint64;
shr ymm1_2 ymm0_2 0x3f@uint64;
shr ymm1_3 ymm0_3 0x3f@uint64;
(* vpsllq $0x1,%ymm0,%ymm0                         #! PC = 0x555555577a53 *)
shl ymm0_0 ymm0_0 0x1@uint64;
shl ymm0_1 ymm0_1 0x1@uint64;
shl ymm0_2 ymm0_2 0x1@uint64;
shl ymm0_3 ymm0_3 0x1@uint64;
(* vmovdqa %ymm7,-0xd0(%rbp)                       #! EA = L0x7fffffffbe80; PC = 0x555555577a58 *)
mov L0x7fffffffbe80 ymm7_0;
mov L0x7fffffffbe88 ymm7_1;
mov L0x7fffffffbe90 ymm7_2;
mov L0x7fffffffbe98 ymm7_3;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x555555577a60 *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x2d0(%rbp),%ymm14,%ymm1                #! EA = L0x7fffffffbc80; Value = 0xa1ffa1e15ff0bd68; PC = 0x555555577a64 *)
xor ymm1_0@uint64 ymm14_0 L0x7fffffffbc80;
xor ymm1_1@uint64 ymm14_1 L0x7fffffffbc88;
xor ymm1_2@uint64 ymm14_2 L0x7fffffffbc90;
xor ymm1_3@uint64 ymm14_3 L0x7fffffffbc98;
(* vpxor  -0x350(%rbp),%ymm14,%ymm14               #! EA = L0x7fffffffbc00; Value = 0x29f78ba152939175; PC = 0x555555577a6c *)
xor ymm14_0@uint64 ymm14_0 L0x7fffffffbc00;
xor ymm14_1@uint64 ymm14_1 L0x7fffffffbc08;
xor ymm14_2@uint64 ymm14_2 L0x7fffffffbc10;
xor ymm14_3@uint64 ymm14_3 L0x7fffffffbc18;
(* vpsrlq $0x27,%ymm9,%ymm3                        #! PC = 0x555555577a74 *)
shr ymm3_0 ymm9_0 0x27@uint64;
shr ymm3_1 ymm9_1 0x27@uint64;
shr ymm3_2 ymm9_2 0x27@uint64;
shr ymm3_3 ymm9_3 0x27@uint64;
(* vpsllq $0x19,%ymm9,%ymm9                        #! PC = 0x555555577a7a *)
shl ymm9_0 ymm9_0 0x19@uint64;
shl ymm9_1 ymm9_1 0x19@uint64;
shl ymm9_2 ymm9_2 0x19@uint64;
shl ymm9_3 ymm9_3 0x19@uint64;
(* vpsrlq $0x3a,%ymm1,%ymm2                        #! PC = 0x555555577a80 *)
shr ymm2_0 ymm1_0 0x3a@uint64;
shr ymm2_1 ymm1_1 0x3a@uint64;
shr ymm2_2 ymm1_2 0x3a@uint64;
shr ymm2_3 ymm1_3 0x3a@uint64;
(* vpsllq $0x6,%ymm1,%ymm1                         #! PC = 0x555555577a85 *)
shl ymm1_0 ymm1_0 0x6@uint64;
shl ymm1_1 ymm1_1 0x6@uint64;
shl ymm1_2 ymm1_2 0x6@uint64;
shl ymm1_3 ymm1_3 0x6@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x555555577a8a *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpor   %ymm3,%ymm9,%ymm2                        #! PC = 0x555555577a8e *)
or ymm2_0@uint64 ymm9_0 ymm3_0;
or ymm2_1@uint64 ymm9_1 ymm3_1;
or ymm2_2@uint64 ymm9_2 ymm3_2;
or ymm2_3@uint64 ymm9_3 ymm3_3;
(* vpandn %ymm2,%ymm1,%ymm3                        #! PC = 0x555555577a92 *)
not ymm1_0n@uint64 ymm1_0;
and ymm3_0@uint64 ymm1_0n ymm2_0;
not ymm1_1n@uint64 ymm1_1;
and ymm3_1@uint64 ymm1_1n ymm2_1;
not ymm1_2n@uint64 ymm1_2;
and ymm3_2@uint64 ymm1_2n ymm2_2;
not ymm1_3n@uint64 ymm1_3;
and ymm3_3@uint64 ymm1_3n ymm2_3;
(* vpxor  %ymm0,%ymm3,%ymm9                        #! PC = 0x555555577a96 *)
xor ymm9_0@uint64 ymm3_0 ymm0_0;
xor ymm9_1@uint64 ymm3_1 ymm0_1;
xor ymm9_2@uint64 ymm3_2 ymm0_2;
xor ymm9_3@uint64 ymm3_3 ymm0_3;
(* vpxor  -0x50(%rbp),%ymm8,%ymm3                  #! EA = L0x7fffffffbf00; Value = 0x45428a665086ac41; PC = 0x555555577a9a *)
xor ymm3_0@uint64 ymm8_0 L0x7fffffffbf00;
xor ymm3_1@uint64 ymm8_1 L0x7fffffffbf08;
xor ymm3_2@uint64 ymm8_2 L0x7fffffffbf10;
xor ymm3_3@uint64 ymm8_3 L0x7fffffffbf18;
(* vmovdqa %ymm9,%ymm15                            #! PC = 0x555555577a9f *)
mov ymm15_0 ymm9_0;
mov ymm15_1 ymm9_1;
mov ymm15_2 ymm9_2;
mov ymm15_3 ymm9_3;
(* vpshufb 0x56453(%rip),%ymm3,%ymm3        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x555555577aa4 *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm3_0, ymm3_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm3_2, ymm3_3, tmp_2, tmp_3);
mov ymm3_0 tmp_0;
mov ymm3_1 tmp_1;
mov ymm3_2 tmp_2;
mov ymm3_3 tmp_3;
(* vmovdqa %ymm15,-0x1d0(%rbp)                     #! EA = L0x7fffffffbd80; PC = 0x555555577aad *)
mov L0x7fffffffbd80 ymm15_0;
mov L0x7fffffffbd88 ymm15_1;
mov L0x7fffffffbd90 ymm15_2;
mov L0x7fffffffbd98 ymm15_3;
(* vpandn %ymm3,%ymm2,%ymm7                        #! PC = 0x555555577ab5 *)
not ymm2_0n@uint64 ymm2_0;
and ymm7_0@uint64 ymm2_0n ymm3_0;
not ymm2_1n@uint64 ymm2_1;
and ymm7_1@uint64 ymm2_1n ymm3_1;
not ymm2_2n@uint64 ymm2_2;
and ymm7_2@uint64 ymm2_2n ymm3_2;
not ymm2_3n@uint64 ymm2_3;
and ymm7_3@uint64 ymm2_3n ymm3_3;
(* vpxor  %ymm1,%ymm7,%ymm12                       #! PC = 0x555555577ab9 *)
xor ymm12_0@uint64 ymm7_0 ymm1_0;
xor ymm12_1@uint64 ymm7_1 ymm1_1;
xor ymm12_2@uint64 ymm7_2 ymm1_2;
xor ymm12_3@uint64 ymm7_3 ymm1_3;
(* vpsrlq $0x2e,%ymm6,%ymm7                        #! PC = 0x555555577abd *)
shr ymm7_0 ymm6_0 0x2e@uint64;
shr ymm7_1 ymm6_1 0x2e@uint64;
shr ymm7_2 ymm6_2 0x2e@uint64;
shr ymm7_3 ymm6_3 0x2e@uint64;
(* vpsllq $0x12,%ymm6,%ymm6                        #! PC = 0x555555577ac2 *)
shl ymm6_0 ymm6_0 0x12@uint64;
shl ymm6_1 ymm6_1 0x12@uint64;
shl ymm6_2 ymm6_2 0x12@uint64;
shl ymm6_3 ymm6_3 0x12@uint64;
(* vmovdqa %ymm12,-0x170(%rbp)                     #! EA = L0x7fffffffbde0; PC = 0x555555577ac7 *)
mov L0x7fffffffbde0 ymm12_0;
mov L0x7fffffffbde8 ymm12_1;
mov L0x7fffffffbdf0 ymm12_2;
mov L0x7fffffffbdf8 ymm12_3;
(* vpor   %ymm7,%ymm6,%ymm12                       #! PC = 0x555555577acf *)
or ymm12_0@uint64 ymm6_0 ymm7_0;
or ymm12_1@uint64 ymm6_1 ymm7_1;
or ymm12_2@uint64 ymm6_2 ymm7_2;
or ymm12_3@uint64 ymm6_3 ymm7_3;
(* vpandn %ymm12,%ymm3,%ymm9                       #! PC = 0x555555577ad3 *)
not ymm3_0n@uint64 ymm3_0;
and ymm9_0@uint64 ymm3_0n ymm12_0;
not ymm3_1n@uint64 ymm3_1;
and ymm9_1@uint64 ymm3_1n ymm12_1;
not ymm3_2n@uint64 ymm3_2;
and ymm9_2@uint64 ymm3_2n ymm12_2;
not ymm3_3n@uint64 ymm3_3;
and ymm9_3@uint64 ymm3_3n ymm12_3;
(* vpxor  %ymm2,%ymm9,%ymm9                        #! PC = 0x555555577ad8 *)
xor ymm9_0@uint64 ymm9_0 ymm2_0;
xor ymm9_1@uint64 ymm9_1 ymm2_1;
xor ymm9_2@uint64 ymm9_2 ymm2_2;
xor ymm9_3@uint64 ymm9_3 ymm2_3;
(* vpandn %ymm0,%ymm12,%ymm2                       #! PC = 0x555555577adc *)
not ymm12_0n@uint64 ymm12_0;
and ymm2_0@uint64 ymm12_0n ymm0_0;
not ymm12_1n@uint64 ymm12_1;
and ymm2_1@uint64 ymm12_1n ymm0_1;
not ymm12_2n@uint64 ymm12_2;
and ymm2_2@uint64 ymm12_2n ymm0_2;
not ymm12_3n@uint64 ymm12_3;
and ymm2_3@uint64 ymm12_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x555555577ae0 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm12,%ymm0,%ymm1                       #! PC = 0x555555577ae4 *)
xor ymm1_0@uint64 ymm0_0 ymm12_0;
xor ymm1_1@uint64 ymm0_1 ymm12_1;
xor ymm1_2@uint64 ymm0_2 ymm12_2;
xor ymm1_3@uint64 ymm0_3 ymm12_3;
(* vpxor  -0x310(%rbp),%ymm8,%ymm0                 #! EA = L0x7fffffffbc40; Value = 0x1ef11c5578706c33; PC = 0x555555577ae9 *)
xor ymm0_0@uint64 ymm8_0 L0x7fffffffbc40;
xor ymm0_1@uint64 ymm8_1 L0x7fffffffbc48;
xor ymm0_2@uint64 ymm8_2 L0x7fffffffbc50;
xor ymm0_3@uint64 ymm8_3 L0x7fffffffbc58;
(* vpxor  %ymm3,%ymm2,%ymm2                        #! PC = 0x555555577af1 *)
xor ymm2_0@uint64 ymm2_0 ymm3_0;
xor ymm2_1@uint64 ymm2_1 ymm3_1;
xor ymm2_2@uint64 ymm2_2 ymm3_2;
xor ymm2_3@uint64 ymm2_3 ymm3_3;
(* vmovdqa %ymm1,-0x150(%rbp)                      #! EA = L0x7fffffffbe00; PC = 0x555555577af5 *)
mov L0x7fffffffbe00 ymm1_0;
mov L0x7fffffffbe08 ymm1_1;
mov L0x7fffffffbe10 ymm1_2;
mov L0x7fffffffbe18 ymm1_3;
(* vpxor  -0x1f0(%rbp),%ymm8,%ymm8                 #! EA = L0x7fffffffbd60; Value = 0x1acec2cda2b1c6c6; PC = 0x555555577afd *)
xor ymm8_0@uint64 ymm8_0 L0x7fffffffbd60;
xor ymm8_1@uint64 ymm8_1 L0x7fffffffbd68;
xor ymm8_2@uint64 ymm8_2 L0x7fffffffbd70;
xor ymm8_3@uint64 ymm8_3 L0x7fffffffbd78;
(* vpsrlq $0x25,%ymm0,%ymm1                        #! PC = 0x555555577b05 *)
shr ymm1_0 ymm0_0 0x25@uint64;
shr ymm1_1 ymm0_1 0x25@uint64;
shr ymm1_2 ymm0_2 0x25@uint64;
shr ymm1_3 ymm0_3 0x25@uint64;
(* vpsllq $0x1b,%ymm0,%ymm0                        #! PC = 0x555555577b0a *)
shl ymm0_0 ymm0_0 0x1b@uint64;
shl ymm0_1 ymm0_1 0x1b@uint64;
shl ymm0_2 ymm0_2 0x1b@uint64;
shl ymm0_3 ymm0_3 0x1b@uint64;
(* vmovdqa %ymm2,-0xb0(%rbp)                       #! EA = L0x7fffffffbea0; PC = 0x555555577b0f *)
mov L0x7fffffffbea0 ymm2_0;
mov L0x7fffffffbea8 ymm2_1;
mov L0x7fffffffbeb0 ymm2_2;
mov L0x7fffffffbeb8 ymm2_3;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x555555577b17 *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x2f0(%rbp),%ymm5,%ymm1                 #! EA = L0x7fffffffbc60; Value = 0x1322254abe46649e; PC = 0x555555577b1b *)
xor ymm1_0@uint64 ymm5_0 L0x7fffffffbc60;
xor ymm1_1@uint64 ymm5_1 L0x7fffffffbc68;
xor ymm1_2@uint64 ymm5_2 L0x7fffffffbc70;
xor ymm1_3@uint64 ymm5_3 L0x7fffffffbc78;
(* vpxor  -0x3b0(%rbp),%ymm5,%ymm5                 #! EA = L0x7fffffffbba0; Value = 0x329a908710cd5a85; PC = 0x555555577b23 *)
xor ymm5_0@uint64 ymm5_0 L0x7fffffffbba0;
xor ymm5_1@uint64 ymm5_1 L0x7fffffffbba8;
xor ymm5_2@uint64 ymm5_2 L0x7fffffffbbb0;
xor ymm5_3@uint64 ymm5_3 L0x7fffffffbbb8;
(* vpsrlq $0x1c,%ymm1,%ymm2                        #! PC = 0x555555577b2b *)
shr ymm2_0 ymm1_0 0x1c@uint64;
shr ymm2_1 ymm1_1 0x1c@uint64;
shr ymm2_2 ymm1_2 0x1c@uint64;
shr ymm2_3 ymm1_3 0x1c@uint64;
(* vpsllq $0x24,%ymm1,%ymm1                        #! PC = 0x555555577b30 *)
shl ymm1_0 ymm1_0 0x24@uint64;
shl ymm1_1 ymm1_1 0x24@uint64;
shl ymm1_2 ymm1_2 0x24@uint64;
shl ymm1_3 ymm1_3 0x24@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x555555577b35 *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpxor  -0x290(%rbp),%ymm4,%ymm2                 #! EA = L0x7fffffffbcc0; Value = 0x02c931b528b695b9; PC = 0x555555577b39 *)
xor ymm2_0@uint64 ymm4_0 L0x7fffffffbcc0;
xor ymm2_1@uint64 ymm4_1 L0x7fffffffbcc8;
xor ymm2_2@uint64 ymm4_2 L0x7fffffffbcd0;
xor ymm2_3@uint64 ymm4_3 L0x7fffffffbcd8;
(* vpxor  -0x270(%rbp),%ymm4,%ymm4                 #! EA = L0x7fffffffbce0; Value = 0x752f2fec3448e9b2; PC = 0x555555577b41 *)
xor ymm4_0@uint64 ymm4_0 L0x7fffffffbce0;
xor ymm4_1@uint64 ymm4_1 L0x7fffffffbce8;
xor ymm4_2@uint64 ymm4_2 L0x7fffffffbcf0;
xor ymm4_3@uint64 ymm4_3 L0x7fffffffbcf8;
(* vpsrlq $0x36,%ymm2,%ymm3                        #! PC = 0x555555577b49 *)
shr ymm3_0 ymm2_0 0x36@uint64;
shr ymm3_1 ymm2_1 0x36@uint64;
shr ymm3_2 ymm2_2 0x36@uint64;
shr ymm3_3 ymm2_3 0x36@uint64;
(* vpsllq $0xa,%ymm2,%ymm2                         #! PC = 0x555555577b4e *)
shl ymm2_0 ymm2_0 0xa@uint64;
shl ymm2_1 ymm2_1 0xa@uint64;
shl ymm2_2 ymm2_2 0xa@uint64;
shl ymm2_3 ymm2_3 0xa@uint64;
(* vpor   %ymm3,%ymm2,%ymm2                        #! PC = 0x555555577b53 *)
or ymm2_0@uint64 ymm2_0 ymm3_0;
or ymm2_1@uint64 ymm2_1 ymm3_1;
or ymm2_2@uint64 ymm2_2 ymm3_2;
or ymm2_3@uint64 ymm2_3 ymm3_3;
(* vpandn %ymm2,%ymm1,%ymm3                        #! PC = 0x555555577b57 *)
not ymm1_0n@uint64 ymm1_0;
and ymm3_0@uint64 ymm1_0n ymm2_0;
not ymm1_1n@uint64 ymm1_1;
and ymm3_1@uint64 ymm1_1n ymm2_1;
not ymm1_2n@uint64 ymm1_2;
and ymm3_2@uint64 ymm1_2n ymm2_2;
not ymm1_3n@uint64 ymm1_3;
and ymm3_3@uint64 ymm1_3n ymm2_3;
(* vpxor  %ymm0,%ymm3,%ymm7                        #! PC = 0x555555577b5b *)
xor ymm7_0@uint64 ymm3_0 ymm0_0;
xor ymm7_1@uint64 ymm3_1 ymm0_1;
xor ymm7_2@uint64 ymm3_2 ymm0_2;
xor ymm7_3@uint64 ymm3_3 ymm0_3;
(* vpsrlq $0x31,%ymm11,%ymm3                       #! PC = 0x555555577b5f *)
shr ymm3_0 ymm11_0 0x31@uint64;
shr ymm3_1 ymm11_1 0x31@uint64;
shr ymm3_2 ymm11_2 0x31@uint64;
shr ymm3_3 ymm11_3 0x31@uint64;
(* vpsllq $0xf,%ymm11,%ymm11                       #! PC = 0x555555577b65 *)
shl ymm11_0 ymm11_0 0xf@uint64;
shl ymm11_1 ymm11_1 0xf@uint64;
shl ymm11_2 ymm11_2 0xf@uint64;
shl ymm11_3 ymm11_3 0xf@uint64;
(* vmovdqa %ymm7,%ymm12                            #! PC = 0x555555577b6b *)
mov ymm12_0 ymm7_0;
mov ymm12_1 ymm7_1;
mov ymm12_2 ymm7_2;
mov ymm12_3 ymm7_3;
(* vpor   %ymm3,%ymm11,%ymm11                      #! PC = 0x555555577b6f *)
or ymm11_0@uint64 ymm11_0 ymm3_0;
or ymm11_1@uint64 ymm11_1 ymm3_1;
or ymm11_2@uint64 ymm11_2 ymm3_2;
or ymm11_3@uint64 ymm11_3 ymm3_3;
(* vpxor  -0x70(%rbp),%ymm13,%ymm3                 #! EA = L0x7fffffffbee0; Value = 0xee429201d6040138; PC = 0x555555577b73 *)
xor ymm3_0@uint64 ymm13_0 L0x7fffffffbee0;
xor ymm3_1@uint64 ymm13_1 L0x7fffffffbee8;
xor ymm3_2@uint64 ymm13_2 L0x7fffffffbef0;
xor ymm3_3@uint64 ymm13_3 L0x7fffffffbef8;
(* vpxor  -0x2b0(%rbp),%ymm13,%ymm13               #! EA = L0x7fffffffbca0; Value = 0xfaf58b3abf5862df; PC = 0x555555577b78 *)
xor ymm13_0@uint64 ymm13_0 L0x7fffffffbca0;
xor ymm13_1@uint64 ymm13_1 L0x7fffffffbca8;
xor ymm13_2@uint64 ymm13_2 L0x7fffffffbcb0;
xor ymm13_3@uint64 ymm13_3 L0x7fffffffbcb8;
(* vmovdqa %ymm12,-0x350(%rbp)                     #! EA = L0x7fffffffbc00; PC = 0x555555577b80 *)
mov L0x7fffffffbc00 ymm12_0;
mov L0x7fffffffbc08 ymm12_1;
mov L0x7fffffffbc10 ymm12_2;
mov L0x7fffffffbc18 ymm12_3;
(* vpandn %ymm11,%ymm2,%ymm7                       #! PC = 0x555555577b88 *)
not ymm2_0n@uint64 ymm2_0;
and ymm7_0@uint64 ymm2_0n ymm11_0;
not ymm2_1n@uint64 ymm2_1;
and ymm7_1@uint64 ymm2_1n ymm11_1;
not ymm2_2n@uint64 ymm2_2;
and ymm7_2@uint64 ymm2_2n ymm11_2;
not ymm2_3n@uint64 ymm2_3;
and ymm7_3@uint64 ymm2_3n ymm11_3;
(* vpshufb 0x5634a(%rip),%ymm3,%ymm3        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x555555577b8d *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm3_0, ymm3_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm3_2, ymm3_3, tmp_2, tmp_3);
mov ymm3_0 tmp_0;
mov ymm3_1 tmp_1;
mov ymm3_2 tmp_2;
mov ymm3_3 tmp_3;
(* vpxor  %ymm1,%ymm7,%ymm7                        #! PC = 0x555555577b96 *)
xor ymm7_0@uint64 ymm7_0 ymm1_0;
xor ymm7_1@uint64 ymm7_1 ymm1_1;
xor ymm7_2@uint64 ymm7_2 ymm1_2;
xor ymm7_3@uint64 ymm7_3 ymm1_3;
(* vpandn %ymm3,%ymm11,%ymm6                       #! PC = 0x555555577b9a *)
not ymm11_0n@uint64 ymm11_0;
and ymm6_0@uint64 ymm11_0n ymm3_0;
not ymm11_1n@uint64 ymm11_1;
and ymm6_1@uint64 ymm11_1n ymm3_1;
not ymm11_2n@uint64 ymm11_2;
and ymm6_2@uint64 ymm11_2n ymm3_2;
not ymm11_3n@uint64 ymm11_3;
and ymm6_3@uint64 ymm11_3n ymm3_3;
(* vpxor  %ymm2,%ymm6,%ymm2                        #! PC = 0x555555577b9e *)
xor ymm2_0@uint64 ymm6_0 ymm2_0;
xor ymm2_1@uint64 ymm6_1 ymm2_1;
xor ymm2_2@uint64 ymm6_2 ymm2_2;
xor ymm2_3@uint64 ymm6_3 ymm2_3;
(* vmovdqa %ymm2,-0x70(%rbp)                       #! EA = L0x7fffffffbee0; PC = 0x555555577ba2 *)
mov L0x7fffffffbee0 ymm2_0;
mov L0x7fffffffbee8 ymm2_1;
mov L0x7fffffffbef0 ymm2_2;
mov L0x7fffffffbef8 ymm2_3;
(* vpandn %ymm0,%ymm3,%ymm2                        #! PC = 0x555555577ba7 *)
not ymm3_0n@uint64 ymm3_0;
and ymm2_0@uint64 ymm3_0n ymm0_0;
not ymm3_1n@uint64 ymm3_1;
and ymm2_1@uint64 ymm3_1n ymm0_1;
not ymm3_2n@uint64 ymm3_2;
and ymm2_2@uint64 ymm3_2n ymm0_2;
not ymm3_3n@uint64 ymm3_3;
and ymm2_3@uint64 ymm3_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x555555577bab *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm3,%ymm0,%ymm3                        #! PC = 0x555555577baf *)
xor ymm3_0@uint64 ymm0_0 ymm3_0;
xor ymm3_1@uint64 ymm0_1 ymm3_1;
xor ymm3_2@uint64 ymm0_2 ymm3_2;
xor ymm3_3@uint64 ymm0_3 ymm3_3;
(* vpsrlq $0x9,%ymm13,%ymm0                        #! PC = 0x555555577bb3 *)
shr ymm0_0 ymm13_0 0x9@uint64;
shr ymm0_1 ymm13_1 0x9@uint64;
shr ymm0_2 ymm13_2 0x9@uint64;
shr ymm0_3 ymm13_3 0x9@uint64;
(* vpxor  %ymm11,%ymm2,%ymm11                      #! PC = 0x555555577bb9 *)
xor ymm11_0@uint64 ymm2_0 ymm11_0;
xor ymm11_1@uint64 ymm2_1 ymm11_1;
xor ymm11_2@uint64 ymm2_2 ymm11_2;
xor ymm11_3@uint64 ymm2_3 ymm11_3;
(* vpsllq $0x37,%ymm13,%ymm13                      #! PC = 0x555555577bbe *)
shl ymm13_0 ymm13_0 0x37@uint64;
shl ymm13_1 ymm13_1 0x37@uint64;
shl ymm13_2 ymm13_2 0x37@uint64;
shl ymm13_3 ymm13_3 0x37@uint64;
(* vpsrlq $0x2,%ymm14,%ymm2                        #! PC = 0x555555577bc4 *)
shr ymm2_0 ymm14_0 0x2@uint64;
shr ymm2_1 ymm14_1 0x2@uint64;
shr ymm2_2 ymm14_2 0x2@uint64;
shr ymm2_3 ymm14_3 0x2@uint64;
(* vmovdqa %ymm3,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x555555577bca *)
mov L0x7fffffffbf00 ymm3_0;
mov L0x7fffffffbf08 ymm3_1;
mov L0x7fffffffbf10 ymm3_2;
mov L0x7fffffffbf18 ymm3_3;
(* vpor   %ymm0,%ymm13,%ymm13                      #! PC = 0x555555577bcf *)
or ymm13_0@uint64 ymm13_0 ymm0_0;
or ymm13_1@uint64 ymm13_1 ymm0_1;
or ymm13_2@uint64 ymm13_2 ymm0_2;
or ymm13_3@uint64 ymm13_3 ymm0_3;
(* vpsrlq $0x19,%ymm8,%ymm0                        #! PC = 0x555555577bd3 *)
shr ymm0_0 ymm8_0 0x19@uint64;
shr ymm0_1 ymm8_1 0x19@uint64;
shr ymm0_2 ymm8_2 0x19@uint64;
shr ymm0_3 ymm8_3 0x19@uint64;
(* vpsllq $0x27,%ymm8,%ymm8                        #! PC = 0x555555577bd9 *)
shl ymm8_0 ymm8_0 0x27@uint64;
shl ymm8_1 ymm8_1 0x27@uint64;
shl ymm8_2 ymm8_2 0x27@uint64;
shl ymm8_3 ymm8_3 0x27@uint64;
(* vpsllq $0x3e,%ymm14,%ymm14                      #! PC = 0x555555577bdf *)
shl ymm14_0 ymm14_0 0x3e@uint64;
shl ymm14_1 ymm14_1 0x3e@uint64;
shl ymm14_2 ymm14_2 0x3e@uint64;
shl ymm14_3 ymm14_3 0x3e@uint64;
(* vpor   %ymm0,%ymm8,%ymm8                        #! PC = 0x555555577be5 *)
or ymm8_0@uint64 ymm8_0 ymm0_0;
or ymm8_1@uint64 ymm8_1 ymm0_1;
or ymm8_2@uint64 ymm8_2 ymm0_2;
or ymm8_3@uint64 ymm8_3 ymm0_3;
(* vpor   %ymm2,%ymm14,%ymm1                       #! PC = 0x555555577be9 *)
or ymm1_0@uint64 ymm14_0 ymm2_0;
or ymm1_1@uint64 ymm14_1 ymm2_1;
or ymm1_2@uint64 ymm14_2 ymm2_2;
or ymm1_3@uint64 ymm14_3 ymm2_3;
(* vpxor  -0x210(%rbp),%ymm15,%ymm0                #! EA = L0x7fffffffbd40; Value = 0xae399b7b6c84633d; PC = 0x555555577bed *)
xor ymm0_0@uint64 ymm15_0 L0x7fffffffbd40;
xor ymm0_1@uint64 ymm15_1 L0x7fffffffbd48;
xor ymm0_2@uint64 ymm15_2 L0x7fffffffbd50;
xor ymm0_3@uint64 ymm15_3 L0x7fffffffbd58;
(* vpandn %ymm8,%ymm13,%ymm6                       #! PC = 0x555555577bf5 *)
not ymm13_0n@uint64 ymm13_0;
and ymm6_0@uint64 ymm13_0n ymm8_0;
not ymm13_1n@uint64 ymm13_1;
and ymm6_1@uint64 ymm13_1n ymm8_1;
not ymm13_2n@uint64 ymm13_2;
and ymm6_2@uint64 ymm13_2n ymm8_2;
not ymm13_3n@uint64 ymm13_3;
and ymm6_3@uint64 ymm13_3n ymm8_3;
(* vpxor  %ymm1,%ymm6,%ymm6                        #! PC = 0x555555577bfa *)
xor ymm6_0@uint64 ymm6_0 ymm1_0;
xor ymm6_1@uint64 ymm6_1 ymm1_1;
xor ymm6_2@uint64 ymm6_2 ymm1_2;
xor ymm6_3@uint64 ymm6_3 ymm1_3;
(* vpxor  %ymm12,%ymm6,%ymm2                       #! PC = 0x555555577bfe *)
xor ymm2_0@uint64 ymm6_0 ymm12_0;
xor ymm2_1@uint64 ymm6_1 ymm12_1;
xor ymm2_2@uint64 ymm6_2 ymm12_2;
xor ymm2_3@uint64 ymm6_3 ymm12_3;
(* vpxor  %ymm0,%ymm2,%ymm2                        #! PC = 0x555555577c03 *)
xor ymm2_0@uint64 ymm2_0 ymm0_0;
xor ymm2_1@uint64 ymm2_1 ymm0_1;
xor ymm2_2@uint64 ymm2_2 ymm0_2;
xor ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x17,%ymm5,%ymm0                        #! PC = 0x555555577c07 *)
shr ymm0_0 ymm5_0 0x17@uint64;
shr ymm0_1 ymm5_1 0x17@uint64;
shr ymm0_2 ymm5_2 0x17@uint64;
shr ymm0_3 ymm5_3 0x17@uint64;
(* vpxor  -0x90(%rbp),%ymm2,%ymm2                  #! EA = L0x7fffffffbec0; Value = 0xf204ab825ec1b98c; PC = 0x555555577c0c *)
xor ymm2_0@uint64 ymm2_0 L0x7fffffffbec0;
xor ymm2_1@uint64 ymm2_1 L0x7fffffffbec8;
xor ymm2_2@uint64 ymm2_2 L0x7fffffffbed0;
xor ymm2_3@uint64 ymm2_3 L0x7fffffffbed8;
(* vpsllq $0x29,%ymm5,%ymm5                        #! PC = 0x555555577c14 *)
shl ymm5_0 ymm5_0 0x29@uint64;
shl ymm5_1 ymm5_1 0x29@uint64;
shl ymm5_2 ymm5_2 0x29@uint64;
shl ymm5_3 ymm5_3 0x29@uint64;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x555555577c19 *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpxor  -0x1b0(%rbp),%ymm10,%ymm0                #! EA = L0x7fffffffbda0; Value = 0x032132932ed2960e; PC = 0x555555577c1d *)
xor ymm0_0@uint64 ymm10_0 L0x7fffffffbda0;
xor ymm0_1@uint64 ymm10_1 L0x7fffffffbda8;
xor ymm0_2@uint64 ymm10_2 L0x7fffffffbdb0;
xor ymm0_3@uint64 ymm10_3 L0x7fffffffbdb8;
(* vpandn %ymm5,%ymm8,%ymm3                        #! PC = 0x555555577c25 *)
not ymm8_0n@uint64 ymm8_0;
and ymm3_0@uint64 ymm8_0n ymm5_0;
not ymm8_1n@uint64 ymm8_1;
and ymm3_1@uint64 ymm8_1n ymm5_1;
not ymm8_2n@uint64 ymm8_2;
and ymm3_2@uint64 ymm8_2n ymm5_2;
not ymm8_3n@uint64 ymm8_3;
and ymm3_3@uint64 ymm8_3n ymm5_3;
(* vpxor  %ymm13,%ymm3,%ymm14                      #! PC = 0x555555577c29 *)
xor ymm14_0@uint64 ymm3_0 ymm13_0;
xor ymm14_1@uint64 ymm3_1 ymm13_1;
xor ymm14_2@uint64 ymm3_2 ymm13_2;
xor ymm14_3@uint64 ymm3_3 ymm13_3;
(* vpsrlq $0x3e,%ymm4,%ymm3                        #! PC = 0x555555577c2e *)
shr ymm3_0 ymm4_0 0x3e@uint64;
shr ymm3_1 ymm4_1 0x3e@uint64;
shr ymm3_2 ymm4_2 0x3e@uint64;
shr ymm3_3 ymm4_3 0x3e@uint64;
(* vmovdqa %ymm14,%ymm15                           #! PC = 0x555555577c33 *)
mov ymm15_0 ymm14_0;
mov ymm15_1 ymm14_1;
mov ymm15_2 ymm14_2;
mov ymm15_3 ymm14_3;
(* vpxor  -0x170(%rbp),%ymm7,%ymm14                #! EA = L0x7fffffffbde0; Value = 0x41ff2cbc6c803fdd; PC = 0x555555577c38 *)
xor ymm14_0@uint64 ymm7_0 L0x7fffffffbde0;
xor ymm14_1@uint64 ymm7_1 L0x7fffffffbde8;
xor ymm14_2@uint64 ymm7_2 L0x7fffffffbdf0;
xor ymm14_3@uint64 ymm7_3 L0x7fffffffbdf8;
(* vpsllq $0x2,%ymm4,%ymm4                         #! PC = 0x555555577c40 *)
shl ymm4_0 ymm4_0 0x2@uint64;
shl ymm4_1 ymm4_1 0x2@uint64;
shl ymm4_2 ymm4_2 0x2@uint64;
shl ymm4_3 ymm4_3 0x2@uint64;
(* vmovdqa %ymm15,-0x310(%rbp)                     #! EA = L0x7fffffffbc40; PC = 0x555555577c45 *)
mov L0x7fffffffbc40 ymm15_0;
mov L0x7fffffffbc48 ymm15_1;
mov L0x7fffffffbc50 ymm15_2;
mov L0x7fffffffbc58 ymm15_3;
(* vpxor  %ymm0,%ymm14,%ymm14                      #! PC = 0x555555577c4d *)
xor ymm14_0@uint64 ymm14_0 ymm0_0;
xor ymm14_1@uint64 ymm14_1 ymm0_1;
xor ymm14_2@uint64 ymm14_2 ymm0_2;
xor ymm14_3@uint64 ymm14_3 ymm0_3;
(* vpor   %ymm3,%ymm4,%ymm0                        #! PC = 0x555555577c51 *)
or ymm0_0@uint64 ymm4_0 ymm3_0;
or ymm0_1@uint64 ymm4_1 ymm3_1;
or ymm0_2@uint64 ymm4_2 ymm3_2;
or ymm0_3@uint64 ymm4_3 ymm3_3;
(* vpandn %ymm0,%ymm5,%ymm3                        #! PC = 0x555555577c55 *)
not ymm5_0n@uint64 ymm5_0;
and ymm3_0@uint64 ymm5_0n ymm0_0;
not ymm5_1n@uint64 ymm5_1;
and ymm3_1@uint64 ymm5_1n ymm0_1;
not ymm5_2n@uint64 ymm5_2;
and ymm3_2@uint64 ymm5_2n ymm0_2;
not ymm5_3n@uint64 ymm5_3;
and ymm3_3@uint64 ymm5_3n ymm0_3;
(* vpxor  %ymm15,%ymm14,%ymm14                     #! PC = 0x555555577c59 *)
xor ymm14_0@uint64 ymm14_0 ymm15_0;
xor ymm14_1@uint64 ymm14_1 ymm15_1;
xor ymm14_2@uint64 ymm14_2 ymm15_2;
xor ymm14_3@uint64 ymm14_3 ymm15_3;
(* vpxor  -0x70(%rbp),%ymm9,%ymm15                 #! EA = L0x7fffffffbee0; Value = 0xde74aeb20a95db03; PC = 0x555555577c5e *)
xor ymm15_0@uint64 ymm9_0 L0x7fffffffbee0;
xor ymm15_1@uint64 ymm9_1 L0x7fffffffbee8;
xor ymm15_2@uint64 ymm9_2 L0x7fffffffbef0;
xor ymm15_3@uint64 ymm9_3 L0x7fffffffbef8;
(* vmovdqa -0xf0(%rbp),%ymm4                       #! EA = L0x7fffffffbe60; Value = 0x83371dd98c8b09f1; PC = 0x555555577c63 *)
mov ymm4_0 L0x7fffffffbe60;
mov ymm4_1 L0x7fffffffbe68;
mov ymm4_2 L0x7fffffffbe70;
mov ymm4_3 L0x7fffffffbe78;
(* vpxor  %ymm8,%ymm3,%ymm8                        #! PC = 0x555555577c6b *)
xor ymm8_0@uint64 ymm3_0 ymm8_0;
xor ymm8_1@uint64 ymm3_1 ymm8_1;
xor ymm8_2@uint64 ymm3_2 ymm8_2;
xor ymm8_3@uint64 ymm3_3 ymm8_3;
(* vpxor  -0x250(%rbp),%ymm4,%ymm3                 #! EA = L0x7fffffffbd00; Value = 0xd2e894a105ce76b0; PC = 0x555555577c70 *)
xor ymm3_0@uint64 ymm4_0 L0x7fffffffbd00;
xor ymm3_1@uint64 ymm4_1 L0x7fffffffbd08;
xor ymm3_2@uint64 ymm4_2 L0x7fffffffbd10;
xor ymm3_3@uint64 ymm4_3 L0x7fffffffbd18;
(* vpandn %ymm13,%ymm1,%ymm4                       #! PC = 0x555555577c78 *)
not ymm1_0n@uint64 ymm1_0;
and ymm4_0@uint64 ymm1_0n ymm13_0;
not ymm1_1n@uint64 ymm1_1;
and ymm4_1@uint64 ymm1_1n ymm13_1;
not ymm1_2n@uint64 ymm1_2;
and ymm4_2@uint64 ymm1_2n ymm13_2;
not ymm1_3n@uint64 ymm1_3;
and ymm4_3@uint64 ymm1_3n ymm13_3;
(* vmovdqa -0xd0(%rbp),%ymm13                      #! EA = L0x7fffffffbe80; Value = 0x8aad4677926b6a65; PC = 0x555555577c7d *)
mov ymm13_0 L0x7fffffffbe80;
mov ymm13_1 L0x7fffffffbe88;
mov ymm13_2 L0x7fffffffbe90;
mov ymm13_3 L0x7fffffffbe98;
(* vpxor  %ymm0,%ymm4,%ymm4                        #! PC = 0x555555577c85 *)
xor ymm4_0@uint64 ymm4_0 ymm0_0;
xor ymm4_1@uint64 ymm4_1 ymm0_1;
xor ymm4_2@uint64 ymm4_2 ymm0_2;
xor ymm4_3@uint64 ymm4_3 ymm0_3;
(* vpxor  %ymm3,%ymm15,%ymm15                      #! PC = 0x555555577c89 *)
xor ymm15_0@uint64 ymm15_0 ymm3_0;
xor ymm15_1@uint64 ymm15_1 ymm3_1;
xor ymm15_2@uint64 ymm15_2 ymm3_2;
xor ymm15_3@uint64 ymm15_3 ymm3_3;
(* vpandn %ymm1,%ymm0,%ymm3                        #! PC = 0x555555577c8d *)
not ymm0_0n@uint64 ymm0_0;
and ymm3_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm3_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm3_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm3_3@uint64 ymm0_3n ymm1_3;
(* vpxor  -0x230(%rbp),%ymm13,%ymm0                #! EA = L0x7fffffffbd20; Value = 0x8c6c05a408afc4ba; PC = 0x555555577c91 *)
xor ymm0_0@uint64 ymm13_0 L0x7fffffffbd20;
xor ymm0_1@uint64 ymm13_1 L0x7fffffffbd28;
xor ymm0_2@uint64 ymm13_2 L0x7fffffffbd30;
xor ymm0_3@uint64 ymm13_3 L0x7fffffffbd38;
(* vpxor  %ymm5,%ymm3,%ymm5                        #! PC = 0x555555577c99 *)
xor ymm5_0@uint64 ymm3_0 ymm5_0;
xor ymm5_1@uint64 ymm3_1 ymm5_1;
xor ymm5_2@uint64 ymm3_2 ymm5_2;
xor ymm5_3@uint64 ymm3_3 ymm5_3;
(* vpxor  -0x150(%rbp),%ymm4,%ymm1                 #! EA = L0x7fffffffbe00; Value = 0x77129c370664d096; PC = 0x555555577c9d *)
xor ymm1_0@uint64 ymm4_0 L0x7fffffffbe00;
xor ymm1_1@uint64 ymm4_1 L0x7fffffffbe08;
xor ymm1_2@uint64 ymm4_2 L0x7fffffffbe10;
xor ymm1_3@uint64 ymm4_3 L0x7fffffffbe18;
(* vmovdqa -0x190(%rbp),%ymm3                      #! EA = L0x7fffffffbdc0; Value = 0x7cf0f6ddaf592ee4; PC = 0x555555577ca5 *)
mov ymm3_0 L0x7fffffffbdc0;
mov ymm3_1 L0x7fffffffbdc8;
mov ymm3_2 L0x7fffffffbdd0;
mov ymm3_3 L0x7fffffffbdd8;
(* vpxor  %ymm8,%ymm15,%ymm15                      #! PC = 0x555555577cad *)
xor ymm15_0@uint64 ymm15_0 ymm8_0;
xor ymm15_1@uint64 ymm15_1 ymm8_1;
xor ymm15_2@uint64 ymm15_2 ymm8_2;
xor ymm15_3@uint64 ymm15_3 ymm8_3;
(* vpxor  -0x110(%rbp),%ymm3,%ymm3                 #! EA = L0x7fffffffbe40; Value = 0x75da76bd87d37a73; PC = 0x555555577cb2 *)
xor ymm3_0@uint64 ymm3_0 L0x7fffffffbe40;
xor ymm3_1@uint64 ymm3_1 L0x7fffffffbe48;
xor ymm3_2@uint64 ymm3_2 L0x7fffffffbe50;
xor ymm3_3@uint64 ymm3_3 L0x7fffffffbe58;
(* vpxor  %ymm5,%ymm11,%ymm12                      #! PC = 0x555555577cba *)
xor ymm12_0@uint64 ymm11_0 ymm5_0;
xor ymm12_1@uint64 ymm11_1 ymm5_1;
xor ymm12_2@uint64 ymm11_2 ymm5_2;
xor ymm12_3@uint64 ymm11_3 ymm5_3;
(* vmovdqa %ymm5,-0x270(%rbp)                      #! EA = L0x7fffffffbce0; PC = 0x555555577cbe *)
mov L0x7fffffffbce0 ymm5_0;
mov L0x7fffffffbce8 ymm5_1;
mov L0x7fffffffbcf0 ymm5_2;
mov L0x7fffffffbcf8 ymm5_3;
(* vpxor  %ymm0,%ymm1,%ymm1                        #! PC = 0x555555577cc6 *)
xor ymm1_0@uint64 ymm1_0 ymm0_0;
xor ymm1_1@uint64 ymm1_1 ymm0_1;
xor ymm1_2@uint64 ymm1_2 ymm0_2;
xor ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsllq $0x1,%ymm14,%ymm5                        #! PC = 0x555555577cca *)
shl ymm5_0 ymm14_0 0x1@uint64;
shl ymm5_1 ymm14_1 0x1@uint64;
shl ymm5_2 ymm14_2 0x1@uint64;
shl ymm5_3 ymm14_3 0x1@uint64;
(* vpxor  -0x50(%rbp),%ymm1,%ymm1                  #! EA = L0x7fffffffbf00; Value = 0x04754b7b9faeff7b; PC = 0x555555577cd0 *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbf00;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbf08;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbf10;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbf18;
(* vpsrlq $0x3f,%ymm14,%ymm0                       #! PC = 0x555555577cd5 *)
shr ymm0_0 ymm14_0 0x3f@uint64;
shr ymm0_1 ymm14_1 0x3f@uint64;
shr ymm0_2 ymm14_2 0x3f@uint64;
shr ymm0_3 ymm14_3 0x3f@uint64;
(* vpxor  %ymm3,%ymm12,%ymm12                      #! PC = 0x555555577cdb *)
xor ymm12_0@uint64 ymm12_0 ymm3_0;
xor ymm12_1@uint64 ymm12_1 ymm3_1;
xor ymm12_2@uint64 ymm12_2 ymm3_2;
xor ymm12_3@uint64 ymm12_3 ymm3_3;
(* vpxor  -0xb0(%rbp),%ymm12,%ymm12                #! EA = L0x7fffffffbea0; Value = 0xe5bbddf2601ea85b; PC = 0x555555577cdf *)
xor ymm12_0@uint64 ymm12_0 L0x7fffffffbea0;
xor ymm12_1@uint64 ymm12_1 L0x7fffffffbea8;
xor ymm12_2@uint64 ymm12_2 L0x7fffffffbeb0;
xor ymm12_3@uint64 ymm12_3 L0x7fffffffbeb8;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x555555577ce7 *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpsllq $0x1,%ymm15,%ymm3                        #! PC = 0x555555577ceb *)
shl ymm3_0 ymm15_0 0x1@uint64;
shl ymm3_1 ymm15_1 0x1@uint64;
shl ymm3_2 ymm15_2 0x1@uint64;
shl ymm3_3 ymm15_3 0x1@uint64;
(* vpsrlq $0x3f,%ymm15,%ymm0                       #! PC = 0x555555577cf1 *)
shr ymm0_0 ymm15_0 0x3f@uint64;
shr ymm0_1 ymm15_1 0x3f@uint64;
shr ymm0_2 ymm15_2 0x3f@uint64;
shr ymm0_3 ymm15_3 0x3f@uint64;
(* vpsrlq $0x3f,%ymm12,%ymm13                      #! PC = 0x555555577cf7 *)
shr ymm13_0 ymm12_0 0x3f@uint64;
shr ymm13_1 ymm12_1 0x3f@uint64;
shr ymm13_2 ymm12_2 0x3f@uint64;
shr ymm13_3 ymm12_3 0x3f@uint64;
(* vpxor  %ymm1,%ymm5,%ymm5                        #! PC = 0x555555577cfd *)
xor ymm5_0@uint64 ymm5_0 ymm1_0;
xor ymm5_1@uint64 ymm5_1 ymm1_1;
xor ymm5_2@uint64 ymm5_2 ymm1_2;
xor ymm5_3@uint64 ymm5_3 ymm1_3;
(* vpor   %ymm0,%ymm3,%ymm3                        #! PC = 0x555555577d01 *)
or ymm3_0@uint64 ymm3_0 ymm0_0;
or ymm3_1@uint64 ymm3_1 ymm0_1;
or ymm3_2@uint64 ymm3_2 ymm0_2;
or ymm3_3@uint64 ymm3_3 ymm0_3;
(* vpsllq $0x1,%ymm12,%ymm0                        #! PC = 0x555555577d05 *)
shl ymm0_0 ymm12_0 0x1@uint64;
shl ymm0_1 ymm12_1 0x1@uint64;
shl ymm0_2 ymm12_2 0x1@uint64;
shl ymm0_3 ymm12_3 0x1@uint64;
(* vpxor  %ymm6,%ymm5,%ymm6                        #! PC = 0x555555577d0b *)
xor ymm6_0@uint64 ymm5_0 ymm6_0;
xor ymm6_1@uint64 ymm5_1 ymm6_1;
xor ymm6_2@uint64 ymm5_2 ymm6_2;
xor ymm6_3@uint64 ymm5_3 ymm6_3;
(* vpor   %ymm13,%ymm0,%ymm0                       #! PC = 0x555555577d0f *)
or ymm0_0@uint64 ymm0_0 ymm13_0;
or ymm0_1@uint64 ymm0_1 ymm13_1;
or ymm0_2@uint64 ymm0_2 ymm13_2;
or ymm0_3@uint64 ymm0_3 ymm13_3;
(* vpxor  %ymm2,%ymm3,%ymm3                        #! PC = 0x555555577d14 *)
xor ymm3_0@uint64 ymm3_0 ymm2_0;
xor ymm3_1@uint64 ymm3_1 ymm2_1;
xor ymm3_2@uint64 ymm3_2 ymm2_2;
xor ymm3_3@uint64 ymm3_3 ymm2_3;
(* vpxor  %ymm14,%ymm0,%ymm14                      #! PC = 0x555555577d18 *)
xor ymm14_0@uint64 ymm0_0 ymm14_0;
xor ymm14_1@uint64 ymm0_1 ymm14_1;
xor ymm14_2@uint64 ymm0_2 ymm14_2;
xor ymm14_3@uint64 ymm0_3 ymm14_3;
(* vpsrlq $0x3f,%ymm1,%ymm0                        #! PC = 0x555555577d1d *)
shr ymm0_0 ymm1_0 0x3f@uint64;
shr ymm0_1 ymm1_1 0x3f@uint64;
shr ymm0_2 ymm1_2 0x3f@uint64;
shr ymm0_3 ymm1_3 0x3f@uint64;
(* vpxor  %ymm10,%ymm3,%ymm10                      #! PC = 0x555555577d22 *)
xor ymm10_0@uint64 ymm3_0 ymm10_0;
xor ymm10_1@uint64 ymm3_1 ymm10_1;
xor ymm10_2@uint64 ymm3_2 ymm10_2;
xor ymm10_3@uint64 ymm3_3 ymm10_3;
(* vpsllq $0x1,%ymm1,%ymm1                         #! PC = 0x555555577d27 *)
shl ymm1_0 ymm1_0 0x1@uint64;
shl ymm1_1 ymm1_1 0x1@uint64;
shl ymm1_2 ymm1_2 0x1@uint64;
shl ymm1_3 ymm1_3 0x1@uint64;
(* vpxor  %ymm9,%ymm14,%ymm9                       #! PC = 0x555555577d2c *)
xor ymm9_0@uint64 ymm14_0 ymm9_0;
xor ymm9_1@uint64 ymm14_1 ymm9_1;
xor ymm9_2@uint64 ymm14_2 ymm9_2;
xor ymm9_3@uint64 ymm14_3 ymm9_3;
(* vpxor  %ymm7,%ymm3,%ymm7                        #! PC = 0x555555577d31 *)
xor ymm7_0@uint64 ymm3_0 ymm7_0;
xor ymm7_1@uint64 ymm3_1 ymm7_1;
xor ymm7_2@uint64 ymm3_2 ymm7_2;
xor ymm7_3@uint64 ymm3_3 ymm7_3;
(* vpor   %ymm0,%ymm1,%ymm1                        #! PC = 0x555555577d35 *)
or ymm1_0@uint64 ymm1_0 ymm0_0;
or ymm1_1@uint64 ymm1_1 ymm0_1;
or ymm1_2@uint64 ymm1_2 ymm0_2;
or ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3f,%ymm2,%ymm0                        #! PC = 0x555555577d39 *)
shr ymm0_0 ymm2_0 0x3f@uint64;
shr ymm0_1 ymm2_1 0x3f@uint64;
shr ymm0_2 ymm2_2 0x3f@uint64;
shr ymm0_3 ymm2_3 0x3f@uint64;
(* vpxor  %ymm8,%ymm14,%ymm8                       #! PC = 0x555555577d3e *)
xor ymm8_0@uint64 ymm14_0 ymm8_0;
xor ymm8_1@uint64 ymm14_1 ymm8_1;
xor ymm8_2@uint64 ymm14_2 ymm8_2;
xor ymm8_3@uint64 ymm14_3 ymm8_3;
(* vpsllq $0x1,%ymm2,%ymm2                         #! PC = 0x555555577d43 *)
shl ymm2_0 ymm2_0 0x1@uint64;
shl ymm2_1 ymm2_1 0x1@uint64;
shl ymm2_2 ymm2_2 0x1@uint64;
shl ymm2_3 ymm2_3 0x1@uint64;
(* vpxor  %ymm15,%ymm1,%ymm15                      #! PC = 0x555555577d48 *)
xor ymm15_0@uint64 ymm1_0 ymm15_0;
xor ymm15_1@uint64 ymm1_1 ymm15_1;
xor ymm15_2@uint64 ymm1_2 ymm15_2;
xor ymm15_3@uint64 ymm1_3 ymm15_3;
(* vpor   %ymm0,%ymm2,%ymm2                        #! PC = 0x555555577d4d *)
or ymm2_0@uint64 ymm2_0 ymm0_0;
or ymm2_1@uint64 ymm2_1 ymm0_1;
or ymm2_2@uint64 ymm2_2 ymm0_2;
or ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x14,%ymm10,%ymm0                       #! PC = 0x555555577d51 *)
shr ymm0_0 ymm10_0 0x14@uint64;
shr ymm0_1 ymm10_1 0x14@uint64;
shr ymm0_2 ymm10_2 0x14@uint64;
shr ymm0_3 ymm10_3 0x14@uint64;
(* vpxor  %ymm11,%ymm15,%ymm11                     #! PC = 0x555555577d57 *)
xor ymm11_0@uint64 ymm15_0 ymm11_0;
xor ymm11_1@uint64 ymm15_1 ymm11_1;
xor ymm11_2@uint64 ymm15_2 ymm11_2;
xor ymm11_3@uint64 ymm15_3 ymm11_3;
(* vpsllq $0x2c,%ymm10,%ymm10                      #! PC = 0x555555577d5c *)
shl ymm10_0 ymm10_0 0x2c@uint64;
shl ymm10_1 ymm10_1 0x2c@uint64;
shl ymm10_2 ymm10_2 0x2c@uint64;
shl ymm10_3 ymm10_3 0x2c@uint64;
(* vpxor  %ymm12,%ymm2,%ymm12                      #! PC = 0x555555577d62 *)
xor ymm12_0@uint64 ymm2_0 ymm12_0;
xor ymm12_1@uint64 ymm2_1 ymm12_1;
xor ymm12_2@uint64 ymm2_2 ymm12_2;
xor ymm12_3@uint64 ymm2_3 ymm12_3;
(* vpxor  -0x90(%rbp),%ymm5,%ymm2                  #! EA = L0x7fffffffbec0; Value = 0xf204ab825ec1b98c; PC = 0x555555577d67 *)
xor ymm2_0@uint64 ymm5_0 L0x7fffffffbec0;
xor ymm2_1@uint64 ymm5_1 L0x7fffffffbec8;
xor ymm2_2@uint64 ymm5_2 L0x7fffffffbed0;
xor ymm2_3@uint64 ymm5_3 L0x7fffffffbed8;
(* vpor   %ymm0,%ymm10,%ymm10                      #! PC = 0x555555577d6f *)
or ymm10_0@uint64 ymm10_0 ymm0_0;
or ymm10_1@uint64 ymm10_1 ymm0_1;
or ymm10_2@uint64 ymm10_2 ymm0_2;
or ymm10_3@uint64 ymm10_3 ymm0_3;
(* vpsrlq $0x15,%ymm9,%ymm0                        #! PC = 0x555555577d73 *)
shr ymm0_0 ymm9_0 0x15@uint64;
shr ymm0_1 ymm9_1 0x15@uint64;
shr ymm0_2 ymm9_2 0x15@uint64;
shr ymm0_3 ymm9_3 0x15@uint64;
(* vpxor  %ymm4,%ymm12,%ymm4                       #! PC = 0x555555577d79 *)
xor ymm4_0@uint64 ymm12_0 ymm4_0;
xor ymm4_1@uint64 ymm12_1 ymm4_1;
xor ymm4_2@uint64 ymm12_2 ymm4_2;
xor ymm4_3@uint64 ymm12_3 ymm4_3;
(* vpsllq $0x2b,%ymm9,%ymm9                        #! PC = 0x555555577d7d *)
shl ymm9_0 ymm9_0 0x2b@uint64;
shl ymm9_1 ymm9_1 0x2b@uint64;
shl ymm9_2 ymm9_2 0x2b@uint64;
shl ymm9_3 ymm9_3 0x2b@uint64;
(* vpor   %ymm0,%ymm9,%ymm9                        #! PC = 0x555555577d83 *)
or ymm9_0@uint64 ymm9_0 ymm0_0;
or ymm9_1@uint64 ymm9_1 ymm0_1;
or ymm9_2@uint64 ymm9_2 ymm0_2;
or ymm9_3@uint64 ymm9_3 ymm0_3;
(* vmovq  %rcx,%xmm0                               #! PC = 0x555555577d87 *)
mov xmm0_0 rcx;
mov xmm0_1 0@uint64;
(* vpandn %ymm9,%ymm10,%ymm1                       #! PC = 0x555555577d8c *)
not ymm10_0n@uint64 ymm10_0;
and ymm1_0@uint64 ymm10_0n ymm9_0;
not ymm10_1n@uint64 ymm10_1;
and ymm1_1@uint64 ymm10_1n ymm9_1;
not ymm10_2n@uint64 ymm10_2;
and ymm1_2@uint64 ymm10_2n ymm9_2;
not ymm10_3n@uint64 ymm10_3;
and ymm1_3@uint64 ymm10_3n ymm9_3;
(* vpbroadcastq %xmm0,%ymm0                        #! PC = 0x555555577d91 *)
mov ymm0_0 xmm0_0;
mov ymm0_1 xmm0_0;
mov ymm0_2 xmm0_0;
mov ymm0_3 xmm0_0;
(* vpxor  %ymm1,%ymm0,%ymm0                        #! PC = 0x555555577d96 *)
xor ymm0_0@uint64 ymm0_0 ymm1_0;
xor ymm0_1@uint64 ymm0_1 ymm1_1;
xor ymm0_2@uint64 ymm0_2 ymm1_2;
xor ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  %ymm2,%ymm0,%ymm0                        #! PC = 0x555555577d9a *)
xor ymm0_0@uint64 ymm0_0 ymm2_0;
xor ymm0_1@uint64 ymm0_1 ymm2_1;
xor ymm0_2@uint64 ymm0_2 ymm2_2;
xor ymm0_3@uint64 ymm0_3 ymm2_3;
(* vmovdqa %ymm0,-0x90(%rbp)                       #! EA = L0x7fffffffbec0; PC = 0x555555577d9e *)
mov L0x7fffffffbec0 ymm0_0;
mov L0x7fffffffbec8 ymm0_1;
mov L0x7fffffffbed0 ymm0_2;
mov L0x7fffffffbed8 ymm0_3;
(* vpsrlq $0x2b,%ymm11,%ymm0                       #! PC = 0x555555577da6 *)
shr ymm0_0 ymm11_0 0x2b@uint64;
shr ymm0_1 ymm11_1 0x2b@uint64;
shr ymm0_2 ymm11_2 0x2b@uint64;
shr ymm0_3 ymm11_3 0x2b@uint64;
(* vpsllq $0x15,%ymm11,%ymm11                      #! PC = 0x555555577dac *)
shl ymm11_0 ymm11_0 0x15@uint64;
shl ymm11_1 ymm11_1 0x15@uint64;
shl ymm11_2 ymm11_2 0x15@uint64;
shl ymm11_3 ymm11_3 0x15@uint64;
(* vpor   %ymm0,%ymm11,%ymm11                      #! PC = 0x555555577db2 *)
or ymm11_0@uint64 ymm11_0 ymm0_0;
or ymm11_1@uint64 ymm11_1 ymm0_1;
or ymm11_2@uint64 ymm11_2 ymm0_2;
or ymm11_3@uint64 ymm11_3 ymm0_3;
(* vpandn %ymm11,%ymm9,%ymm0                       #! PC = 0x555555577db6 *)
not ymm9_0n@uint64 ymm9_0;
and ymm0_0@uint64 ymm9_0n ymm11_0;
not ymm9_1n@uint64 ymm9_1;
and ymm0_1@uint64 ymm9_1n ymm11_1;
not ymm9_2n@uint64 ymm9_2;
and ymm0_2@uint64 ymm9_2n ymm11_2;
not ymm9_3n@uint64 ymm9_3;
and ymm0_3@uint64 ymm9_3n ymm11_3;
(* vpxor  %ymm10,%ymm0,%ymm0                       #! PC = 0x555555577dbb *)
xor ymm0_0@uint64 ymm0_0 ymm10_0;
xor ymm0_1@uint64 ymm0_1 ymm10_1;
xor ymm0_2@uint64 ymm0_2 ymm10_2;
xor ymm0_3@uint64 ymm0_3 ymm10_3;
(* vpandn %ymm10,%ymm2,%ymm10                      #! PC = 0x555555577dc0 *)
not ymm2_0n@uint64 ymm2_0;
and ymm10_0@uint64 ymm2_0n ymm10_0;
not ymm2_1n@uint64 ymm2_1;
and ymm10_1@uint64 ymm2_1n ymm10_1;
not ymm2_2n@uint64 ymm2_2;
and ymm10_2@uint64 ymm2_2n ymm10_2;
not ymm2_3n@uint64 ymm2_3;
and ymm10_3@uint64 ymm2_3n ymm10_3;
(* vmovdqa %ymm0,-0x2f0(%rbp)                      #! EA = L0x7fffffffbc60; PC = 0x555555577dc5 *)
mov L0x7fffffffbc60 ymm0_0;
mov L0x7fffffffbc68 ymm0_1;
mov L0x7fffffffbc70 ymm0_2;
mov L0x7fffffffbc78 ymm0_3;
(* vpsrlq $0x32,%ymm4,%ymm0                        #! PC = 0x555555577dcd *)
shr ymm0_0 ymm4_0 0x32@uint64;
shr ymm0_1 ymm4_1 0x32@uint64;
shr ymm0_2 ymm4_2 0x32@uint64;
shr ymm0_3 ymm4_3 0x32@uint64;
(* vpsllq $0xe,%ymm4,%ymm4                         #! PC = 0x555555577dd2 *)
shl ymm4_0 ymm4_0 0xe@uint64;
shl ymm4_1 ymm4_1 0xe@uint64;
shl ymm4_2 ymm4_2 0xe@uint64;
shl ymm4_3 ymm4_3 0xe@uint64;
(* vpor   %ymm0,%ymm4,%ymm4                        #! PC = 0x555555577dd7 *)
or ymm4_0@uint64 ymm4_0 ymm0_0;
or ymm4_1@uint64 ymm4_1 ymm0_1;
or ymm4_2@uint64 ymm4_2 ymm0_2;
or ymm4_3@uint64 ymm4_3 ymm0_3;
(* vpandn %ymm4,%ymm11,%ymm0                       #! PC = 0x555555577ddb *)
not ymm11_0n@uint64 ymm11_0;
and ymm0_0@uint64 ymm11_0n ymm4_0;
not ymm11_1n@uint64 ymm11_1;
and ymm0_1@uint64 ymm11_1n ymm4_1;
not ymm11_2n@uint64 ymm11_2;
and ymm0_2@uint64 ymm11_2n ymm4_2;
not ymm11_3n@uint64 ymm11_3;
and ymm0_3@uint64 ymm11_3n ymm4_3;
(* vpxor  %ymm9,%ymm0,%ymm9                        #! PC = 0x555555577ddf *)
xor ymm9_0@uint64 ymm0_0 ymm9_0;
xor ymm9_1@uint64 ymm0_1 ymm9_1;
xor ymm9_2@uint64 ymm0_2 ymm9_2;
xor ymm9_3@uint64 ymm0_3 ymm9_3;
(* vpandn %ymm2,%ymm4,%ymm0                        #! PC = 0x555555577de4 *)
not ymm4_0n@uint64 ymm4_0;
and ymm0_0@uint64 ymm4_0n ymm2_0;
not ymm4_1n@uint64 ymm4_1;
and ymm0_1@uint64 ymm4_1n ymm2_1;
not ymm4_2n@uint64 ymm4_2;
and ymm0_2@uint64 ymm4_2n ymm2_2;
not ymm4_3n@uint64 ymm4_3;
and ymm0_3@uint64 ymm4_3n ymm2_3;
(* vpxor  %ymm11,%ymm0,%ymm11                      #! PC = 0x555555577de8 *)
xor ymm11_0@uint64 ymm0_0 ymm11_0;
xor ymm11_1@uint64 ymm0_1 ymm11_1;
xor ymm11_2@uint64 ymm0_2 ymm11_2;
xor ymm11_3@uint64 ymm0_3 ymm11_3;
(* vmovdqa %ymm9,-0x2d0(%rbp)                      #! EA = L0x7fffffffbc80; PC = 0x555555577ded *)
mov L0x7fffffffbc80 ymm9_0;
mov L0x7fffffffbc88 ymm9_1;
mov L0x7fffffffbc90 ymm9_2;
mov L0x7fffffffbc98 ymm9_3;
(* vmovdqa %ymm11,-0x130(%rbp)                     #! EA = L0x7fffffffbe20; PC = 0x555555577df5 *)
mov L0x7fffffffbe20 ymm11_0;
mov L0x7fffffffbe28 ymm11_1;
mov L0x7fffffffbe30 ymm11_2;
mov L0x7fffffffbe38 ymm11_3;
(* vpxor  %ymm4,%ymm10,%ymm11                      #! PC = 0x555555577dfd *)
xor ymm11_0@uint64 ymm10_0 ymm4_0;
xor ymm11_1@uint64 ymm10_1 ymm4_1;
xor ymm11_2@uint64 ymm10_2 ymm4_2;
xor ymm11_3@uint64 ymm10_3 ymm4_3;
(* vmovdqa %ymm11,-0x2b0(%rbp)                     #! EA = L0x7fffffffbca0; PC = 0x555555577e01 *)
mov L0x7fffffffbca0 ymm11_0;
mov L0x7fffffffbca8 ymm11_1;
mov L0x7fffffffbcb0 ymm11_2;
mov L0x7fffffffbcb8 ymm11_3;
(* vpxor  -0x110(%rbp),%ymm15,%ymm11               #! EA = L0x7fffffffbe40; Value = 0x75da76bd87d37a73; PC = 0x555555577e09 *)
xor ymm11_0@uint64 ymm15_0 L0x7fffffffbe40;
xor ymm11_1@uint64 ymm15_1 L0x7fffffffbe48;
xor ymm11_2@uint64 ymm15_2 L0x7fffffffbe50;
xor ymm11_3@uint64 ymm15_3 L0x7fffffffbe58;
(* vpsrlq $0x24,%ymm11,%ymm0                       #! PC = 0x555555577e11 *)
shr ymm0_0 ymm11_0 0x24@uint64;
shr ymm0_1 ymm11_1 0x24@uint64;
shr ymm0_2 ymm11_2 0x24@uint64;
shr ymm0_3 ymm11_3 0x24@uint64;
(* vpsllq $0x1c,%ymm11,%ymm2                       #! PC = 0x555555577e17 *)
shl ymm2_0 ymm11_0 0x1c@uint64;
shl ymm2_1 ymm11_1 0x1c@uint64;
shl ymm2_2 ymm11_2 0x1c@uint64;
shl ymm2_3 ymm11_3 0x1c@uint64;
(* vpor   %ymm0,%ymm2,%ymm2                        #! PC = 0x555555577e1d *)
or ymm2_0@uint64 ymm2_0 ymm0_0;
or ymm2_1@uint64 ymm2_1 ymm0_1;
or ymm2_2@uint64 ymm2_2 ymm0_2;
or ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpxor  -0xd0(%rbp),%ymm12,%ymm0                 #! EA = L0x7fffffffbe80; Value = 0x8aad4677926b6a65; PC = 0x555555577e21 *)
xor ymm0_0@uint64 ymm12_0 L0x7fffffffbe80;
xor ymm0_1@uint64 ymm12_1 L0x7fffffffbe88;
xor ymm0_2@uint64 ymm12_2 L0x7fffffffbe90;
xor ymm0_3@uint64 ymm12_3 L0x7fffffffbe98;
(* vpsrlq $0x2c,%ymm0,%ymm1                        #! PC = 0x555555577e29 *)
shr ymm1_0 ymm0_0 0x2c@uint64;
shr ymm1_1 ymm0_1 0x2c@uint64;
shr ymm1_2 ymm0_2 0x2c@uint64;
shr ymm1_3 ymm0_3 0x2c@uint64;
(* vpsllq $0x14,%ymm0,%ymm0                        #! PC = 0x555555577e2e *)
shl ymm0_0 ymm0_0 0x14@uint64;
shl ymm0_1 ymm0_1 0x14@uint64;
shl ymm0_2 ymm0_2 0x14@uint64;
shl ymm0_3 ymm0_3 0x14@uint64;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x555555577e33 *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x1d0(%rbp),%ymm5,%ymm1                 #! EA = L0x7fffffffbd80; Value = 0xf42645f1aeb80189; PC = 0x555555577e37 *)
xor ymm1_0@uint64 ymm5_0 L0x7fffffffbd80;
xor ymm1_1@uint64 ymm5_1 L0x7fffffffbd88;
xor ymm1_2@uint64 ymm5_2 L0x7fffffffbd90;
xor ymm1_3@uint64 ymm5_3 L0x7fffffffbd98;
(* vpsrlq $0x3d,%ymm1,%ymm4                        #! PC = 0x555555577e3f *)
shr ymm4_0 ymm1_0 0x3d@uint64;
shr ymm4_1 ymm1_1 0x3d@uint64;
shr ymm4_2 ymm1_2 0x3d@uint64;
shr ymm4_3 ymm1_3 0x3d@uint64;
(* vpsllq $0x3,%ymm1,%ymm1                         #! PC = 0x555555577e44 *)
shl ymm1_0 ymm1_0 0x3@uint64;
shl ymm1_1 ymm1_1 0x3@uint64;
shl ymm1_2 ymm1_2 0x3@uint64;
shl ymm1_3 ymm1_3 0x3@uint64;
(* vpor   %ymm4,%ymm1,%ymm1                        #! PC = 0x555555577e49 *)
or ymm1_0@uint64 ymm1_0 ymm4_0;
or ymm1_1@uint64 ymm1_1 ymm4_1;
or ymm1_2@uint64 ymm1_2 ymm4_2;
or ymm1_3@uint64 ymm1_3 ymm4_3;
(* vpandn %ymm1,%ymm0,%ymm4                        #! PC = 0x555555577e4d *)
not ymm0_0n@uint64 ymm0_0;
and ymm4_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm4_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm4_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm4_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm2,%ymm4,%ymm9                        #! PC = 0x555555577e51 *)
xor ymm9_0@uint64 ymm4_0 ymm2_0;
xor ymm9_1@uint64 ymm4_1 ymm2_1;
xor ymm9_2@uint64 ymm4_2 ymm2_2;
xor ymm9_3@uint64 ymm4_3 ymm2_3;
(* vpsrlq $0x13,%ymm7,%ymm4                        #! PC = 0x555555577e55 *)
shr ymm4_0 ymm7_0 0x13@uint64;
shr ymm4_1 ymm7_1 0x13@uint64;
shr ymm4_2 ymm7_2 0x13@uint64;
shr ymm4_3 ymm7_3 0x13@uint64;
(* vpsllq $0x2d,%ymm7,%ymm7                        #! PC = 0x555555577e5a *)
shl ymm7_0 ymm7_0 0x2d@uint64;
shl ymm7_1 ymm7_1 0x2d@uint64;
shl ymm7_2 ymm7_2 0x2d@uint64;
shl ymm7_3 ymm7_3 0x2d@uint64;
(* vmovdqa %ymm9,-0x290(%rbp)                      #! EA = L0x7fffffffbcc0; PC = 0x555555577e5f *)
mov L0x7fffffffbcc0 ymm9_0;
mov L0x7fffffffbcc8 ymm9_1;
mov L0x7fffffffbcd0 ymm9_2;
mov L0x7fffffffbcd8 ymm9_3;
(* vpor   %ymm4,%ymm7,%ymm13                       #! PC = 0x555555577e67 *)
or ymm13_0@uint64 ymm7_0 ymm4_0;
or ymm13_1@uint64 ymm7_1 ymm4_1;
or ymm13_2@uint64 ymm7_2 ymm4_2;
or ymm13_3@uint64 ymm7_3 ymm4_3;
(* vpsrlq $0x3,%ymm8,%ymm4                         #! PC = 0x555555577e6b *)
shr ymm4_0 ymm8_0 0x3@uint64;
shr ymm4_1 ymm8_1 0x3@uint64;
shr ymm4_2 ymm8_2 0x3@uint64;
shr ymm4_3 ymm8_3 0x3@uint64;
(* vpsllq $0x3d,%ymm8,%ymm8                        #! PC = 0x555555577e71 *)
shl ymm8_0 ymm8_0 0x3d@uint64;
shl ymm8_1 ymm8_1 0x3d@uint64;
shl ymm8_2 ymm8_2 0x3d@uint64;
shl ymm8_3 ymm8_3 0x3d@uint64;
(* vpandn %ymm13,%ymm1,%ymm9                       #! PC = 0x555555577e77 *)
not ymm1_0n@uint64 ymm1_0;
and ymm9_0@uint64 ymm1_0n ymm13_0;
not ymm1_1n@uint64 ymm1_1;
and ymm9_1@uint64 ymm1_1n ymm13_1;
not ymm1_2n@uint64 ymm1_2;
and ymm9_2@uint64 ymm1_2n ymm13_2;
not ymm1_3n@uint64 ymm1_3;
and ymm9_3@uint64 ymm1_3n ymm13_3;
(* vpor   %ymm4,%ymm8,%ymm8                        #! PC = 0x555555577e7c *)
or ymm8_0@uint64 ymm8_0 ymm4_0;
or ymm8_1@uint64 ymm8_1 ymm4_1;
or ymm8_2@uint64 ymm8_2 ymm4_2;
or ymm8_3@uint64 ymm8_3 ymm4_3;
(* vpxor  %ymm0,%ymm9,%ymm9                        #! PC = 0x555555577e80 *)
xor ymm9_0@uint64 ymm9_0 ymm0_0;
xor ymm9_1@uint64 ymm9_1 ymm0_1;
xor ymm9_2@uint64 ymm9_2 ymm0_2;
xor ymm9_3@uint64 ymm9_3 ymm0_3;
(* vpandn %ymm8,%ymm13,%ymm4                       #! PC = 0x555555577e84 *)
not ymm13_0n@uint64 ymm13_0;
and ymm4_0@uint64 ymm13_0n ymm8_0;
not ymm13_1n@uint64 ymm13_1;
and ymm4_1@uint64 ymm13_1n ymm8_1;
not ymm13_2n@uint64 ymm13_2;
and ymm4_2@uint64 ymm13_2n ymm8_2;
not ymm13_3n@uint64 ymm13_3;
and ymm4_3@uint64 ymm13_3n ymm8_3;
(* vpxor  %ymm1,%ymm4,%ymm10                       #! PC = 0x555555577e89 *)
xor ymm10_0@uint64 ymm4_0 ymm1_0;
xor ymm10_1@uint64 ymm4_1 ymm1_1;
xor ymm10_2@uint64 ymm4_2 ymm1_2;
xor ymm10_3@uint64 ymm4_3 ymm1_3;
(* vpandn %ymm2,%ymm8,%ymm1                        #! PC = 0x555555577e8d *)
not ymm8_0n@uint64 ymm8_0;
and ymm1_0@uint64 ymm8_0n ymm2_0;
not ymm8_1n@uint64 ymm8_1;
and ymm1_1@uint64 ymm8_1n ymm2_1;
not ymm8_2n@uint64 ymm8_2;
and ymm1_2@uint64 ymm8_2n ymm2_2;
not ymm8_3n@uint64 ymm8_3;
and ymm1_3@uint64 ymm8_3n ymm2_3;
(* vpandn %ymm0,%ymm2,%ymm2                        #! PC = 0x555555577e91 *)
not ymm2_0n@uint64 ymm2_0;
and ymm2_0@uint64 ymm2_0n ymm0_0;
not ymm2_1n@uint64 ymm2_1;
and ymm2_1@uint64 ymm2_1n ymm0_1;
not ymm2_2n@uint64 ymm2_2;
and ymm2_2@uint64 ymm2_2n ymm0_2;
not ymm2_3n@uint64 ymm2_3;
and ymm2_3@uint64 ymm2_3n ymm0_3;
(* vpxor  %ymm8,%ymm2,%ymm8                        #! PC = 0x555555577e95 *)
xor ymm8_0@uint64 ymm2_0 ymm8_0;
xor ymm8_1@uint64 ymm2_1 ymm8_1;
xor ymm8_2@uint64 ymm2_2 ymm8_2;
xor ymm8_3@uint64 ymm2_3 ymm8_3;
(* vpxor  -0x1b0(%rbp),%ymm3,%ymm2                 #! EA = L0x7fffffffbda0; Value = 0x032132932ed2960e; PC = 0x555555577e9a *)
xor ymm2_0@uint64 ymm3_0 L0x7fffffffbda0;
xor ymm2_1@uint64 ymm3_1 L0x7fffffffbda8;
xor ymm2_2@uint64 ymm3_2 L0x7fffffffbdb0;
xor ymm2_3@uint64 ymm3_3 L0x7fffffffbdb8;
(* vpxor  %ymm13,%ymm1,%ymm7                       #! PC = 0x555555577ea2 *)
xor ymm7_0@uint64 ymm1_0 ymm13_0;
xor ymm7_1@uint64 ymm1_1 ymm13_1;
xor ymm7_2@uint64 ymm1_2 ymm13_2;
xor ymm7_3@uint64 ymm1_3 ymm13_3;
(* vmovdqa %ymm10,-0x1f0(%rbp)                     #! EA = L0x7fffffffbd60; PC = 0x555555577ea7 *)
mov L0x7fffffffbd60 ymm10_0;
mov L0x7fffffffbd68 ymm10_1;
mov L0x7fffffffbd70 ymm10_2;
mov L0x7fffffffbd78 ymm10_3;
(* vmovdqa %ymm8,-0x110(%rbp)                      #! EA = L0x7fffffffbe40; PC = 0x555555577eaf *)
mov L0x7fffffffbe40 ymm8_0;
mov L0x7fffffffbe48 ymm8_1;
mov L0x7fffffffbe50 ymm8_2;
mov L0x7fffffffbe58 ymm8_3;
(* vpxor  -0xb0(%rbp),%ymm15,%ymm8                 #! EA = L0x7fffffffbea0; Value = 0xe5bbddf2601ea85b; PC = 0x555555577eb7 *)
xor ymm8_0@uint64 ymm15_0 L0x7fffffffbea0;
xor ymm8_1@uint64 ymm15_1 L0x7fffffffbea8;
xor ymm8_2@uint64 ymm15_2 L0x7fffffffbeb0;
xor ymm8_3@uint64 ymm15_3 L0x7fffffffbeb8;
(* vpsrlq $0x3f,%ymm2,%ymm1                        #! PC = 0x555555577ebf *)
shr ymm1_0 ymm2_0 0x3f@uint64;
shr ymm1_1 ymm2_1 0x3f@uint64;
shr ymm1_2 ymm2_2 0x3f@uint64;
shr ymm1_3 ymm2_3 0x3f@uint64;
(* vpsllq $0x1,%ymm2,%ymm2                         #! PC = 0x555555577ec4 *)
shl ymm2_0 ymm2_0 0x1@uint64;
shl ymm2_1 ymm2_1 0x1@uint64;
shl ymm2_2 ymm2_2 0x1@uint64;
shl ymm2_3 ymm2_3 0x1@uint64;
(* vmovdqa %ymm7,-0x1d0(%rbp)                      #! EA = L0x7fffffffbd80; PC = 0x555555577ec9 *)
mov L0x7fffffffbd80 ymm7_0;
mov L0x7fffffffbd88 ymm7_1;
mov L0x7fffffffbd90 ymm7_2;
mov L0x7fffffffbd98 ymm7_3;
(* vpor   %ymm1,%ymm2,%ymm0                        #! PC = 0x555555577ed1 *)
or ymm0_0@uint64 ymm2_0 ymm1_0;
or ymm0_1@uint64 ymm2_1 ymm1_1;
or ymm0_2@uint64 ymm2_2 ymm1_2;
or ymm0_3@uint64 ymm2_3 ymm1_3;
(* vpxor  -0xf0(%rbp),%ymm14,%ymm2                 #! EA = L0x7fffffffbe60; Value = 0x83371dd98c8b09f1; PC = 0x555555577ed5 *)
xor ymm2_0@uint64 ymm14_0 L0x7fffffffbe60;
xor ymm2_1@uint64 ymm14_1 L0x7fffffffbe68;
xor ymm2_2@uint64 ymm14_2 L0x7fffffffbe70;
xor ymm2_3@uint64 ymm14_3 L0x7fffffffbe78;
(* vpsrlq $0x27,%ymm8,%ymm4                        #! PC = 0x555555577edd *)
shr ymm4_0 ymm8_0 0x27@uint64;
shr ymm4_1 ymm8_1 0x27@uint64;
shr ymm4_2 ymm8_2 0x27@uint64;
shr ymm4_3 ymm8_3 0x27@uint64;
(* vpsllq $0x19,%ymm8,%ymm8                        #! PC = 0x555555577ee3 *)
shl ymm8_0 ymm8_0 0x19@uint64;
shl ymm8_1 ymm8_1 0x19@uint64;
shl ymm8_2 ymm8_2 0x19@uint64;
shl ymm8_3 ymm8_3 0x19@uint64;
(* vpsrlq $0x3a,%ymm2,%ymm1                        #! PC = 0x555555577ee9 *)
shr ymm1_0 ymm2_0 0x3a@uint64;
shr ymm1_1 ymm2_1 0x3a@uint64;
shr ymm1_2 ymm2_2 0x3a@uint64;
shr ymm1_3 ymm2_3 0x3a@uint64;
(* vpsllq $0x6,%ymm2,%ymm2                         #! PC = 0x555555577eee *)
shl ymm2_0 ymm2_0 0x6@uint64;
shl ymm2_1 ymm2_1 0x6@uint64;
shl ymm2_2 ymm2_2 0x6@uint64;
shl ymm2_3 ymm2_3 0x6@uint64;
(* vpor   %ymm1,%ymm2,%ymm2                        #! PC = 0x555555577ef3 *)
or ymm2_0@uint64 ymm2_0 ymm1_0;
or ymm2_1@uint64 ymm2_1 ymm1_1;
or ymm2_2@uint64 ymm2_2 ymm1_2;
or ymm2_3@uint64 ymm2_3 ymm1_3;
(* vpor   %ymm4,%ymm8,%ymm1                        #! PC = 0x555555577ef7 *)
or ymm1_0@uint64 ymm8_0 ymm4_0;
or ymm1_1@uint64 ymm8_1 ymm4_1;
or ymm1_2@uint64 ymm8_2 ymm4_2;
or ymm1_3@uint64 ymm8_3 ymm4_3;
(* vpandn %ymm1,%ymm2,%ymm4                        #! PC = 0x555555577efb *)
not ymm2_0n@uint64 ymm2_0;
and ymm4_0@uint64 ymm2_0n ymm1_0;
not ymm2_1n@uint64 ymm2_1;
and ymm4_1@uint64 ymm2_1n ymm1_1;
not ymm2_2n@uint64 ymm2_2;
and ymm4_2@uint64 ymm2_2n ymm1_2;
not ymm2_3n@uint64 ymm2_3;
and ymm4_3@uint64 ymm2_3n ymm1_3;
(* vpxor  %ymm0,%ymm4,%ymm8                        #! PC = 0x555555577eff *)
xor ymm8_0@uint64 ymm4_0 ymm0_0;
xor ymm8_1@uint64 ymm4_1 ymm0_1;
xor ymm8_2@uint64 ymm4_2 ymm0_2;
xor ymm8_3@uint64 ymm4_3 ymm0_3;
(* vpxor  -0x50(%rbp),%ymm12,%ymm4                 #! EA = L0x7fffffffbf00; Value = 0x04754b7b9faeff7b; PC = 0x555555577f03 *)
xor ymm4_0@uint64 ymm12_0 L0x7fffffffbf00;
xor ymm4_1@uint64 ymm12_1 L0x7fffffffbf08;
xor ymm4_2@uint64 ymm12_2 L0x7fffffffbf10;
xor ymm4_3@uint64 ymm12_3 L0x7fffffffbf18;
(* vmovdqa %ymm8,-0xd0(%rbp)                       #! EA = L0x7fffffffbe80; PC = 0x555555577f08 *)
mov L0x7fffffffbe80 ymm8_0;
mov L0x7fffffffbe88 ymm8_1;
mov L0x7fffffffbe90 ymm8_2;
mov L0x7fffffffbe98 ymm8_3;
(* vpshufb 0x55fe7(%rip),%ymm4,%ymm4        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x555555577f10 *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm4_0, ymm4_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm4_2, ymm4_3, tmp_2, tmp_3);
mov ymm4_0 tmp_0;
mov ymm4_1 tmp_1;
mov ymm4_2 tmp_2;
mov ymm4_3 tmp_3;
(* vpandn %ymm4,%ymm1,%ymm7                        #! PC = 0x555555577f19 *)
not ymm1_0n@uint64 ymm1_0;
and ymm7_0@uint64 ymm1_0n ymm4_0;
not ymm1_1n@uint64 ymm1_1;
and ymm7_1@uint64 ymm1_1n ymm4_1;
not ymm1_2n@uint64 ymm1_2;
and ymm7_2@uint64 ymm1_2n ymm4_2;
not ymm1_3n@uint64 ymm1_3;
and ymm7_3@uint64 ymm1_3n ymm4_3;
(* vpxor  %ymm2,%ymm7,%ymm13                       #! PC = 0x555555577f1d *)
xor ymm13_0@uint64 ymm7_0 ymm2_0;
xor ymm13_1@uint64 ymm7_1 ymm2_1;
xor ymm13_2@uint64 ymm7_2 ymm2_2;
xor ymm13_3@uint64 ymm7_3 ymm2_3;
(* vpsrlq $0x2e,%ymm6,%ymm7                        #! PC = 0x555555577f21 *)
shr ymm7_0 ymm6_0 0x2e@uint64;
shr ymm7_1 ymm6_1 0x2e@uint64;
shr ymm7_2 ymm6_2 0x2e@uint64;
shr ymm7_3 ymm6_3 0x2e@uint64;
(* vpsllq $0x12,%ymm6,%ymm6                        #! PC = 0x555555577f26 *)
shl ymm6_0 ymm6_0 0x12@uint64;
shl ymm6_1 ymm6_1 0x12@uint64;
shl ymm6_2 ymm6_2 0x12@uint64;
shl ymm6_3 ymm6_3 0x12@uint64;
(* vmovdqa %ymm13,-0x1b0(%rbp)                     #! EA = L0x7fffffffbda0; PC = 0x555555577f2b *)
mov L0x7fffffffbda0 ymm13_0;
mov L0x7fffffffbda8 ymm13_1;
mov L0x7fffffffbdb0 ymm13_2;
mov L0x7fffffffbdb8 ymm13_3;
(* vpor   %ymm7,%ymm6,%ymm11                       #! PC = 0x555555577f33 *)
or ymm11_0@uint64 ymm6_0 ymm7_0;
or ymm11_1@uint64 ymm6_1 ymm7_1;
or ymm11_2@uint64 ymm6_2 ymm7_2;
or ymm11_3@uint64 ymm6_3 ymm7_3;
(* vpandn %ymm11,%ymm4,%ymm8                       #! PC = 0x555555577f37 *)
not ymm4_0n@uint64 ymm4_0;
and ymm8_0@uint64 ymm4_0n ymm11_0;
not ymm4_1n@uint64 ymm4_1;
and ymm8_1@uint64 ymm4_1n ymm11_1;
not ymm4_2n@uint64 ymm4_2;
and ymm8_2@uint64 ymm4_2n ymm11_2;
not ymm4_3n@uint64 ymm4_3;
and ymm8_3@uint64 ymm4_3n ymm11_3;
(* vpxor  %ymm1,%ymm8,%ymm8                        #! PC = 0x555555577f3c *)
xor ymm8_0@uint64 ymm8_0 ymm1_0;
xor ymm8_1@uint64 ymm8_1 ymm1_1;
xor ymm8_2@uint64 ymm8_2 ymm1_2;
xor ymm8_3@uint64 ymm8_3 ymm1_3;
(* vpandn %ymm0,%ymm11,%ymm1                       #! PC = 0x555555577f40 *)
not ymm11_0n@uint64 ymm11_0;
and ymm1_0@uint64 ymm11_0n ymm0_0;
not ymm11_1n@uint64 ymm11_1;
and ymm1_1@uint64 ymm11_1n ymm0_1;
not ymm11_2n@uint64 ymm11_2;
and ymm1_2@uint64 ymm11_2n ymm0_2;
not ymm11_3n@uint64 ymm11_3;
and ymm1_3@uint64 ymm11_3n ymm0_3;
(* vpandn %ymm2,%ymm0,%ymm0                        #! PC = 0x555555577f44 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm2_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm2_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm2_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm2_3;
(* vpxor  %ymm11,%ymm0,%ymm2                       #! PC = 0x555555577f48 *)
xor ymm2_0@uint64 ymm0_0 ymm11_0;
xor ymm2_1@uint64 ymm0_1 ymm11_1;
xor ymm2_2@uint64 ymm0_2 ymm11_2;
xor ymm2_3@uint64 ymm0_3 ymm11_3;
(* vpxor  %ymm4,%ymm1,%ymm4                        #! PC = 0x555555577f4d *)
xor ymm4_0@uint64 ymm1_0 ymm4_0;
xor ymm4_1@uint64 ymm1_1 ymm4_1;
xor ymm4_2@uint64 ymm1_2 ymm4_2;
xor ymm4_3@uint64 ymm1_3 ymm4_3;
(* vmovdqa %ymm4,-0xb0(%rbp)                       #! EA = L0x7fffffffbea0; PC = 0x555555577f51 *)
mov L0x7fffffffbea0 ymm4_0;
mov L0x7fffffffbea8 ymm4_1;
mov L0x7fffffffbeb0 ymm4_2;
mov L0x7fffffffbeb8 ymm4_3;
(* vmovdqa %ymm2,-0xf0(%rbp)                       #! EA = L0x7fffffffbe60; PC = 0x555555577f59 *)
mov L0x7fffffffbe60 ymm2_0;
mov L0x7fffffffbe68 ymm2_1;
mov L0x7fffffffbe70 ymm2_2;
mov L0x7fffffffbe78 ymm2_3;
(* vpxor  -0x230(%rbp),%ymm12,%ymm10               #! EA = L0x7fffffffbd20; Value = 0x8c6c05a408afc4ba; PC = 0x555555577f61 *)
xor ymm10_0@uint64 ymm12_0 L0x7fffffffbd20;
xor ymm10_1@uint64 ymm12_1 L0x7fffffffbd28;
xor ymm10_2@uint64 ymm12_2 L0x7fffffffbd30;
xor ymm10_3@uint64 ymm12_3 L0x7fffffffbd38;
(* vpxor  -0x210(%rbp),%ymm5,%ymm0                 #! EA = L0x7fffffffbd40; Value = 0xae399b7b6c84633d; PC = 0x555555577f69 *)
xor ymm0_0@uint64 ymm5_0 L0x7fffffffbd40;
xor ymm0_1@uint64 ymm5_1 L0x7fffffffbd48;
xor ymm0_2@uint64 ymm5_2 L0x7fffffffbd50;
xor ymm0_3@uint64 ymm5_3 L0x7fffffffbd58;
(* vpxor  -0x170(%rbp),%ymm3,%ymm7                 #! EA = L0x7fffffffbde0; Value = 0x41ff2cbc6c803fdd; PC = 0x555555577f71 *)
xor ymm7_0@uint64 ymm3_0 L0x7fffffffbde0;
xor ymm7_1@uint64 ymm3_1 L0x7fffffffbde8;
xor ymm7_2@uint64 ymm3_2 L0x7fffffffbdf0;
xor ymm7_3@uint64 ymm3_3 L0x7fffffffbdf8;
(* vpxor  -0x270(%rbp),%ymm15,%ymm6                #! EA = L0x7fffffffbce0; Value = 0x1ea140c98ab9faef; PC = 0x555555577f79 *)
xor ymm6_0@uint64 ymm15_0 L0x7fffffffbce0;
xor ymm6_1@uint64 ymm15_1 L0x7fffffffbce8;
xor ymm6_2@uint64 ymm15_2 L0x7fffffffbcf0;
xor ymm6_3@uint64 ymm15_3 L0x7fffffffbcf8;
(* vpsrlq $0x25,%ymm10,%ymm2                       #! PC = 0x555555577f81 *)
shr ymm2_0 ymm10_0 0x25@uint64;
shr ymm2_1 ymm10_1 0x25@uint64;
shr ymm2_2 ymm10_2 0x25@uint64;
shr ymm2_3 ymm10_3 0x25@uint64;
(* vpsllq $0x1b,%ymm10,%ymm1                       #! PC = 0x555555577f87 *)
shl ymm1_0 ymm10_0 0x1b@uint64;
shl ymm1_1 ymm10_1 0x1b@uint64;
shl ymm1_2 ymm10_2 0x1b@uint64;
shl ymm1_3 ymm10_3 0x1b@uint64;
(* vpxor  -0x190(%rbp),%ymm15,%ymm15               #! EA = L0x7fffffffbdc0; Value = 0x7cf0f6ddaf592ee4; PC = 0x555555577f8d *)
xor ymm15_0@uint64 ymm15_0 L0x7fffffffbdc0;
xor ymm15_1@uint64 ymm15_1 L0x7fffffffbdc8;
xor ymm15_2@uint64 ymm15_2 L0x7fffffffbdd0;
xor ymm15_3@uint64 ymm15_3 L0x7fffffffbdd8;
(* vmovdqa -0xd0(%rbp),%ymm13                      #! EA = L0x7fffffffbe80; Value = 0x03fb7294d1438d9b; PC = 0x555555577f95 *)
mov ymm13_0 L0x7fffffffbe80;
mov ymm13_1 L0x7fffffffbe88;
mov ymm13_2 L0x7fffffffbe90;
mov ymm13_3 L0x7fffffffbe98;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x555555577f9d *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpsrlq $0x1c,%ymm0,%ymm2                        #! PC = 0x555555577fa1 *)
shr ymm2_0 ymm0_0 0x1c@uint64;
shr ymm2_1 ymm0_1 0x1c@uint64;
shr ymm2_2 ymm0_2 0x1c@uint64;
shr ymm2_3 ymm0_3 0x1c@uint64;
(* vpshufb 0x55f31(%rip),%ymm6,%ymm6        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x555555577fa6 *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm6_0, ymm6_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm6_2, ymm6_3, tmp_2, tmp_3);
mov ymm6_0 tmp_0;
mov ymm6_1 tmp_1;
mov ymm6_2 tmp_2;
mov ymm6_3 tmp_3;
(* vpsllq $0x24,%ymm0,%ymm0                        #! PC = 0x555555577faf *)
shl ymm0_0 ymm0_0 0x24@uint64;
shl ymm0_1 ymm0_1 0x24@uint64;
shl ymm0_2 ymm0_2 0x24@uint64;
shl ymm0_3 ymm0_3 0x24@uint64;
(* vpxor  -0x150(%rbp),%ymm12,%ymm12               #! EA = L0x7fffffffbe00; Value = 0x77129c370664d096; PC = 0x555555577fb4 *)
xor ymm12_0@uint64 ymm12_0 L0x7fffffffbe00;
xor ymm12_1@uint64 ymm12_1 L0x7fffffffbe08;
xor ymm12_2@uint64 ymm12_2 L0x7fffffffbe10;
xor ymm12_3@uint64 ymm12_3 L0x7fffffffbe18;
(* vpxor  -0x350(%rbp),%ymm5,%ymm5                 #! EA = L0x7fffffffbc00; Value = 0x38f5de3ed1cbf84f; PC = 0x555555577fbc *)
xor ymm5_0@uint64 ymm5_0 L0x7fffffffbc00;
xor ymm5_1@uint64 ymm5_1 L0x7fffffffbc08;
xor ymm5_2@uint64 ymm5_2 L0x7fffffffbc10;
xor ymm5_3@uint64 ymm5_3 L0x7fffffffbc18;
(* vpor   %ymm2,%ymm0,%ymm0                        #! PC = 0x555555577fc4 *)
or ymm0_0@uint64 ymm0_0 ymm2_0;
or ymm0_1@uint64 ymm0_1 ymm2_1;
or ymm0_2@uint64 ymm0_2 ymm2_2;
or ymm0_3@uint64 ymm0_3 ymm2_3;
(* vpsrlq $0x36,%ymm7,%ymm2                        #! PC = 0x555555577fc8 *)
shr ymm2_0 ymm7_0 0x36@uint64;
shr ymm2_1 ymm7_1 0x36@uint64;
shr ymm2_2 ymm7_2 0x36@uint64;
shr ymm2_3 ymm7_3 0x36@uint64;
(* vpxor  -0x310(%rbp),%ymm3,%ymm3                 #! EA = L0x7fffffffbc40; Value = 0x31b09e98df7db202; PC = 0x555555577fcd *)
xor ymm3_0@uint64 ymm3_0 L0x7fffffffbc40;
xor ymm3_1@uint64 ymm3_1 L0x7fffffffbc48;
xor ymm3_2@uint64 ymm3_2 L0x7fffffffbc50;
xor ymm3_3@uint64 ymm3_3 L0x7fffffffbc58;
(* vpsllq $0xa,%ymm7,%ymm7                         #! PC = 0x555555577fd5 *)
shl ymm7_0 ymm7_0 0xa@uint64;
shl ymm7_1 ymm7_1 0xa@uint64;
shl ymm7_2 ymm7_2 0xa@uint64;
shl ymm7_3 ymm7_3 0xa@uint64;
(* vpor   %ymm2,%ymm7,%ymm4                        #! PC = 0x555555577fda *)
or ymm4_0@uint64 ymm7_0 ymm2_0;
or ymm4_1@uint64 ymm7_1 ymm2_1;
or ymm4_2@uint64 ymm7_2 ymm2_2;
or ymm4_3@uint64 ymm7_3 ymm2_3;
(* vpandn %ymm4,%ymm0,%ymm2                        #! PC = 0x555555577fde *)
not ymm0_0n@uint64 ymm0_0;
and ymm2_0@uint64 ymm0_0n ymm4_0;
not ymm0_1n@uint64 ymm0_1;
and ymm2_1@uint64 ymm0_1n ymm4_1;
not ymm0_2n@uint64 ymm0_2;
and ymm2_2@uint64 ymm0_2n ymm4_2;
not ymm0_3n@uint64 ymm0_3;
and ymm2_3@uint64 ymm0_3n ymm4_3;
(* vpxor  %ymm1,%ymm2,%ymm7                        #! PC = 0x555555577fe2 *)
xor ymm7_0@uint64 ymm2_0 ymm1_0;
xor ymm7_1@uint64 ymm2_1 ymm1_1;
xor ymm7_2@uint64 ymm2_2 ymm1_2;
xor ymm7_3@uint64 ymm2_3 ymm1_3;
(* vpxor  -0x70(%rbp),%ymm14,%ymm2                 #! EA = L0x7fffffffbee0; Value = 0xde74aeb20a95db03; PC = 0x555555577fe6 *)
xor ymm2_0@uint64 ymm14_0 L0x7fffffffbee0;
xor ymm2_1@uint64 ymm14_1 L0x7fffffffbee8;
xor ymm2_2@uint64 ymm14_2 L0x7fffffffbef0;
xor ymm2_3@uint64 ymm14_3 L0x7fffffffbef8;
(* vpxor  -0x250(%rbp),%ymm14,%ymm14               #! EA = L0x7fffffffbd00; Value = 0xd2e894a105ce76b0; PC = 0x555555577feb *)
xor ymm14_0@uint64 ymm14_0 L0x7fffffffbd00;
xor ymm14_1@uint64 ymm14_1 L0x7fffffffbd08;
xor ymm14_2@uint64 ymm14_2 L0x7fffffffbd10;
xor ymm14_3@uint64 ymm14_3 L0x7fffffffbd18;
(* vmovdqa %ymm7,%ymm11                            #! PC = 0x555555577ff3 *)
mov ymm11_0 ymm7_0;
mov ymm11_1 ymm7_1;
mov ymm11_2 ymm7_2;
mov ymm11_3 ymm7_3;
(* vpsrlq $0x31,%ymm2,%ymm7                        #! PC = 0x555555577ff7 *)
shr ymm7_0 ymm2_0 0x31@uint64;
shr ymm7_1 ymm2_1 0x31@uint64;
shr ymm7_2 ymm2_2 0x31@uint64;
shr ymm7_3 ymm2_3 0x31@uint64;
(* vpsllq $0xf,%ymm2,%ymm2                         #! PC = 0x555555577ffc *)
shl ymm2_0 ymm2_0 0xf@uint64;
shl ymm2_1 ymm2_1 0xf@uint64;
shl ymm2_2 ymm2_2 0xf@uint64;
shl ymm2_3 ymm2_3 0xf@uint64;
(* vmovdqa %ymm11,-0x330(%rbp)                     #! EA = L0x7fffffffbc20; PC = 0x555555578001 *)
mov L0x7fffffffbc20 ymm11_0;
mov L0x7fffffffbc28 ymm11_1;
mov L0x7fffffffbc30 ymm11_2;
mov L0x7fffffffbc38 ymm11_3;
(* vpor   %ymm7,%ymm2,%ymm2                        #! PC = 0x555555578009 *)
or ymm2_0@uint64 ymm2_0 ymm7_0;
or ymm2_1@uint64 ymm2_1 ymm7_1;
or ymm2_2@uint64 ymm2_2 ymm7_2;
or ymm2_3@uint64 ymm2_3 ymm7_3;
(* vpandn %ymm6,%ymm2,%ymm10                       #! PC = 0x55555557800d *)
not ymm2_0n@uint64 ymm2_0;
and ymm10_0@uint64 ymm2_0n ymm6_0;
not ymm2_1n@uint64 ymm2_1;
and ymm10_1@uint64 ymm2_1n ymm6_1;
not ymm2_2n@uint64 ymm2_2;
and ymm10_2@uint64 ymm2_2n ymm6_2;
not ymm2_3n@uint64 ymm2_3;
and ymm10_3@uint64 ymm2_3n ymm6_3;
(* vpandn %ymm2,%ymm4,%ymm7                        #! PC = 0x555555578011 *)
not ymm4_0n@uint64 ymm4_0;
and ymm7_0@uint64 ymm4_0n ymm2_0;
not ymm4_1n@uint64 ymm4_1;
and ymm7_1@uint64 ymm4_1n ymm2_1;
not ymm4_2n@uint64 ymm4_2;
and ymm7_2@uint64 ymm4_2n ymm2_2;
not ymm4_3n@uint64 ymm4_3;
and ymm7_3@uint64 ymm4_3n ymm2_3;
(* vpxor  %ymm4,%ymm10,%ymm10                      #! PC = 0x555555578015 *)
xor ymm10_0@uint64 ymm10_0 ymm4_0;
xor ymm10_1@uint64 ymm10_1 ymm4_1;
xor ymm10_2@uint64 ymm10_2 ymm4_2;
xor ymm10_3@uint64 ymm10_3 ymm4_3;
(* vpxor  %ymm0,%ymm7,%ymm7                        #! PC = 0x555555578019 *)
xor ymm7_0@uint64 ymm7_0 ymm0_0;
xor ymm7_1@uint64 ymm7_1 ymm0_1;
xor ymm7_2@uint64 ymm7_2 ymm0_2;
xor ymm7_3@uint64 ymm7_3 ymm0_3;
(* vmovdqa %ymm10,-0x70(%rbp)                      #! EA = L0x7fffffffbee0; PC = 0x55555557801d *)
mov L0x7fffffffbee0 ymm10_0;
mov L0x7fffffffbee8 ymm10_1;
mov L0x7fffffffbef0 ymm10_2;
mov L0x7fffffffbef8 ymm10_3;
(* vpandn %ymm1,%ymm6,%ymm10                       #! PC = 0x555555578022 *)
not ymm6_0n@uint64 ymm6_0;
and ymm10_0@uint64 ymm6_0n ymm1_0;
not ymm6_1n@uint64 ymm6_1;
and ymm10_1@uint64 ymm6_1n ymm1_1;
not ymm6_2n@uint64 ymm6_2;
and ymm10_2@uint64 ymm6_2n ymm1_2;
not ymm6_3n@uint64 ymm6_3;
and ymm10_3@uint64 ymm6_3n ymm1_3;
(* vpandn %ymm0,%ymm1,%ymm1                        #! PC = 0x555555578026 *)
not ymm1_0n@uint64 ymm1_0;
and ymm1_0@uint64 ymm1_0n ymm0_0;
not ymm1_1n@uint64 ymm1_1;
and ymm1_1@uint64 ymm1_1n ymm0_1;
not ymm1_2n@uint64 ymm1_2;
and ymm1_2@uint64 ymm1_2n ymm0_2;
not ymm1_3n@uint64 ymm1_3;
and ymm1_3@uint64 ymm1_3n ymm0_3;
(* vpxor  %ymm2,%ymm10,%ymm10                      #! PC = 0x55555557802a *)
xor ymm10_0@uint64 ymm10_0 ymm2_0;
xor ymm10_1@uint64 ymm10_1 ymm2_1;
xor ymm10_2@uint64 ymm10_2 ymm2_2;
xor ymm10_3@uint64 ymm10_3 ymm2_3;
(* vpsrlq $0x9,%ymm15,%ymm0                        #! PC = 0x55555557802e *)
shr ymm0_0 ymm15_0 0x9@uint64;
shr ymm0_1 ymm15_1 0x9@uint64;
shr ymm0_2 ymm15_2 0x9@uint64;
shr ymm0_3 ymm15_3 0x9@uint64;
(* vpxor  %ymm6,%ymm1,%ymm2                        #! PC = 0x555555578034 *)
xor ymm2_0@uint64 ymm1_0 ymm6_0;
xor ymm2_1@uint64 ymm1_1 ymm6_1;
xor ymm2_2@uint64 ymm1_2 ymm6_2;
xor ymm2_3@uint64 ymm1_3 ymm6_3;
(* vpsllq $0x37,%ymm15,%ymm15                      #! PC = 0x555555578038 *)
shl ymm15_0 ymm15_0 0x37@uint64;
shl ymm15_1 ymm15_1 0x37@uint64;
shl ymm15_2 ymm15_2 0x37@uint64;
shl ymm15_3 ymm15_3 0x37@uint64;
(* vpsrlq $0x19,%ymm12,%ymm6                       #! PC = 0x55555557803e *)
shr ymm6_0 ymm12_0 0x19@uint64;
shr ymm6_1 ymm12_1 0x19@uint64;
shr ymm6_2 ymm12_2 0x19@uint64;
shr ymm6_3 ymm12_3 0x19@uint64;
(* vmovdqa %ymm2,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x555555578044 *)
mov L0x7fffffffbf00 ymm2_0;
mov L0x7fffffffbf08 ymm2_1;
mov L0x7fffffffbf10 ymm2_2;
mov L0x7fffffffbf18 ymm2_3;
(* vpsllq $0x27,%ymm12,%ymm12                      #! PC = 0x555555578049 *)
shl ymm12_0 ymm12_0 0x27@uint64;
shl ymm12_1 ymm12_1 0x27@uint64;
shl ymm12_2 ymm12_2 0x27@uint64;
shl ymm12_3 ymm12_3 0x27@uint64;
(* vpsrlq $0x2,%ymm14,%ymm1                        #! PC = 0x55555557804f *)
shr ymm1_0 ymm14_0 0x2@uint64;
shr ymm1_1 ymm14_1 0x2@uint64;
shr ymm1_2 ymm14_2 0x2@uint64;
shr ymm1_3 ymm14_3 0x2@uint64;
(* vpor   %ymm0,%ymm15,%ymm15                      #! PC = 0x555555578055 *)
or ymm15_0@uint64 ymm15_0 ymm0_0;
or ymm15_1@uint64 ymm15_1 ymm0_1;
or ymm15_2@uint64 ymm15_2 ymm0_2;
or ymm15_3@uint64 ymm15_3 ymm0_3;
(* vpor   %ymm6,%ymm12,%ymm12                      #! PC = 0x555555578059 *)
or ymm12_0@uint64 ymm12_0 ymm6_0;
or ymm12_1@uint64 ymm12_1 ymm6_1;
or ymm12_2@uint64 ymm12_2 ymm6_2;
or ymm12_3@uint64 ymm12_3 ymm6_3;
(* vpsllq $0x3e,%ymm14,%ymm14                      #! PC = 0x55555557805d *)
shl ymm14_0 ymm14_0 0x3e@uint64;
shl ymm14_1 ymm14_1 0x3e@uint64;
shl ymm14_2 ymm14_2 0x3e@uint64;
shl ymm14_3 ymm14_3 0x3e@uint64;
(* vpxor  -0x290(%rbp),%ymm13,%ymm0                #! EA = L0x7fffffffbcc0; Value = 0xd2ea3cac414eb63b; PC = 0x555555578063 *)
xor ymm0_0@uint64 ymm13_0 L0x7fffffffbcc0;
xor ymm0_1@uint64 ymm13_1 L0x7fffffffbcc8;
xor ymm0_2@uint64 ymm13_2 L0x7fffffffbcd0;
xor ymm0_3@uint64 ymm13_3 L0x7fffffffbcd8;
(* vpor   %ymm1,%ymm14,%ymm4                       #! PC = 0x55555557806b *)
or ymm4_0@uint64 ymm14_0 ymm1_0;
or ymm4_1@uint64 ymm14_1 ymm1_1;
or ymm4_2@uint64 ymm14_2 ymm1_2;
or ymm4_3@uint64 ymm14_3 ymm1_3;
(* vpandn %ymm12,%ymm15,%ymm6                      #! PC = 0x55555557806f *)
not ymm15_0n@uint64 ymm15_0;
and ymm6_0@uint64 ymm15_0n ymm12_0;
not ymm15_1n@uint64 ymm15_1;
and ymm6_1@uint64 ymm15_1n ymm12_1;
not ymm15_2n@uint64 ymm15_2;
and ymm6_2@uint64 ymm15_2n ymm12_2;
not ymm15_3n@uint64 ymm15_3;
and ymm6_3@uint64 ymm15_3n ymm12_3;
(* vpxor  -0x70(%rbp),%ymm8,%ymm13                 #! EA = L0x7fffffffbee0; Value = 0x46230e694915f13e; PC = 0x555555578074 *)
xor ymm13_0@uint64 ymm8_0 L0x7fffffffbee0;
xor ymm13_1@uint64 ymm8_1 L0x7fffffffbee8;
xor ymm13_2@uint64 ymm8_2 L0x7fffffffbef0;
xor ymm13_3@uint64 ymm8_3 L0x7fffffffbef8;
(* vpxor  %ymm4,%ymm6,%ymm6                        #! PC = 0x555555578079 *)
xor ymm6_0@uint64 ymm6_0 ymm4_0;
xor ymm6_1@uint64 ymm6_1 ymm4_1;
xor ymm6_2@uint64 ymm6_2 ymm4_2;
xor ymm6_3@uint64 ymm6_3 ymm4_3;
(* vpxor  -0x1b0(%rbp),%ymm7,%ymm14                #! EA = L0x7fffffffbda0; Value = 0x40b5bb60943d81fe; PC = 0x55555557807d *)
xor ymm14_0@uint64 ymm7_0 L0x7fffffffbda0;
xor ymm14_1@uint64 ymm7_1 L0x7fffffffbda8;
xor ymm14_2@uint64 ymm7_2 L0x7fffffffbdb0;
xor ymm14_3@uint64 ymm7_3 L0x7fffffffbdb8;
(* vpxor  %ymm11,%ymm6,%ymm2                       #! PC = 0x555555578085 *)
xor ymm2_0@uint64 ymm6_0 ymm11_0;
xor ymm2_1@uint64 ymm6_1 ymm11_1;
xor ymm2_2@uint64 ymm6_2 ymm11_2;
xor ymm2_3@uint64 ymm6_3 ymm11_3;
(* vpxor  %ymm0,%ymm2,%ymm2                        #! PC = 0x55555557808a *)
xor ymm2_0@uint64 ymm2_0 ymm0_0;
xor ymm2_1@uint64 ymm2_1 ymm0_1;
xor ymm2_2@uint64 ymm2_2 ymm0_2;
xor ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x17,%ymm5,%ymm0                        #! PC = 0x55555557808e *)
shr ymm0_0 ymm5_0 0x17@uint64;
shr ymm0_1 ymm5_1 0x17@uint64;
shr ymm0_2 ymm5_2 0x17@uint64;
shr ymm0_3 ymm5_3 0x17@uint64;
(* vpxor  -0x90(%rbp),%ymm2,%ymm2                  #! EA = L0x7fffffffbec0; Value = 0x06ee364e771b6bcf; PC = 0x555555578093 *)
xor ymm2_0@uint64 ymm2_0 L0x7fffffffbec0;
xor ymm2_1@uint64 ymm2_1 L0x7fffffffbec8;
xor ymm2_2@uint64 ymm2_2 L0x7fffffffbed0;
xor ymm2_3@uint64 ymm2_3 L0x7fffffffbed8;
(* vpsllq $0x29,%ymm5,%ymm5                        #! PC = 0x55555557809b *)
shl ymm5_0 ymm5_0 0x29@uint64;
shl ymm5_1 ymm5_1 0x29@uint64;
shl ymm5_2 ymm5_2 0x29@uint64;
shl ymm5_3 ymm5_3 0x29@uint64;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x5555555780a0 *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpandn %ymm5,%ymm12,%ymm0                       #! PC = 0x5555555780a4 *)
not ymm12_0n@uint64 ymm12_0;
and ymm0_0@uint64 ymm12_0n ymm5_0;
not ymm12_1n@uint64 ymm12_1;
and ymm0_1@uint64 ymm12_1n ymm5_1;
not ymm12_2n@uint64 ymm12_2;
and ymm0_2@uint64 ymm12_2n ymm5_2;
not ymm12_3n@uint64 ymm12_3;
and ymm0_3@uint64 ymm12_3n ymm5_3;
(* vpxor  %ymm15,%ymm0,%ymm1                       #! PC = 0x5555555780a8 *)
xor ymm1_0@uint64 ymm0_0 ymm15_0;
xor ymm1_1@uint64 ymm0_1 ymm15_1;
xor ymm1_2@uint64 ymm0_2 ymm15_2;
xor ymm1_3@uint64 ymm0_3 ymm15_3;
(* vpxor  -0x2f0(%rbp),%ymm9,%ymm0                 #! EA = L0x7fffffffbc60; Value = 0xe658feaa402f4bd5; PC = 0x5555555780ad *)
xor ymm0_0@uint64 ymm9_0 L0x7fffffffbc60;
xor ymm0_1@uint64 ymm9_1 L0x7fffffffbc68;
xor ymm0_2@uint64 ymm9_2 L0x7fffffffbc70;
xor ymm0_3@uint64 ymm9_3 L0x7fffffffbc78;
(* vmovdqa %ymm1,-0x350(%rbp)                      #! EA = L0x7fffffffbc00; PC = 0x5555555780b5 *)
mov L0x7fffffffbc00 ymm1_0;
mov L0x7fffffffbc08 ymm1_1;
mov L0x7fffffffbc10 ymm1_2;
mov L0x7fffffffbc18 ymm1_3;
(* vpxor  %ymm0,%ymm14,%ymm14                      #! PC = 0x5555555780bd *)
xor ymm14_0@uint64 ymm14_0 ymm0_0;
xor ymm14_1@uint64 ymm14_1 ymm0_1;
xor ymm14_2@uint64 ymm14_2 ymm0_2;
xor ymm14_3@uint64 ymm14_3 ymm0_3;
(* vpxor  %ymm1,%ymm14,%ymm14                      #! PC = 0x5555555780c1 *)
xor ymm14_0@uint64 ymm14_0 ymm1_0;
xor ymm14_1@uint64 ymm14_1 ymm1_1;
xor ymm14_2@uint64 ymm14_2 ymm1_2;
xor ymm14_3@uint64 ymm14_3 ymm1_3;
(* vpsrlq $0x3e,%ymm3,%ymm1                        #! PC = 0x5555555780c5 *)
shr ymm1_0 ymm3_0 0x3e@uint64;
shr ymm1_1 ymm3_1 0x3e@uint64;
shr ymm1_2 ymm3_2 0x3e@uint64;
shr ymm1_3 ymm3_3 0x3e@uint64;
(* vpsllq $0x2,%ymm3,%ymm3                         #! PC = 0x5555555780ca *)
shl ymm3_0 ymm3_0 0x2@uint64;
shl ymm3_1 ymm3_1 0x2@uint64;
shl ymm3_2 ymm3_2 0x2@uint64;
shl ymm3_3 ymm3_3 0x2@uint64;
(* vpor   %ymm1,%ymm3,%ymm0                        #! PC = 0x5555555780cf *)
or ymm0_0@uint64 ymm3_0 ymm1_0;
or ymm0_1@uint64 ymm3_1 ymm1_1;
or ymm0_2@uint64 ymm3_2 ymm1_2;
or ymm0_3@uint64 ymm3_3 ymm1_3;
(* vmovdqa -0x1d0(%rbp),%ymm3                      #! EA = L0x7fffffffbd80; Value = 0xc9e47ebe6f50efb2; PC = 0x5555555780d3 *)
mov ymm3_0 L0x7fffffffbd80;
mov ymm3_1 L0x7fffffffbd88;
mov ymm3_2 L0x7fffffffbd90;
mov ymm3_3 L0x7fffffffbd98;
(* vpandn %ymm0,%ymm5,%ymm1                        #! PC = 0x5555555780db *)
not ymm5_0n@uint64 ymm5_0;
and ymm1_0@uint64 ymm5_0n ymm0_0;
not ymm5_1n@uint64 ymm5_1;
and ymm1_1@uint64 ymm5_1n ymm0_1;
not ymm5_2n@uint64 ymm5_2;
and ymm1_2@uint64 ymm5_2n ymm0_2;
not ymm5_3n@uint64 ymm5_3;
and ymm1_3@uint64 ymm5_3n ymm0_3;
(* vpxor  %ymm12,%ymm1,%ymm12                      #! PC = 0x5555555780df *)
xor ymm12_0@uint64 ymm1_0 ymm12_0;
xor ymm12_1@uint64 ymm1_1 ymm12_1;
xor ymm12_2@uint64 ymm1_2 ymm12_2;
xor ymm12_3@uint64 ymm1_3 ymm12_3;
(* vmovdqa -0x1f0(%rbp),%ymm1                      #! EA = L0x7fffffffbd60; Value = 0xf666c3286f380b1d; PC = 0x5555555780e4 *)
mov ymm1_0 L0x7fffffffbd60;
mov ymm1_1 L0x7fffffffbd68;
mov ymm1_2 L0x7fffffffbd70;
mov ymm1_3 L0x7fffffffbd78;
(* vpxor  -0x2d0(%rbp),%ymm1,%ymm1                 #! EA = L0x7fffffffbc80; Value = 0xd9c51e50969aab69; PC = 0x5555555780ec *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbc80;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbc88;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbc90;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbc98;
(* vpxor  %ymm1,%ymm13,%ymm13                      #! PC = 0x5555555780f4 *)
xor ymm13_0@uint64 ymm13_0 ymm1_0;
xor ymm13_1@uint64 ymm13_1 ymm1_1;
xor ymm13_2@uint64 ymm13_2 ymm1_2;
xor ymm13_3@uint64 ymm13_3 ymm1_3;
(* vpandn %ymm4,%ymm0,%ymm1                        #! PC = 0x5555555780f8 *)
not ymm0_0n@uint64 ymm0_0;
and ymm1_0@uint64 ymm0_0n ymm4_0;
not ymm0_1n@uint64 ymm0_1;
and ymm1_1@uint64 ymm0_1n ymm4_1;
not ymm0_2n@uint64 ymm0_2;
and ymm1_2@uint64 ymm0_2n ymm4_2;
not ymm0_3n@uint64 ymm0_3;
and ymm1_3@uint64 ymm0_3n ymm4_3;
(* vpxor  %ymm5,%ymm1,%ymm5                        #! PC = 0x5555555780fc *)
xor ymm5_0@uint64 ymm1_0 ymm5_0;
xor ymm5_1@uint64 ymm1_1 ymm5_1;
xor ymm5_2@uint64 ymm1_2 ymm5_2;
xor ymm5_3@uint64 ymm1_3 ymm5_3;
(* vpxor  -0x130(%rbp),%ymm3,%ymm1                 #! EA = L0x7fffffffbe20; Value = 0x9473996fe1bb4ceb; PC = 0x555555578100 *)
xor ymm1_0@uint64 ymm3_0 L0x7fffffffbe20;
xor ymm1_1@uint64 ymm3_1 L0x7fffffffbe28;
xor ymm1_2@uint64 ymm3_2 L0x7fffffffbe30;
xor ymm1_3@uint64 ymm3_3 L0x7fffffffbe38;
(* vpandn %ymm15,%ymm4,%ymm3                       #! PC = 0x555555578108 *)
not ymm4_0n@uint64 ymm4_0;
and ymm3_0@uint64 ymm4_0n ymm15_0;
not ymm4_1n@uint64 ymm4_1;
and ymm3_1@uint64 ymm4_1n ymm15_1;
not ymm4_2n@uint64 ymm4_2;
and ymm3_2@uint64 ymm4_2n ymm15_2;
not ymm4_3n@uint64 ymm4_3;
and ymm3_3@uint64 ymm4_3n ymm15_3;
(* vmovdqa -0x110(%rbp),%ymm15                     #! EA = L0x7fffffffbe40; Value = 0xd19df82730ea16b5; PC = 0x55555557810d *)
mov ymm15_0 L0x7fffffffbe40;
mov ymm15_1 L0x7fffffffbe48;
mov ymm15_2 L0x7fffffffbe50;
mov ymm15_3 L0x7fffffffbe58;
(* vpxor  %ymm5,%ymm10,%ymm11                      #! PC = 0x555555578115 *)
xor ymm11_0@uint64 ymm10_0 ymm5_0;
xor ymm11_1@uint64 ymm10_1 ymm5_1;
xor ymm11_2@uint64 ymm10_2 ymm5_2;
xor ymm11_3@uint64 ymm10_3 ymm5_3;
(* vpxor  %ymm0,%ymm3,%ymm3                        #! PC = 0x555555578119 *)
xor ymm3_0@uint64 ymm3_0 ymm0_0;
xor ymm3_1@uint64 ymm3_1 ymm0_1;
xor ymm3_2@uint64 ymm3_2 ymm0_2;
xor ymm3_3@uint64 ymm3_3 ymm0_3;
(* vpxor  -0x2b0(%rbp),%ymm15,%ymm0                #! EA = L0x7fffffffbca0; Value = 0xf51cb0626da6f085; PC = 0x55555557811d *)
xor ymm0_0@uint64 ymm15_0 L0x7fffffffbca0;
xor ymm0_1@uint64 ymm15_1 L0x7fffffffbca8;
xor ymm0_2@uint64 ymm15_2 L0x7fffffffbcb0;
xor ymm0_3@uint64 ymm15_3 L0x7fffffffbcb8;
(* vmovdqa %ymm5,-0x310(%rbp)                      #! EA = L0x7fffffffbc40; PC = 0x555555578125 *)
mov L0x7fffffffbc40 ymm5_0;
mov L0x7fffffffbc48 ymm5_1;
mov L0x7fffffffbc50 ymm5_2;
mov L0x7fffffffbc58 ymm5_3;
(* vpxor  %ymm1,%ymm11,%ymm11                      #! PC = 0x55555557812d *)
xor ymm11_0@uint64 ymm11_0 ymm1_0;
xor ymm11_1@uint64 ymm11_1 ymm1_1;
xor ymm11_2@uint64 ymm11_2 ymm1_2;
xor ymm11_3@uint64 ymm11_3 ymm1_3;
(* vpxor  -0xf0(%rbp),%ymm3,%ymm1                  #! EA = L0x7fffffffbe60; Value = 0x908c33b016575e0b; PC = 0x555555578131 *)
xor ymm1_0@uint64 ymm3_0 L0x7fffffffbe60;
xor ymm1_1@uint64 ymm3_1 L0x7fffffffbe68;
xor ymm1_2@uint64 ymm3_2 L0x7fffffffbe70;
xor ymm1_3@uint64 ymm3_3 L0x7fffffffbe78;
(* vpxor  %ymm12,%ymm13,%ymm13                     #! PC = 0x555555578139 *)
xor ymm13_0@uint64 ymm13_0 ymm12_0;
xor ymm13_1@uint64 ymm13_1 ymm12_1;
xor ymm13_2@uint64 ymm13_2 ymm12_2;
xor ymm13_3@uint64 ymm13_3 ymm12_3;
(* vpxor  -0xb0(%rbp),%ymm11,%ymm11                #! EA = L0x7fffffffbea0; Value = 0x53e3c5f2892e6bdb; PC = 0x55555557813e *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbea0;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbea8;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbeb0;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbeb8;
(* vpsllq $0x1,%ymm14,%ymm5                        #! PC = 0x555555578146 *)
shl ymm5_0 ymm14_0 0x1@uint64;
shl ymm5_1 ymm14_1 0x1@uint64;
shl ymm5_2 ymm14_2 0x1@uint64;
shl ymm5_3 ymm14_3 0x1@uint64;
(* vpxor  %ymm0,%ymm1,%ymm1                        #! PC = 0x55555557814c *)
xor ymm1_0@uint64 ymm1_0 ymm0_0;
xor ymm1_1@uint64 ymm1_1 ymm0_1;
xor ymm1_2@uint64 ymm1_2 ymm0_2;
xor ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3f,%ymm14,%ymm0                       #! PC = 0x555555578150 *)
shr ymm0_0 ymm14_0 0x3f@uint64;
shr ymm0_1 ymm14_1 0x3f@uint64;
shr ymm0_2 ymm14_2 0x3f@uint64;
shr ymm0_3 ymm14_3 0x3f@uint64;
(* vpxor  -0x50(%rbp),%ymm1,%ymm1                  #! EA = L0x7fffffffbf00; Value = 0x50b4ad11e821c365; PC = 0x555555578156 *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbf00;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbf08;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbf10;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbf18;
(* vpsllq $0x1,%ymm13,%ymm4                        #! PC = 0x55555557815b *)
shl ymm4_0 ymm13_0 0x1@uint64;
shl ymm4_1 ymm13_1 0x1@uint64;
shl ymm4_2 ymm13_2 0x1@uint64;
shl ymm4_3 ymm13_3 0x1@uint64;
(* vpsrlq $0x3f,%ymm11,%ymm15                      #! PC = 0x555555578161 *)
shr ymm15_0 ymm11_0 0x3f@uint64;
shr ymm15_1 ymm11_1 0x3f@uint64;
shr ymm15_2 ymm11_2 0x3f@uint64;
shr ymm15_3 ymm11_3 0x3f@uint64;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x555555578167 *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpsrlq $0x3f,%ymm13,%ymm0                       #! PC = 0x55555557816b *)
shr ymm0_0 ymm13_0 0x3f@uint64;
shr ymm0_1 ymm13_1 0x3f@uint64;
shr ymm0_2 ymm13_2 0x3f@uint64;
shr ymm0_3 ymm13_3 0x3f@uint64;
(* vpxor  %ymm1,%ymm5,%ymm5                        #! PC = 0x555555578171 *)
xor ymm5_0@uint64 ymm5_0 ymm1_0;
xor ymm5_1@uint64 ymm5_1 ymm1_1;
xor ymm5_2@uint64 ymm5_2 ymm1_2;
xor ymm5_3@uint64 ymm5_3 ymm1_3;
(* vpor   %ymm0,%ymm4,%ymm4                        #! PC = 0x555555578175 *)
or ymm4_0@uint64 ymm4_0 ymm0_0;
or ymm4_1@uint64 ymm4_1 ymm0_1;
or ymm4_2@uint64 ymm4_2 ymm0_2;
or ymm4_3@uint64 ymm4_3 ymm0_3;
(* vpsllq $0x1,%ymm11,%ymm0                        #! PC = 0x555555578179 *)
shl ymm0_0 ymm11_0 0x1@uint64;
shl ymm0_1 ymm11_1 0x1@uint64;
shl ymm0_2 ymm11_2 0x1@uint64;
shl ymm0_3 ymm11_3 0x1@uint64;
(* vpxor  %ymm6,%ymm5,%ymm6                        #! PC = 0x55555557817f *)
xor ymm6_0@uint64 ymm5_0 ymm6_0;
xor ymm6_1@uint64 ymm5_1 ymm6_1;
xor ymm6_2@uint64 ymm5_2 ymm6_2;
xor ymm6_3@uint64 ymm5_3 ymm6_3;
(* vpor   %ymm15,%ymm0,%ymm0                       #! PC = 0x555555578183 *)
or ymm0_0@uint64 ymm0_0 ymm15_0;
or ymm0_1@uint64 ymm0_1 ymm15_1;
or ymm0_2@uint64 ymm0_2 ymm15_2;
or ymm0_3@uint64 ymm0_3 ymm15_3;
(* vpxor  %ymm2,%ymm4,%ymm4                        #! PC = 0x555555578188 *)
xor ymm4_0@uint64 ymm4_0 ymm2_0;
xor ymm4_1@uint64 ymm4_1 ymm2_1;
xor ymm4_2@uint64 ymm4_2 ymm2_2;
xor ymm4_3@uint64 ymm4_3 ymm2_3;
(* vmovq  %rsi,%xmm15                              #! PC = 0x55555557818c *)
mov xmm15_0 rsi;
mov xmm15_1 0@uint64;
(* vpxor  %ymm14,%ymm0,%ymm14                      #! PC = 0x555555578191 *)
xor ymm14_0@uint64 ymm0_0 ymm14_0;
xor ymm14_1@uint64 ymm0_1 ymm14_1;
xor ymm14_2@uint64 ymm0_2 ymm14_2;
xor ymm14_3@uint64 ymm0_3 ymm14_3;
(* vpsrlq $0x3f,%ymm1,%ymm0                        #! PC = 0x555555578196 *)
shr ymm0_0 ymm1_0 0x3f@uint64;
shr ymm0_1 ymm1_1 0x3f@uint64;
shr ymm0_2 ymm1_2 0x3f@uint64;
shr ymm0_3 ymm1_3 0x3f@uint64;
(* vpxor  %ymm9,%ymm4,%ymm9                        #! PC = 0x55555557819b *)
xor ymm9_0@uint64 ymm4_0 ymm9_0;
xor ymm9_1@uint64 ymm4_1 ymm9_1;
xor ymm9_2@uint64 ymm4_2 ymm9_2;
xor ymm9_3@uint64 ymm4_3 ymm9_3;
(* vpsllq $0x1,%ymm1,%ymm1                         #! PC = 0x5555555781a0 *)
shl ymm1_0 ymm1_0 0x1@uint64;
shl ymm1_1 ymm1_1 0x1@uint64;
shl ymm1_2 ymm1_2 0x1@uint64;
shl ymm1_3 ymm1_3 0x1@uint64;
(* vpxor  %ymm8,%ymm14,%ymm8                       #! PC = 0x5555555781a5 *)
xor ymm8_0@uint64 ymm14_0 ymm8_0;
xor ymm8_1@uint64 ymm14_1 ymm8_1;
xor ymm8_2@uint64 ymm14_2 ymm8_2;
xor ymm8_3@uint64 ymm14_3 ymm8_3;
(* vpxor  %ymm7,%ymm4,%ymm7                        #! PC = 0x5555555781aa *)
xor ymm7_0@uint64 ymm4_0 ymm7_0;
xor ymm7_1@uint64 ymm4_1 ymm7_1;
xor ymm7_2@uint64 ymm4_2 ymm7_2;
xor ymm7_3@uint64 ymm4_3 ymm7_3;
(* vpor   %ymm0,%ymm1,%ymm1                        #! PC = 0x5555555781ae *)
or ymm1_0@uint64 ymm1_0 ymm0_0;
or ymm1_1@uint64 ymm1_1 ymm0_1;
or ymm1_2@uint64 ymm1_2 ymm0_2;
or ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3f,%ymm2,%ymm0                        #! PC = 0x5555555781b2 *)
shr ymm0_0 ymm2_0 0x3f@uint64;
shr ymm0_1 ymm2_1 0x3f@uint64;
shr ymm0_2 ymm2_2 0x3f@uint64;
shr ymm0_3 ymm2_3 0x3f@uint64;
(* vpxor  %ymm12,%ymm14,%ymm12                     #! PC = 0x5555555781b7 *)
xor ymm12_0@uint64 ymm14_0 ymm12_0;
xor ymm12_1@uint64 ymm14_1 ymm12_1;
xor ymm12_2@uint64 ymm14_2 ymm12_2;
xor ymm12_3@uint64 ymm14_3 ymm12_3;
(* vpxor  %ymm13,%ymm1,%ymm13                      #! PC = 0x5555555781bc *)
xor ymm13_0@uint64 ymm1_0 ymm13_0;
xor ymm13_1@uint64 ymm1_1 ymm13_1;
xor ymm13_2@uint64 ymm1_2 ymm13_2;
xor ymm13_3@uint64 ymm1_3 ymm13_3;
(* vpsrlq $0x14,%ymm9,%ymm1                        #! PC = 0x5555555781c1 *)
shr ymm1_0 ymm9_0 0x14@uint64;
shr ymm1_1 ymm9_1 0x14@uint64;
shr ymm1_2 ymm9_2 0x14@uint64;
shr ymm1_3 ymm9_3 0x14@uint64;
(* vpsllq $0x2c,%ymm9,%ymm9                        #! PC = 0x5555555781c7 *)
shl ymm9_0 ymm9_0 0x2c@uint64;
shl ymm9_1 ymm9_1 0x2c@uint64;
shl ymm9_2 ymm9_2 0x2c@uint64;
shl ymm9_3 ymm9_3 0x2c@uint64;
(* vpsllq $0x1,%ymm2,%ymm2                         #! PC = 0x5555555781cd *)
shl ymm2_0 ymm2_0 0x1@uint64;
shl ymm2_1 ymm2_1 0x1@uint64;
shl ymm2_2 ymm2_2 0x1@uint64;
shl ymm2_3 ymm2_3 0x1@uint64;
(* vpxor  %ymm10,%ymm13,%ymm10                     #! PC = 0x5555555781d2 *)
xor ymm10_0@uint64 ymm13_0 ymm10_0;
xor ymm10_1@uint64 ymm13_1 ymm10_1;
xor ymm10_2@uint64 ymm13_2 ymm10_2;
xor ymm10_3@uint64 ymm13_3 ymm10_3;
(* vpor   %ymm1,%ymm9,%ymm9                        #! PC = 0x5555555781d7 *)
or ymm9_0@uint64 ymm9_0 ymm1_0;
or ymm9_1@uint64 ymm9_1 ymm1_1;
or ymm9_2@uint64 ymm9_2 ymm1_2;
or ymm9_3@uint64 ymm9_3 ymm1_3;
(* vpsrlq $0x15,%ymm8,%ymm1                        #! PC = 0x5555555781db *)
shr ymm1_0 ymm8_0 0x15@uint64;
shr ymm1_1 ymm8_1 0x15@uint64;
shr ymm1_2 ymm8_2 0x15@uint64;
shr ymm1_3 ymm8_3 0x15@uint64;
(* vpor   %ymm0,%ymm2,%ymm2                        #! PC = 0x5555555781e1 *)
or ymm2_0@uint64 ymm2_0 ymm0_0;
or ymm2_1@uint64 ymm2_1 ymm0_1;
or ymm2_2@uint64 ymm2_2 ymm0_2;
or ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsllq $0x2b,%ymm8,%ymm8                        #! PC = 0x5555555781e5 *)
shl ymm8_0 ymm8_0 0x2b@uint64;
shl ymm8_1 ymm8_1 0x2b@uint64;
shl ymm8_2 ymm8_2 0x2b@uint64;
shl ymm8_3 ymm8_3 0x2b@uint64;
(* vpxor  %ymm11,%ymm2,%ymm11                      #! PC = 0x5555555781eb *)
xor ymm11_0@uint64 ymm2_0 ymm11_0;
xor ymm11_1@uint64 ymm2_1 ymm11_1;
xor ymm11_2@uint64 ymm2_2 ymm11_2;
xor ymm11_3@uint64 ymm2_3 ymm11_3;
(* vpxor  -0x90(%rbp),%ymm5,%ymm0                  #! EA = L0x7fffffffbec0; Value = 0x06ee364e771b6bcf; PC = 0x5555555781f0 *)
xor ymm0_0@uint64 ymm5_0 L0x7fffffffbec0;
xor ymm0_1@uint64 ymm5_1 L0x7fffffffbec8;
xor ymm0_2@uint64 ymm5_2 L0x7fffffffbed0;
xor ymm0_3@uint64 ymm5_3 L0x7fffffffbed8;
(* vpor   %ymm1,%ymm8,%ymm8                        #! PC = 0x5555555781f8 *)
or ymm8_0@uint64 ymm8_0 ymm1_0;
or ymm8_1@uint64 ymm8_1 ymm1_1;
or ymm8_2@uint64 ymm8_2 ymm1_2;
or ymm8_3@uint64 ymm8_3 ymm1_3;
(* vpbroadcastq %xmm15,%ymm1                       #! PC = 0x5555555781fc *)
mov ymm1_0 xmm15_0;
mov ymm1_1 xmm15_0;
mov ymm1_2 xmm15_0;
mov ymm1_3 xmm15_0;
(* vpxor  %ymm3,%ymm11,%ymm3                       #! PC = 0x555555578201 *)
xor ymm3_0@uint64 ymm11_0 ymm3_0;
xor ymm3_1@uint64 ymm11_1 ymm3_1;
xor ymm3_2@uint64 ymm11_2 ymm3_2;
xor ymm3_3@uint64 ymm11_3 ymm3_3;
(* vpandn %ymm8,%ymm9,%ymm2                        #! PC = 0x555555578205 *)
not ymm9_0n@uint64 ymm9_0;
and ymm2_0@uint64 ymm9_0n ymm8_0;
not ymm9_1n@uint64 ymm9_1;
and ymm2_1@uint64 ymm9_1n ymm8_1;
not ymm9_2n@uint64 ymm9_2;
and ymm2_2@uint64 ymm9_2n ymm8_2;
not ymm9_3n@uint64 ymm9_3;
and ymm2_3@uint64 ymm9_3n ymm8_3;
(* vpxor  %ymm2,%ymm1,%ymm1                        #! PC = 0x55555557820a *)
xor ymm1_0@uint64 ymm1_0 ymm2_0;
xor ymm1_1@uint64 ymm1_1 ymm2_1;
xor ymm1_2@uint64 ymm1_2 ymm2_2;
xor ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpxor  %ymm0,%ymm1,%ymm2                        #! PC = 0x55555557820e *)
xor ymm2_0@uint64 ymm1_0 ymm0_0;
xor ymm2_1@uint64 ymm1_1 ymm0_1;
xor ymm2_2@uint64 ymm1_2 ymm0_2;
xor ymm2_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x2b,%ymm10,%ymm1                       #! PC = 0x555555578212 *)
shr ymm1_0 ymm10_0 0x2b@uint64;
shr ymm1_1 ymm10_1 0x2b@uint64;
shr ymm1_2 ymm10_2 0x2b@uint64;
shr ymm1_3 ymm10_3 0x2b@uint64;
(* vpsllq $0x15,%ymm10,%ymm10                      #! PC = 0x555555578218 *)
shl ymm10_0 ymm10_0 0x15@uint64;
shl ymm10_1 ymm10_1 0x15@uint64;
shl ymm10_2 ymm10_2 0x15@uint64;
shl ymm10_3 ymm10_3 0x15@uint64;
(* vmovdqa %ymm2,-0x90(%rbp)                       #! EA = L0x7fffffffbec0; PC = 0x55555557821e *)
mov L0x7fffffffbec0 ymm2_0;
mov L0x7fffffffbec8 ymm2_1;
mov L0x7fffffffbed0 ymm2_2;
mov L0x7fffffffbed8 ymm2_3;
(* vpor   %ymm1,%ymm10,%ymm10                      #! PC = 0x555555578226 *)
or ymm10_0@uint64 ymm10_0 ymm1_0;
or ymm10_1@uint64 ymm10_1 ymm1_1;
or ymm10_2@uint64 ymm10_2 ymm1_2;
or ymm10_3@uint64 ymm10_3 ymm1_3;
(* vpandn %ymm10,%ymm8,%ymm1                       #! PC = 0x55555557822a *)
not ymm8_0n@uint64 ymm8_0;
and ymm1_0@uint64 ymm8_0n ymm10_0;
not ymm8_1n@uint64 ymm8_1;
and ymm1_1@uint64 ymm8_1n ymm10_1;
not ymm8_2n@uint64 ymm8_2;
and ymm1_2@uint64 ymm8_2n ymm10_2;
not ymm8_3n@uint64 ymm8_3;
and ymm1_3@uint64 ymm8_3n ymm10_3;
(* vpxor  %ymm9,%ymm1,%ymm15                       #! PC = 0x55555557822f *)
xor ymm15_0@uint64 ymm1_0 ymm9_0;
xor ymm15_1@uint64 ymm1_1 ymm9_1;
xor ymm15_2@uint64 ymm1_2 ymm9_2;
xor ymm15_3@uint64 ymm1_3 ymm9_3;
(* vpsrlq $0x32,%ymm3,%ymm1                        #! PC = 0x555555578234 *)
shr ymm1_0 ymm3_0 0x32@uint64;
shr ymm1_1 ymm3_1 0x32@uint64;
shr ymm1_2 ymm3_2 0x32@uint64;
shr ymm1_3 ymm3_3 0x32@uint64;
(* vpandn %ymm9,%ymm0,%ymm9                        #! PC = 0x555555578239 *)
not ymm0_0n@uint64 ymm0_0;
and ymm9_0@uint64 ymm0_0n ymm9_0;
not ymm0_1n@uint64 ymm0_1;
and ymm9_1@uint64 ymm0_1n ymm9_1;
not ymm0_2n@uint64 ymm0_2;
and ymm9_2@uint64 ymm0_2n ymm9_2;
not ymm0_3n@uint64 ymm0_3;
and ymm9_3@uint64 ymm0_3n ymm9_3;
(* vpsllq $0xe,%ymm3,%ymm3                         #! PC = 0x55555557823e *)
shl ymm3_0 ymm3_0 0xe@uint64;
shl ymm3_1 ymm3_1 0xe@uint64;
shl ymm3_2 ymm3_2 0xe@uint64;
shl ymm3_3 ymm3_3 0xe@uint64;
(* vmovdqa %ymm15,-0x190(%rbp)                     #! EA = L0x7fffffffbdc0; PC = 0x555555578243 *)
mov L0x7fffffffbdc0 ymm15_0;
mov L0x7fffffffbdc8 ymm15_1;
mov L0x7fffffffbdd0 ymm15_2;
mov L0x7fffffffbdd8 ymm15_3;
(* vpor   %ymm1,%ymm3,%ymm3                        #! PC = 0x55555557824b *)
or ymm3_0@uint64 ymm3_0 ymm1_0;
or ymm3_1@uint64 ymm3_1 ymm1_1;
or ymm3_2@uint64 ymm3_2 ymm1_2;
or ymm3_3@uint64 ymm3_3 ymm1_3;
(* vpandn %ymm3,%ymm10,%ymm1                       #! PC = 0x55555557824f *)
not ymm10_0n@uint64 ymm10_0;
and ymm1_0@uint64 ymm10_0n ymm3_0;
not ymm10_1n@uint64 ymm10_1;
and ymm1_1@uint64 ymm10_1n ymm3_1;
not ymm10_2n@uint64 ymm10_2;
and ymm1_2@uint64 ymm10_2n ymm3_2;
not ymm10_3n@uint64 ymm10_3;
and ymm1_3@uint64 ymm10_3n ymm3_3;
(* vpxor  %ymm3,%ymm9,%ymm9                        #! PC = 0x555555578253 *)
xor ymm9_0@uint64 ymm9_0 ymm3_0;
xor ymm9_1@uint64 ymm9_1 ymm3_1;
xor ymm9_2@uint64 ymm9_2 ymm3_2;
xor ymm9_3@uint64 ymm9_3 ymm3_3;
(* vpxor  %ymm8,%ymm1,%ymm8                        #! PC = 0x555555578257 *)
xor ymm8_0@uint64 ymm1_0 ymm8_0;
xor ymm8_1@uint64 ymm1_1 ymm8_1;
xor ymm8_2@uint64 ymm1_2 ymm8_2;
xor ymm8_3@uint64 ymm1_3 ymm8_3;
(* vpandn %ymm0,%ymm3,%ymm1                        #! PC = 0x55555557825c *)
not ymm3_0n@uint64 ymm3_0;
and ymm1_0@uint64 ymm3_0n ymm0_0;
not ymm3_1n@uint64 ymm3_1;
and ymm1_1@uint64 ymm3_1n ymm0_1;
not ymm3_2n@uint64 ymm3_2;
and ymm1_2@uint64 ymm3_2n ymm0_2;
not ymm3_3n@uint64 ymm3_3;
and ymm1_3@uint64 ymm3_3n ymm0_3;
(* vpxor  -0x130(%rbp),%ymm13,%ymm0                #! EA = L0x7fffffffbe20; Value = 0x9473996fe1bb4ceb; PC = 0x555555578260 *)
xor ymm0_0@uint64 ymm13_0 L0x7fffffffbe20;
xor ymm0_1@uint64 ymm13_1 L0x7fffffffbe28;
xor ymm0_2@uint64 ymm13_2 L0x7fffffffbe30;
xor ymm0_3@uint64 ymm13_3 L0x7fffffffbe38;
(* vmovdqa %ymm9,-0x250(%rbp)                      #! EA = L0x7fffffffbd00; PC = 0x555555578268 *)
mov L0x7fffffffbd00 ymm9_0;
mov L0x7fffffffbd08 ymm9_1;
mov L0x7fffffffbd10 ymm9_2;
mov L0x7fffffffbd18 ymm9_3;
(* vpxor  %ymm10,%ymm1,%ymm10                      #! PC = 0x555555578270 *)
xor ymm10_0@uint64 ymm1_0 ymm10_0;
xor ymm10_1@uint64 ymm1_1 ymm10_1;
xor ymm10_2@uint64 ymm1_2 ymm10_2;
xor ymm10_3@uint64 ymm1_3 ymm10_3;
(* vmovdqa %ymm8,-0x270(%rbp)                      #! EA = L0x7fffffffbce0; PC = 0x555555578275 *)
mov L0x7fffffffbce0 ymm8_0;
mov L0x7fffffffbce8 ymm8_1;
mov L0x7fffffffbcf0 ymm8_2;
mov L0x7fffffffbcf8 ymm8_3;
(* vpsrlq $0x24,%ymm0,%ymm1                        #! PC = 0x55555557827d *)
shr ymm1_0 ymm0_0 0x24@uint64;
shr ymm1_1 ymm0_1 0x24@uint64;
shr ymm1_2 ymm0_2 0x24@uint64;
shr ymm1_3 ymm0_3 0x24@uint64;
(* vpsllq $0x1c,%ymm0,%ymm0                        #! PC = 0x555555578282 *)
shl ymm0_0 ymm0_0 0x1c@uint64;
shl ymm0_1 ymm0_1 0x1c@uint64;
shl ymm0_2 ymm0_2 0x1c@uint64;
shl ymm0_3 ymm0_3 0x1c@uint64;
(* vmovdqa %ymm10,-0x170(%rbp)                     #! EA = L0x7fffffffbde0; PC = 0x555555578287 *)
mov L0x7fffffffbde0 ymm10_0;
mov L0x7fffffffbde8 ymm10_1;
mov L0x7fffffffbdf0 ymm10_2;
mov L0x7fffffffbdf8 ymm10_3;
(* vpxor  -0x2b0(%rbp),%ymm11,%ymm10               #! EA = L0x7fffffffbca0; Value = 0xf51cb0626da6f085; PC = 0x55555557828f *)
xor ymm10_0@uint64 ymm11_0 L0x7fffffffbca0;
xor ymm10_1@uint64 ymm11_1 L0x7fffffffbca8;
xor ymm10_2@uint64 ymm11_2 L0x7fffffffbcb0;
xor ymm10_3@uint64 ymm11_3 L0x7fffffffbcb8;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x555555578297 *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x110(%rbp),%ymm11,%ymm1                #! EA = L0x7fffffffbe40; Value = 0xd19df82730ea16b5; PC = 0x55555557829b *)
xor ymm1_0@uint64 ymm11_0 L0x7fffffffbe40;
xor ymm1_1@uint64 ymm11_1 L0x7fffffffbe48;
xor ymm1_2@uint64 ymm11_2 L0x7fffffffbe50;
xor ymm1_3@uint64 ymm11_3 L0x7fffffffbe58;
(* vpsrlq $0x2c,%ymm1,%ymm2                        #! PC = 0x5555555782a3 *)
shr ymm2_0 ymm1_0 0x2c@uint64;
shr ymm2_1 ymm1_1 0x2c@uint64;
shr ymm2_2 ymm1_2 0x2c@uint64;
shr ymm2_3 ymm1_3 0x2c@uint64;
(* vpsllq $0x14,%ymm1,%ymm1                        #! PC = 0x5555555782a8 *)
shl ymm1_0 ymm1_0 0x14@uint64;
shl ymm1_1 ymm1_1 0x14@uint64;
shl ymm1_2 ymm1_2 0x14@uint64;
shl ymm1_3 ymm1_3 0x14@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x5555555782ad *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpxor  -0xd0(%rbp),%ymm5,%ymm2                  #! EA = L0x7fffffffbe80; Value = 0x03fb7294d1438d9b; PC = 0x5555555782b1 *)
xor ymm2_0@uint64 ymm5_0 L0x7fffffffbe80;
xor ymm2_1@uint64 ymm5_1 L0x7fffffffbe88;
xor ymm2_2@uint64 ymm5_2 L0x7fffffffbe90;
xor ymm2_3@uint64 ymm5_3 L0x7fffffffbe98;
(* vpsrlq $0x3d,%ymm2,%ymm3                        #! PC = 0x5555555782b9 *)
shr ymm3_0 ymm2_0 0x3d@uint64;
shr ymm3_1 ymm2_1 0x3d@uint64;
shr ymm3_2 ymm2_2 0x3d@uint64;
shr ymm3_3 ymm2_3 0x3d@uint64;
(* vpsllq $0x3,%ymm2,%ymm2                         #! PC = 0x5555555782be *)
shl ymm2_0 ymm2_0 0x3@uint64;
shl ymm2_1 ymm2_1 0x3@uint64;
shl ymm2_2 ymm2_2 0x3@uint64;
shl ymm2_3 ymm2_3 0x3@uint64;
(* vpor   %ymm3,%ymm2,%ymm2                        #! PC = 0x5555555782c3 *)
or ymm2_0@uint64 ymm2_0 ymm3_0;
or ymm2_1@uint64 ymm2_1 ymm3_1;
or ymm2_2@uint64 ymm2_2 ymm3_2;
or ymm2_3@uint64 ymm2_3 ymm3_3;
(* vpandn %ymm2,%ymm1,%ymm3                        #! PC = 0x5555555782c7 *)
not ymm1_0n@uint64 ymm1_0;
and ymm3_0@uint64 ymm1_0n ymm2_0;
not ymm1_1n@uint64 ymm1_1;
and ymm3_1@uint64 ymm1_1n ymm2_1;
not ymm1_2n@uint64 ymm1_2;
and ymm3_2@uint64 ymm1_2n ymm2_2;
not ymm1_3n@uint64 ymm1_3;
and ymm3_3@uint64 ymm1_3n ymm2_3;
(* vpxor  %ymm0,%ymm3,%ymm8                        #! PC = 0x5555555782cb *)
xor ymm8_0@uint64 ymm3_0 ymm0_0;
xor ymm8_1@uint64 ymm3_1 ymm0_1;
xor ymm8_2@uint64 ymm3_2 ymm0_2;
xor ymm8_3@uint64 ymm3_3 ymm0_3;
(* vpsrlq $0x13,%ymm7,%ymm3                        #! PC = 0x5555555782cf *)
shr ymm3_0 ymm7_0 0x13@uint64;
shr ymm3_1 ymm7_1 0x13@uint64;
shr ymm3_2 ymm7_2 0x13@uint64;
shr ymm3_3 ymm7_3 0x13@uint64;
(* vpsllq $0x2d,%ymm7,%ymm7                        #! PC = 0x5555555782d4 *)
shl ymm7_0 ymm7_0 0x2d@uint64;
shl ymm7_1 ymm7_1 0x2d@uint64;
shl ymm7_2 ymm7_2 0x2d@uint64;
shl ymm7_3 ymm7_3 0x2d@uint64;
(* vmovdqa %ymm8,-0x230(%rbp)                      #! EA = L0x7fffffffbd20; PC = 0x5555555782d9 *)
mov L0x7fffffffbd20 ymm8_0;
mov L0x7fffffffbd28 ymm8_1;
mov L0x7fffffffbd30 ymm8_2;
mov L0x7fffffffbd38 ymm8_3;
(* vpor   %ymm3,%ymm7,%ymm15                       #! PC = 0x5555555782e1 *)
or ymm15_0@uint64 ymm7_0 ymm3_0;
or ymm15_1@uint64 ymm7_1 ymm3_1;
or ymm15_2@uint64 ymm7_2 ymm3_2;
or ymm15_3@uint64 ymm7_3 ymm3_3;
(* vpsrlq $0x3,%ymm12,%ymm3                        #! PC = 0x5555555782e5 *)
shr ymm3_0 ymm12_0 0x3@uint64;
shr ymm3_1 ymm12_1 0x3@uint64;
shr ymm3_2 ymm12_2 0x3@uint64;
shr ymm3_3 ymm12_3 0x3@uint64;
(* vpsllq $0x3d,%ymm12,%ymm12                      #! PC = 0x5555555782eb *)
shl ymm12_0 ymm12_0 0x3d@uint64;
shl ymm12_1 ymm12_1 0x3d@uint64;
shl ymm12_2 ymm12_2 0x3d@uint64;
shl ymm12_3 ymm12_3 0x3d@uint64;
(* vpandn %ymm15,%ymm2,%ymm8                       #! PC = 0x5555555782f1 *)
not ymm2_0n@uint64 ymm2_0;
and ymm8_0@uint64 ymm2_0n ymm15_0;
not ymm2_1n@uint64 ymm2_1;
and ymm8_1@uint64 ymm2_1n ymm15_1;
not ymm2_2n@uint64 ymm2_2;
and ymm8_2@uint64 ymm2_2n ymm15_2;
not ymm2_3n@uint64 ymm2_3;
and ymm8_3@uint64 ymm2_3n ymm15_3;
(* vpor   %ymm3,%ymm12,%ymm12                      #! PC = 0x5555555782f6 *)
or ymm12_0@uint64 ymm12_0 ymm3_0;
or ymm12_1@uint64 ymm12_1 ymm3_1;
or ymm12_2@uint64 ymm12_2 ymm3_2;
or ymm12_3@uint64 ymm12_3 ymm3_3;
(* vpxor  %ymm1,%ymm8,%ymm8                        #! PC = 0x5555555782fa *)
xor ymm8_0@uint64 ymm8_0 ymm1_0;
xor ymm8_1@uint64 ymm8_1 ymm1_1;
xor ymm8_2@uint64 ymm8_2 ymm1_2;
xor ymm8_3@uint64 ymm8_3 ymm1_3;
(* vpandn %ymm12,%ymm15,%ymm3                      #! PC = 0x5555555782fe *)
not ymm15_0n@uint64 ymm15_0;
and ymm3_0@uint64 ymm15_0n ymm12_0;
not ymm15_1n@uint64 ymm15_1;
and ymm3_1@uint64 ymm15_1n ymm12_1;
not ymm15_2n@uint64 ymm15_2;
and ymm3_2@uint64 ymm15_2n ymm12_2;
not ymm15_3n@uint64 ymm15_3;
and ymm3_3@uint64 ymm15_3n ymm12_3;
(* vpxor  %ymm2,%ymm3,%ymm7                        #! PC = 0x555555578303 *)
xor ymm7_0@uint64 ymm3_0 ymm2_0;
xor ymm7_1@uint64 ymm3_1 ymm2_1;
xor ymm7_2@uint64 ymm3_2 ymm2_2;
xor ymm7_3@uint64 ymm3_3 ymm2_3;
(* vpandn %ymm0,%ymm12,%ymm2                       #! PC = 0x555555578307 *)
not ymm12_0n@uint64 ymm12_0;
and ymm2_0@uint64 ymm12_0n ymm0_0;
not ymm12_1n@uint64 ymm12_1;
and ymm2_1@uint64 ymm12_1n ymm0_1;
not ymm12_2n@uint64 ymm12_2;
and ymm2_2@uint64 ymm12_2n ymm0_2;
not ymm12_3n@uint64 ymm12_3;
and ymm2_3@uint64 ymm12_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x55555557830b *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm12,%ymm0,%ymm12                      #! PC = 0x55555557830f *)
xor ymm12_0@uint64 ymm0_0 ymm12_0;
xor ymm12_1@uint64 ymm0_1 ymm12_1;
xor ymm12_2@uint64 ymm0_2 ymm12_2;
xor ymm12_3@uint64 ymm0_3 ymm12_3;
(* vpxor  -0x2f0(%rbp),%ymm4,%ymm0                 #! EA = L0x7fffffffbc60; Value = 0xe658feaa402f4bd5; PC = 0x555555578314 *)
xor ymm0_0@uint64 ymm4_0 L0x7fffffffbc60;
xor ymm0_1@uint64 ymm4_1 L0x7fffffffbc68;
xor ymm0_2@uint64 ymm4_2 L0x7fffffffbc70;
xor ymm0_3@uint64 ymm4_3 L0x7fffffffbc78;
(* vpxor  %ymm15,%ymm2,%ymm9                       #! PC = 0x55555557831c *)
xor ymm9_0@uint64 ymm2_0 ymm15_0;
xor ymm9_1@uint64 ymm2_1 ymm15_1;
xor ymm9_2@uint64 ymm2_2 ymm15_2;
xor ymm9_3@uint64 ymm2_3 ymm15_3;
(* vmovdqa %ymm7,-0x150(%rbp)                      #! EA = L0x7fffffffbe00; PC = 0x555555578321 *)
mov L0x7fffffffbe00 ymm7_0;
mov L0x7fffffffbe08 ymm7_1;
mov L0x7fffffffbe10 ymm7_2;
mov L0x7fffffffbe18 ymm7_3;
(* vmovdqa %ymm9,-0x210(%rbp)                      #! EA = L0x7fffffffbd40; PC = 0x555555578329 *)
mov L0x7fffffffbd40 ymm9_0;
mov L0x7fffffffbd48 ymm9_1;
mov L0x7fffffffbd50 ymm9_2;
mov L0x7fffffffbd58 ymm9_3;
(* vpxor  -0xb0(%rbp),%ymm13,%ymm9                 #! EA = L0x7fffffffbea0; Value = 0x53e3c5f2892e6bdb; PC = 0x555555578331 *)
xor ymm9_0@uint64 ymm13_0 L0x7fffffffbea0;
xor ymm9_1@uint64 ymm13_1 L0x7fffffffbea8;
xor ymm9_2@uint64 ymm13_2 L0x7fffffffbeb0;
xor ymm9_3@uint64 ymm13_3 L0x7fffffffbeb8;
(* vpsrlq $0x3f,%ymm0,%ymm1                        #! PC = 0x555555578339 *)
shr ymm1_0 ymm0_0 0x3f@uint64;
shr ymm1_1 ymm0_1 0x3f@uint64;
shr ymm1_2 ymm0_2 0x3f@uint64;
shr ymm1_3 ymm0_3 0x3f@uint64;
(* vpsllq $0x1,%ymm0,%ymm0                         #! PC = 0x55555557833e *)
shl ymm0_0 ymm0_0 0x1@uint64;
shl ymm0_1 ymm0_1 0x1@uint64;
shl ymm0_2 ymm0_2 0x1@uint64;
shl ymm0_3 ymm0_3 0x1@uint64;
(* vmovdqa %ymm12,-0x130(%rbp)                     #! EA = L0x7fffffffbe20; PC = 0x555555578343 *)
mov L0x7fffffffbe20 ymm12_0;
mov L0x7fffffffbe28 ymm12_1;
mov L0x7fffffffbe30 ymm12_2;
mov L0x7fffffffbe38 ymm12_3;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x55555557834b *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x1f0(%rbp),%ymm14,%ymm1                #! EA = L0x7fffffffbd60; Value = 0xf666c3286f380b1d; PC = 0x55555557834f *)
xor ymm1_0@uint64 ymm14_0 L0x7fffffffbd60;
xor ymm1_1@uint64 ymm14_1 L0x7fffffffbd68;
xor ymm1_2@uint64 ymm14_2 L0x7fffffffbd70;
xor ymm1_3@uint64 ymm14_3 L0x7fffffffbd78;
(* vpsrlq $0x27,%ymm9,%ymm3                        #! PC = 0x555555578357 *)
shr ymm3_0 ymm9_0 0x27@uint64;
shr ymm3_1 ymm9_1 0x27@uint64;
shr ymm3_2 ymm9_2 0x27@uint64;
shr ymm3_3 ymm9_3 0x27@uint64;
(* vpsllq $0x19,%ymm9,%ymm9                        #! PC = 0x55555557835d *)
shl ymm9_0 ymm9_0 0x19@uint64;
shl ymm9_1 ymm9_1 0x19@uint64;
shl ymm9_2 ymm9_2 0x19@uint64;
shl ymm9_3 ymm9_3 0x19@uint64;
(* vpsrlq $0x3a,%ymm1,%ymm2                        #! PC = 0x555555578363 *)
shr ymm2_0 ymm1_0 0x3a@uint64;
shr ymm2_1 ymm1_1 0x3a@uint64;
shr ymm2_2 ymm1_2 0x3a@uint64;
shr ymm2_3 ymm1_3 0x3a@uint64;
(* vpsllq $0x6,%ymm1,%ymm1                         #! PC = 0x555555578368 *)
shl ymm1_0 ymm1_0 0x6@uint64;
shl ymm1_1 ymm1_1 0x6@uint64;
shl ymm1_2 ymm1_2 0x6@uint64;
shl ymm1_3 ymm1_3 0x6@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x55555557836d *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpor   %ymm3,%ymm9,%ymm2                        #! PC = 0x555555578371 *)
or ymm2_0@uint64 ymm9_0 ymm3_0;
or ymm2_1@uint64 ymm9_1 ymm3_1;
or ymm2_2@uint64 ymm9_2 ymm3_2;
or ymm2_3@uint64 ymm9_3 ymm3_3;
(* vpandn %ymm2,%ymm1,%ymm3                        #! PC = 0x555555578375 *)
not ymm1_0n@uint64 ymm1_0;
and ymm3_0@uint64 ymm1_0n ymm2_0;
not ymm1_1n@uint64 ymm1_1;
and ymm3_1@uint64 ymm1_1n ymm2_1;
not ymm1_2n@uint64 ymm1_2;
and ymm3_2@uint64 ymm1_2n ymm2_2;
not ymm1_3n@uint64 ymm1_3;
and ymm3_3@uint64 ymm1_3n ymm2_3;
(* vpxor  %ymm0,%ymm3,%ymm12                       #! PC = 0x555555578379 *)
xor ymm12_0@uint64 ymm3_0 ymm0_0;
xor ymm12_1@uint64 ymm3_1 ymm0_1;
xor ymm12_2@uint64 ymm3_2 ymm0_2;
xor ymm12_3@uint64 ymm3_3 ymm0_3;
(* vpxor  -0x50(%rbp),%ymm11,%ymm3                 #! EA = L0x7fffffffbf00; Value = 0x50b4ad11e821c365; PC = 0x55555557837d *)
xor ymm3_0@uint64 ymm11_0 L0x7fffffffbf00;
xor ymm3_1@uint64 ymm11_1 L0x7fffffffbf08;
xor ymm3_2@uint64 ymm11_2 L0x7fffffffbf10;
xor ymm3_3@uint64 ymm11_3 L0x7fffffffbf18;
(* vpxor  -0xf0(%rbp),%ymm11,%ymm11                #! EA = L0x7fffffffbe60; Value = 0x908c33b016575e0b; PC = 0x555555578382 *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbe60;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbe68;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbe70;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbe78;
(* vmovdqa %ymm12,-0xd0(%rbp)                      #! EA = L0x7fffffffbe80; PC = 0x55555557838a *)
mov L0x7fffffffbe80 ymm12_0;
mov L0x7fffffffbe88 ymm12_1;
mov L0x7fffffffbe90 ymm12_2;
mov L0x7fffffffbe98 ymm12_3;
(* vpshufb 0x55b65(%rip),%ymm3,%ymm3        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x555555578392 *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm3_0, ymm3_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm3_2, ymm3_3, tmp_2, tmp_3);
mov ymm3_0 tmp_0;
mov ymm3_1 tmp_1;
mov ymm3_2 tmp_2;
mov ymm3_3 tmp_3;
(* vpandn %ymm3,%ymm2,%ymm7                        #! PC = 0x55555557839b *)
not ymm2_0n@uint64 ymm2_0;
and ymm7_0@uint64 ymm2_0n ymm3_0;
not ymm2_1n@uint64 ymm2_1;
and ymm7_1@uint64 ymm2_1n ymm3_1;
not ymm2_2n@uint64 ymm2_2;
and ymm7_2@uint64 ymm2_2n ymm3_2;
not ymm2_3n@uint64 ymm2_3;
and ymm7_3@uint64 ymm2_3n ymm3_3;
(* vpxor  %ymm1,%ymm7,%ymm15                       #! PC = 0x55555557839f *)
xor ymm15_0@uint64 ymm7_0 ymm1_0;
xor ymm15_1@uint64 ymm7_1 ymm1_1;
xor ymm15_2@uint64 ymm7_2 ymm1_2;
xor ymm15_3@uint64 ymm7_3 ymm1_3;
(* vpsrlq $0x2e,%ymm6,%ymm7                        #! PC = 0x5555555783a3 *)
shr ymm7_0 ymm6_0 0x2e@uint64;
shr ymm7_1 ymm6_1 0x2e@uint64;
shr ymm7_2 ymm6_2 0x2e@uint64;
shr ymm7_3 ymm6_3 0x2e@uint64;
(* vpsllq $0x12,%ymm6,%ymm6                        #! PC = 0x5555555783a8 *)
shl ymm6_0 ymm6_0 0x12@uint64;
shl ymm6_1 ymm6_1 0x12@uint64;
shl ymm6_2 ymm6_2 0x12@uint64;
shl ymm6_3 ymm6_3 0x12@uint64;
(* vpor   %ymm7,%ymm6,%ymm12                       #! PC = 0x5555555783ad *)
or ymm12_0@uint64 ymm6_0 ymm7_0;
or ymm12_1@uint64 ymm6_1 ymm7_1;
or ymm12_2@uint64 ymm6_2 ymm7_2;
or ymm12_3@uint64 ymm6_3 ymm7_3;
(* vpxor  -0x1b0(%rbp),%ymm4,%ymm7                 #! EA = L0x7fffffffbda0; Value = 0x40b5bb60943d81fe; PC = 0x5555555783b1 *)
xor ymm7_0@uint64 ymm4_0 L0x7fffffffbda0;
xor ymm7_1@uint64 ymm4_1 L0x7fffffffbda8;
xor ymm7_2@uint64 ymm4_2 L0x7fffffffbdb0;
xor ymm7_3@uint64 ymm4_3 L0x7fffffffbdb8;
(* vpandn %ymm12,%ymm3,%ymm9                       #! PC = 0x5555555783b9 *)
not ymm3_0n@uint64 ymm3_0;
and ymm9_0@uint64 ymm3_0n ymm12_0;
not ymm3_1n@uint64 ymm3_1;
and ymm9_1@uint64 ymm3_1n ymm12_1;
not ymm3_2n@uint64 ymm3_2;
and ymm9_2@uint64 ymm3_2n ymm12_2;
not ymm3_3n@uint64 ymm3_3;
and ymm9_3@uint64 ymm3_3n ymm12_3;
(* vpxor  %ymm2,%ymm9,%ymm9                        #! PC = 0x5555555783be *)
xor ymm9_0@uint64 ymm9_0 ymm2_0;
xor ymm9_1@uint64 ymm9_1 ymm2_1;
xor ymm9_2@uint64 ymm9_2 ymm2_2;
xor ymm9_3@uint64 ymm9_3 ymm2_3;
(* vpandn %ymm0,%ymm12,%ymm2                       #! PC = 0x5555555783c2 *)
not ymm12_0n@uint64 ymm12_0;
and ymm2_0@uint64 ymm12_0n ymm0_0;
not ymm12_1n@uint64 ymm12_1;
and ymm2_1@uint64 ymm12_1n ymm0_1;
not ymm12_2n@uint64 ymm12_2;
and ymm2_2@uint64 ymm12_2n ymm0_2;
not ymm12_3n@uint64 ymm12_3;
and ymm2_3@uint64 ymm12_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x5555555783c6 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm12,%ymm0,%ymm0                       #! PC = 0x5555555783ca *)
xor ymm0_0@uint64 ymm0_0 ymm12_0;
xor ymm0_1@uint64 ymm0_1 ymm12_1;
xor ymm0_2@uint64 ymm0_2 ymm12_2;
xor ymm0_3@uint64 ymm0_3 ymm12_3;
(* vpsrlq $0x25,%ymm10,%ymm1                       #! PC = 0x5555555783cf *)
shr ymm1_0 ymm10_0 0x25@uint64;
shr ymm1_1 ymm10_1 0x25@uint64;
shr ymm1_2 ymm10_2 0x25@uint64;
shr ymm1_3 ymm10_3 0x25@uint64;
(* vpxor  %ymm3,%ymm2,%ymm2                        #! PC = 0x5555555783d5 *)
xor ymm2_0@uint64 ymm2_0 ymm3_0;
xor ymm2_1@uint64 ymm2_1 ymm3_1;
xor ymm2_2@uint64 ymm2_2 ymm3_2;
xor ymm2_3@uint64 ymm2_3 ymm3_3;
(* vpsllq $0x1b,%ymm10,%ymm10                      #! PC = 0x5555555783d9 *)
shl ymm10_0 ymm10_0 0x1b@uint64;
shl ymm10_1 ymm10_1 0x1b@uint64;
shl ymm10_2 ymm10_2 0x1b@uint64;
shl ymm10_3 ymm10_3 0x1b@uint64;
(* vmovdqa %ymm0,-0x110(%rbp)                      #! EA = L0x7fffffffbe40; PC = 0x5555555783df *)
mov L0x7fffffffbe40 ymm0_0;
mov L0x7fffffffbe48 ymm0_1;
mov L0x7fffffffbe50 ymm0_2;
mov L0x7fffffffbe58 ymm0_3;
(* vpsrlq $0x36,%ymm7,%ymm3                        #! PC = 0x5555555783e7 *)
shr ymm3_0 ymm7_0 0x36@uint64;
shr ymm3_1 ymm7_1 0x36@uint64;
shr ymm3_2 ymm7_2 0x36@uint64;
shr ymm3_3 ymm7_3 0x36@uint64;
(* vpor   %ymm1,%ymm10,%ymm0                       #! PC = 0x5555555783ec *)
or ymm0_0@uint64 ymm10_0 ymm1_0;
or ymm0_1@uint64 ymm10_1 ymm1_1;
or ymm0_2@uint64 ymm10_2 ymm1_2;
or ymm0_3@uint64 ymm10_3 ymm1_3;
(* vpsllq $0xa,%ymm7,%ymm7                         #! PC = 0x5555555783f0 *)
shl ymm7_0 ymm7_0 0xa@uint64;
shl ymm7_1 ymm7_1 0xa@uint64;
shl ymm7_2 ymm7_2 0xa@uint64;
shl ymm7_3 ymm7_3 0xa@uint64;
(* vpxor  -0x70(%rbp),%ymm14,%ymm10                #! EA = L0x7fffffffbee0; Value = 0x46230e694915f13e; PC = 0x5555555783f5 *)
xor ymm10_0@uint64 ymm14_0 L0x7fffffffbee0;
xor ymm10_1@uint64 ymm14_1 L0x7fffffffbee8;
xor ymm10_2@uint64 ymm14_2 L0x7fffffffbef0;
xor ymm10_3@uint64 ymm14_3 L0x7fffffffbef8;
(* vmovdqa %ymm2,-0xb0(%rbp)                       #! EA = L0x7fffffffbea0; PC = 0x5555555783fa *)
mov L0x7fffffffbea0 ymm2_0;
mov L0x7fffffffbea8 ymm2_1;
mov L0x7fffffffbeb0 ymm2_2;
mov L0x7fffffffbeb8 ymm2_3;
(* vpxor  -0x290(%rbp),%ymm5,%ymm1                 #! EA = L0x7fffffffbcc0; Value = 0xd2ea3cac414eb63b; PC = 0x555555578402 *)
xor ymm1_0@uint64 ymm5_0 L0x7fffffffbcc0;
xor ymm1_1@uint64 ymm5_1 L0x7fffffffbcc8;
xor ymm1_2@uint64 ymm5_2 L0x7fffffffbcd0;
xor ymm1_3@uint64 ymm5_3 L0x7fffffffbcd8;
(* vpxor  -0x2d0(%rbp),%ymm14,%ymm14               #! EA = L0x7fffffffbc80; Value = 0xd9c51e50969aab69; PC = 0x55555557840a *)
xor ymm14_0@uint64 ymm14_0 L0x7fffffffbc80;
xor ymm14_1@uint64 ymm14_1 L0x7fffffffbc88;
xor ymm14_2@uint64 ymm14_2 L0x7fffffffbc90;
xor ymm14_3@uint64 ymm14_3 L0x7fffffffbc98;
(* vpsrlq $0x1c,%ymm1,%ymm2                        #! PC = 0x555555578412 *)
shr ymm2_0 ymm1_0 0x1c@uint64;
shr ymm2_1 ymm1_1 0x1c@uint64;
shr ymm2_2 ymm1_2 0x1c@uint64;
shr ymm2_3 ymm1_3 0x1c@uint64;
(* vpsllq $0x24,%ymm1,%ymm1                        #! PC = 0x555555578417 *)
shl ymm1_0 ymm1_0 0x24@uint64;
shl ymm1_1 ymm1_1 0x24@uint64;
shl ymm1_2 ymm1_2 0x24@uint64;
shl ymm1_3 ymm1_3 0x24@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x55555557841c *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpor   %ymm3,%ymm7,%ymm2                        #! PC = 0x555555578420 *)
or ymm2_0@uint64 ymm7_0 ymm3_0;
or ymm2_1@uint64 ymm7_1 ymm3_1;
or ymm2_2@uint64 ymm7_2 ymm3_2;
or ymm2_3@uint64 ymm7_3 ymm3_3;
(* vpandn %ymm2,%ymm1,%ymm3                        #! PC = 0x555555578424 *)
not ymm1_0n@uint64 ymm1_0;
and ymm3_0@uint64 ymm1_0n ymm2_0;
not ymm1_1n@uint64 ymm1_1;
and ymm3_1@uint64 ymm1_1n ymm2_1;
not ymm1_2n@uint64 ymm1_2;
and ymm3_2@uint64 ymm1_2n ymm2_2;
not ymm1_3n@uint64 ymm1_3;
and ymm3_3@uint64 ymm1_3n ymm2_3;
(* vpxor  %ymm0,%ymm3,%ymm6                        #! PC = 0x555555578428 *)
xor ymm6_0@uint64 ymm3_0 ymm0_0;
xor ymm6_1@uint64 ymm3_1 ymm0_1;
xor ymm6_2@uint64 ymm3_2 ymm0_2;
xor ymm6_3@uint64 ymm3_3 ymm0_3;
(* vpsrlq $0x31,%ymm10,%ymm3                       #! PC = 0x55555557842c *)
shr ymm3_0 ymm10_0 0x31@uint64;
shr ymm3_1 ymm10_1 0x31@uint64;
shr ymm3_2 ymm10_2 0x31@uint64;
shr ymm3_3 ymm10_3 0x31@uint64;
(* vpsllq $0xf,%ymm10,%ymm10                       #! PC = 0x555555578432 *)
shl ymm10_0 ymm10_0 0xf@uint64;
shl ymm10_1 ymm10_1 0xf@uint64;
shl ymm10_2 ymm10_2 0xf@uint64;
shl ymm10_3 ymm10_3 0xf@uint64;
(* vmovdqa %ymm6,%ymm12                            #! PC = 0x555555578438 *)
mov ymm12_0 ymm6_0;
mov ymm12_1 ymm6_1;
mov ymm12_2 ymm6_2;
mov ymm12_3 ymm6_3;
(* vpor   %ymm3,%ymm10,%ymm10                      #! PC = 0x55555557843c *)
or ymm10_0@uint64 ymm10_0 ymm3_0;
or ymm10_1@uint64 ymm10_1 ymm3_1;
or ymm10_2@uint64 ymm10_2 ymm3_2;
or ymm10_3@uint64 ymm10_3 ymm3_3;
(* vpxor  -0x310(%rbp),%ymm13,%ymm3                #! EA = L0x7fffffffbc40; Value = 0x637658693c47003f; PC = 0x555555578440 *)
xor ymm3_0@uint64 ymm13_0 L0x7fffffffbc40;
xor ymm3_1@uint64 ymm13_1 L0x7fffffffbc48;
xor ymm3_2@uint64 ymm13_2 L0x7fffffffbc50;
xor ymm3_3@uint64 ymm13_3 L0x7fffffffbc58;
(* vpxor  -0x1d0(%rbp),%ymm13,%ymm13               #! EA = L0x7fffffffbd80; Value = 0xc9e47ebe6f50efb2; PC = 0x555555578448 *)
xor ymm13_0@uint64 ymm13_0 L0x7fffffffbd80;
xor ymm13_1@uint64 ymm13_1 L0x7fffffffbd88;
xor ymm13_2@uint64 ymm13_2 L0x7fffffffbd90;
xor ymm13_3@uint64 ymm13_3 L0x7fffffffbd98;
(* vmovdqa %ymm12,-0x310(%rbp)                     #! EA = L0x7fffffffbc40; PC = 0x555555578450 *)
mov L0x7fffffffbc40 ymm12_0;
mov L0x7fffffffbc48 ymm12_1;
mov L0x7fffffffbc50 ymm12_2;
mov L0x7fffffffbc58 ymm12_3;
(* vpandn %ymm10,%ymm2,%ymm7                       #! PC = 0x555555578458 *)
not ymm2_0n@uint64 ymm2_0;
and ymm7_0@uint64 ymm2_0n ymm10_0;
not ymm2_1n@uint64 ymm2_1;
and ymm7_1@uint64 ymm2_1n ymm10_1;
not ymm2_2n@uint64 ymm2_2;
and ymm7_2@uint64 ymm2_2n ymm10_2;
not ymm2_3n@uint64 ymm2_3;
and ymm7_3@uint64 ymm2_3n ymm10_3;
(* vpshufb 0x55a7a(%rip),%ymm3,%ymm3        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x55555557845d *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm3_0, ymm3_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm3_2, ymm3_3, tmp_2, tmp_3);
mov ymm3_0 tmp_0;
mov ymm3_1 tmp_1;
mov ymm3_2 tmp_2;
mov ymm3_3 tmp_3;
(* vpxor  %ymm1,%ymm7,%ymm7                        #! PC = 0x555555578466 *)
xor ymm7_0@uint64 ymm7_0 ymm1_0;
xor ymm7_1@uint64 ymm7_1 ymm1_1;
xor ymm7_2@uint64 ymm7_2 ymm1_2;
xor ymm7_3@uint64 ymm7_3 ymm1_3;
(* vpandn %ymm3,%ymm10,%ymm6                       #! PC = 0x55555557846a *)
not ymm10_0n@uint64 ymm10_0;
and ymm6_0@uint64 ymm10_0n ymm3_0;
not ymm10_1n@uint64 ymm10_1;
and ymm6_1@uint64 ymm10_1n ymm3_1;
not ymm10_2n@uint64 ymm10_2;
and ymm6_2@uint64 ymm10_2n ymm3_2;
not ymm10_3n@uint64 ymm10_3;
and ymm6_3@uint64 ymm10_3n ymm3_3;
(* vpxor  %ymm2,%ymm6,%ymm6                        #! PC = 0x55555557846e *)
xor ymm6_0@uint64 ymm6_0 ymm2_0;
xor ymm6_1@uint64 ymm6_1 ymm2_1;
xor ymm6_2@uint64 ymm6_2 ymm2_2;
xor ymm6_3@uint64 ymm6_3 ymm2_3;
(* vpandn %ymm0,%ymm3,%ymm2                        #! PC = 0x555555578472 *)
not ymm3_0n@uint64 ymm3_0;
and ymm2_0@uint64 ymm3_0n ymm0_0;
not ymm3_1n@uint64 ymm3_1;
and ymm2_1@uint64 ymm3_1n ymm0_1;
not ymm3_2n@uint64 ymm3_2;
and ymm2_2@uint64 ymm3_2n ymm0_2;
not ymm3_3n@uint64 ymm3_3;
and ymm2_3@uint64 ymm3_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x555555578476 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpsrlq $0x2,%ymm14,%ymm1                        #! PC = 0x55555557847a *)
shr ymm1_0 ymm14_0 0x2@uint64;
shr ymm1_1 ymm14_1 0x2@uint64;
shr ymm1_2 ymm14_2 0x2@uint64;
shr ymm1_3 ymm14_3 0x2@uint64;
(* vpsllq $0x3e,%ymm14,%ymm14                      #! PC = 0x555555578480 *)
shl ymm14_0 ymm14_0 0x3e@uint64;
shl ymm14_1 ymm14_1 0x3e@uint64;
shl ymm14_2 ymm14_2 0x3e@uint64;
shl ymm14_3 ymm14_3 0x3e@uint64;
(* vmovdqa %ymm6,-0x70(%rbp)                       #! EA = L0x7fffffffbee0; PC = 0x555555578486 *)
mov L0x7fffffffbee0 ymm6_0;
mov L0x7fffffffbee8 ymm6_1;
mov L0x7fffffffbef0 ymm6_2;
mov L0x7fffffffbef8 ymm6_3;
(* vpxor  %ymm3,%ymm0,%ymm6                        #! PC = 0x55555557848b *)
xor ymm6_0@uint64 ymm0_0 ymm3_0;
xor ymm6_1@uint64 ymm0_1 ymm3_1;
xor ymm6_2@uint64 ymm0_2 ymm3_2;
xor ymm6_3@uint64 ymm0_3 ymm3_3;
(* vpor   %ymm1,%ymm14,%ymm3                       #! PC = 0x55555557848f *)
or ymm3_0@uint64 ymm14_0 ymm1_0;
or ymm3_1@uint64 ymm14_1 ymm1_1;
or ymm3_2@uint64 ymm14_2 ymm1_2;
or ymm3_3@uint64 ymm14_3 ymm1_3;
(* vpsrlq $0x9,%ymm13,%ymm0                        #! PC = 0x555555578493 *)
shr ymm0_0 ymm13_0 0x9@uint64;
shr ymm0_1 ymm13_1 0x9@uint64;
shr ymm0_2 ymm13_2 0x9@uint64;
shr ymm0_3 ymm13_3 0x9@uint64;
(* vmovdqa %ymm6,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x555555578499 *)
mov L0x7fffffffbf00 ymm6_0;
mov L0x7fffffffbf08 ymm6_1;
mov L0x7fffffffbf10 ymm6_2;
mov L0x7fffffffbf18 ymm6_3;
(* vpxor  %ymm10,%ymm2,%ymm10                      #! PC = 0x55555557849e *)
xor ymm10_0@uint64 ymm2_0 ymm10_0;
xor ymm10_1@uint64 ymm2_1 ymm10_1;
xor ymm10_2@uint64 ymm2_2 ymm10_2;
xor ymm10_3@uint64 ymm2_3 ymm10_3;
(* vpsrlq $0x19,%ymm11,%ymm1                       #! PC = 0x5555555784a3 *)
shr ymm1_0 ymm11_0 0x19@uint64;
shr ymm1_1 ymm11_1 0x19@uint64;
shr ymm1_2 ymm11_2 0x19@uint64;
shr ymm1_3 ymm11_3 0x19@uint64;
(* vpsllq $0x37,%ymm13,%ymm13                      #! PC = 0x5555555784a9 *)
shl ymm13_0 ymm13_0 0x37@uint64;
shl ymm13_1 ymm13_1 0x37@uint64;
shl ymm13_2 ymm13_2 0x37@uint64;
shl ymm13_3 ymm13_3 0x37@uint64;
(* vpsllq $0x27,%ymm11,%ymm11                      #! PC = 0x5555555784af *)
shl ymm11_0 ymm11_0 0x27@uint64;
shl ymm11_1 ymm11_1 0x27@uint64;
shl ymm11_2 ymm11_2 0x27@uint64;
shl ymm11_3 ymm11_3 0x27@uint64;
(* vpor   %ymm0,%ymm13,%ymm13                      #! PC = 0x5555555784b5 *)
or ymm13_0@uint64 ymm13_0 ymm0_0;
or ymm13_1@uint64 ymm13_1 ymm0_1;
or ymm13_2@uint64 ymm13_2 ymm0_2;
or ymm13_3@uint64 ymm13_3 ymm0_3;
(* vmovdqa -0xd0(%rbp),%ymm0                       #! EA = L0x7fffffffbe80; Value = 0x7e1ba84748e80bee; PC = 0x5555555784b9 *)
mov ymm0_0 L0x7fffffffbe80;
mov ymm0_1 L0x7fffffffbe88;
mov ymm0_2 L0x7fffffffbe90;
mov ymm0_3 L0x7fffffffbe98;
(* vpxor  -0x230(%rbp),%ymm0,%ymm0                 #! EA = L0x7fffffffbd20; Value = 0x2dfca2b3ca5eda71; PC = 0x5555555784c1 *)
xor ymm0_0@uint64 ymm0_0 L0x7fffffffbd20;
xor ymm0_1@uint64 ymm0_1 L0x7fffffffbd28;
xor ymm0_2@uint64 ymm0_2 L0x7fffffffbd30;
xor ymm0_3@uint64 ymm0_3 L0x7fffffffbd38;
(* vpor   %ymm1,%ymm11,%ymm11                      #! PC = 0x5555555784c9 *)
or ymm11_0@uint64 ymm11_0 ymm1_0;
or ymm11_1@uint64 ymm11_1 ymm1_1;
or ymm11_2@uint64 ymm11_2 ymm1_2;
or ymm11_3@uint64 ymm11_3 ymm1_3;
(* vpxor  -0x330(%rbp),%ymm5,%ymm5                 #! EA = L0x7fffffffbc20; Value = 0xf135e2a8960f74e2; PC = 0x5555555784cd *)
xor ymm5_0@uint64 ymm5_0 L0x7fffffffbc20;
xor ymm5_1@uint64 ymm5_1 L0x7fffffffbc28;
xor ymm5_2@uint64 ymm5_2 L0x7fffffffbc30;
xor ymm5_3@uint64 ymm5_3 L0x7fffffffbc38;
(* vpxor  -0x350(%rbp),%ymm4,%ymm4                 #! EA = L0x7fffffffbc00; Value = 0x29ff28d8da171496; PC = 0x5555555784d5 *)
xor ymm4_0@uint64 ymm4_0 L0x7fffffffbc00;
xor ymm4_1@uint64 ymm4_1 L0x7fffffffbc08;
xor ymm4_2@uint64 ymm4_2 L0x7fffffffbc10;
xor ymm4_3@uint64 ymm4_3 L0x7fffffffbc18;
(* vmovdqa %ymm15,-0x330(%rbp)                     #! EA = L0x7fffffffbc20; PC = 0x5555555784dd *)
mov L0x7fffffffbc20 ymm15_0;
mov L0x7fffffffbc28 ymm15_1;
mov L0x7fffffffbc30 ymm15_2;
mov L0x7fffffffbc38 ymm15_3;
(* vpandn %ymm11,%ymm13,%ymm6                      #! PC = 0x5555555784e5 *)
not ymm13_0n@uint64 ymm13_0;
and ymm6_0@uint64 ymm13_0n ymm11_0;
not ymm13_1n@uint64 ymm13_1;
and ymm6_1@uint64 ymm13_1n ymm11_1;
not ymm13_2n@uint64 ymm13_2;
and ymm6_2@uint64 ymm13_2n ymm11_2;
not ymm13_3n@uint64 ymm13_3;
and ymm6_3@uint64 ymm13_3n ymm11_3;
(* vpxor  %ymm3,%ymm6,%ymm6                        #! PC = 0x5555555784ea *)
xor ymm6_0@uint64 ymm6_0 ymm3_0;
xor ymm6_1@uint64 ymm6_1 ymm3_1;
xor ymm6_2@uint64 ymm6_2 ymm3_2;
xor ymm6_3@uint64 ymm6_3 ymm3_3;
(* vpsrlq $0x3e,%ymm4,%ymm1                        #! PC = 0x5555555784ee *)
shr ymm1_0 ymm4_0 0x3e@uint64;
shr ymm1_1 ymm4_1 0x3e@uint64;
shr ymm1_2 ymm4_2 0x3e@uint64;
shr ymm1_3 ymm4_3 0x3e@uint64;
(* vpxor  %ymm12,%ymm6,%ymm2                       #! PC = 0x5555555784f3 *)
xor ymm2_0@uint64 ymm6_0 ymm12_0;
xor ymm2_1@uint64 ymm6_1 ymm12_1;
xor ymm2_2@uint64 ymm6_2 ymm12_2;
xor ymm2_3@uint64 ymm6_3 ymm12_3;
(* vpsllq $0x2,%ymm4,%ymm4                         #! PC = 0x5555555784f8 *)
shl ymm4_0 ymm4_0 0x2@uint64;
shl ymm4_1 ymm4_1 0x2@uint64;
shl ymm4_2 ymm4_2 0x2@uint64;
shl ymm4_3 ymm4_3 0x2@uint64;
(* vpxor  %ymm0,%ymm2,%ymm2                        #! PC = 0x5555555784fd *)
xor ymm2_0@uint64 ymm2_0 ymm0_0;
xor ymm2_1@uint64 ymm2_1 ymm0_1;
xor ymm2_2@uint64 ymm2_2 ymm0_2;
xor ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x17,%ymm5,%ymm0                        #! PC = 0x555555578501 *)
shr ymm0_0 ymm5_0 0x17@uint64;
shr ymm0_1 ymm5_1 0x17@uint64;
shr ymm0_2 ymm5_2 0x17@uint64;
shr ymm0_3 ymm5_3 0x17@uint64;
(* vpxor  -0x90(%rbp),%ymm2,%ymm2                  #! EA = L0x7fffffffbec0; Value = 0x0294d0853009a2e9; PC = 0x555555578506 *)
xor ymm2_0@uint64 ymm2_0 L0x7fffffffbec0;
xor ymm2_1@uint64 ymm2_1 L0x7fffffffbec8;
xor ymm2_2@uint64 ymm2_2 L0x7fffffffbed0;
xor ymm2_3@uint64 ymm2_3 L0x7fffffffbed8;
(* vpsllq $0x29,%ymm5,%ymm5                        #! PC = 0x55555557850e *)
shl ymm5_0 ymm5_0 0x29@uint64;
shl ymm5_1 ymm5_1 0x29@uint64;
shl ymm5_2 ymm5_2 0x29@uint64;
shl ymm5_3 ymm5_3 0x29@uint64;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x555555578513 *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpandn %ymm5,%ymm11,%ymm0                       #! PC = 0x555555578517 *)
not ymm11_0n@uint64 ymm11_0;
and ymm0_0@uint64 ymm11_0n ymm5_0;
not ymm11_1n@uint64 ymm11_1;
and ymm0_1@uint64 ymm11_1n ymm5_1;
not ymm11_2n@uint64 ymm11_2;
and ymm0_2@uint64 ymm11_2n ymm5_2;
not ymm11_3n@uint64 ymm11_3;
and ymm0_3@uint64 ymm11_3n ymm5_3;
(* vpxor  %ymm13,%ymm0,%ymm14                      #! PC = 0x55555557851b *)
xor ymm14_0@uint64 ymm0_0 ymm13_0;
xor ymm14_1@uint64 ymm0_1 ymm13_1;
xor ymm14_2@uint64 ymm0_2 ymm13_2;
xor ymm14_3@uint64 ymm0_3 ymm13_3;
(* vpxor  -0x190(%rbp),%ymm8,%ymm0                 #! EA = L0x7fffffffbdc0; Value = 0x6d4b92a0afd4a76b; PC = 0x555555578520 *)
xor ymm0_0@uint64 ymm8_0 L0x7fffffffbdc0;
xor ymm0_1@uint64 ymm8_1 L0x7fffffffbdc8;
xor ymm0_2@uint64 ymm8_2 L0x7fffffffbdd0;
xor ymm0_3@uint64 ymm8_3 L0x7fffffffbdd8;
(* vmovdqa %ymm14,%ymm12                           #! PC = 0x555555578528 *)
mov ymm12_0 ymm14_0;
mov ymm12_1 ymm14_1;
mov ymm12_2 ymm14_2;
mov ymm12_3 ymm14_3;
(* vpxor  %ymm15,%ymm7,%ymm14                      #! PC = 0x55555557852d *)
xor ymm14_0@uint64 ymm7_0 ymm15_0;
xor ymm14_1@uint64 ymm7_1 ymm15_1;
xor ymm14_2@uint64 ymm7_2 ymm15_2;
xor ymm14_3@uint64 ymm7_3 ymm15_3;
(* vpxor  -0x70(%rbp),%ymm9,%ymm15                 #! EA = L0x7fffffffbee0; Value = 0x70433fd50a2fca56; PC = 0x555555578532 *)
xor ymm15_0@uint64 ymm9_0 L0x7fffffffbee0;
xor ymm15_1@uint64 ymm9_1 L0x7fffffffbee8;
xor ymm15_2@uint64 ymm9_2 L0x7fffffffbef0;
xor ymm15_3@uint64 ymm9_3 L0x7fffffffbef8;
(* vpxor  %ymm0,%ymm14,%ymm14                      #! PC = 0x555555578537 *)
xor ymm14_0@uint64 ymm14_0 ymm0_0;
xor ymm14_1@uint64 ymm14_1 ymm0_1;
xor ymm14_2@uint64 ymm14_2 ymm0_2;
xor ymm14_3@uint64 ymm14_3 ymm0_3;
(* vpor   %ymm1,%ymm4,%ymm0                        #! PC = 0x55555557853b *)
or ymm0_0@uint64 ymm4_0 ymm1_0;
or ymm0_1@uint64 ymm4_1 ymm1_1;
or ymm0_2@uint64 ymm4_2 ymm1_2;
or ymm0_3@uint64 ymm4_3 ymm1_3;
(* vmovdqa %ymm12,-0x2f0(%rbp)                     #! EA = L0x7fffffffbc60; PC = 0x55555557853f *)
mov L0x7fffffffbc60 ymm12_0;
mov L0x7fffffffbc68 ymm12_1;
mov L0x7fffffffbc70 ymm12_2;
mov L0x7fffffffbc78 ymm12_3;
(* vpandn %ymm13,%ymm3,%ymm4                       #! PC = 0x555555578547 *)
not ymm3_0n@uint64 ymm3_0;
and ymm4_0@uint64 ymm3_0n ymm13_0;
not ymm3_1n@uint64 ymm3_1;
and ymm4_1@uint64 ymm3_1n ymm13_1;
not ymm3_2n@uint64 ymm3_2;
and ymm4_2@uint64 ymm3_2n ymm13_2;
not ymm3_3n@uint64 ymm3_3;
and ymm4_3@uint64 ymm3_3n ymm13_3;
(* vpandn %ymm0,%ymm5,%ymm1                        #! PC = 0x55555557854c *)
not ymm5_0n@uint64 ymm5_0;
and ymm1_0@uint64 ymm5_0n ymm0_0;
not ymm5_1n@uint64 ymm5_1;
and ymm1_1@uint64 ymm5_1n ymm0_1;
not ymm5_2n@uint64 ymm5_2;
and ymm1_2@uint64 ymm5_2n ymm0_2;
not ymm5_3n@uint64 ymm5_3;
and ymm1_3@uint64 ymm5_3n ymm0_3;
(* vpxor  %ymm12,%ymm14,%ymm14                     #! PC = 0x555555578550 *)
xor ymm14_0@uint64 ymm14_0 ymm12_0;
xor ymm14_1@uint64 ymm14_1 ymm12_1;
xor ymm14_2@uint64 ymm14_2 ymm12_2;
xor ymm14_3@uint64 ymm14_3 ymm12_3;
(* vpxor  %ymm0,%ymm4,%ymm4                        #! PC = 0x555555578555 *)
xor ymm4_0@uint64 ymm4_0 ymm0_0;
xor ymm4_1@uint64 ymm4_1 ymm0_1;
xor ymm4_2@uint64 ymm4_2 ymm0_2;
xor ymm4_3@uint64 ymm4_3 ymm0_3;
(* vmovdqa -0x130(%rbp),%ymm13                     #! EA = L0x7fffffffbe20; Value = 0xc54f3e2ef42a032e; PC = 0x555555578559 *)
mov ymm13_0 L0x7fffffffbe20;
mov ymm13_1 L0x7fffffffbe28;
mov ymm13_2 L0x7fffffffbe30;
mov ymm13_3 L0x7fffffffbe38;
(* vpxor  %ymm11,%ymm1,%ymm11                      #! PC = 0x555555578561 *)
xor ymm11_0@uint64 ymm1_0 ymm11_0;
xor ymm11_1@uint64 ymm1_1 ymm11_1;
xor ymm11_2@uint64 ymm1_2 ymm11_2;
xor ymm11_3@uint64 ymm1_3 ymm11_3;
(* vmovdqa -0x150(%rbp),%ymm1                      #! EA = L0x7fffffffbe00; Value = 0x780c883422dea4d0; PC = 0x555555578566 *)
mov ymm1_0 L0x7fffffffbe00;
mov ymm1_1 L0x7fffffffbe08;
mov ymm1_2 L0x7fffffffbe10;
mov ymm1_3 L0x7fffffffbe18;
(* vpxor  -0x270(%rbp),%ymm1,%ymm1                 #! EA = L0x7fffffffbce0; Value = 0x86a9931dbbbe2820; PC = 0x55555557856e *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbce0;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbce8;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbcf0;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbcf8;
(* vpxor  %ymm1,%ymm15,%ymm15                      #! PC = 0x555555578576 *)
xor ymm15_0@uint64 ymm15_0 ymm1_0;
xor ymm15_1@uint64 ymm15_1 ymm1_1;
xor ymm15_2@uint64 ymm15_2 ymm1_2;
xor ymm15_3@uint64 ymm15_3 ymm1_3;
(* vpandn %ymm3,%ymm0,%ymm1                        #! PC = 0x55555557857a *)
not ymm0_0n@uint64 ymm0_0;
and ymm1_0@uint64 ymm0_0n ymm3_0;
not ymm0_1n@uint64 ymm0_1;
and ymm1_1@uint64 ymm0_1n ymm3_1;
not ymm0_2n@uint64 ymm0_2;
and ymm1_2@uint64 ymm0_2n ymm3_2;
not ymm0_3n@uint64 ymm0_3;
and ymm1_3@uint64 ymm0_3n ymm3_3;
(* vpxor  -0x250(%rbp),%ymm13,%ymm0                #! EA = L0x7fffffffbd00; Value = 0x322c45cc79b07a6c; PC = 0x55555557857e *)
xor ymm0_0@uint64 ymm13_0 L0x7fffffffbd00;
xor ymm0_1@uint64 ymm13_1 L0x7fffffffbd08;
xor ymm0_2@uint64 ymm13_2 L0x7fffffffbd10;
xor ymm0_3@uint64 ymm13_3 L0x7fffffffbd18;
(* vpxor  %ymm5,%ymm1,%ymm5                        #! PC = 0x555555578586 *)
xor ymm5_0@uint64 ymm1_0 ymm5_0;
xor ymm5_1@uint64 ymm1_1 ymm5_1;
xor ymm5_2@uint64 ymm1_2 ymm5_2;
xor ymm5_3@uint64 ymm1_3 ymm5_3;
(* vmovdqa -0x210(%rbp),%ymm1                      #! EA = L0x7fffffffbd40; Value = 0x3cefe3bf645dc61f; PC = 0x55555557858a *)
mov ymm1_0 L0x7fffffffbd40;
mov ymm1_1 L0x7fffffffbd48;
mov ymm1_2 L0x7fffffffbd50;
mov ymm1_3 L0x7fffffffbd58;
(* vpxor  %ymm11,%ymm15,%ymm15                     #! PC = 0x555555578592 *)
xor ymm15_0@uint64 ymm15_0 ymm11_0;
xor ymm15_1@uint64 ymm15_1 ymm11_1;
xor ymm15_2@uint64 ymm15_2 ymm11_2;
xor ymm15_3@uint64 ymm15_3 ymm11_3;
(* vpxor  -0x170(%rbp),%ymm1,%ymm1                 #! EA = L0x7fffffffbde0; Value = 0x5c3ce9b0ea86020e; PC = 0x555555578597 *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbde0;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbde8;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbdf0;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbdf8;
(* vpxor  %ymm5,%ymm10,%ymm12                      #! PC = 0x55555557859f *)
xor ymm12_0@uint64 ymm10_0 ymm5_0;
xor ymm12_1@uint64 ymm10_1 ymm5_1;
xor ymm12_2@uint64 ymm10_2 ymm5_2;
xor ymm12_3@uint64 ymm10_3 ymm5_3;
(* vmovdqa %ymm5,-0x290(%rbp)                      #! EA = L0x7fffffffbcc0; PC = 0x5555555785a3 *)
mov L0x7fffffffbcc0 ymm5_0;
mov L0x7fffffffbcc8 ymm5_1;
mov L0x7fffffffbcd0 ymm5_2;
mov L0x7fffffffbcd8 ymm5_3;
(* vpsllq $0x1,%ymm14,%ymm5                        #! PC = 0x5555555785ab *)
shl ymm5_0 ymm14_0 0x1@uint64;
shl ymm5_1 ymm14_1 0x1@uint64;
shl ymm5_2 ymm14_2 0x1@uint64;
shl ymm5_3 ymm14_3 0x1@uint64;
(* vpxor  %ymm1,%ymm12,%ymm12                      #! PC = 0x5555555785b1 *)
xor ymm12_0@uint64 ymm12_0 ymm1_0;
xor ymm12_1@uint64 ymm12_1 ymm1_1;
xor ymm12_2@uint64 ymm12_2 ymm1_2;
xor ymm12_3@uint64 ymm12_3 ymm1_3;
(* vpxor  -0x110(%rbp),%ymm4,%ymm1                 #! EA = L0x7fffffffbe40; Value = 0xdc8e9ad663eb67f4; PC = 0x5555555785b5 *)
xor ymm1_0@uint64 ymm4_0 L0x7fffffffbe40;
xor ymm1_1@uint64 ymm4_1 L0x7fffffffbe48;
xor ymm1_2@uint64 ymm4_2 L0x7fffffffbe50;
xor ymm1_3@uint64 ymm4_3 L0x7fffffffbe58;
(* vpxor  -0xb0(%rbp),%ymm12,%ymm12                #! EA = L0x7fffffffbea0; Value = 0x8e5c9db63e29f72a; PC = 0x5555555785bd *)
xor ymm12_0@uint64 ymm12_0 L0x7fffffffbea0;
xor ymm12_1@uint64 ymm12_1 L0x7fffffffbea8;
xor ymm12_2@uint64 ymm12_2 L0x7fffffffbeb0;
xor ymm12_3@uint64 ymm12_3 L0x7fffffffbeb8;
(* vpsllq $0x1,%ymm15,%ymm3                        #! PC = 0x5555555785c5 *)
shl ymm3_0 ymm15_0 0x1@uint64;
shl ymm3_1 ymm15_1 0x1@uint64;
shl ymm3_2 ymm15_2 0x1@uint64;
shl ymm3_3 ymm15_3 0x1@uint64;
(* vpxor  %ymm0,%ymm1,%ymm1                        #! PC = 0x5555555785cb *)
xor ymm1_0@uint64 ymm1_0 ymm0_0;
xor ymm1_1@uint64 ymm1_1 ymm0_1;
xor ymm1_2@uint64 ymm1_2 ymm0_2;
xor ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3f,%ymm14,%ymm0                       #! PC = 0x5555555785cf *)
shr ymm0_0 ymm14_0 0x3f@uint64;
shr ymm0_1 ymm14_1 0x3f@uint64;
shr ymm0_2 ymm14_2 0x3f@uint64;
shr ymm0_3 ymm14_3 0x3f@uint64;
(* vpxor  -0x50(%rbp),%ymm1,%ymm1                  #! EA = L0x7fffffffbf00; Value = 0xcf709311541f6f05; PC = 0x5555555785d5 *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbf00;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbf08;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbf10;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbf18;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x5555555785da *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpsrlq $0x3f,%ymm15,%ymm0                       #! PC = 0x5555555785de *)
shr ymm0_0 ymm15_0 0x3f@uint64;
shr ymm0_1 ymm15_1 0x3f@uint64;
shr ymm0_2 ymm15_2 0x3f@uint64;
shr ymm0_3 ymm15_3 0x3f@uint64;
(* vpsrlq $0x3f,%ymm12,%ymm13                      #! PC = 0x5555555785e4 *)
shr ymm13_0 ymm12_0 0x3f@uint64;
shr ymm13_1 ymm12_1 0x3f@uint64;
shr ymm13_2 ymm12_2 0x3f@uint64;
shr ymm13_3 ymm12_3 0x3f@uint64;
(* vpor   %ymm0,%ymm3,%ymm3                        #! PC = 0x5555555785ea *)
or ymm3_0@uint64 ymm3_0 ymm0_0;
or ymm3_1@uint64 ymm3_1 ymm0_1;
or ymm3_2@uint64 ymm3_2 ymm0_2;
or ymm3_3@uint64 ymm3_3 ymm0_3;
(* vpxor  %ymm1,%ymm5,%ymm5                        #! PC = 0x5555555785ee *)
xor ymm5_0@uint64 ymm5_0 ymm1_0;
xor ymm5_1@uint64 ymm5_1 ymm1_1;
xor ymm5_2@uint64 ymm5_2 ymm1_2;
xor ymm5_3@uint64 ymm5_3 ymm1_3;
(* vpsllq $0x1,%ymm12,%ymm0                        #! PC = 0x5555555785f2 *)
shl ymm0_0 ymm12_0 0x1@uint64;
shl ymm0_1 ymm12_1 0x1@uint64;
shl ymm0_2 ymm12_2 0x1@uint64;
shl ymm0_3 ymm12_3 0x1@uint64;
(* vpxor  %ymm2,%ymm3,%ymm3                        #! PC = 0x5555555785f8 *)
xor ymm3_0@uint64 ymm3_0 ymm2_0;
xor ymm3_1@uint64 ymm3_1 ymm2_1;
xor ymm3_2@uint64 ymm3_2 ymm2_2;
xor ymm3_3@uint64 ymm3_3 ymm2_3;
(* vpxor  %ymm6,%ymm5,%ymm6                        #! PC = 0x5555555785fc *)
xor ymm6_0@uint64 ymm5_0 ymm6_0;
xor ymm6_1@uint64 ymm5_1 ymm6_1;
xor ymm6_2@uint64 ymm5_2 ymm6_2;
xor ymm6_3@uint64 ymm5_3 ymm6_3;
(* vpor   %ymm13,%ymm0,%ymm0                       #! PC = 0x555555578600 *)
or ymm0_0@uint64 ymm0_0 ymm13_0;
or ymm0_1@uint64 ymm0_1 ymm13_1;
or ymm0_2@uint64 ymm0_2 ymm13_2;
or ymm0_3@uint64 ymm0_3 ymm13_3;
(* vpxor  %ymm8,%ymm3,%ymm8                        #! PC = 0x555555578605 *)
xor ymm8_0@uint64 ymm3_0 ymm8_0;
xor ymm8_1@uint64 ymm3_1 ymm8_1;
xor ymm8_2@uint64 ymm3_2 ymm8_2;
xor ymm8_3@uint64 ymm3_3 ymm8_3;
(* vmovq  %r8,%xmm13                               #! PC = 0x55555557860a *)
mov xmm13_0 r8;
mov xmm13_1 0@uint64;
(* vpxor  %ymm14,%ymm0,%ymm14                      #! PC = 0x55555557860f *)
xor ymm14_0@uint64 ymm0_0 ymm14_0;
xor ymm14_1@uint64 ymm0_1 ymm14_1;
xor ymm14_2@uint64 ymm0_2 ymm14_2;
xor ymm14_3@uint64 ymm0_3 ymm14_3;
(* vpsrlq $0x3f,%ymm1,%ymm0                        #! PC = 0x555555578614 *)
shr ymm0_0 ymm1_0 0x3f@uint64;
shr ymm0_1 ymm1_1 0x3f@uint64;
shr ymm0_2 ymm1_2 0x3f@uint64;
shr ymm0_3 ymm1_3 0x3f@uint64;
(* vpxor  %ymm7,%ymm3,%ymm7                        #! PC = 0x555555578619 *)
xor ymm7_0@uint64 ymm3_0 ymm7_0;
xor ymm7_1@uint64 ymm3_1 ymm7_1;
xor ymm7_2@uint64 ymm3_2 ymm7_2;
xor ymm7_3@uint64 ymm3_3 ymm7_3;
(* vpsllq $0x1,%ymm1,%ymm1                         #! PC = 0x55555557861d *)
shl ymm1_0 ymm1_0 0x1@uint64;
shl ymm1_1 ymm1_1 0x1@uint64;
shl ymm1_2 ymm1_2 0x1@uint64;
shl ymm1_3 ymm1_3 0x1@uint64;
(* vpxor  %ymm9,%ymm14,%ymm9                       #! PC = 0x555555578622 *)
xor ymm9_0@uint64 ymm14_0 ymm9_0;
xor ymm9_1@uint64 ymm14_1 ymm9_1;
xor ymm9_2@uint64 ymm14_2 ymm9_2;
xor ymm9_3@uint64 ymm14_3 ymm9_3;
(* vpxor  %ymm11,%ymm14,%ymm11                     #! PC = 0x555555578627 *)
xor ymm11_0@uint64 ymm14_0 ymm11_0;
xor ymm11_1@uint64 ymm14_1 ymm11_1;
xor ymm11_2@uint64 ymm14_2 ymm11_2;
xor ymm11_3@uint64 ymm14_3 ymm11_3;
(* vpor   %ymm0,%ymm1,%ymm1                        #! PC = 0x55555557862c *)
or ymm1_0@uint64 ymm1_0 ymm0_0;
or ymm1_1@uint64 ymm1_1 ymm0_1;
or ymm1_2@uint64 ymm1_2 ymm0_2;
or ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3f,%ymm2,%ymm0                        #! PC = 0x555555578630 *)
shr ymm0_0 ymm2_0 0x3f@uint64;
shr ymm0_1 ymm2_1 0x3f@uint64;
shr ymm0_2 ymm2_2 0x3f@uint64;
shr ymm0_3 ymm2_3 0x3f@uint64;
(* vpsllq $0x1,%ymm2,%ymm2                         #! PC = 0x555555578635 *)
shl ymm2_0 ymm2_0 0x1@uint64;
shl ymm2_1 ymm2_1 0x1@uint64;
shl ymm2_2 ymm2_2 0x1@uint64;
shl ymm2_3 ymm2_3 0x1@uint64;
(* vpxor  %ymm15,%ymm1,%ymm15                      #! PC = 0x55555557863a *)
xor ymm15_0@uint64 ymm1_0 ymm15_0;
xor ymm15_1@uint64 ymm1_1 ymm15_1;
xor ymm15_2@uint64 ymm1_2 ymm15_2;
xor ymm15_3@uint64 ymm1_3 ymm15_3;
(* vpxor  -0x90(%rbp),%ymm5,%ymm1                  #! EA = L0x7fffffffbec0; Value = 0x0294d0853009a2e9; PC = 0x55555557863f *)
xor ymm1_0@uint64 ymm5_0 L0x7fffffffbec0;
xor ymm1_1@uint64 ymm5_1 L0x7fffffffbec8;
xor ymm1_2@uint64 ymm5_2 L0x7fffffffbed0;
xor ymm1_3@uint64 ymm5_3 L0x7fffffffbed8;
(* vpor   %ymm0,%ymm2,%ymm2                        #! PC = 0x555555578647 *)
or ymm2_0@uint64 ymm2_0 ymm0_0;
or ymm2_1@uint64 ymm2_1 ymm0_1;
or ymm2_2@uint64 ymm2_2 ymm0_2;
or ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x14,%ymm8,%ymm0                        #! PC = 0x55555557864b *)
shr ymm0_0 ymm8_0 0x14@uint64;
shr ymm0_1 ymm8_1 0x14@uint64;
shr ymm0_2 ymm8_2 0x14@uint64;
shr ymm0_3 ymm8_3 0x14@uint64;
(* vpxor  %ymm10,%ymm15,%ymm10                     #! PC = 0x555555578651 *)
xor ymm10_0@uint64 ymm15_0 ymm10_0;
xor ymm10_1@uint64 ymm15_1 ymm10_1;
xor ymm10_2@uint64 ymm15_2 ymm10_2;
xor ymm10_3@uint64 ymm15_3 ymm10_3;
(* vpsllq $0x2c,%ymm8,%ymm8                        #! PC = 0x555555578656 *)
shl ymm8_0 ymm8_0 0x2c@uint64;
shl ymm8_1 ymm8_1 0x2c@uint64;
shl ymm8_2 ymm8_2 0x2c@uint64;
shl ymm8_3 ymm8_3 0x2c@uint64;
(* vpxor  %ymm12,%ymm2,%ymm12                      #! PC = 0x55555557865c *)
xor ymm12_0@uint64 ymm2_0 ymm12_0;
xor ymm12_1@uint64 ymm2_1 ymm12_1;
xor ymm12_2@uint64 ymm2_2 ymm12_2;
xor ymm12_3@uint64 ymm2_3 ymm12_3;
(* vpor   %ymm0,%ymm8,%ymm8                        #! PC = 0x555555578661 *)
or ymm8_0@uint64 ymm8_0 ymm0_0;
or ymm8_1@uint64 ymm8_1 ymm0_1;
or ymm8_2@uint64 ymm8_2 ymm0_2;
or ymm8_3@uint64 ymm8_3 ymm0_3;
(* vpsrlq $0x15,%ymm9,%ymm0                        #! PC = 0x555555578665 *)
shr ymm0_0 ymm9_0 0x15@uint64;
shr ymm0_1 ymm9_1 0x15@uint64;
shr ymm0_2 ymm9_2 0x15@uint64;
shr ymm0_3 ymm9_3 0x15@uint64;
(* vpxor  %ymm4,%ymm12,%ymm4                       #! PC = 0x55555557866b *)
xor ymm4_0@uint64 ymm12_0 ymm4_0;
xor ymm4_1@uint64 ymm12_1 ymm4_1;
xor ymm4_2@uint64 ymm12_2 ymm4_2;
xor ymm4_3@uint64 ymm12_3 ymm4_3;
(* vpsllq $0x2b,%ymm9,%ymm9                        #! PC = 0x55555557866f *)
shl ymm9_0 ymm9_0 0x2b@uint64;
shl ymm9_1 ymm9_1 0x2b@uint64;
shl ymm9_2 ymm9_2 0x2b@uint64;
shl ymm9_3 ymm9_3 0x2b@uint64;
(* vpor   %ymm0,%ymm9,%ymm9                        #! PC = 0x555555578675 *)
or ymm9_0@uint64 ymm9_0 ymm0_0;
or ymm9_1@uint64 ymm9_1 ymm0_1;
or ymm9_2@uint64 ymm9_2 ymm0_2;
or ymm9_3@uint64 ymm9_3 ymm0_3;
(* vpbroadcastq %xmm13,%ymm0                       #! PC = 0x555555578679 *)
mov ymm0_0 xmm13_0;
mov ymm0_1 xmm13_0;
mov ymm0_2 xmm13_0;
mov ymm0_3 xmm13_0;
(* vpandn %ymm9,%ymm8,%ymm2                        #! PC = 0x55555557867e *)
not ymm8_0n@uint64 ymm8_0;
and ymm2_0@uint64 ymm8_0n ymm9_0;
not ymm8_1n@uint64 ymm8_1;
and ymm2_1@uint64 ymm8_1n ymm9_1;
not ymm8_2n@uint64 ymm8_2;
and ymm2_2@uint64 ymm8_2n ymm9_2;
not ymm8_3n@uint64 ymm8_3;
and ymm2_3@uint64 ymm8_3n ymm9_3;
(* vpxor  %ymm2,%ymm0,%ymm0                        #! PC = 0x555555578683 *)
xor ymm0_0@uint64 ymm0_0 ymm2_0;
xor ymm0_1@uint64 ymm0_1 ymm2_1;
xor ymm0_2@uint64 ymm0_2 ymm2_2;
xor ymm0_3@uint64 ymm0_3 ymm2_3;
(* vpxor  %ymm1,%ymm0,%ymm2                        #! PC = 0x555555578687 *)
xor ymm2_0@uint64 ymm0_0 ymm1_0;
xor ymm2_1@uint64 ymm0_1 ymm1_1;
xor ymm2_2@uint64 ymm0_2 ymm1_2;
xor ymm2_3@uint64 ymm0_3 ymm1_3;
(* vpsrlq $0x2b,%ymm10,%ymm0                       #! PC = 0x55555557868b *)
shr ymm0_0 ymm10_0 0x2b@uint64;
shr ymm0_1 ymm10_1 0x2b@uint64;
shr ymm0_2 ymm10_2 0x2b@uint64;
shr ymm0_3 ymm10_3 0x2b@uint64;
(* vpsllq $0x15,%ymm10,%ymm10                      #! PC = 0x555555578691 *)
shl ymm10_0 ymm10_0 0x15@uint64;
shl ymm10_1 ymm10_1 0x15@uint64;
shl ymm10_2 ymm10_2 0x15@uint64;
shl ymm10_3 ymm10_3 0x15@uint64;
(* vmovdqa %ymm2,-0x90(%rbp)                       #! EA = L0x7fffffffbec0; PC = 0x555555578697 *)
mov L0x7fffffffbec0 ymm2_0;
mov L0x7fffffffbec8 ymm2_1;
mov L0x7fffffffbed0 ymm2_2;
mov L0x7fffffffbed8 ymm2_3;
(* vpor   %ymm0,%ymm10,%ymm10                      #! PC = 0x55555557869f *)
or ymm10_0@uint64 ymm10_0 ymm0_0;
or ymm10_1@uint64 ymm10_1 ymm0_1;
or ymm10_2@uint64 ymm10_2 ymm0_2;
or ymm10_3@uint64 ymm10_3 ymm0_3;
(* vpandn %ymm10,%ymm9,%ymm0                       #! PC = 0x5555555786a3 *)
not ymm9_0n@uint64 ymm9_0;
and ymm0_0@uint64 ymm9_0n ymm10_0;
not ymm9_1n@uint64 ymm9_1;
and ymm0_1@uint64 ymm9_1n ymm10_1;
not ymm9_2n@uint64 ymm9_2;
and ymm0_2@uint64 ymm9_2n ymm10_2;
not ymm9_3n@uint64 ymm9_3;
and ymm0_3@uint64 ymm9_3n ymm10_3;
(* vpxor  %ymm8,%ymm0,%ymm13                       #! PC = 0x5555555786a8 *)
xor ymm13_0@uint64 ymm0_0 ymm8_0;
xor ymm13_1@uint64 ymm0_1 ymm8_1;
xor ymm13_2@uint64 ymm0_2 ymm8_2;
xor ymm13_3@uint64 ymm0_3 ymm8_3;
(* vpsrlq $0x32,%ymm4,%ymm0                        #! PC = 0x5555555786ad *)
shr ymm0_0 ymm4_0 0x32@uint64;
shr ymm0_1 ymm4_1 0x32@uint64;
shr ymm0_2 ymm4_2 0x32@uint64;
shr ymm0_3 ymm4_3 0x32@uint64;
(* vpandn %ymm8,%ymm1,%ymm8                        #! PC = 0x5555555786b2 *)
not ymm1_0n@uint64 ymm1_0;
and ymm8_0@uint64 ymm1_0n ymm8_0;
not ymm1_1n@uint64 ymm1_1;
and ymm8_1@uint64 ymm1_1n ymm8_1;
not ymm1_2n@uint64 ymm1_2;
and ymm8_2@uint64 ymm1_2n ymm8_2;
not ymm1_3n@uint64 ymm1_3;
and ymm8_3@uint64 ymm1_3n ymm8_3;
(* vpsllq $0xe,%ymm4,%ymm4                         #! PC = 0x5555555786b7 *)
shl ymm4_0 ymm4_0 0xe@uint64;
shl ymm4_1 ymm4_1 0xe@uint64;
shl ymm4_2 ymm4_2 0xe@uint64;
shl ymm4_3 ymm4_3 0xe@uint64;
(* vmovdqa %ymm13,-0x1d0(%rbp)                     #! EA = L0x7fffffffbd80; PC = 0x5555555786bc *)
mov L0x7fffffffbd80 ymm13_0;
mov L0x7fffffffbd88 ymm13_1;
mov L0x7fffffffbd90 ymm13_2;
mov L0x7fffffffbd98 ymm13_3;
(* vpor   %ymm0,%ymm4,%ymm4                        #! PC = 0x5555555786c4 *)
or ymm4_0@uint64 ymm4_0 ymm0_0;
or ymm4_1@uint64 ymm4_1 ymm0_1;
or ymm4_2@uint64 ymm4_2 ymm0_2;
or ymm4_3@uint64 ymm4_3 ymm0_3;
(* vpandn %ymm4,%ymm10,%ymm0                       #! PC = 0x5555555786c8 *)
not ymm10_0n@uint64 ymm10_0;
and ymm0_0@uint64 ymm10_0n ymm4_0;
not ymm10_1n@uint64 ymm10_1;
and ymm0_1@uint64 ymm10_1n ymm4_1;
not ymm10_2n@uint64 ymm10_2;
and ymm0_2@uint64 ymm10_2n ymm4_2;
not ymm10_3n@uint64 ymm10_3;
and ymm0_3@uint64 ymm10_3n ymm4_3;
(* vpxor  %ymm4,%ymm8,%ymm8                        #! PC = 0x5555555786cc *)
xor ymm8_0@uint64 ymm8_0 ymm4_0;
xor ymm8_1@uint64 ymm8_1 ymm4_1;
xor ymm8_2@uint64 ymm8_2 ymm4_2;
xor ymm8_3@uint64 ymm8_3 ymm4_3;
(* vpxor  %ymm9,%ymm0,%ymm9                        #! PC = 0x5555555786d0 *)
xor ymm9_0@uint64 ymm0_0 ymm9_0;
xor ymm9_1@uint64 ymm0_1 ymm9_1;
xor ymm9_2@uint64 ymm0_2 ymm9_2;
xor ymm9_3@uint64 ymm0_3 ymm9_3;
(* vpandn %ymm1,%ymm4,%ymm0                        #! PC = 0x5555555786d5 *)
not ymm4_0n@uint64 ymm4_0;
and ymm0_0@uint64 ymm4_0n ymm1_0;
not ymm4_1n@uint64 ymm4_1;
and ymm0_1@uint64 ymm4_1n ymm1_1;
not ymm4_2n@uint64 ymm4_2;
and ymm0_2@uint64 ymm4_2n ymm1_2;
not ymm4_3n@uint64 ymm4_3;
and ymm0_3@uint64 ymm4_3n ymm1_3;
(* vmovdqa %ymm8,-0x2b0(%rbp)                      #! EA = L0x7fffffffbca0; PC = 0x5555555786d9 *)
mov L0x7fffffffbca0 ymm8_0;
mov L0x7fffffffbca8 ymm8_1;
mov L0x7fffffffbcb0 ymm8_2;
mov L0x7fffffffbcb8 ymm8_3;
(* vpxor  %ymm10,%ymm0,%ymm10                      #! PC = 0x5555555786e1 *)
xor ymm10_0@uint64 ymm0_0 ymm10_0;
xor ymm10_1@uint64 ymm0_1 ymm10_1;
xor ymm10_2@uint64 ymm0_2 ymm10_2;
xor ymm10_3@uint64 ymm0_3 ymm10_3;
(* vmovdqa %ymm9,-0x2d0(%rbp)                      #! EA = L0x7fffffffbc80; PC = 0x5555555786e6 *)
mov L0x7fffffffbc80 ymm9_0;
mov L0x7fffffffbc88 ymm9_1;
mov L0x7fffffffbc90 ymm9_2;
mov L0x7fffffffbc98 ymm9_3;
(* vmovdqa %ymm10,-0xf0(%rbp)                      #! EA = L0x7fffffffbe60; PC = 0x5555555786ee *)
mov L0x7fffffffbe60 ymm10_0;
mov L0x7fffffffbe68 ymm10_1;
mov L0x7fffffffbe70 ymm10_2;
mov L0x7fffffffbe78 ymm10_3;
(* vpxor  -0x170(%rbp),%ymm15,%ymm10               #! EA = L0x7fffffffbde0; Value = 0x5c3ce9b0ea86020e; PC = 0x5555555786f6 *)
xor ymm10_0@uint64 ymm15_0 L0x7fffffffbde0;
xor ymm10_1@uint64 ymm15_1 L0x7fffffffbde8;
xor ymm10_2@uint64 ymm15_2 L0x7fffffffbdf0;
xor ymm10_3@uint64 ymm15_3 L0x7fffffffbdf8;
(* vpsrlq $0x24,%ymm10,%ymm0                       #! PC = 0x5555555786fe *)
shr ymm0_0 ymm10_0 0x24@uint64;
shr ymm0_1 ymm10_1 0x24@uint64;
shr ymm0_2 ymm10_2 0x24@uint64;
shr ymm0_3 ymm10_3 0x24@uint64;
(* vpsllq $0x1c,%ymm10,%ymm1                       #! PC = 0x555555578704 *)
shl ymm1_0 ymm10_0 0x1c@uint64;
shl ymm1_1 ymm10_1 0x1c@uint64;
shl ymm1_2 ymm10_2 0x1c@uint64;
shl ymm1_3 ymm10_3 0x1c@uint64;
(* vpor   %ymm0,%ymm1,%ymm1                        #! PC = 0x55555557870a *)
or ymm1_0@uint64 ymm1_0 ymm0_0;
or ymm1_1@uint64 ymm1_1 ymm0_1;
or ymm1_2@uint64 ymm1_2 ymm0_2;
or ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpxor  -0x130(%rbp),%ymm12,%ymm0                #! EA = L0x7fffffffbe20; Value = 0xc54f3e2ef42a032e; PC = 0x55555557870e *)
xor ymm0_0@uint64 ymm12_0 L0x7fffffffbe20;
xor ymm0_1@uint64 ymm12_1 L0x7fffffffbe28;
xor ymm0_2@uint64 ymm12_2 L0x7fffffffbe30;
xor ymm0_3@uint64 ymm12_3 L0x7fffffffbe38;
(* vpsrlq $0x2c,%ymm0,%ymm2                        #! PC = 0x555555578716 *)
shr ymm2_0 ymm0_0 0x2c@uint64;
shr ymm2_1 ymm0_1 0x2c@uint64;
shr ymm2_2 ymm0_2 0x2c@uint64;
shr ymm2_3 ymm0_3 0x2c@uint64;
(* vpsllq $0x14,%ymm0,%ymm0                        #! PC = 0x55555557871b *)
shl ymm0_0 ymm0_0 0x14@uint64;
shl ymm0_1 ymm0_1 0x14@uint64;
shl ymm0_2 ymm0_2 0x14@uint64;
shl ymm0_3 ymm0_3 0x14@uint64;
(* vpor   %ymm2,%ymm0,%ymm0                        #! PC = 0x555555578720 *)
or ymm0_0@uint64 ymm0_0 ymm2_0;
or ymm0_1@uint64 ymm0_1 ymm2_1;
or ymm0_2@uint64 ymm0_2 ymm2_2;
or ymm0_3@uint64 ymm0_3 ymm2_3;
(* vpxor  -0xd0(%rbp),%ymm5,%ymm2                  #! EA = L0x7fffffffbe80; Value = 0x7e1ba84748e80bee; PC = 0x555555578724 *)
xor ymm2_0@uint64 ymm5_0 L0x7fffffffbe80;
xor ymm2_1@uint64 ymm5_1 L0x7fffffffbe88;
xor ymm2_2@uint64 ymm5_2 L0x7fffffffbe90;
xor ymm2_3@uint64 ymm5_3 L0x7fffffffbe98;
(* vpsrlq $0x3d,%ymm2,%ymm4                        #! PC = 0x55555557872c *)
shr ymm4_0 ymm2_0 0x3d@uint64;
shr ymm4_1 ymm2_1 0x3d@uint64;
shr ymm4_2 ymm2_2 0x3d@uint64;
shr ymm4_3 ymm2_3 0x3d@uint64;
(* vpsllq $0x3,%ymm2,%ymm2                         #! PC = 0x555555578731 *)
shl ymm2_0 ymm2_0 0x3@uint64;
shl ymm2_1 ymm2_1 0x3@uint64;
shl ymm2_2 ymm2_2 0x3@uint64;
shl ymm2_3 ymm2_3 0x3@uint64;
(* vpor   %ymm4,%ymm2,%ymm2                        #! PC = 0x555555578736 *)
or ymm2_0@uint64 ymm2_0 ymm4_0;
or ymm2_1@uint64 ymm2_1 ymm4_1;
or ymm2_2@uint64 ymm2_2 ymm4_2;
or ymm2_3@uint64 ymm2_3 ymm4_3;
(* vpandn %ymm2,%ymm0,%ymm4                        #! PC = 0x55555557873a *)
not ymm0_0n@uint64 ymm0_0;
and ymm4_0@uint64 ymm0_0n ymm2_0;
not ymm0_1n@uint64 ymm0_1;
and ymm4_1@uint64 ymm0_1n ymm2_1;
not ymm0_2n@uint64 ymm0_2;
and ymm4_2@uint64 ymm0_2n ymm2_2;
not ymm0_3n@uint64 ymm0_3;
and ymm4_3@uint64 ymm0_3n ymm2_3;
(* vpxor  %ymm1,%ymm4,%ymm13                       #! PC = 0x55555557873e *)
xor ymm13_0@uint64 ymm4_0 ymm1_0;
xor ymm13_1@uint64 ymm4_1 ymm1_1;
xor ymm13_2@uint64 ymm4_2 ymm1_2;
xor ymm13_3@uint64 ymm4_3 ymm1_3;
(* vpsrlq $0x13,%ymm7,%ymm4                        #! PC = 0x555555578742 *)
shr ymm4_0 ymm7_0 0x13@uint64;
shr ymm4_1 ymm7_1 0x13@uint64;
shr ymm4_2 ymm7_2 0x13@uint64;
shr ymm4_3 ymm7_3 0x13@uint64;
(* vpsllq $0x2d,%ymm7,%ymm7                        #! PC = 0x555555578747 *)
shl ymm7_0 ymm7_0 0x2d@uint64;
shl ymm7_1 ymm7_1 0x2d@uint64;
shl ymm7_2 ymm7_2 0x2d@uint64;
shl ymm7_3 ymm7_3 0x2d@uint64;
(* vmovdqa %ymm13,-0x1f0(%rbp)                     #! EA = L0x7fffffffbd60; PC = 0x55555557874c *)
mov L0x7fffffffbd60 ymm13_0;
mov L0x7fffffffbd68 ymm13_1;
mov L0x7fffffffbd70 ymm13_2;
mov L0x7fffffffbd78 ymm13_3;
(* vpor   %ymm4,%ymm7,%ymm13                       #! PC = 0x555555578754 *)
or ymm13_0@uint64 ymm7_0 ymm4_0;
or ymm13_1@uint64 ymm7_1 ymm4_1;
or ymm13_2@uint64 ymm7_2 ymm4_2;
or ymm13_3@uint64 ymm7_3 ymm4_3;
(* vpsrlq $0x3,%ymm11,%ymm4                        #! PC = 0x555555578758 *)
shr ymm4_0 ymm11_0 0x3@uint64;
shr ymm4_1 ymm11_1 0x3@uint64;
shr ymm4_2 ymm11_2 0x3@uint64;
shr ymm4_3 ymm11_3 0x3@uint64;
(* vpsllq $0x3d,%ymm11,%ymm11                      #! PC = 0x55555557875e *)
shl ymm11_0 ymm11_0 0x3d@uint64;
shl ymm11_1 ymm11_1 0x3d@uint64;
shl ymm11_2 ymm11_2 0x3d@uint64;
shl ymm11_3 ymm11_3 0x3d@uint64;
(* vpandn %ymm13,%ymm2,%ymm8                       #! PC = 0x555555578764 *)
not ymm2_0n@uint64 ymm2_0;
and ymm8_0@uint64 ymm2_0n ymm13_0;
not ymm2_1n@uint64 ymm2_1;
and ymm8_1@uint64 ymm2_1n ymm13_1;
not ymm2_2n@uint64 ymm2_2;
and ymm8_2@uint64 ymm2_2n ymm13_2;
not ymm2_3n@uint64 ymm2_3;
and ymm8_3@uint64 ymm2_3n ymm13_3;
(* vpor   %ymm4,%ymm11,%ymm11                      #! PC = 0x555555578769 *)
or ymm11_0@uint64 ymm11_0 ymm4_0;
or ymm11_1@uint64 ymm11_1 ymm4_1;
or ymm11_2@uint64 ymm11_2 ymm4_2;
or ymm11_3@uint64 ymm11_3 ymm4_3;
(* vpxor  %ymm0,%ymm8,%ymm8                        #! PC = 0x55555557876d *)
xor ymm8_0@uint64 ymm8_0 ymm0_0;
xor ymm8_1@uint64 ymm8_1 ymm0_1;
xor ymm8_2@uint64 ymm8_2 ymm0_2;
xor ymm8_3@uint64 ymm8_3 ymm0_3;
(* vpandn %ymm11,%ymm13,%ymm4                      #! PC = 0x555555578771 *)
not ymm13_0n@uint64 ymm13_0;
and ymm4_0@uint64 ymm13_0n ymm11_0;
not ymm13_1n@uint64 ymm13_1;
and ymm4_1@uint64 ymm13_1n ymm11_1;
not ymm13_2n@uint64 ymm13_2;
and ymm4_2@uint64 ymm13_2n ymm11_2;
not ymm13_3n@uint64 ymm13_3;
and ymm4_3@uint64 ymm13_3n ymm11_3;
(* vpxor  %ymm2,%ymm4,%ymm10                       #! PC = 0x555555578776 *)
xor ymm10_0@uint64 ymm4_0 ymm2_0;
xor ymm10_1@uint64 ymm4_1 ymm2_1;
xor ymm10_2@uint64 ymm4_2 ymm2_2;
xor ymm10_3@uint64 ymm4_3 ymm2_3;
(* vpandn %ymm1,%ymm11,%ymm2                       #! PC = 0x55555557877a *)
not ymm11_0n@uint64 ymm11_0;
and ymm2_0@uint64 ymm11_0n ymm1_0;
not ymm11_1n@uint64 ymm11_1;
and ymm2_1@uint64 ymm11_1n ymm1_1;
not ymm11_2n@uint64 ymm11_2;
and ymm2_2@uint64 ymm11_2n ymm1_2;
not ymm11_3n@uint64 ymm11_3;
and ymm2_3@uint64 ymm11_3n ymm1_3;
(* vpandn %ymm0,%ymm1,%ymm1                        #! PC = 0x55555557877e *)
not ymm1_0n@uint64 ymm1_0;
and ymm1_0@uint64 ymm1_0n ymm0_0;
not ymm1_1n@uint64 ymm1_1;
and ymm1_1@uint64 ymm1_1n ymm0_1;
not ymm1_2n@uint64 ymm1_2;
and ymm1_2@uint64 ymm1_2n ymm0_2;
not ymm1_3n@uint64 ymm1_3;
and ymm1_3@uint64 ymm1_3n ymm0_3;
(* vpxor  %ymm13,%ymm2,%ymm9                       #! PC = 0x555555578782 *)
xor ymm9_0@uint64 ymm2_0 ymm13_0;
xor ymm9_1@uint64 ymm2_1 ymm13_1;
xor ymm9_2@uint64 ymm2_2 ymm13_2;
xor ymm9_3@uint64 ymm2_3 ymm13_3;
(* vpxor  %ymm11,%ymm1,%ymm11                      #! PC = 0x555555578787 *)
xor ymm11_0@uint64 ymm1_0 ymm11_0;
xor ymm11_1@uint64 ymm1_1 ymm11_1;
xor ymm11_2@uint64 ymm1_2 ymm11_2;
xor ymm11_3@uint64 ymm1_3 ymm11_3;
(* vpxor  -0x190(%rbp),%ymm3,%ymm1                 #! EA = L0x7fffffffbdc0; Value = 0x6d4b92a0afd4a76b; PC = 0x55555557878c *)
xor ymm1_0@uint64 ymm3_0 L0x7fffffffbdc0;
xor ymm1_1@uint64 ymm3_1 L0x7fffffffbdc8;
xor ymm1_2@uint64 ymm3_2 L0x7fffffffbdd0;
xor ymm1_3@uint64 ymm3_3 L0x7fffffffbdd8;
(* vmovdqa %ymm10,-0x170(%rbp)                     #! EA = L0x7fffffffbde0; PC = 0x555555578794 *)
mov L0x7fffffffbde0 ymm10_0;
mov L0x7fffffffbde8 ymm10_1;
mov L0x7fffffffbdf0 ymm10_2;
mov L0x7fffffffbdf8 ymm10_3;
(* vmovdqa %ymm9,-0x1b0(%rbp)                      #! EA = L0x7fffffffbda0; PC = 0x55555557879c *)
mov L0x7fffffffbda0 ymm9_0;
mov L0x7fffffffbda8 ymm9_1;
mov L0x7fffffffbdb0 ymm9_2;
mov L0x7fffffffbdb8 ymm9_3;
(* vmovdqa %ymm11,-0xd0(%rbp)                      #! EA = L0x7fffffffbe80; PC = 0x5555555787a4 *)
mov L0x7fffffffbe80 ymm11_0;
mov L0x7fffffffbe88 ymm11_1;
mov L0x7fffffffbe90 ymm11_2;
mov L0x7fffffffbe98 ymm11_3;
(* vpxor  -0x150(%rbp),%ymm14,%ymm2                #! EA = L0x7fffffffbe00; Value = 0x780c883422dea4d0; PC = 0x5555555787ac *)
xor ymm2_0@uint64 ymm14_0 L0x7fffffffbe00;
xor ymm2_1@uint64 ymm14_1 L0x7fffffffbe08;
xor ymm2_2@uint64 ymm14_2 L0x7fffffffbe10;
xor ymm2_3@uint64 ymm14_3 L0x7fffffffbe18;
(* vpxor  -0xb0(%rbp),%ymm15,%ymm9                 #! EA = L0x7fffffffbea0; Value = 0x8e5c9db63e29f72a; PC = 0x5555555787b4 *)
xor ymm9_0@uint64 ymm15_0 L0x7fffffffbea0;
xor ymm9_1@uint64 ymm15_1 L0x7fffffffbea8;
xor ymm9_2@uint64 ymm15_2 L0x7fffffffbeb0;
xor ymm9_3@uint64 ymm15_3 L0x7fffffffbeb8;
(* vpsrlq $0x3f,%ymm1,%ymm0                        #! PC = 0x5555555787bc *)
shr ymm0_0 ymm1_0 0x3f@uint64;
shr ymm0_1 ymm1_1 0x3f@uint64;
shr ymm0_2 ymm1_2 0x3f@uint64;
shr ymm0_3 ymm1_3 0x3f@uint64;
(* vpsllq $0x1,%ymm1,%ymm1                         #! PC = 0x5555555787c1 *)
shl ymm1_0 ymm1_0 0x1@uint64;
shl ymm1_1 ymm1_1 0x1@uint64;
shl ymm1_2 ymm1_2 0x1@uint64;
shl ymm1_3 ymm1_3 0x1@uint64;
(* vpor   %ymm0,%ymm1,%ymm1                        #! PC = 0x5555555787c6 *)
or ymm1_0@uint64 ymm1_0 ymm0_0;
or ymm1_1@uint64 ymm1_1 ymm0_1;
or ymm1_2@uint64 ymm1_2 ymm0_2;
or ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x27,%ymm9,%ymm4                        #! PC = 0x5555555787ca *)
shr ymm4_0 ymm9_0 0x27@uint64;
shr ymm4_1 ymm9_1 0x27@uint64;
shr ymm4_2 ymm9_2 0x27@uint64;
shr ymm4_3 ymm9_3 0x27@uint64;
(* vpsrlq $0x3a,%ymm2,%ymm0                        #! PC = 0x5555555787d0 *)
shr ymm0_0 ymm2_0 0x3a@uint64;
shr ymm0_1 ymm2_1 0x3a@uint64;
shr ymm0_2 ymm2_2 0x3a@uint64;
shr ymm0_3 ymm2_3 0x3a@uint64;
(* vpsllq $0x19,%ymm9,%ymm9                        #! PC = 0x5555555787d5 *)
shl ymm9_0 ymm9_0 0x19@uint64;
shl ymm9_1 ymm9_1 0x19@uint64;
shl ymm9_2 ymm9_2 0x19@uint64;
shl ymm9_3 ymm9_3 0x19@uint64;
(* vpsllq $0x6,%ymm2,%ymm2                         #! PC = 0x5555555787db *)
shl ymm2_0 ymm2_0 0x6@uint64;
shl ymm2_1 ymm2_1 0x6@uint64;
shl ymm2_2 ymm2_2 0x6@uint64;
shl ymm2_3 ymm2_3 0x6@uint64;
(* vpor   %ymm0,%ymm2,%ymm2                        #! PC = 0x5555555787e0 *)
or ymm2_0@uint64 ymm2_0 ymm0_0;
or ymm2_1@uint64 ymm2_1 ymm0_1;
or ymm2_2@uint64 ymm2_2 ymm0_2;
or ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpor   %ymm4,%ymm9,%ymm0                        #! PC = 0x5555555787e4 *)
or ymm0_0@uint64 ymm9_0 ymm4_0;
or ymm0_1@uint64 ymm9_1 ymm4_1;
or ymm0_2@uint64 ymm9_2 ymm4_2;
or ymm0_3@uint64 ymm9_3 ymm4_3;
(* vpandn %ymm0,%ymm2,%ymm4                        #! PC = 0x5555555787e8 *)
not ymm2_0n@uint64 ymm2_0;
and ymm4_0@uint64 ymm2_0n ymm0_0;
not ymm2_1n@uint64 ymm2_1;
and ymm4_1@uint64 ymm2_1n ymm0_1;
not ymm2_2n@uint64 ymm2_2;
and ymm4_2@uint64 ymm2_2n ymm0_2;
not ymm2_3n@uint64 ymm2_3;
and ymm4_3@uint64 ymm2_3n ymm0_3;
(* vpxor  %ymm1,%ymm4,%ymm7                        #! PC = 0x5555555787ec *)
xor ymm7_0@uint64 ymm4_0 ymm1_0;
xor ymm7_1@uint64 ymm4_1 ymm1_1;
xor ymm7_2@uint64 ymm4_2 ymm1_2;
xor ymm7_3@uint64 ymm4_3 ymm1_3;
(* vpxor  -0x50(%rbp),%ymm12,%ymm4                 #! EA = L0x7fffffffbf00; Value = 0xcf709311541f6f05; PC = 0x5555555787f0 *)
xor ymm4_0@uint64 ymm12_0 L0x7fffffffbf00;
xor ymm4_1@uint64 ymm12_1 L0x7fffffffbf08;
xor ymm4_2@uint64 ymm12_2 L0x7fffffffbf10;
xor ymm4_3@uint64 ymm12_3 L0x7fffffffbf18;
(* vmovdqa %ymm7,%ymm13                            #! PC = 0x5555555787f5 *)
mov ymm13_0 ymm7_0;
mov ymm13_1 ymm7_1;
mov ymm13_2 ymm7_2;
mov ymm13_3 ymm7_3;
(* vpshufb 0x556fe(%rip),%ymm4,%ymm4        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x5555555787f9 *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm4_0, ymm4_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm4_2, ymm4_3, tmp_2, tmp_3);
mov ymm4_0 tmp_0;
mov ymm4_1 tmp_1;
mov ymm4_2 tmp_2;
mov ymm4_3 tmp_3;
(* vpandn %ymm4,%ymm0,%ymm7                        #! PC = 0x555555578802 *)
not ymm0_0n@uint64 ymm0_0;
and ymm7_0@uint64 ymm0_0n ymm4_0;
not ymm0_1n@uint64 ymm0_1;
and ymm7_1@uint64 ymm0_1n ymm4_1;
not ymm0_2n@uint64 ymm0_2;
and ymm7_2@uint64 ymm0_2n ymm4_2;
not ymm0_3n@uint64 ymm0_3;
and ymm7_3@uint64 ymm0_3n ymm4_3;
(* vpxor  %ymm2,%ymm7,%ymm7                        #! PC = 0x555555578806 *)
xor ymm7_0@uint64 ymm7_0 ymm2_0;
xor ymm7_1@uint64 ymm7_1 ymm2_1;
xor ymm7_2@uint64 ymm7_2 ymm2_2;
xor ymm7_3@uint64 ymm7_3 ymm2_3;
(* vmovdqa %ymm7,-0x190(%rbp)                      #! EA = L0x7fffffffbdc0; PC = 0x55555557880a *)
mov L0x7fffffffbdc0 ymm7_0;
mov L0x7fffffffbdc8 ymm7_1;
mov L0x7fffffffbdd0 ymm7_2;
mov L0x7fffffffbdd8 ymm7_3;
(* vpsrlq $0x2e,%ymm6,%ymm7                        #! PC = 0x555555578812 *)
shr ymm7_0 ymm6_0 0x2e@uint64;
shr ymm7_1 ymm6_1 0x2e@uint64;
shr ymm7_2 ymm6_2 0x2e@uint64;
shr ymm7_3 ymm6_3 0x2e@uint64;
(* vpsllq $0x12,%ymm6,%ymm6                        #! PC = 0x555555578817 *)
shl ymm6_0 ymm6_0 0x12@uint64;
shl ymm6_1 ymm6_1 0x12@uint64;
shl ymm6_2 ymm6_2 0x12@uint64;
shl ymm6_3 ymm6_3 0x12@uint64;
(* vpor   %ymm7,%ymm6,%ymm11                       #! PC = 0x55555557881c *)
or ymm11_0@uint64 ymm6_0 ymm7_0;
or ymm11_1@uint64 ymm6_1 ymm7_1;
or ymm11_2@uint64 ymm6_2 ymm7_2;
or ymm11_3@uint64 ymm6_3 ymm7_3;
(* vpxor  -0x290(%rbp),%ymm15,%ymm6                #! EA = L0x7fffffffbcc0; Value = 0x2bd80f68cc4e6008; PC = 0x555555578820 *)
xor ymm6_0@uint64 ymm15_0 L0x7fffffffbcc0;
xor ymm6_1@uint64 ymm15_1 L0x7fffffffbcc8;
xor ymm6_2@uint64 ymm15_2 L0x7fffffffbcd0;
xor ymm6_3@uint64 ymm15_3 L0x7fffffffbcd8;
(* vpxor  -0x210(%rbp),%ymm15,%ymm15               #! EA = L0x7fffffffbd40; Value = 0x3cefe3bf645dc61f; PC = 0x555555578828 *)
xor ymm15_0@uint64 ymm15_0 L0x7fffffffbd40;
xor ymm15_1@uint64 ymm15_1 L0x7fffffffbd48;
xor ymm15_2@uint64 ymm15_2 L0x7fffffffbd50;
xor ymm15_3@uint64 ymm15_3 L0x7fffffffbd58;
(* vpandn %ymm11,%ymm4,%ymm9                       #! PC = 0x555555578830 *)
not ymm4_0n@uint64 ymm4_0;
and ymm9_0@uint64 ymm4_0n ymm11_0;
not ymm4_1n@uint64 ymm4_1;
and ymm9_1@uint64 ymm4_1n ymm11_1;
not ymm4_2n@uint64 ymm4_2;
and ymm9_2@uint64 ymm4_2n ymm11_2;
not ymm4_3n@uint64 ymm4_3;
and ymm9_3@uint64 ymm4_3n ymm11_3;
(* vpshufb 0x556a2(%rip),%ymm6,%ymm6        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x555555578835 *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm6_0, ymm6_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm6_2, ymm6_3, tmp_2, tmp_3);
mov ymm6_0 tmp_0;
mov ymm6_1 tmp_1;
mov ymm6_2 tmp_2;
mov ymm6_3 tmp_3;
(* vpxor  %ymm0,%ymm9,%ymm9                        #! PC = 0x55555557883e *)
xor ymm9_0@uint64 ymm9_0 ymm0_0;
xor ymm9_1@uint64 ymm9_1 ymm0_1;
xor ymm9_2@uint64 ymm9_2 ymm0_2;
xor ymm9_3@uint64 ymm9_3 ymm0_3;
(* vpandn %ymm1,%ymm11,%ymm0                       #! PC = 0x555555578842 *)
not ymm11_0n@uint64 ymm11_0;
and ymm0_0@uint64 ymm11_0n ymm1_0;
not ymm11_1n@uint64 ymm11_1;
and ymm0_1@uint64 ymm11_1n ymm1_1;
not ymm11_2n@uint64 ymm11_2;
and ymm0_2@uint64 ymm11_2n ymm1_2;
not ymm11_3n@uint64 ymm11_3;
and ymm0_3@uint64 ymm11_3n ymm1_3;
(* vpxor  %ymm4,%ymm0,%ymm7                        #! PC = 0x555555578846 *)
xor ymm7_0@uint64 ymm0_0 ymm4_0;
xor ymm7_1@uint64 ymm0_1 ymm4_1;
xor ymm7_2@uint64 ymm0_2 ymm4_2;
xor ymm7_3@uint64 ymm0_3 ymm4_3;
(* vpxor  -0x250(%rbp),%ymm12,%ymm0                #! EA = L0x7fffffffbd00; Value = 0x322c45cc79b07a6c; PC = 0x55555557884a *)
xor ymm0_0@uint64 ymm12_0 L0x7fffffffbd00;
xor ymm0_1@uint64 ymm12_1 L0x7fffffffbd08;
xor ymm0_2@uint64 ymm12_2 L0x7fffffffbd10;
xor ymm0_3@uint64 ymm12_3 L0x7fffffffbd18;
(* vpandn %ymm2,%ymm1,%ymm1                        #! PC = 0x555555578852 *)
not ymm1_0n@uint64 ymm1_0;
and ymm1_0@uint64 ymm1_0n ymm2_0;
not ymm1_1n@uint64 ymm1_1;
and ymm1_1@uint64 ymm1_1n ymm2_1;
not ymm1_2n@uint64 ymm1_2;
and ymm1_2@uint64 ymm1_2n ymm2_2;
not ymm1_3n@uint64 ymm1_3;
and ymm1_3@uint64 ymm1_3n ymm2_3;
(* vpxor  %ymm11,%ymm1,%ymm11                      #! PC = 0x555555578856 *)
xor ymm11_0@uint64 ymm1_0 ymm11_0;
xor ymm11_1@uint64 ymm1_1 ymm11_1;
xor ymm11_2@uint64 ymm1_2 ymm11_2;
xor ymm11_3@uint64 ymm1_3 ymm11_3;
(* vmovdqa %ymm7,-0xb0(%rbp)                       #! EA = L0x7fffffffbea0; PC = 0x55555557885b *)
mov L0x7fffffffbea0 ymm7_0;
mov L0x7fffffffbea8 ymm7_1;
mov L0x7fffffffbeb0 ymm7_2;
mov L0x7fffffffbeb8 ymm7_3;
(* vpxor  -0x330(%rbp),%ymm3,%ymm7                 #! EA = L0x7fffffffbc20; Value = 0xc777b862349466ae; PC = 0x555555578863 *)
xor ymm7_0@uint64 ymm3_0 L0x7fffffffbc20;
xor ymm7_1@uint64 ymm3_1 L0x7fffffffbc28;
xor ymm7_2@uint64 ymm3_2 L0x7fffffffbc30;
xor ymm7_3@uint64 ymm3_3 L0x7fffffffbc38;
(* vpxor  -0x110(%rbp),%ymm12,%ymm12               #! EA = L0x7fffffffbe40; Value = 0xdc8e9ad663eb67f4; PC = 0x55555557886b *)
xor ymm12_0@uint64 ymm12_0 L0x7fffffffbe40;
xor ymm12_1@uint64 ymm12_1 L0x7fffffffbe48;
xor ymm12_2@uint64 ymm12_2 L0x7fffffffbe50;
xor ymm12_3@uint64 ymm12_3 L0x7fffffffbe58;
(* vpsrlq $0x25,%ymm0,%ymm1                        #! PC = 0x555555578873 *)
shr ymm1_0 ymm0_0 0x25@uint64;
shr ymm1_1 ymm0_1 0x25@uint64;
shr ymm1_2 ymm0_2 0x25@uint64;
shr ymm1_3 ymm0_3 0x25@uint64;
(* vpsllq $0x1b,%ymm0,%ymm0                        #! PC = 0x555555578878 *)
shl ymm0_0 ymm0_0 0x1b@uint64;
shl ymm0_1 ymm0_1 0x1b@uint64;
shl ymm0_2 ymm0_2 0x1b@uint64;
shl ymm0_3 ymm0_3 0x1b@uint64;
(* vmovdqa %ymm11,-0x150(%rbp)                     #! EA = L0x7fffffffbe00; PC = 0x55555557887d *)
mov L0x7fffffffbe00 ymm11_0;
mov L0x7fffffffbe08 ymm11_1;
mov L0x7fffffffbe10 ymm11_2;
mov L0x7fffffffbe18 ymm11_3;
(* vpxor  -0x2f0(%rbp),%ymm3,%ymm3                 #! EA = L0x7fffffffbc60; Value = 0x39f99b067422b547; PC = 0x555555578885 *)
xor ymm3_0@uint64 ymm3_0 L0x7fffffffbc60;
xor ymm3_1@uint64 ymm3_1 L0x7fffffffbc68;
xor ymm3_2@uint64 ymm3_2 L0x7fffffffbc70;
xor ymm3_3@uint64 ymm3_3 L0x7fffffffbc78;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x55555557888d *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x230(%rbp),%ymm5,%ymm1                 #! EA = L0x7fffffffbd20; Value = 0x2dfca2b3ca5eda71; PC = 0x555555578891 *)
xor ymm1_0@uint64 ymm5_0 L0x7fffffffbd20;
xor ymm1_1@uint64 ymm5_1 L0x7fffffffbd28;
xor ymm1_2@uint64 ymm5_2 L0x7fffffffbd30;
xor ymm1_3@uint64 ymm5_3 L0x7fffffffbd38;
(* vpxor  -0x310(%rbp),%ymm5,%ymm5                 #! EA = L0x7fffffffbc40; Value = 0xc3d4c4c0a4002291; PC = 0x555555578899 *)
xor ymm5_0@uint64 ymm5_0 L0x7fffffffbc40;
xor ymm5_1@uint64 ymm5_1 L0x7fffffffbc48;
xor ymm5_2@uint64 ymm5_2 L0x7fffffffbc50;
xor ymm5_3@uint64 ymm5_3 L0x7fffffffbc58;
(* vmovdqa %ymm13,-0x110(%rbp)                     #! EA = L0x7fffffffbe40; PC = 0x5555555788a1 *)
mov L0x7fffffffbe40 ymm13_0;
mov L0x7fffffffbe48 ymm13_1;
mov L0x7fffffffbe50 ymm13_2;
mov L0x7fffffffbe58 ymm13_3;
(* vpsrlq $0x1c,%ymm1,%ymm2                        #! PC = 0x5555555788a9 *)
shr ymm2_0 ymm1_0 0x1c@uint64;
shr ymm2_1 ymm1_1 0x1c@uint64;
shr ymm2_2 ymm1_2 0x1c@uint64;
shr ymm2_3 ymm1_3 0x1c@uint64;
(* vpsllq $0x24,%ymm1,%ymm1                        #! PC = 0x5555555788ae *)
shl ymm1_0 ymm1_0 0x24@uint64;
shl ymm1_1 ymm1_1 0x24@uint64;
shl ymm1_2 ymm1_2 0x24@uint64;
shl ymm1_3 ymm1_3 0x24@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x5555555788b3 *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpsrlq $0x36,%ymm7,%ymm2                        #! PC = 0x5555555788b7 *)
shr ymm2_0 ymm7_0 0x36@uint64;
shr ymm2_1 ymm7_1 0x36@uint64;
shr ymm2_2 ymm7_2 0x36@uint64;
shr ymm2_3 ymm7_3 0x36@uint64;
(* vpsllq $0xa,%ymm7,%ymm7                         #! PC = 0x5555555788bc *)
shl ymm7_0 ymm7_0 0xa@uint64;
shl ymm7_1 ymm7_1 0xa@uint64;
shl ymm7_2 ymm7_2 0xa@uint64;
shl ymm7_3 ymm7_3 0xa@uint64;
(* vpor   %ymm2,%ymm7,%ymm4                        #! PC = 0x5555555788c1 *)
or ymm4_0@uint64 ymm7_0 ymm2_0;
or ymm4_1@uint64 ymm7_1 ymm2_1;
or ymm4_2@uint64 ymm7_2 ymm2_2;
or ymm4_3@uint64 ymm7_3 ymm2_3;
(* vpandn %ymm4,%ymm1,%ymm2                        #! PC = 0x5555555788c5 *)
not ymm1_0n@uint64 ymm1_0;
and ymm2_0@uint64 ymm1_0n ymm4_0;
not ymm1_1n@uint64 ymm1_1;
and ymm2_1@uint64 ymm1_1n ymm4_1;
not ymm1_2n@uint64 ymm1_2;
and ymm2_2@uint64 ymm1_2n ymm4_2;
not ymm1_3n@uint64 ymm1_3;
and ymm2_3@uint64 ymm1_3n ymm4_3;
(* vpxor  %ymm0,%ymm2,%ymm11                       #! PC = 0x5555555788c9 *)
xor ymm11_0@uint64 ymm2_0 ymm0_0;
xor ymm11_1@uint64 ymm2_1 ymm0_1;
xor ymm11_2@uint64 ymm2_2 ymm0_2;
xor ymm11_3@uint64 ymm2_3 ymm0_3;
(* vpxor  -0x70(%rbp),%ymm14,%ymm2                 #! EA = L0x7fffffffbee0; Value = 0x70433fd50a2fca56; PC = 0x5555555788cd *)
xor ymm2_0@uint64 ymm14_0 L0x7fffffffbee0;
xor ymm2_1@uint64 ymm14_1 L0x7fffffffbee8;
xor ymm2_2@uint64 ymm14_2 L0x7fffffffbef0;
xor ymm2_3@uint64 ymm14_3 L0x7fffffffbef8;
(* vpxor  -0x270(%rbp),%ymm14,%ymm14               #! EA = L0x7fffffffbce0; Value = 0x86a9931dbbbe2820; PC = 0x5555555788d2 *)
xor ymm14_0@uint64 ymm14_0 L0x7fffffffbce0;
xor ymm14_1@uint64 ymm14_1 L0x7fffffffbce8;
xor ymm14_2@uint64 ymm14_2 L0x7fffffffbcf0;
xor ymm14_3@uint64 ymm14_3 L0x7fffffffbcf8;
(* vmovdqa %ymm11,-0x330(%rbp)                     #! EA = L0x7fffffffbc20; PC = 0x5555555788da *)
mov L0x7fffffffbc20 ymm11_0;
mov L0x7fffffffbc28 ymm11_1;
mov L0x7fffffffbc30 ymm11_2;
mov L0x7fffffffbc38 ymm11_3;
(* vpsrlq $0x31,%ymm2,%ymm7                        #! PC = 0x5555555788e2 *)
shr ymm7_0 ymm2_0 0x31@uint64;
shr ymm7_1 ymm2_1 0x31@uint64;
shr ymm7_2 ymm2_2 0x31@uint64;
shr ymm7_3 ymm2_3 0x31@uint64;
(* vpsllq $0xf,%ymm2,%ymm2                         #! PC = 0x5555555788e7 *)
shl ymm2_0 ymm2_0 0xf@uint64;
shl ymm2_1 ymm2_1 0xf@uint64;
shl ymm2_2 ymm2_2 0xf@uint64;
shl ymm2_3 ymm2_3 0xf@uint64;
(* vpor   %ymm7,%ymm2,%ymm2                        #! PC = 0x5555555788ec *)
or ymm2_0@uint64 ymm2_0 ymm7_0;
or ymm2_1@uint64 ymm2_1 ymm7_1;
or ymm2_2@uint64 ymm2_2 ymm7_2;
or ymm2_3@uint64 ymm2_3 ymm7_3;
(* vpandn %ymm6,%ymm2,%ymm10                       #! PC = 0x5555555788f0 *)
not ymm2_0n@uint64 ymm2_0;
and ymm10_0@uint64 ymm2_0n ymm6_0;
not ymm2_1n@uint64 ymm2_1;
and ymm10_1@uint64 ymm2_1n ymm6_1;
not ymm2_2n@uint64 ymm2_2;
and ymm10_2@uint64 ymm2_2n ymm6_2;
not ymm2_3n@uint64 ymm2_3;
and ymm10_3@uint64 ymm2_3n ymm6_3;
(* vpandn %ymm2,%ymm4,%ymm7                        #! PC = 0x5555555788f4 *)
not ymm4_0n@uint64 ymm4_0;
and ymm7_0@uint64 ymm4_0n ymm2_0;
not ymm4_1n@uint64 ymm4_1;
and ymm7_1@uint64 ymm4_1n ymm2_1;
not ymm4_2n@uint64 ymm4_2;
and ymm7_2@uint64 ymm4_2n ymm2_2;
not ymm4_3n@uint64 ymm4_3;
and ymm7_3@uint64 ymm4_3n ymm2_3;
(* vpxor  %ymm4,%ymm10,%ymm4                       #! PC = 0x5555555788f8 *)
xor ymm4_0@uint64 ymm10_0 ymm4_0;
xor ymm4_1@uint64 ymm10_1 ymm4_1;
xor ymm4_2@uint64 ymm10_2 ymm4_2;
xor ymm4_3@uint64 ymm10_3 ymm4_3;
(* vpandn %ymm0,%ymm6,%ymm10                       #! PC = 0x5555555788fc *)
not ymm6_0n@uint64 ymm6_0;
and ymm10_0@uint64 ymm6_0n ymm0_0;
not ymm6_1n@uint64 ymm6_1;
and ymm10_1@uint64 ymm6_1n ymm0_1;
not ymm6_2n@uint64 ymm6_2;
and ymm10_2@uint64 ymm6_2n ymm0_2;
not ymm6_3n@uint64 ymm6_3;
and ymm10_3@uint64 ymm6_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x555555578900 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm6,%ymm0,%ymm6                        #! PC = 0x555555578904 *)
xor ymm6_0@uint64 ymm0_0 ymm6_0;
xor ymm6_1@uint64 ymm0_1 ymm6_1;
xor ymm6_2@uint64 ymm0_2 ymm6_2;
xor ymm6_3@uint64 ymm0_3 ymm6_3;
(* vpsrlq $0x9,%ymm15,%ymm0                        #! PC = 0x555555578908 *)
shr ymm0_0 ymm15_0 0x9@uint64;
shr ymm0_1 ymm15_1 0x9@uint64;
shr ymm0_2 ymm15_2 0x9@uint64;
shr ymm0_3 ymm15_3 0x9@uint64;
(* vpxor  %ymm2,%ymm10,%ymm10                      #! PC = 0x55555557890e *)
xor ymm10_0@uint64 ymm10_0 ymm2_0;
xor ymm10_1@uint64 ymm10_1 ymm2_1;
xor ymm10_2@uint64 ymm10_2 ymm2_2;
xor ymm10_3@uint64 ymm10_3 ymm2_3;
(* vmovdqa %ymm4,-0x70(%rbp)                       #! EA = L0x7fffffffbee0; PC = 0x555555578912 *)
mov L0x7fffffffbee0 ymm4_0;
mov L0x7fffffffbee8 ymm4_1;
mov L0x7fffffffbef0 ymm4_2;
mov L0x7fffffffbef8 ymm4_3;
(* vmovdqa %ymm6,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x555555578917 *)
mov L0x7fffffffbf00 ymm6_0;
mov L0x7fffffffbf08 ymm6_1;
mov L0x7fffffffbf10 ymm6_2;
mov L0x7fffffffbf18 ymm6_3;
(* vpsllq $0x37,%ymm15,%ymm15                      #! PC = 0x55555557891c *)
shl ymm15_0 ymm15_0 0x37@uint64;
shl ymm15_1 ymm15_1 0x37@uint64;
shl ymm15_2 ymm15_2 0x37@uint64;
shl ymm15_3 ymm15_3 0x37@uint64;
(* vpsrlq $0x19,%ymm12,%ymm6                       #! PC = 0x555555578922 *)
shr ymm6_0 ymm12_0 0x19@uint64;
shr ymm6_1 ymm12_1 0x19@uint64;
shr ymm6_2 ymm12_2 0x19@uint64;
shr ymm6_3 ymm12_3 0x19@uint64;
(* vpxor  %ymm1,%ymm7,%ymm7                        #! PC = 0x555555578928 *)
xor ymm7_0@uint64 ymm7_0 ymm1_0;
xor ymm7_1@uint64 ymm7_1 ymm1_1;
xor ymm7_2@uint64 ymm7_2 ymm1_2;
xor ymm7_3@uint64 ymm7_3 ymm1_3;
(* vpsllq $0x27,%ymm12,%ymm12                      #! PC = 0x55555557892c *)
shl ymm12_0 ymm12_0 0x27@uint64;
shl ymm12_1 ymm12_1 0x27@uint64;
shl ymm12_2 ymm12_2 0x27@uint64;
shl ymm12_3 ymm12_3 0x27@uint64;
(* vpsrlq $0x2,%ymm14,%ymm2                        #! PC = 0x555555578932 *)
shr ymm2_0 ymm14_0 0x2@uint64;
shr ymm2_1 ymm14_1 0x2@uint64;
shr ymm2_2 ymm14_2 0x2@uint64;
shr ymm2_3 ymm14_3 0x2@uint64;
(* vpor   %ymm0,%ymm15,%ymm15                      #! PC = 0x555555578938 *)
or ymm15_0@uint64 ymm15_0 ymm0_0;
or ymm15_1@uint64 ymm15_1 ymm0_1;
or ymm15_2@uint64 ymm15_2 ymm0_2;
or ymm15_3@uint64 ymm15_3 ymm0_3;
(* vpor   %ymm6,%ymm12,%ymm12                      #! PC = 0x55555557893c *)
or ymm12_0@uint64 ymm12_0 ymm6_0;
or ymm12_1@uint64 ymm12_1 ymm6_1;
or ymm12_2@uint64 ymm12_2 ymm6_2;
or ymm12_3@uint64 ymm12_3 ymm6_3;
(* vpsllq $0x3e,%ymm14,%ymm14                      #! PC = 0x555555578940 *)
shl ymm14_0 ymm14_0 0x3e@uint64;
shl ymm14_1 ymm14_1 0x3e@uint64;
shl ymm14_2 ymm14_2 0x3e@uint64;
shl ymm14_3 ymm14_3 0x3e@uint64;
(* vpxor  -0x1f0(%rbp),%ymm13,%ymm0                #! EA = L0x7fffffffbd60; Value = 0x2f3b2ad03fba7fb4; PC = 0x555555578946 *)
xor ymm0_0@uint64 ymm13_0 L0x7fffffffbd60;
xor ymm0_1@uint64 ymm13_1 L0x7fffffffbd68;
xor ymm0_2@uint64 ymm13_2 L0x7fffffffbd70;
xor ymm0_3@uint64 ymm13_3 L0x7fffffffbd78;
(* vpor   %ymm2,%ymm14,%ymm4                       #! PC = 0x55555557894e *)
or ymm4_0@uint64 ymm14_0 ymm2_0;
or ymm4_1@uint64 ymm14_1 ymm2_1;
or ymm4_2@uint64 ymm14_2 ymm2_2;
or ymm4_3@uint64 ymm14_3 ymm2_3;
(* vpandn %ymm12,%ymm15,%ymm6                      #! PC = 0x555555578952 *)
not ymm15_0n@uint64 ymm15_0;
and ymm6_0@uint64 ymm15_0n ymm12_0;
not ymm15_1n@uint64 ymm15_1;
and ymm6_1@uint64 ymm15_1n ymm12_1;
not ymm15_2n@uint64 ymm15_2;
and ymm6_2@uint64 ymm15_2n ymm12_2;
not ymm15_3n@uint64 ymm15_3;
and ymm6_3@uint64 ymm15_3n ymm12_3;
(* vpxor  -0x70(%rbp),%ymm9,%ymm13                 #! EA = L0x7fffffffbee0; Value = 0x6fb9736051442cc0; PC = 0x555555578957 *)
xor ymm13_0@uint64 ymm9_0 L0x7fffffffbee0;
xor ymm13_1@uint64 ymm9_1 L0x7fffffffbee8;
xor ymm13_2@uint64 ymm9_2 L0x7fffffffbef0;
xor ymm13_3@uint64 ymm9_3 L0x7fffffffbef8;
(* vpxor  %ymm4,%ymm6,%ymm6                        #! PC = 0x55555557895c *)
xor ymm6_0@uint64 ymm6_0 ymm4_0;
xor ymm6_1@uint64 ymm6_1 ymm4_1;
xor ymm6_2@uint64 ymm6_2 ymm4_2;
xor ymm6_3@uint64 ymm6_3 ymm4_3;
(* vpxor  -0x190(%rbp),%ymm7,%ymm14                #! EA = L0x7fffffffbdc0; Value = 0xf1d6dfc015c6d99d; PC = 0x555555578960 *)
xor ymm14_0@uint64 ymm7_0 L0x7fffffffbdc0;
xor ymm14_1@uint64 ymm7_1 L0x7fffffffbdc8;
xor ymm14_2@uint64 ymm7_2 L0x7fffffffbdd0;
xor ymm14_3@uint64 ymm7_3 L0x7fffffffbdd8;
(* vpxor  %ymm11,%ymm6,%ymm2                       #! PC = 0x555555578968 *)
xor ymm2_0@uint64 ymm6_0 ymm11_0;
xor ymm2_1@uint64 ymm6_1 ymm11_1;
xor ymm2_2@uint64 ymm6_2 ymm11_2;
xor ymm2_3@uint64 ymm6_3 ymm11_3;
(* vmovdqa -0x170(%rbp),%ymm11                     #! EA = L0x7fffffffbde0; Value = 0x8e03bdc59cf78585; PC = 0x55555557896d *)
mov ymm11_0 L0x7fffffffbde0;
mov ymm11_1 L0x7fffffffbde8;
mov ymm11_2 L0x7fffffffbdf0;
mov ymm11_3 L0x7fffffffbdf8;
(* vpxor  %ymm0,%ymm2,%ymm2                        #! PC = 0x555555578975 *)
xor ymm2_0@uint64 ymm2_0 ymm0_0;
xor ymm2_1@uint64 ymm2_1 ymm0_1;
xor ymm2_2@uint64 ymm2_2 ymm0_2;
xor ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x17,%ymm5,%ymm0                        #! PC = 0x555555578979 *)
shr ymm0_0 ymm5_0 0x17@uint64;
shr ymm0_1 ymm5_1 0x17@uint64;
shr ymm0_2 ymm5_2 0x17@uint64;
shr ymm0_3 ymm5_3 0x17@uint64;
(* vpxor  -0x90(%rbp),%ymm2,%ymm2                  #! EA = L0x7fffffffbec0; Value = 0x5d350f71494fc8f7; PC = 0x55555557897e *)
xor ymm2_0@uint64 ymm2_0 L0x7fffffffbec0;
xor ymm2_1@uint64 ymm2_1 L0x7fffffffbec8;
xor ymm2_2@uint64 ymm2_2 L0x7fffffffbed0;
xor ymm2_3@uint64 ymm2_3 L0x7fffffffbed8;
(* vpsllq $0x29,%ymm5,%ymm5                        #! PC = 0x555555578986 *)
shl ymm5_0 ymm5_0 0x29@uint64;
shl ymm5_1 ymm5_1 0x29@uint64;
shl ymm5_2 ymm5_2 0x29@uint64;
shl ymm5_3 ymm5_3 0x29@uint64;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x55555557898b *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpandn %ymm5,%ymm12,%ymm0                       #! PC = 0x55555557898f *)
not ymm12_0n@uint64 ymm12_0;
and ymm0_0@uint64 ymm12_0n ymm5_0;
not ymm12_1n@uint64 ymm12_1;
and ymm0_1@uint64 ymm12_1n ymm5_1;
not ymm12_2n@uint64 ymm12_2;
and ymm0_2@uint64 ymm12_2n ymm5_2;
not ymm12_3n@uint64 ymm12_3;
and ymm0_3@uint64 ymm12_3n ymm5_3;
(* vpxor  %ymm15,%ymm0,%ymm1                       #! PC = 0x555555578993 *)
xor ymm1_0@uint64 ymm0_0 ymm15_0;
xor ymm1_1@uint64 ymm0_1 ymm15_1;
xor ymm1_2@uint64 ymm0_2 ymm15_2;
xor ymm1_3@uint64 ymm0_3 ymm15_3;
(* vpxor  -0x1d0(%rbp),%ymm8,%ymm0                 #! EA = L0x7fffffffbd80; Value = 0xe37bf573d8aafe2d; PC = 0x555555578998 *)
xor ymm0_0@uint64 ymm8_0 L0x7fffffffbd80;
xor ymm0_1@uint64 ymm8_1 L0x7fffffffbd88;
xor ymm0_2@uint64 ymm8_2 L0x7fffffffbd90;
xor ymm0_3@uint64 ymm8_3 L0x7fffffffbd98;
(* vmovdqa %ymm1,-0x310(%rbp)                      #! EA = L0x7fffffffbc40; PC = 0x5555555789a0 *)
mov L0x7fffffffbc40 ymm1_0;
mov L0x7fffffffbc48 ymm1_1;
mov L0x7fffffffbc50 ymm1_2;
mov L0x7fffffffbc58 ymm1_3;
(* vpxor  %ymm0,%ymm14,%ymm14                      #! PC = 0x5555555789a8 *)
xor ymm14_0@uint64 ymm14_0 ymm0_0;
xor ymm14_1@uint64 ymm14_1 ymm0_1;
xor ymm14_2@uint64 ymm14_2 ymm0_2;
xor ymm14_3@uint64 ymm14_3 ymm0_3;
(* vpxor  %ymm1,%ymm14,%ymm14                      #! PC = 0x5555555789ac *)
xor ymm14_0@uint64 ymm14_0 ymm1_0;
xor ymm14_1@uint64 ymm14_1 ymm1_1;
xor ymm14_2@uint64 ymm14_2 ymm1_2;
xor ymm14_3@uint64 ymm14_3 ymm1_3;
(* vpsrlq $0x3e,%ymm3,%ymm1                        #! PC = 0x5555555789b0 *)
shr ymm1_0 ymm3_0 0x3e@uint64;
shr ymm1_1 ymm3_1 0x3e@uint64;
shr ymm1_2 ymm3_2 0x3e@uint64;
shr ymm1_3 ymm3_3 0x3e@uint64;
(* vpsllq $0x2,%ymm3,%ymm3                         #! PC = 0x5555555789b5 *)
shl ymm3_0 ymm3_0 0x2@uint64;
shl ymm3_1 ymm3_1 0x2@uint64;
shl ymm3_2 ymm3_2 0x2@uint64;
shl ymm3_3 ymm3_3 0x2@uint64;
(* vpor   %ymm1,%ymm3,%ymm0                        #! PC = 0x5555555789ba *)
or ymm0_0@uint64 ymm3_0 ymm1_0;
or ymm0_1@uint64 ymm3_1 ymm1_1;
or ymm0_2@uint64 ymm3_2 ymm1_2;
or ymm0_3@uint64 ymm3_3 ymm1_3;
(* vmovdqa -0x1b0(%rbp),%ymm3                      #! EA = L0x7fffffffbda0; Value = 0x982733e2d63b636a; PC = 0x5555555789be *)
mov ymm3_0 L0x7fffffffbda0;
mov ymm3_1 L0x7fffffffbda8;
mov ymm3_2 L0x7fffffffbdb0;
mov ymm3_3 L0x7fffffffbdb8;
(* vpandn %ymm0,%ymm5,%ymm1                        #! PC = 0x5555555789c6 *)
not ymm5_0n@uint64 ymm5_0;
and ymm1_0@uint64 ymm5_0n ymm0_0;
not ymm5_1n@uint64 ymm5_1;
and ymm1_1@uint64 ymm5_1n ymm0_1;
not ymm5_2n@uint64 ymm5_2;
and ymm1_2@uint64 ymm5_2n ymm0_2;
not ymm5_3n@uint64 ymm5_3;
and ymm1_3@uint64 ymm5_3n ymm0_3;
(* vpxor  %ymm12,%ymm1,%ymm12                      #! PC = 0x5555555789ca *)
xor ymm12_0@uint64 ymm1_0 ymm12_0;
xor ymm12_1@uint64 ymm1_1 ymm12_1;
xor ymm12_2@uint64 ymm1_2 ymm12_2;
xor ymm12_3@uint64 ymm1_3 ymm12_3;
(* vpxor  -0x2d0(%rbp),%ymm11,%ymm1                #! EA = L0x7fffffffbc80; Value = 0x26c80c388310b9ed; PC = 0x5555555789cf *)
xor ymm1_0@uint64 ymm11_0 L0x7fffffffbc80;
xor ymm1_1@uint64 ymm11_1 L0x7fffffffbc88;
xor ymm1_2@uint64 ymm11_2 L0x7fffffffbc90;
xor ymm1_3@uint64 ymm11_3 L0x7fffffffbc98;
(* vpxor  %ymm1,%ymm13,%ymm13                      #! PC = 0x5555555789d7 *)
xor ymm13_0@uint64 ymm13_0 ymm1_0;
xor ymm13_1@uint64 ymm13_1 ymm1_1;
xor ymm13_2@uint64 ymm13_2 ymm1_2;
xor ymm13_3@uint64 ymm13_3 ymm1_3;
(* vpandn %ymm4,%ymm0,%ymm1                        #! PC = 0x5555555789db *)
not ymm0_0n@uint64 ymm0_0;
and ymm1_0@uint64 ymm0_0n ymm4_0;
not ymm0_1n@uint64 ymm0_1;
and ymm1_1@uint64 ymm0_1n ymm4_1;
not ymm0_2n@uint64 ymm0_2;
and ymm1_2@uint64 ymm0_2n ymm4_2;
not ymm0_3n@uint64 ymm0_3;
and ymm1_3@uint64 ymm0_3n ymm4_3;
(* vpxor  %ymm5,%ymm1,%ymm5                        #! PC = 0x5555555789df *)
xor ymm5_0@uint64 ymm1_0 ymm5_0;
xor ymm5_1@uint64 ymm1_1 ymm5_1;
xor ymm5_2@uint64 ymm1_2 ymm5_2;
xor ymm5_3@uint64 ymm1_3 ymm5_3;
(* vpxor  %ymm12,%ymm13,%ymm13                     #! PC = 0x5555555789e3 *)
xor ymm13_0@uint64 ymm13_0 ymm12_0;
xor ymm13_1@uint64 ymm13_1 ymm12_1;
xor ymm13_2@uint64 ymm13_2 ymm12_2;
xor ymm13_3@uint64 ymm13_3 ymm12_3;
(* vmovdqa %ymm5,-0x2f0(%rbp)                      #! EA = L0x7fffffffbc60; PC = 0x5555555789e8 *)
mov L0x7fffffffbc60 ymm5_0;
mov L0x7fffffffbc68 ymm5_1;
mov L0x7fffffffbc70 ymm5_2;
mov L0x7fffffffbc78 ymm5_3;
(* vpxor  -0xf0(%rbp),%ymm3,%ymm1                  #! EA = L0x7fffffffbe60; Value = 0xa284df955f5574b8; PC = 0x5555555789f0 *)
xor ymm1_0@uint64 ymm3_0 L0x7fffffffbe60;
xor ymm1_1@uint64 ymm3_1 L0x7fffffffbe68;
xor ymm1_2@uint64 ymm3_2 L0x7fffffffbe70;
xor ymm1_3@uint64 ymm3_3 L0x7fffffffbe78;
(* vpxor  %ymm5,%ymm10,%ymm11                      #! PC = 0x5555555789f8 *)
xor ymm11_0@uint64 ymm10_0 ymm5_0;
xor ymm11_1@uint64 ymm10_1 ymm5_1;
xor ymm11_2@uint64 ymm10_2 ymm5_2;
xor ymm11_3@uint64 ymm10_3 ymm5_3;
(* vpandn %ymm15,%ymm4,%ymm3                       #! PC = 0x5555555789fc *)
not ymm4_0n@uint64 ymm4_0;
and ymm3_0@uint64 ymm4_0n ymm15_0;
not ymm4_1n@uint64 ymm4_1;
and ymm3_1@uint64 ymm4_1n ymm15_1;
not ymm4_2n@uint64 ymm4_2;
and ymm3_2@uint64 ymm4_2n ymm15_2;
not ymm4_3n@uint64 ymm4_3;
and ymm3_3@uint64 ymm4_3n ymm15_3;
(* vpsllq $0x1,%ymm14,%ymm5                        #! PC = 0x555555578a01 *)
shl ymm5_0 ymm14_0 0x1@uint64;
shl ymm5_1 ymm14_1 0x1@uint64;
shl ymm5_2 ymm14_2 0x1@uint64;
shl ymm5_3 ymm14_3 0x1@uint64;
(* vpsllq $0x1,%ymm13,%ymm4                        #! PC = 0x555555578a07 *)
shl ymm4_0 ymm13_0 0x1@uint64;
shl ymm4_1 ymm13_1 0x1@uint64;
shl ymm4_2 ymm13_2 0x1@uint64;
shl ymm4_3 ymm13_3 0x1@uint64;
(* vpxor  %ymm0,%ymm3,%ymm3                        #! PC = 0x555555578a0d *)
xor ymm3_0@uint64 ymm3_0 ymm0_0;
xor ymm3_1@uint64 ymm3_1 ymm0_1;
xor ymm3_2@uint64 ymm3_2 ymm0_2;
xor ymm3_3@uint64 ymm3_3 ymm0_3;
(* vmovdqa -0xd0(%rbp),%ymm15                      #! EA = L0x7fffffffbe80; Value = 0x7734677d193a2578; PC = 0x555555578a11 *)
mov ymm15_0 L0x7fffffffbe80;
mov ymm15_1 L0x7fffffffbe88;
mov ymm15_2 L0x7fffffffbe90;
mov ymm15_3 L0x7fffffffbe98;
(* vpxor  %ymm1,%ymm11,%ymm11                      #! PC = 0x555555578a19 *)
xor ymm11_0@uint64 ymm11_0 ymm1_0;
xor ymm11_1@uint64 ymm11_1 ymm1_1;
xor ymm11_2@uint64 ymm11_2 ymm1_2;
xor ymm11_3@uint64 ymm11_3 ymm1_3;
(* vpxor  -0x2b0(%rbp),%ymm15,%ymm0                #! EA = L0x7fffffffbca0; Value = 0x3807beb7e976fca5; PC = 0x555555578a1d *)
xor ymm0_0@uint64 ymm15_0 L0x7fffffffbca0;
xor ymm0_1@uint64 ymm15_1 L0x7fffffffbca8;
xor ymm0_2@uint64 ymm15_2 L0x7fffffffbcb0;
xor ymm0_3@uint64 ymm15_3 L0x7fffffffbcb8;
(* vpxor  -0x150(%rbp),%ymm3,%ymm1                 #! EA = L0x7fffffffbe00; Value = 0x87bcb30fe85bd280; PC = 0x555555578a25 *)
xor ymm1_0@uint64 ymm3_0 L0x7fffffffbe00;
xor ymm1_1@uint64 ymm3_1 L0x7fffffffbe08;
xor ymm1_2@uint64 ymm3_2 L0x7fffffffbe10;
xor ymm1_3@uint64 ymm3_3 L0x7fffffffbe18;
(* vpxor  -0xb0(%rbp),%ymm11,%ymm11                #! EA = L0x7fffffffbea0; Value = 0x3ce653f16dc02838; PC = 0x555555578a2d *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbea0;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbea8;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbeb0;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbeb8;
(* vpxor  %ymm0,%ymm1,%ymm1                        #! PC = 0x555555578a35 *)
xor ymm1_0@uint64 ymm1_0 ymm0_0;
xor ymm1_1@uint64 ymm1_1 ymm0_1;
xor ymm1_2@uint64 ymm1_2 ymm0_2;
xor ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3f,%ymm14,%ymm0                       #! PC = 0x555555578a39 *)
shr ymm0_0 ymm14_0 0x3f@uint64;
shr ymm0_1 ymm14_1 0x3f@uint64;
shr ymm0_2 ymm14_2 0x3f@uint64;
shr ymm0_3 ymm14_3 0x3f@uint64;
(* vpxor  -0x50(%rbp),%ymm1,%ymm1                  #! EA = L0x7fffffffbf00; Value = 0x84501b4167d1303f; PC = 0x555555578a3f *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbf00;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbf08;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbf10;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbf18;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x555555578a44 *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpsrlq $0x3f,%ymm13,%ymm0                       #! PC = 0x555555578a48 *)
shr ymm0_0 ymm13_0 0x3f@uint64;
shr ymm0_1 ymm13_1 0x3f@uint64;
shr ymm0_2 ymm13_2 0x3f@uint64;
shr ymm0_3 ymm13_3 0x3f@uint64;
(* vpsrlq $0x3f,%ymm11,%ymm15                      #! PC = 0x555555578a4e *)
shr ymm15_0 ymm11_0 0x3f@uint64;
shr ymm15_1 ymm11_1 0x3f@uint64;
shr ymm15_2 ymm11_2 0x3f@uint64;
shr ymm15_3 ymm11_3 0x3f@uint64;
(* vpor   %ymm0,%ymm4,%ymm4                        #! PC = 0x555555578a54 *)
or ymm4_0@uint64 ymm4_0 ymm0_0;
or ymm4_1@uint64 ymm4_1 ymm0_1;
or ymm4_2@uint64 ymm4_2 ymm0_2;
or ymm4_3@uint64 ymm4_3 ymm0_3;
(* vpxor  %ymm1,%ymm5,%ymm5                        #! PC = 0x555555578a58 *)
xor ymm5_0@uint64 ymm5_0 ymm1_0;
xor ymm5_1@uint64 ymm5_1 ymm1_1;
xor ymm5_2@uint64 ymm5_2 ymm1_2;
xor ymm5_3@uint64 ymm5_3 ymm1_3;
(* vpsllq $0x1,%ymm11,%ymm0                        #! PC = 0x555555578a5c *)
shl ymm0_0 ymm11_0 0x1@uint64;
shl ymm0_1 ymm11_1 0x1@uint64;
shl ymm0_2 ymm11_2 0x1@uint64;
shl ymm0_3 ymm11_3 0x1@uint64;
(* vpxor  %ymm2,%ymm4,%ymm4                        #! PC = 0x555555578a62 *)
xor ymm4_0@uint64 ymm4_0 ymm2_0;
xor ymm4_1@uint64 ymm4_1 ymm2_1;
xor ymm4_2@uint64 ymm4_2 ymm2_2;
xor ymm4_3@uint64 ymm4_3 ymm2_3;
(* vpxor  %ymm6,%ymm5,%ymm6                        #! PC = 0x555555578a66 *)
xor ymm6_0@uint64 ymm5_0 ymm6_0;
xor ymm6_1@uint64 ymm5_1 ymm6_1;
xor ymm6_2@uint64 ymm5_2 ymm6_2;
xor ymm6_3@uint64 ymm5_3 ymm6_3;
(* vpor   %ymm15,%ymm0,%ymm0                       #! PC = 0x555555578a6a *)
or ymm0_0@uint64 ymm0_0 ymm15_0;
or ymm0_1@uint64 ymm0_1 ymm15_1;
or ymm0_2@uint64 ymm0_2 ymm15_2;
or ymm0_3@uint64 ymm0_3 ymm15_3;
(* vpxor  %ymm8,%ymm4,%ymm8                        #! PC = 0x555555578a6f *)
xor ymm8_0@uint64 ymm4_0 ymm8_0;
xor ymm8_1@uint64 ymm4_1 ymm8_1;
xor ymm8_2@uint64 ymm4_2 ymm8_2;
xor ymm8_3@uint64 ymm4_3 ymm8_3;
(* vpxor  %ymm7,%ymm4,%ymm7                        #! PC = 0x555555578a74 *)
xor ymm7_0@uint64 ymm4_0 ymm7_0;
xor ymm7_1@uint64 ymm4_1 ymm7_1;
xor ymm7_2@uint64 ymm4_2 ymm7_2;
xor ymm7_3@uint64 ymm4_3 ymm7_3;
(* vpxor  %ymm14,%ymm0,%ymm14                      #! PC = 0x555555578a78 *)
xor ymm14_0@uint64 ymm0_0 ymm14_0;
xor ymm14_1@uint64 ymm0_1 ymm14_1;
xor ymm14_2@uint64 ymm0_2 ymm14_2;
xor ymm14_3@uint64 ymm0_3 ymm14_3;
(* vpsrlq $0x3f,%ymm1,%ymm0                        #! PC = 0x555555578a7d *)
shr ymm0_0 ymm1_0 0x3f@uint64;
shr ymm0_1 ymm1_1 0x3f@uint64;
shr ymm0_2 ymm1_2 0x3f@uint64;
shr ymm0_3 ymm1_3 0x3f@uint64;
(* vpsllq $0x1,%ymm1,%ymm1                         #! PC = 0x555555578a82 *)
shl ymm1_0 ymm1_0 0x1@uint64;
shl ymm1_1 ymm1_1 0x1@uint64;
shl ymm1_2 ymm1_2 0x1@uint64;
shl ymm1_3 ymm1_3 0x1@uint64;
(* vpxor  %ymm9,%ymm14,%ymm9                       #! PC = 0x555555578a87 *)
xor ymm9_0@uint64 ymm14_0 ymm9_0;
xor ymm9_1@uint64 ymm14_1 ymm9_1;
xor ymm9_2@uint64 ymm14_2 ymm9_2;
xor ymm9_3@uint64 ymm14_3 ymm9_3;
(* vpxor  %ymm12,%ymm14,%ymm12                     #! PC = 0x555555578a8c *)
xor ymm12_0@uint64 ymm14_0 ymm12_0;
xor ymm12_1@uint64 ymm14_1 ymm12_1;
xor ymm12_2@uint64 ymm14_2 ymm12_2;
xor ymm12_3@uint64 ymm14_3 ymm12_3;
(* vpor   %ymm0,%ymm1,%ymm1                        #! PC = 0x555555578a91 *)
or ymm1_0@uint64 ymm1_0 ymm0_0;
or ymm1_1@uint64 ymm1_1 ymm0_1;
or ymm1_2@uint64 ymm1_2 ymm0_2;
or ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3f,%ymm2,%ymm0                        #! PC = 0x555555578a95 *)
shr ymm0_0 ymm2_0 0x3f@uint64;
shr ymm0_1 ymm2_1 0x3f@uint64;
shr ymm0_2 ymm2_2 0x3f@uint64;
shr ymm0_3 ymm2_3 0x3f@uint64;
(* vpxor  %ymm13,%ymm1,%ymm13                      #! PC = 0x555555578a9a *)
xor ymm13_0@uint64 ymm1_0 ymm13_0;
xor ymm13_1@uint64 ymm1_1 ymm13_1;
xor ymm13_2@uint64 ymm1_2 ymm13_2;
xor ymm13_3@uint64 ymm1_3 ymm13_3;
(* vpsrlq $0x14,%ymm8,%ymm1                        #! PC = 0x555555578a9f *)
shr ymm1_0 ymm8_0 0x14@uint64;
shr ymm1_1 ymm8_1 0x14@uint64;
shr ymm1_2 ymm8_2 0x14@uint64;
shr ymm1_3 ymm8_3 0x14@uint64;
(* vpsllq $0x2c,%ymm8,%ymm8                        #! PC = 0x555555578aa5 *)
shl ymm8_0 ymm8_0 0x2c@uint64;
shl ymm8_1 ymm8_1 0x2c@uint64;
shl ymm8_2 ymm8_2 0x2c@uint64;
shl ymm8_3 ymm8_3 0x2c@uint64;
(* vpsllq $0x1,%ymm2,%ymm2                         #! PC = 0x555555578aab *)
shl ymm2_0 ymm2_0 0x1@uint64;
shl ymm2_1 ymm2_1 0x1@uint64;
shl ymm2_2 ymm2_2 0x1@uint64;
shl ymm2_3 ymm2_3 0x1@uint64;
(* vpxor  %ymm10,%ymm13,%ymm10                     #! PC = 0x555555578ab0 *)
xor ymm10_0@uint64 ymm13_0 ymm10_0;
xor ymm10_1@uint64 ymm13_1 ymm10_1;
xor ymm10_2@uint64 ymm13_2 ymm10_2;
xor ymm10_3@uint64 ymm13_3 ymm10_3;
(* vpor   %ymm1,%ymm8,%ymm8                        #! PC = 0x555555578ab5 *)
or ymm8_0@uint64 ymm8_0 ymm1_0;
or ymm8_1@uint64 ymm8_1 ymm1_1;
or ymm8_2@uint64 ymm8_2 ymm1_2;
or ymm8_3@uint64 ymm8_3 ymm1_3;
(* vpsrlq $0x15,%ymm9,%ymm1                        #! PC = 0x555555578ab9 *)
shr ymm1_0 ymm9_0 0x15@uint64;
shr ymm1_1 ymm9_1 0x15@uint64;
shr ymm1_2 ymm9_2 0x15@uint64;
shr ymm1_3 ymm9_3 0x15@uint64;
(* vpor   %ymm0,%ymm2,%ymm2                        #! PC = 0x555555578abf *)
or ymm2_0@uint64 ymm2_0 ymm0_0;
or ymm2_1@uint64 ymm2_1 ymm0_1;
or ymm2_2@uint64 ymm2_2 ymm0_2;
or ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsllq $0x2b,%ymm9,%ymm9                        #! PC = 0x555555578ac3 *)
shl ymm9_0 ymm9_0 0x2b@uint64;
shl ymm9_1 ymm9_1 0x2b@uint64;
shl ymm9_2 ymm9_2 0x2b@uint64;
shl ymm9_3 ymm9_3 0x2b@uint64;
(* vpxor  -0x90(%rbp),%ymm5,%ymm0                  #! EA = L0x7fffffffbec0; Value = 0x5d350f71494fc8f7; PC = 0x555555578ac9 *)
xor ymm0_0@uint64 ymm5_0 L0x7fffffffbec0;
xor ymm0_1@uint64 ymm5_1 L0x7fffffffbec8;
xor ymm0_2@uint64 ymm5_2 L0x7fffffffbed0;
xor ymm0_3@uint64 ymm5_3 L0x7fffffffbed8;
(* vpxor  %ymm11,%ymm2,%ymm11                      #! PC = 0x555555578ad1 *)
xor ymm11_0@uint64 ymm2_0 ymm11_0;
xor ymm11_1@uint64 ymm2_1 ymm11_1;
xor ymm11_2@uint64 ymm2_2 ymm11_2;
xor ymm11_3@uint64 ymm2_3 ymm11_3;
(* vpor   %ymm1,%ymm9,%ymm9                        #! PC = 0x555555578ad6 *)
or ymm9_0@uint64 ymm9_0 ymm1_0;
or ymm9_1@uint64 ymm9_1 ymm1_1;
or ymm9_2@uint64 ymm9_2 ymm1_2;
or ymm9_3@uint64 ymm9_3 ymm1_3;
(* vmovq  %r9,%xmm1                                #! PC = 0x555555578ada *)
mov xmm1_0 r9;
mov xmm1_1 0@uint64;
(* vpxor  %ymm3,%ymm11,%ymm3                       #! PC = 0x555555578adf *)
xor ymm3_0@uint64 ymm11_0 ymm3_0;
xor ymm3_1@uint64 ymm11_1 ymm3_1;
xor ymm3_2@uint64 ymm11_2 ymm3_2;
xor ymm3_3@uint64 ymm11_3 ymm3_3;
(* vpandn %ymm9,%ymm8,%ymm2                        #! PC = 0x555555578ae3 *)
not ymm8_0n@uint64 ymm8_0;
and ymm2_0@uint64 ymm8_0n ymm9_0;
not ymm8_1n@uint64 ymm8_1;
and ymm2_1@uint64 ymm8_1n ymm9_1;
not ymm8_2n@uint64 ymm8_2;
and ymm2_2@uint64 ymm8_2n ymm9_2;
not ymm8_3n@uint64 ymm8_3;
and ymm2_3@uint64 ymm8_3n ymm9_3;
(* vpbroadcastq %xmm1,%ymm1                        #! PC = 0x555555578ae8 *)
mov ymm1_0 xmm1_0;
mov ymm1_1 xmm1_0;
mov ymm1_2 xmm1_0;
mov ymm1_3 xmm1_0;
(* vpxor  %ymm2,%ymm1,%ymm1                        #! PC = 0x555555578aed *)
xor ymm1_0@uint64 ymm1_0 ymm2_0;
xor ymm1_1@uint64 ymm1_1 ymm2_1;
xor ymm1_2@uint64 ymm1_2 ymm2_2;
xor ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpxor  %ymm0,%ymm1,%ymm2                        #! PC = 0x555555578af1 *)
xor ymm2_0@uint64 ymm1_0 ymm0_0;
xor ymm2_1@uint64 ymm1_1 ymm0_1;
xor ymm2_2@uint64 ymm1_2 ymm0_2;
xor ymm2_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x2b,%ymm10,%ymm1                       #! PC = 0x555555578af5 *)
shr ymm1_0 ymm10_0 0x2b@uint64;
shr ymm1_1 ymm10_1 0x2b@uint64;
shr ymm1_2 ymm10_2 0x2b@uint64;
shr ymm1_3 ymm10_3 0x2b@uint64;
(* vpsllq $0x15,%ymm10,%ymm10                      #! PC = 0x555555578afb *)
shl ymm10_0 ymm10_0 0x15@uint64;
shl ymm10_1 ymm10_1 0x15@uint64;
shl ymm10_2 ymm10_2 0x15@uint64;
shl ymm10_3 ymm10_3 0x15@uint64;
(* vmovdqa %ymm2,-0x90(%rbp)                       #! EA = L0x7fffffffbec0; PC = 0x555555578b01 *)
mov L0x7fffffffbec0 ymm2_0;
mov L0x7fffffffbec8 ymm2_1;
mov L0x7fffffffbed0 ymm2_2;
mov L0x7fffffffbed8 ymm2_3;
(* vpor   %ymm1,%ymm10,%ymm10                      #! PC = 0x555555578b09 *)
or ymm10_0@uint64 ymm10_0 ymm1_0;
or ymm10_1@uint64 ymm10_1 ymm1_1;
or ymm10_2@uint64 ymm10_2 ymm1_2;
or ymm10_3@uint64 ymm10_3 ymm1_3;
(* vpandn %ymm10,%ymm9,%ymm1                       #! PC = 0x555555578b0d *)
not ymm9_0n@uint64 ymm9_0;
and ymm1_0@uint64 ymm9_0n ymm10_0;
not ymm9_1n@uint64 ymm9_1;
and ymm1_1@uint64 ymm9_1n ymm10_1;
not ymm9_2n@uint64 ymm9_2;
and ymm1_2@uint64 ymm9_2n ymm10_2;
not ymm9_3n@uint64 ymm9_3;
and ymm1_3@uint64 ymm9_3n ymm10_3;
(* vpxor  %ymm8,%ymm1,%ymm2                        #! PC = 0x555555578b12 *)
xor ymm2_0@uint64 ymm1_0 ymm8_0;
xor ymm2_1@uint64 ymm1_1 ymm8_1;
xor ymm2_2@uint64 ymm1_2 ymm8_2;
xor ymm2_3@uint64 ymm1_3 ymm8_3;
(* vpsrlq $0x32,%ymm3,%ymm1                        #! PC = 0x555555578b17 *)
shr ymm1_0 ymm3_0 0x32@uint64;
shr ymm1_1 ymm3_1 0x32@uint64;
shr ymm1_2 ymm3_2 0x32@uint64;
shr ymm1_3 ymm3_3 0x32@uint64;
(* vpandn %ymm8,%ymm0,%ymm8                        #! PC = 0x555555578b1c *)
not ymm0_0n@uint64 ymm0_0;
and ymm8_0@uint64 ymm0_0n ymm8_0;
not ymm0_1n@uint64 ymm0_1;
and ymm8_1@uint64 ymm0_1n ymm8_1;
not ymm0_2n@uint64 ymm0_2;
and ymm8_2@uint64 ymm0_2n ymm8_2;
not ymm0_3n@uint64 ymm0_3;
and ymm8_3@uint64 ymm0_3n ymm8_3;
(* vpsllq $0xe,%ymm3,%ymm3                         #! PC = 0x555555578b21 *)
shl ymm3_0 ymm3_0 0xe@uint64;
shl ymm3_1 ymm3_1 0xe@uint64;
shl ymm3_2 ymm3_2 0xe@uint64;
shl ymm3_3 ymm3_3 0xe@uint64;
(* vmovdqa %ymm2,-0x290(%rbp)                      #! EA = L0x7fffffffbcc0; PC = 0x555555578b26 *)
mov L0x7fffffffbcc0 ymm2_0;
mov L0x7fffffffbcc8 ymm2_1;
mov L0x7fffffffbcd0 ymm2_2;
mov L0x7fffffffbcd8 ymm2_3;
(* vpor   %ymm1,%ymm3,%ymm3                        #! PC = 0x555555578b2e *)
or ymm3_0@uint64 ymm3_0 ymm1_0;
or ymm3_1@uint64 ymm3_1 ymm1_1;
or ymm3_2@uint64 ymm3_2 ymm1_2;
or ymm3_3@uint64 ymm3_3 ymm1_3;
(* vpandn %ymm3,%ymm10,%ymm1                       #! PC = 0x555555578b32 *)
not ymm10_0n@uint64 ymm10_0;
and ymm1_0@uint64 ymm10_0n ymm3_0;
not ymm10_1n@uint64 ymm10_1;
and ymm1_1@uint64 ymm10_1n ymm3_1;
not ymm10_2n@uint64 ymm10_2;
and ymm1_2@uint64 ymm10_2n ymm3_2;
not ymm10_3n@uint64 ymm10_3;
and ymm1_3@uint64 ymm10_3n ymm3_3;
(* vpxor  %ymm3,%ymm8,%ymm8                        #! PC = 0x555555578b36 *)
xor ymm8_0@uint64 ymm8_0 ymm3_0;
xor ymm8_1@uint64 ymm8_1 ymm3_1;
xor ymm8_2@uint64 ymm8_2 ymm3_2;
xor ymm8_3@uint64 ymm8_3 ymm3_3;
(* vpxor  %ymm9,%ymm1,%ymm9                        #! PC = 0x555555578b3a *)
xor ymm9_0@uint64 ymm1_0 ymm9_0;
xor ymm9_1@uint64 ymm1_1 ymm9_1;
xor ymm9_2@uint64 ymm1_2 ymm9_2;
xor ymm9_3@uint64 ymm1_3 ymm9_3;
(* vpandn %ymm0,%ymm3,%ymm1                        #! PC = 0x555555578b3f *)
not ymm3_0n@uint64 ymm3_0;
and ymm1_0@uint64 ymm3_0n ymm0_0;
not ymm3_1n@uint64 ymm3_1;
and ymm1_1@uint64 ymm3_1n ymm0_1;
not ymm3_2n@uint64 ymm3_2;
and ymm1_2@uint64 ymm3_2n ymm0_2;
not ymm3_3n@uint64 ymm3_3;
and ymm1_3@uint64 ymm3_3n ymm0_3;
(* vmovdqa %ymm8,-0x250(%rbp)                      #! EA = L0x7fffffffbd00; PC = 0x555555578b43 *)
mov L0x7fffffffbd00 ymm8_0;
mov L0x7fffffffbd08 ymm8_1;
mov L0x7fffffffbd10 ymm8_2;
mov L0x7fffffffbd18 ymm8_3;
(* vmovdqa %ymm9,-0x270(%rbp)                      #! EA = L0x7fffffffbce0; PC = 0x555555578b4b *)
mov L0x7fffffffbce0 ymm9_0;
mov L0x7fffffffbce8 ymm9_1;
mov L0x7fffffffbcf0 ymm9_2;
mov L0x7fffffffbcf8 ymm9_3;
(* vpxor  %ymm10,%ymm1,%ymm9                       #! PC = 0x555555578b53 *)
xor ymm9_0@uint64 ymm1_0 ymm10_0;
xor ymm9_1@uint64 ymm1_1 ymm10_1;
xor ymm9_2@uint64 ymm1_2 ymm10_2;
xor ymm9_3@uint64 ymm1_3 ymm10_3;
(* vpxor  -0xf0(%rbp),%ymm13,%ymm10                #! EA = L0x7fffffffbe60; Value = 0xa284df955f5574b8; PC = 0x555555578b58 *)
xor ymm10_0@uint64 ymm13_0 L0x7fffffffbe60;
xor ymm10_1@uint64 ymm13_1 L0x7fffffffbe68;
xor ymm10_2@uint64 ymm13_2 L0x7fffffffbe70;
xor ymm10_3@uint64 ymm13_3 L0x7fffffffbe78;
(* vmovdqa %ymm9,-0x130(%rbp)                      #! EA = L0x7fffffffbe20; PC = 0x555555578b60 *)
mov L0x7fffffffbe20 ymm9_0;
mov L0x7fffffffbe28 ymm9_1;
mov L0x7fffffffbe30 ymm9_2;
mov L0x7fffffffbe38 ymm9_3;
(* vpxor  -0xb0(%rbp),%ymm13,%ymm9                 #! EA = L0x7fffffffbea0; Value = 0x3ce653f16dc02838; PC = 0x555555578b68 *)
xor ymm9_0@uint64 ymm13_0 L0x7fffffffbea0;
xor ymm9_1@uint64 ymm13_1 L0x7fffffffbea8;
xor ymm9_2@uint64 ymm13_2 L0x7fffffffbeb0;
xor ymm9_3@uint64 ymm13_3 L0x7fffffffbeb8;
(* vpsrlq $0x24,%ymm10,%ymm1                       #! PC = 0x555555578b70 *)
shr ymm1_0 ymm10_0 0x24@uint64;
shr ymm1_1 ymm10_1 0x24@uint64;
shr ymm1_2 ymm10_2 0x24@uint64;
shr ymm1_3 ymm10_3 0x24@uint64;
(* vpsllq $0x1c,%ymm10,%ymm0                       #! PC = 0x555555578b76 *)
shl ymm0_0 ymm10_0 0x1c@uint64;
shl ymm0_1 ymm10_1 0x1c@uint64;
shl ymm0_2 ymm10_2 0x1c@uint64;
shl ymm0_3 ymm10_3 0x1c@uint64;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x555555578b7c *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0xd0(%rbp),%ymm11,%ymm1                 #! EA = L0x7fffffffbe80; Value = 0x7734677d193a2578; PC = 0x555555578b80 *)
xor ymm1_0@uint64 ymm11_0 L0x7fffffffbe80;
xor ymm1_1@uint64 ymm11_1 L0x7fffffffbe88;
xor ymm1_2@uint64 ymm11_2 L0x7fffffffbe90;
xor ymm1_3@uint64 ymm11_3 L0x7fffffffbe98;
(* vpsrlq $0x2c,%ymm1,%ymm2                        #! PC = 0x555555578b88 *)
shr ymm2_0 ymm1_0 0x2c@uint64;
shr ymm2_1 ymm1_1 0x2c@uint64;
shr ymm2_2 ymm1_2 0x2c@uint64;
shr ymm2_3 ymm1_3 0x2c@uint64;
(* vpsllq $0x14,%ymm1,%ymm1                        #! PC = 0x555555578b8d *)
shl ymm1_0 ymm1_0 0x14@uint64;
shl ymm1_1 ymm1_1 0x14@uint64;
shl ymm1_2 ymm1_2 0x14@uint64;
shl ymm1_3 ymm1_3 0x14@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x555555578b92 *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpxor  -0x110(%rbp),%ymm5,%ymm2                 #! EA = L0x7fffffffbe40; Value = 0x3525810e4ec2258f; PC = 0x555555578b96 *)
xor ymm2_0@uint64 ymm5_0 L0x7fffffffbe40;
xor ymm2_1@uint64 ymm5_1 L0x7fffffffbe48;
xor ymm2_2@uint64 ymm5_2 L0x7fffffffbe50;
xor ymm2_3@uint64 ymm5_3 L0x7fffffffbe58;
(* vpsrlq $0x3d,%ymm2,%ymm3                        #! PC = 0x555555578b9e *)
shr ymm3_0 ymm2_0 0x3d@uint64;
shr ymm3_1 ymm2_1 0x3d@uint64;
shr ymm3_2 ymm2_2 0x3d@uint64;
shr ymm3_3 ymm2_3 0x3d@uint64;
(* vpsllq $0x3,%ymm2,%ymm2                         #! PC = 0x555555578ba3 *)
shl ymm2_0 ymm2_0 0x3@uint64;
shl ymm2_1 ymm2_1 0x3@uint64;
shl ymm2_2 ymm2_2 0x3@uint64;
shl ymm2_3 ymm2_3 0x3@uint64;
(* vpor   %ymm3,%ymm2,%ymm2                        #! PC = 0x555555578ba8 *)
or ymm2_0@uint64 ymm2_0 ymm3_0;
or ymm2_1@uint64 ymm2_1 ymm3_1;
or ymm2_2@uint64 ymm2_2 ymm3_2;
or ymm2_3@uint64 ymm2_3 ymm3_3;
(* vpandn %ymm2,%ymm1,%ymm3                        #! PC = 0x555555578bac *)
not ymm1_0n@uint64 ymm1_0;
and ymm3_0@uint64 ymm1_0n ymm2_0;
not ymm1_1n@uint64 ymm1_1;
and ymm3_1@uint64 ymm1_1n ymm2_1;
not ymm1_2n@uint64 ymm1_2;
and ymm3_2@uint64 ymm1_2n ymm2_2;
not ymm1_3n@uint64 ymm1_3;
and ymm3_3@uint64 ymm1_3n ymm2_3;
(* vpxor  %ymm0,%ymm3,%ymm8                        #! PC = 0x555555578bb0 *)
xor ymm8_0@uint64 ymm3_0 ymm0_0;
xor ymm8_1@uint64 ymm3_1 ymm0_1;
xor ymm8_2@uint64 ymm3_2 ymm0_2;
xor ymm8_3@uint64 ymm3_3 ymm0_3;
(* vpsrlq $0x13,%ymm7,%ymm3                        #! PC = 0x555555578bb4 *)
shr ymm3_0 ymm7_0 0x13@uint64;
shr ymm3_1 ymm7_1 0x13@uint64;
shr ymm3_2 ymm7_2 0x13@uint64;
shr ymm3_3 ymm7_3 0x13@uint64;
(* vpsllq $0x2d,%ymm7,%ymm7                        #! PC = 0x555555578bb9 *)
shl ymm7_0 ymm7_0 0x2d@uint64;
shl ymm7_1 ymm7_1 0x2d@uint64;
shl ymm7_2 ymm7_2 0x2d@uint64;
shl ymm7_3 ymm7_3 0x2d@uint64;
(* vmovdqa %ymm8,-0x230(%rbp)                      #! EA = L0x7fffffffbd20; PC = 0x555555578bbe *)
mov L0x7fffffffbd20 ymm8_0;
mov L0x7fffffffbd28 ymm8_1;
mov L0x7fffffffbd30 ymm8_2;
mov L0x7fffffffbd38 ymm8_3;
(* vpor   %ymm3,%ymm7,%ymm15                       #! PC = 0x555555578bc6 *)
or ymm15_0@uint64 ymm7_0 ymm3_0;
or ymm15_1@uint64 ymm7_1 ymm3_1;
or ymm15_2@uint64 ymm7_2 ymm3_2;
or ymm15_3@uint64 ymm7_3 ymm3_3;
(* vpsrlq $0x3,%ymm12,%ymm3                        #! PC = 0x555555578bca *)
shr ymm3_0 ymm12_0 0x3@uint64;
shr ymm3_1 ymm12_1 0x3@uint64;
shr ymm3_2 ymm12_2 0x3@uint64;
shr ymm3_3 ymm12_3 0x3@uint64;
(* vpsllq $0x3d,%ymm12,%ymm12                      #! PC = 0x555555578bd0 *)
shl ymm12_0 ymm12_0 0x3d@uint64;
shl ymm12_1 ymm12_1 0x3d@uint64;
shl ymm12_2 ymm12_2 0x3d@uint64;
shl ymm12_3 ymm12_3 0x3d@uint64;
(* vpandn %ymm15,%ymm2,%ymm8                       #! PC = 0x555555578bd6 *)
not ymm2_0n@uint64 ymm2_0;
and ymm8_0@uint64 ymm2_0n ymm15_0;
not ymm2_1n@uint64 ymm2_1;
and ymm8_1@uint64 ymm2_1n ymm15_1;
not ymm2_2n@uint64 ymm2_2;
and ymm8_2@uint64 ymm2_2n ymm15_2;
not ymm2_3n@uint64 ymm2_3;
and ymm8_3@uint64 ymm2_3n ymm15_3;
(* vpor   %ymm3,%ymm12,%ymm12                      #! PC = 0x555555578bdb *)
or ymm12_0@uint64 ymm12_0 ymm3_0;
or ymm12_1@uint64 ymm12_1 ymm3_1;
or ymm12_2@uint64 ymm12_2 ymm3_2;
or ymm12_3@uint64 ymm12_3 ymm3_3;
(* vpxor  %ymm1,%ymm8,%ymm8                        #! PC = 0x555555578bdf *)
xor ymm8_0@uint64 ymm8_0 ymm1_0;
xor ymm8_1@uint64 ymm8_1 ymm1_1;
xor ymm8_2@uint64 ymm8_2 ymm1_2;
xor ymm8_3@uint64 ymm8_3 ymm1_3;
(* vpandn %ymm12,%ymm15,%ymm3                      #! PC = 0x555555578be3 *)
not ymm15_0n@uint64 ymm15_0;
and ymm3_0@uint64 ymm15_0n ymm12_0;
not ymm15_1n@uint64 ymm15_1;
and ymm3_1@uint64 ymm15_1n ymm12_1;
not ymm15_2n@uint64 ymm15_2;
and ymm3_2@uint64 ymm15_2n ymm12_2;
not ymm15_3n@uint64 ymm15_3;
and ymm3_3@uint64 ymm15_3n ymm12_3;
(* vpxor  %ymm2,%ymm3,%ymm10                       #! PC = 0x555555578be8 *)
xor ymm10_0@uint64 ymm3_0 ymm2_0;
xor ymm10_1@uint64 ymm3_1 ymm2_1;
xor ymm10_2@uint64 ymm3_2 ymm2_2;
xor ymm10_3@uint64 ymm3_3 ymm2_3;
(* vpsrlq $0x27,%ymm9,%ymm3                        #! PC = 0x555555578bec *)
shr ymm3_0 ymm9_0 0x27@uint64;
shr ymm3_1 ymm9_1 0x27@uint64;
shr ymm3_2 ymm9_2 0x27@uint64;
shr ymm3_3 ymm9_3 0x27@uint64;
(* vpandn %ymm0,%ymm12,%ymm2                       #! PC = 0x555555578bf2 *)
not ymm12_0n@uint64 ymm12_0;
and ymm2_0@uint64 ymm12_0n ymm0_0;
not ymm12_1n@uint64 ymm12_1;
and ymm2_1@uint64 ymm12_1n ymm0_1;
not ymm12_2n@uint64 ymm12_2;
and ymm2_2@uint64 ymm12_2n ymm0_2;
not ymm12_3n@uint64 ymm12_3;
and ymm2_3@uint64 ymm12_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x555555578bf6 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm15,%ymm2,%ymm2                       #! PC = 0x555555578bfa *)
xor ymm2_0@uint64 ymm2_0 ymm15_0;
xor ymm2_1@uint64 ymm2_1 ymm15_1;
xor ymm2_2@uint64 ymm2_2 ymm15_2;
xor ymm2_3@uint64 ymm2_3 ymm15_3;
(* vmovdqa %ymm10,-0x110(%rbp)                     #! EA = L0x7fffffffbe40; PC = 0x555555578bff *)
mov L0x7fffffffbe40 ymm10_0;
mov L0x7fffffffbe48 ymm10_1;
mov L0x7fffffffbe50 ymm10_2;
mov L0x7fffffffbe58 ymm10_3;
(* vpxor  %ymm12,%ymm0,%ymm12                      #! PC = 0x555555578c07 *)
xor ymm12_0@uint64 ymm0_0 ymm12_0;
xor ymm12_1@uint64 ymm0_1 ymm12_1;
xor ymm12_2@uint64 ymm0_2 ymm12_2;
xor ymm12_3@uint64 ymm0_3 ymm12_3;
(* vpxor  -0x1d0(%rbp),%ymm4,%ymm0                 #! EA = L0x7fffffffbd80; Value = 0xe37bf573d8aafe2d; PC = 0x555555578c0c *)
xor ymm0_0@uint64 ymm4_0 L0x7fffffffbd80;
xor ymm0_1@uint64 ymm4_1 L0x7fffffffbd88;
xor ymm0_2@uint64 ymm4_2 L0x7fffffffbd90;
xor ymm0_3@uint64 ymm4_3 L0x7fffffffbd98;
(* vmovdqa %ymm2,-0x210(%rbp)                      #! EA = L0x7fffffffbd40; PC = 0x555555578c14 *)
mov L0x7fffffffbd40 ymm2_0;
mov L0x7fffffffbd48 ymm2_1;
mov L0x7fffffffbd50 ymm2_2;
mov L0x7fffffffbd58 ymm2_3;
(* vpxor  -0x170(%rbp),%ymm14,%ymm2                #! EA = L0x7fffffffbde0; Value = 0x8e03bdc59cf78585; PC = 0x555555578c1c *)
xor ymm2_0@uint64 ymm14_0 L0x7fffffffbde0;
xor ymm2_1@uint64 ymm14_1 L0x7fffffffbde8;
xor ymm2_2@uint64 ymm14_2 L0x7fffffffbdf0;
xor ymm2_3@uint64 ymm14_3 L0x7fffffffbdf8;
(* vpsllq $0x19,%ymm9,%ymm9                        #! PC = 0x555555578c24 *)
shl ymm9_0 ymm9_0 0x19@uint64;
shl ymm9_1 ymm9_1 0x19@uint64;
shl ymm9_2 ymm9_2 0x19@uint64;
shl ymm9_3 ymm9_3 0x19@uint64;
(* vmovdqa %ymm12,-0xf0(%rbp)                      #! EA = L0x7fffffffbe60; PC = 0x555555578c2a *)
mov L0x7fffffffbe60 ymm12_0;
mov L0x7fffffffbe68 ymm12_1;
mov L0x7fffffffbe70 ymm12_2;
mov L0x7fffffffbe78 ymm12_3;
(* vpsrlq $0x3f,%ymm0,%ymm1                        #! PC = 0x555555578c32 *)
shr ymm1_0 ymm0_0 0x3f@uint64;
shr ymm1_1 ymm0_1 0x3f@uint64;
shr ymm1_2 ymm0_2 0x3f@uint64;
shr ymm1_3 ymm0_3 0x3f@uint64;
(* vpsllq $0x1,%ymm0,%ymm0                         #! PC = 0x555555578c37 *)
shl ymm0_0 ymm0_0 0x1@uint64;
shl ymm0_1 ymm0_1 0x1@uint64;
shl ymm0_2 ymm0_2 0x1@uint64;
shl ymm0_3 ymm0_3 0x1@uint64;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x555555578c3c *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpsrlq $0x3a,%ymm2,%ymm1                        #! PC = 0x555555578c40 *)
shr ymm1_0 ymm2_0 0x3a@uint64;
shr ymm1_1 ymm2_1 0x3a@uint64;
shr ymm1_2 ymm2_2 0x3a@uint64;
shr ymm1_3 ymm2_3 0x3a@uint64;
(* vpsllq $0x6,%ymm2,%ymm2                         #! PC = 0x555555578c45 *)
shl ymm2_0 ymm2_0 0x6@uint64;
shl ymm2_1 ymm2_1 0x6@uint64;
shl ymm2_2 ymm2_2 0x6@uint64;
shl ymm2_3 ymm2_3 0x6@uint64;
(* vpor   %ymm1,%ymm2,%ymm2                        #! PC = 0x555555578c4a *)
or ymm2_0@uint64 ymm2_0 ymm1_0;
or ymm2_1@uint64 ymm2_1 ymm1_1;
or ymm2_2@uint64 ymm2_2 ymm1_2;
or ymm2_3@uint64 ymm2_3 ymm1_3;
(* vpor   %ymm3,%ymm9,%ymm1                        #! PC = 0x555555578c4e *)
or ymm1_0@uint64 ymm9_0 ymm3_0;
or ymm1_1@uint64 ymm9_1 ymm3_1;
or ymm1_2@uint64 ymm9_2 ymm3_2;
or ymm1_3@uint64 ymm9_3 ymm3_3;
(* vpandn %ymm1,%ymm2,%ymm3                        #! PC = 0x555555578c52 *)
not ymm2_0n@uint64 ymm2_0;
and ymm3_0@uint64 ymm2_0n ymm1_0;
not ymm2_1n@uint64 ymm2_1;
and ymm3_1@uint64 ymm2_1n ymm1_1;
not ymm2_2n@uint64 ymm2_2;
and ymm3_2@uint64 ymm2_2n ymm1_2;
not ymm2_3n@uint64 ymm2_3;
and ymm3_3@uint64 ymm2_3n ymm1_3;
(* vpxor  %ymm0,%ymm3,%ymm12                       #! PC = 0x555555578c56 *)
xor ymm12_0@uint64 ymm3_0 ymm0_0;
xor ymm12_1@uint64 ymm3_1 ymm0_1;
xor ymm12_2@uint64 ymm3_2 ymm0_2;
xor ymm12_3@uint64 ymm3_3 ymm0_3;
(* vpxor  -0x50(%rbp),%ymm11,%ymm3                 #! EA = L0x7fffffffbf00; Value = 0x84501b4167d1303f; PC = 0x555555578c5a *)
xor ymm3_0@uint64 ymm11_0 L0x7fffffffbf00;
xor ymm3_1@uint64 ymm11_1 L0x7fffffffbf08;
xor ymm3_2@uint64 ymm11_2 L0x7fffffffbf10;
xor ymm3_3@uint64 ymm11_3 L0x7fffffffbf18;
(* vmovdqa %ymm12,-0xd0(%rbp)                      #! EA = L0x7fffffffbe80; PC = 0x555555578c5f *)
mov L0x7fffffffbe80 ymm12_0;
mov L0x7fffffffbe88 ymm12_1;
mov L0x7fffffffbe90 ymm12_2;
mov L0x7fffffffbe98 ymm12_3;
(* vpshufb 0x55290(%rip),%ymm3,%ymm3        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x555555578c67 *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm3_0, ymm3_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm3_2, ymm3_3, tmp_2, tmp_3);
mov ymm3_0 tmp_0;
mov ymm3_1 tmp_1;
mov ymm3_2 tmp_2;
mov ymm3_3 tmp_3;
(* vpandn %ymm3,%ymm1,%ymm7                        #! PC = 0x555555578c70 *)
not ymm1_0n@uint64 ymm1_0;
and ymm7_0@uint64 ymm1_0n ymm3_0;
not ymm1_1n@uint64 ymm1_1;
and ymm7_1@uint64 ymm1_1n ymm3_1;
not ymm1_2n@uint64 ymm1_2;
and ymm7_2@uint64 ymm1_2n ymm3_2;
not ymm1_3n@uint64 ymm1_3;
and ymm7_3@uint64 ymm1_3n ymm3_3;
(* vpxor  %ymm2,%ymm7,%ymm10                       #! PC = 0x555555578c74 *)
xor ymm10_0@uint64 ymm7_0 ymm2_0;
xor ymm10_1@uint64 ymm7_1 ymm2_1;
xor ymm10_2@uint64 ymm7_2 ymm2_2;
xor ymm10_3@uint64 ymm7_3 ymm2_3;
(* vpsrlq $0x2e,%ymm6,%ymm7                        #! PC = 0x555555578c78 *)
shr ymm7_0 ymm6_0 0x2e@uint64;
shr ymm7_1 ymm6_1 0x2e@uint64;
shr ymm7_2 ymm6_2 0x2e@uint64;
shr ymm7_3 ymm6_3 0x2e@uint64;
(* vpsllq $0x12,%ymm6,%ymm6                        #! PC = 0x555555578c7d *)
shl ymm6_0 ymm6_0 0x12@uint64;
shl ymm6_1 ymm6_1 0x12@uint64;
shl ymm6_2 ymm6_2 0x12@uint64;
shl ymm6_3 ymm6_3 0x12@uint64;
(* vmovdqa %ymm10,-0x1d0(%rbp)                     #! EA = L0x7fffffffbd80; PC = 0x555555578c82 *)
mov L0x7fffffffbd80 ymm10_0;
mov L0x7fffffffbd88 ymm10_1;
mov L0x7fffffffbd90 ymm10_2;
mov L0x7fffffffbd98 ymm10_3;
(* vpxor  -0x70(%rbp),%ymm14,%ymm10                #! EA = L0x7fffffffbee0; Value = 0x6fb9736051442cc0; PC = 0x555555578c8a *)
xor ymm10_0@uint64 ymm14_0 L0x7fffffffbee0;
xor ymm10_1@uint64 ymm14_1 L0x7fffffffbee8;
xor ymm10_2@uint64 ymm14_2 L0x7fffffffbef0;
xor ymm10_3@uint64 ymm14_3 L0x7fffffffbef8;
(* vpor   %ymm7,%ymm6,%ymm12                       #! PC = 0x555555578c8f *)
or ymm12_0@uint64 ymm6_0 ymm7_0;
or ymm12_1@uint64 ymm6_1 ymm7_1;
or ymm12_2@uint64 ymm6_2 ymm7_2;
or ymm12_3@uint64 ymm6_3 ymm7_3;
(* vpxor  -0x190(%rbp),%ymm4,%ymm7                 #! EA = L0x7fffffffbdc0; Value = 0xf1d6dfc015c6d99d; PC = 0x555555578c93 *)
xor ymm7_0@uint64 ymm4_0 L0x7fffffffbdc0;
xor ymm7_1@uint64 ymm4_1 L0x7fffffffbdc8;
xor ymm7_2@uint64 ymm4_2 L0x7fffffffbdd0;
xor ymm7_3@uint64 ymm4_3 L0x7fffffffbdd8;
(* vpandn %ymm12,%ymm3,%ymm9                       #! PC = 0x555555578c9b *)
not ymm3_0n@uint64 ymm3_0;
and ymm9_0@uint64 ymm3_0n ymm12_0;
not ymm3_1n@uint64 ymm3_1;
and ymm9_1@uint64 ymm3_1n ymm12_1;
not ymm3_2n@uint64 ymm3_2;
and ymm9_2@uint64 ymm3_2n ymm12_2;
not ymm3_3n@uint64 ymm3_3;
and ymm9_3@uint64 ymm3_3n ymm12_3;
(* vpxor  %ymm1,%ymm9,%ymm9                        #! PC = 0x555555578ca0 *)
xor ymm9_0@uint64 ymm9_0 ymm1_0;
xor ymm9_1@uint64 ymm9_1 ymm1_1;
xor ymm9_2@uint64 ymm9_2 ymm1_2;
xor ymm9_3@uint64 ymm9_3 ymm1_3;
(* vpandn %ymm0,%ymm12,%ymm1                       #! PC = 0x555555578ca4 *)
not ymm12_0n@uint64 ymm12_0;
and ymm1_0@uint64 ymm12_0n ymm0_0;
not ymm12_1n@uint64 ymm12_1;
and ymm1_1@uint64 ymm12_1n ymm0_1;
not ymm12_2n@uint64 ymm12_2;
and ymm1_2@uint64 ymm12_2n ymm0_2;
not ymm12_3n@uint64 ymm12_3;
and ymm1_3@uint64 ymm12_3n ymm0_3;
(* vpandn %ymm2,%ymm0,%ymm0                        #! PC = 0x555555578ca8 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm2_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm2_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm2_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm2_3;
(* vpxor  %ymm3,%ymm1,%ymm15                       #! PC = 0x555555578cac *)
xor ymm15_0@uint64 ymm1_0 ymm3_0;
xor ymm15_1@uint64 ymm1_1 ymm3_1;
xor ymm15_2@uint64 ymm1_2 ymm3_2;
xor ymm15_3@uint64 ymm1_3 ymm3_3;
(* vpxor  -0x2b0(%rbp),%ymm11,%ymm1                #! EA = L0x7fffffffbca0; Value = 0x3807beb7e976fca5; PC = 0x555555578cb0 *)
xor ymm1_0@uint64 ymm11_0 L0x7fffffffbca0;
xor ymm1_1@uint64 ymm11_1 L0x7fffffffbca8;
xor ymm1_2@uint64 ymm11_2 L0x7fffffffbcb0;
xor ymm1_3@uint64 ymm11_3 L0x7fffffffbcb8;
(* vpxor  %ymm12,%ymm0,%ymm0                       #! PC = 0x555555578cb8 *)
xor ymm0_0@uint64 ymm0_0 ymm12_0;
xor ymm0_1@uint64 ymm0_1 ymm12_1;
xor ymm0_2@uint64 ymm0_2 ymm12_2;
xor ymm0_3@uint64 ymm0_3 ymm12_3;
(* vmovdqa %ymm0,-0x170(%rbp)                      #! EA = L0x7fffffffbde0; PC = 0x555555578cbd *)
mov L0x7fffffffbde0 ymm0_0;
mov L0x7fffffffbde8 ymm0_1;
mov L0x7fffffffbdf0 ymm0_2;
mov L0x7fffffffbdf8 ymm0_3;
(* vpsrlq $0x36,%ymm7,%ymm3                        #! PC = 0x555555578cc5 *)
shr ymm3_0 ymm7_0 0x36@uint64;
shr ymm3_1 ymm7_1 0x36@uint64;
shr ymm3_2 ymm7_2 0x36@uint64;
shr ymm3_3 ymm7_3 0x36@uint64;
(* vpsllq $0xa,%ymm7,%ymm7                         #! PC = 0x555555578cca *)
shl ymm7_0 ymm7_0 0xa@uint64;
shl ymm7_1 ymm7_1 0xa@uint64;
shl ymm7_2 ymm7_2 0xa@uint64;
shl ymm7_3 ymm7_3 0xa@uint64;
(* vpsrlq $0x25,%ymm1,%ymm2                        #! PC = 0x555555578ccf *)
shr ymm2_0 ymm1_0 0x25@uint64;
shr ymm2_1 ymm1_1 0x25@uint64;
shr ymm2_2 ymm1_2 0x25@uint64;
shr ymm2_3 ymm1_3 0x25@uint64;
(* vpsllq $0x1b,%ymm1,%ymm0                        #! PC = 0x555555578cd4 *)
shl ymm0_0 ymm1_0 0x1b@uint64;
shl ymm0_1 ymm1_1 0x1b@uint64;
shl ymm0_2 ymm1_2 0x1b@uint64;
shl ymm0_3 ymm1_3 0x1b@uint64;
(* vpxor  -0x1f0(%rbp),%ymm5,%ymm1                 #! EA = L0x7fffffffbd60; Value = 0x2f3b2ad03fba7fb4; PC = 0x555555578cd9 *)
xor ymm1_0@uint64 ymm5_0 L0x7fffffffbd60;
xor ymm1_1@uint64 ymm5_1 L0x7fffffffbd68;
xor ymm1_2@uint64 ymm5_2 L0x7fffffffbd70;
xor ymm1_3@uint64 ymm5_3 L0x7fffffffbd78;
(* vmovdqa %ymm15,-0xb0(%rbp)                      #! EA = L0x7fffffffbea0; PC = 0x555555578ce1 *)
mov L0x7fffffffbea0 ymm15_0;
mov L0x7fffffffbea8 ymm15_1;
mov L0x7fffffffbeb0 ymm15_2;
mov L0x7fffffffbeb8 ymm15_3;
(* vpor   %ymm2,%ymm0,%ymm0                        #! PC = 0x555555578ce9 *)
or ymm0_0@uint64 ymm0_0 ymm2_0;
or ymm0_1@uint64 ymm0_1 ymm2_1;
or ymm0_2@uint64 ymm0_2 ymm2_2;
or ymm0_3@uint64 ymm0_3 ymm2_3;
(* vpsrlq $0x1c,%ymm1,%ymm2                        #! PC = 0x555555578ced *)
shr ymm2_0 ymm1_0 0x1c@uint64;
shr ymm2_1 ymm1_1 0x1c@uint64;
shr ymm2_2 ymm1_2 0x1c@uint64;
shr ymm2_3 ymm1_3 0x1c@uint64;
(* vpsllq $0x24,%ymm1,%ymm1                        #! PC = 0x555555578cf2 *)
shl ymm1_0 ymm1_0 0x24@uint64;
shl ymm1_1 ymm1_1 0x24@uint64;
shl ymm1_2 ymm1_2 0x24@uint64;
shl ymm1_3 ymm1_3 0x24@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x555555578cf7 *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpor   %ymm3,%ymm7,%ymm2                        #! PC = 0x555555578cfb *)
or ymm2_0@uint64 ymm7_0 ymm3_0;
or ymm2_1@uint64 ymm7_1 ymm3_1;
or ymm2_2@uint64 ymm7_2 ymm3_2;
or ymm2_3@uint64 ymm7_3 ymm3_3;
(* vpandn %ymm2,%ymm1,%ymm3                        #! PC = 0x555555578cff *)
not ymm1_0n@uint64 ymm1_0;
and ymm3_0@uint64 ymm1_0n ymm2_0;
not ymm1_1n@uint64 ymm1_1;
and ymm3_1@uint64 ymm1_1n ymm2_1;
not ymm1_2n@uint64 ymm1_2;
and ymm3_2@uint64 ymm1_2n ymm2_2;
not ymm1_3n@uint64 ymm1_3;
and ymm3_3@uint64 ymm1_3n ymm2_3;
(* vpxor  %ymm0,%ymm3,%ymm7                        #! PC = 0x555555578d03 *)
xor ymm7_0@uint64 ymm3_0 ymm0_0;
xor ymm7_1@uint64 ymm3_1 ymm0_1;
xor ymm7_2@uint64 ymm3_2 ymm0_2;
xor ymm7_3@uint64 ymm3_3 ymm0_3;
(* vpsrlq $0x31,%ymm10,%ymm3                       #! PC = 0x555555578d07 *)
shr ymm3_0 ymm10_0 0x31@uint64;
shr ymm3_1 ymm10_1 0x31@uint64;
shr ymm3_2 ymm10_2 0x31@uint64;
shr ymm3_3 ymm10_3 0x31@uint64;
(* vpsllq $0xf,%ymm10,%ymm10                       #! PC = 0x555555578d0d *)
shl ymm10_0 ymm10_0 0xf@uint64;
shl ymm10_1 ymm10_1 0xf@uint64;
shl ymm10_2 ymm10_2 0xf@uint64;
shl ymm10_3 ymm10_3 0xf@uint64;
(* vmovdqa %ymm7,%ymm12                            #! PC = 0x555555578d13 *)
mov ymm12_0 ymm7_0;
mov ymm12_1 ymm7_1;
mov ymm12_2 ymm7_2;
mov ymm12_3 ymm7_3;
(* vpor   %ymm3,%ymm10,%ymm10                      #! PC = 0x555555578d17 *)
or ymm10_0@uint64 ymm10_0 ymm3_0;
or ymm10_1@uint64 ymm10_1 ymm3_1;
or ymm10_2@uint64 ymm10_2 ymm3_2;
or ymm10_3@uint64 ymm10_3 ymm3_3;
(* vpxor  -0x2f0(%rbp),%ymm13,%ymm3                #! EA = L0x7fffffffbc60; Value = 0x2e9689056e4699b8; PC = 0x555555578d1b *)
xor ymm3_0@uint64 ymm13_0 L0x7fffffffbc60;
xor ymm3_1@uint64 ymm13_1 L0x7fffffffbc68;
xor ymm3_2@uint64 ymm13_2 L0x7fffffffbc70;
xor ymm3_3@uint64 ymm13_3 L0x7fffffffbc78;
(* vpxor  -0x1b0(%rbp),%ymm13,%ymm13               #! EA = L0x7fffffffbda0; Value = 0x982733e2d63b636a; PC = 0x555555578d23 *)
xor ymm13_0@uint64 ymm13_0 L0x7fffffffbda0;
xor ymm13_1@uint64 ymm13_1 L0x7fffffffbda8;
xor ymm13_2@uint64 ymm13_2 L0x7fffffffbdb0;
xor ymm13_3@uint64 ymm13_3 L0x7fffffffbdb8;
(* vmovdqa %ymm12,-0x350(%rbp)                     #! EA = L0x7fffffffbc00; PC = 0x555555578d2b *)
mov L0x7fffffffbc00 ymm12_0;
mov L0x7fffffffbc08 ymm12_1;
mov L0x7fffffffbc10 ymm12_2;
mov L0x7fffffffbc18 ymm12_3;
(* vpxor  -0x150(%rbp),%ymm11,%ymm11               #! EA = L0x7fffffffbe00; Value = 0x87bcb30fe85bd280; PC = 0x555555578d33 *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbe00;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbe08;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbe10;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbe18;
(* vpandn %ymm10,%ymm2,%ymm7                       #! PC = 0x555555578d3b *)
not ymm2_0n@uint64 ymm2_0;
and ymm7_0@uint64 ymm2_0n ymm10_0;
not ymm2_1n@uint64 ymm2_1;
and ymm7_1@uint64 ymm2_1n ymm10_1;
not ymm2_2n@uint64 ymm2_2;
and ymm7_2@uint64 ymm2_2n ymm10_2;
not ymm2_3n@uint64 ymm2_3;
and ymm7_3@uint64 ymm2_3n ymm10_3;
(* vpxor  -0x2d0(%rbp),%ymm14,%ymm14               #! EA = L0x7fffffffbc80; Value = 0x26c80c388310b9ed; PC = 0x555555578d40 *)
xor ymm14_0@uint64 ymm14_0 L0x7fffffffbc80;
xor ymm14_1@uint64 ymm14_1 L0x7fffffffbc88;
xor ymm14_2@uint64 ymm14_2 L0x7fffffffbc90;
xor ymm14_3@uint64 ymm14_3 L0x7fffffffbc98;
(* vpshufb 0x5518f(%rip),%ymm3,%ymm3        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x555555578d48 *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm3_0, ymm3_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm3_2, ymm3_3, tmp_2, tmp_3);
mov ymm3_0 tmp_0;
mov ymm3_1 tmp_1;
mov ymm3_2 tmp_2;
mov ymm3_3 tmp_3;
(* vpxor  %ymm1,%ymm7,%ymm7                        #! PC = 0x555555578d51 *)
xor ymm7_0@uint64 ymm7_0 ymm1_0;
xor ymm7_1@uint64 ymm7_1 ymm1_1;
xor ymm7_2@uint64 ymm7_2 ymm1_2;
xor ymm7_3@uint64 ymm7_3 ymm1_3;
(* vpxor  -0x330(%rbp),%ymm5,%ymm5                 #! EA = L0x7fffffffbc20; Value = 0x9c4e438b1642898e; PC = 0x555555578d55 *)
xor ymm5_0@uint64 ymm5_0 L0x7fffffffbc20;
xor ymm5_1@uint64 ymm5_1 L0x7fffffffbc28;
xor ymm5_2@uint64 ymm5_2 L0x7fffffffbc30;
xor ymm5_3@uint64 ymm5_3 L0x7fffffffbc38;
(* vpandn %ymm3,%ymm10,%ymm6                       #! PC = 0x555555578d5d *)
not ymm10_0n@uint64 ymm10_0;
and ymm6_0@uint64 ymm10_0n ymm3_0;
not ymm10_1n@uint64 ymm10_1;
and ymm6_1@uint64 ymm10_1n ymm3_1;
not ymm10_2n@uint64 ymm10_2;
and ymm6_2@uint64 ymm10_2n ymm3_2;
not ymm10_3n@uint64 ymm10_3;
and ymm6_3@uint64 ymm10_3n ymm3_3;
(* vpxor  -0x310(%rbp),%ymm4,%ymm4                 #! EA = L0x7fffffffbc40; Value = 0xe9e1987c2ef60a70; PC = 0x555555578d61 *)
xor ymm4_0@uint64 ymm4_0 L0x7fffffffbc40;
xor ymm4_1@uint64 ymm4_1 L0x7fffffffbc48;
xor ymm4_2@uint64 ymm4_2 L0x7fffffffbc50;
xor ymm4_3@uint64 ymm4_3 L0x7fffffffbc58;
(* vpxor  %ymm2,%ymm6,%ymm15                       #! PC = 0x555555578d69 *)
xor ymm15_0@uint64 ymm6_0 ymm2_0;
xor ymm15_1@uint64 ymm6_1 ymm2_1;
xor ymm15_2@uint64 ymm6_2 ymm2_2;
xor ymm15_3@uint64 ymm6_3 ymm2_3;
(* vpandn %ymm0,%ymm3,%ymm2                        #! PC = 0x555555578d6d *)
not ymm3_0n@uint64 ymm3_0;
and ymm2_0@uint64 ymm3_0n ymm0_0;
not ymm3_1n@uint64 ymm3_1;
and ymm2_1@uint64 ymm3_1n ymm0_1;
not ymm3_2n@uint64 ymm3_2;
and ymm2_2@uint64 ymm3_2n ymm0_2;
not ymm3_3n@uint64 ymm3_3;
and ymm2_3@uint64 ymm3_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x555555578d71 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm3,%ymm0,%ymm6                        #! PC = 0x555555578d75 *)
xor ymm6_0@uint64 ymm0_0 ymm3_0;
xor ymm6_1@uint64 ymm0_1 ymm3_1;
xor ymm6_2@uint64 ymm0_2 ymm3_2;
xor ymm6_3@uint64 ymm0_3 ymm3_3;
(* vpsrlq $0x9,%ymm13,%ymm0                        #! PC = 0x555555578d79 *)
shr ymm0_0 ymm13_0 0x9@uint64;
shr ymm0_1 ymm13_1 0x9@uint64;
shr ymm0_2 ymm13_2 0x9@uint64;
shr ymm0_3 ymm13_3 0x9@uint64;
(* vmovdqa %ymm15,-0x70(%rbp)                      #! EA = L0x7fffffffbee0; PC = 0x555555578d7f *)
mov L0x7fffffffbee0 ymm15_0;
mov L0x7fffffffbee8 ymm15_1;
mov L0x7fffffffbef0 ymm15_2;
mov L0x7fffffffbef8 ymm15_3;
(* vpxor  %ymm10,%ymm2,%ymm10                      #! PC = 0x555555578d84 *)
xor ymm10_0@uint64 ymm2_0 ymm10_0;
xor ymm10_1@uint64 ymm2_1 ymm10_1;
xor ymm10_2@uint64 ymm2_2 ymm10_2;
xor ymm10_3@uint64 ymm2_3 ymm10_3;
(* vpsllq $0x37,%ymm13,%ymm13                      #! PC = 0x555555578d89 *)
shl ymm13_0 ymm13_0 0x37@uint64;
shl ymm13_1 ymm13_1 0x37@uint64;
shl ymm13_2 ymm13_2 0x37@uint64;
shl ymm13_3 ymm13_3 0x37@uint64;
(* vpsrlq $0x2,%ymm14,%ymm1                        #! PC = 0x555555578d8f *)
shr ymm1_0 ymm14_0 0x2@uint64;
shr ymm1_1 ymm14_1 0x2@uint64;
shr ymm1_2 ymm14_2 0x2@uint64;
shr ymm1_3 ymm14_3 0x2@uint64;
(* vmovdqa %ymm6,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x555555578d95 *)
mov L0x7fffffffbf00 ymm6_0;
mov L0x7fffffffbf08 ymm6_1;
mov L0x7fffffffbf10 ymm6_2;
mov L0x7fffffffbf18 ymm6_3;
(* vmovdqa -0xd0(%rbp),%ymm15                      #! EA = L0x7fffffffbe80; Value = 0x91282d2c07d3c3ea; PC = 0x555555578d9a *)
mov ymm15_0 L0x7fffffffbe80;
mov ymm15_1 L0x7fffffffbe88;
mov ymm15_2 L0x7fffffffbe90;
mov ymm15_3 L0x7fffffffbe98;
(* vpor   %ymm0,%ymm13,%ymm13                      #! PC = 0x555555578da2 *)
or ymm13_0@uint64 ymm13_0 ymm0_0;
or ymm13_1@uint64 ymm13_1 ymm0_1;
or ymm13_2@uint64 ymm13_2 ymm0_2;
or ymm13_3@uint64 ymm13_3 ymm0_3;
(* vpsrlq $0x19,%ymm11,%ymm0                       #! PC = 0x555555578da6 *)
shr ymm0_0 ymm11_0 0x19@uint64;
shr ymm0_1 ymm11_1 0x19@uint64;
shr ymm0_2 ymm11_2 0x19@uint64;
shr ymm0_3 ymm11_3 0x19@uint64;
(* vpsllq $0x27,%ymm11,%ymm11                      #! PC = 0x555555578dac *)
shl ymm11_0 ymm11_0 0x27@uint64;
shl ymm11_1 ymm11_1 0x27@uint64;
shl ymm11_2 ymm11_2 0x27@uint64;
shl ymm11_3 ymm11_3 0x27@uint64;
(* vpsllq $0x3e,%ymm14,%ymm14                      #! PC = 0x555555578db2 *)
shl ymm14_0 ymm14_0 0x3e@uint64;
shl ymm14_1 ymm14_1 0x3e@uint64;
shl ymm14_2 ymm14_2 0x3e@uint64;
shl ymm14_3 ymm14_3 0x3e@uint64;
(* vpor   %ymm0,%ymm11,%ymm11                      #! PC = 0x555555578db8 *)
or ymm11_0@uint64 ymm11_0 ymm0_0;
or ymm11_1@uint64 ymm11_1 ymm0_1;
or ymm11_2@uint64 ymm11_2 ymm0_2;
or ymm11_3@uint64 ymm11_3 ymm0_3;
(* vpor   %ymm1,%ymm14,%ymm3                       #! PC = 0x555555578dbc *)
or ymm3_0@uint64 ymm14_0 ymm1_0;
or ymm3_1@uint64 ymm14_1 ymm1_1;
or ymm3_2@uint64 ymm14_2 ymm1_2;
or ymm3_3@uint64 ymm14_3 ymm1_3;
(* vpxor  -0x230(%rbp),%ymm15,%ymm0                #! EA = L0x7fffffffbd20; Value = 0x4bc8b925db11fc4e; PC = 0x555555578dc0 *)
xor ymm0_0@uint64 ymm15_0 L0x7fffffffbd20;
xor ymm0_1@uint64 ymm15_1 L0x7fffffffbd28;
xor ymm0_2@uint64 ymm15_2 L0x7fffffffbd30;
xor ymm0_3@uint64 ymm15_3 L0x7fffffffbd38;
(* vpandn %ymm11,%ymm13,%ymm6                      #! PC = 0x555555578dc8 *)
not ymm13_0n@uint64 ymm13_0;
and ymm6_0@uint64 ymm13_0n ymm11_0;
not ymm13_1n@uint64 ymm13_1;
and ymm6_1@uint64 ymm13_1n ymm11_1;
not ymm13_2n@uint64 ymm13_2;
and ymm6_2@uint64 ymm13_2n ymm11_2;
not ymm13_3n@uint64 ymm13_3;
and ymm6_3@uint64 ymm13_3n ymm11_3;
(* vpsrlq $0x3e,%ymm4,%ymm1                        #! PC = 0x555555578dcd *)
shr ymm1_0 ymm4_0 0x3e@uint64;
shr ymm1_1 ymm4_1 0x3e@uint64;
shr ymm1_2 ymm4_2 0x3e@uint64;
shr ymm1_3 ymm4_3 0x3e@uint64;
(* vpxor  -0x70(%rbp),%ymm9,%ymm15                 #! EA = L0x7fffffffbee0; Value = 0x84b0356a433bf9a8; PC = 0x555555578dd2 *)
xor ymm15_0@uint64 ymm9_0 L0x7fffffffbee0;
xor ymm15_1@uint64 ymm9_1 L0x7fffffffbee8;
xor ymm15_2@uint64 ymm9_2 L0x7fffffffbef0;
xor ymm15_3@uint64 ymm9_3 L0x7fffffffbef8;
(* vpxor  %ymm3,%ymm6,%ymm6                        #! PC = 0x555555578dd7 *)
xor ymm6_0@uint64 ymm6_0 ymm3_0;
xor ymm6_1@uint64 ymm6_1 ymm3_1;
xor ymm6_2@uint64 ymm6_2 ymm3_2;
xor ymm6_3@uint64 ymm6_3 ymm3_3;
(* vpsllq $0x2,%ymm4,%ymm4                         #! PC = 0x555555578ddb *)
shl ymm4_0 ymm4_0 0x2@uint64;
shl ymm4_1 ymm4_1 0x2@uint64;
shl ymm4_2 ymm4_2 0x2@uint64;
shl ymm4_3 ymm4_3 0x2@uint64;
(* vpxor  %ymm12,%ymm6,%ymm2                       #! PC = 0x555555578de0 *)
xor ymm2_0@uint64 ymm6_0 ymm12_0;
xor ymm2_1@uint64 ymm6_1 ymm12_1;
xor ymm2_2@uint64 ymm6_2 ymm12_2;
xor ymm2_3@uint64 ymm6_3 ymm12_3;
(* vpxor  %ymm0,%ymm2,%ymm2                        #! PC = 0x555555578de5 *)
xor ymm2_0@uint64 ymm2_0 ymm0_0;
xor ymm2_1@uint64 ymm2_1 ymm0_1;
xor ymm2_2@uint64 ymm2_2 ymm0_2;
xor ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x17,%ymm5,%ymm0                        #! PC = 0x555555578de9 *)
shr ymm0_0 ymm5_0 0x17@uint64;
shr ymm0_1 ymm5_1 0x17@uint64;
shr ymm0_2 ymm5_2 0x17@uint64;
shr ymm0_3 ymm5_3 0x17@uint64;
(* vpxor  -0x90(%rbp),%ymm2,%ymm2                  #! EA = L0x7fffffffbec0; Value = 0xfbdf32a8511a215a; PC = 0x555555578dee *)
xor ymm2_0@uint64 ymm2_0 L0x7fffffffbec0;
xor ymm2_1@uint64 ymm2_1 L0x7fffffffbec8;
xor ymm2_2@uint64 ymm2_2 L0x7fffffffbed0;
xor ymm2_3@uint64 ymm2_3 L0x7fffffffbed8;
(* vpsllq $0x29,%ymm5,%ymm5                        #! PC = 0x555555578df6 *)
shl ymm5_0 ymm5_0 0x29@uint64;
shl ymm5_1 ymm5_1 0x29@uint64;
shl ymm5_2 ymm5_2 0x29@uint64;
shl ymm5_3 ymm5_3 0x29@uint64;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x555555578dfb *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpandn %ymm5,%ymm11,%ymm0                       #! PC = 0x555555578dff *)
not ymm11_0n@uint64 ymm11_0;
and ymm0_0@uint64 ymm11_0n ymm5_0;
not ymm11_1n@uint64 ymm11_1;
and ymm0_1@uint64 ymm11_1n ymm5_1;
not ymm11_2n@uint64 ymm11_2;
and ymm0_2@uint64 ymm11_2n ymm5_2;
not ymm11_3n@uint64 ymm11_3;
and ymm0_3@uint64 ymm11_3n ymm5_3;
(* vpxor  %ymm13,%ymm0,%ymm14                      #! PC = 0x555555578e03 *)
xor ymm14_0@uint64 ymm0_0 ymm13_0;
xor ymm14_1@uint64 ymm0_1 ymm13_1;
xor ymm14_2@uint64 ymm0_2 ymm13_2;
xor ymm14_3@uint64 ymm0_3 ymm13_3;
(* vpxor  -0x290(%rbp),%ymm8,%ymm0                 #! EA = L0x7fffffffbcc0; Value = 0x077cc8c29c4070d1; PC = 0x555555578e08 *)
xor ymm0_0@uint64 ymm8_0 L0x7fffffffbcc0;
xor ymm0_1@uint64 ymm8_1 L0x7fffffffbcc8;
xor ymm0_2@uint64 ymm8_2 L0x7fffffffbcd0;
xor ymm0_3@uint64 ymm8_3 L0x7fffffffbcd8;
(* vmovdqa %ymm14,%ymm12                           #! PC = 0x555555578e10 *)
mov ymm12_0 ymm14_0;
mov ymm12_1 ymm14_1;
mov ymm12_2 ymm14_2;
mov ymm12_3 ymm14_3;
(* vpxor  -0x1d0(%rbp),%ymm7,%ymm14                #! EA = L0x7fffffffbd80; Value = 0x55c1963ad365f063; PC = 0x555555578e15 *)
xor ymm14_0@uint64 ymm7_0 L0x7fffffffbd80;
xor ymm14_1@uint64 ymm7_1 L0x7fffffffbd88;
xor ymm14_2@uint64 ymm7_2 L0x7fffffffbd90;
xor ymm14_3@uint64 ymm7_3 L0x7fffffffbd98;
(* vmovdqa %ymm12,-0x330(%rbp)                     #! EA = L0x7fffffffbc20; PC = 0x555555578e1d *)
mov L0x7fffffffbc20 ymm12_0;
mov L0x7fffffffbc28 ymm12_1;
mov L0x7fffffffbc30 ymm12_2;
mov L0x7fffffffbc38 ymm12_3;
(* vpxor  %ymm0,%ymm14,%ymm14                      #! PC = 0x555555578e25 *)
xor ymm14_0@uint64 ymm14_0 ymm0_0;
xor ymm14_1@uint64 ymm14_1 ymm0_1;
xor ymm14_2@uint64 ymm14_2 ymm0_2;
xor ymm14_3@uint64 ymm14_3 ymm0_3;
(* vpor   %ymm1,%ymm4,%ymm0                        #! PC = 0x555555578e29 *)
or ymm0_0@uint64 ymm4_0 ymm1_0;
or ymm0_1@uint64 ymm4_1 ymm1_1;
or ymm0_2@uint64 ymm4_2 ymm1_2;
or ymm0_3@uint64 ymm4_3 ymm1_3;
(* vpandn %ymm13,%ymm3,%ymm4                       #! PC = 0x555555578e2d *)
not ymm3_0n@uint64 ymm3_0;
and ymm4_0@uint64 ymm3_0n ymm13_0;
not ymm3_1n@uint64 ymm3_1;
and ymm4_1@uint64 ymm3_1n ymm13_1;
not ymm3_2n@uint64 ymm3_2;
and ymm4_2@uint64 ymm3_2n ymm13_2;
not ymm3_3n@uint64 ymm3_3;
and ymm4_3@uint64 ymm3_3n ymm13_3;
(* vmovdqa -0xf0(%rbp),%ymm13                      #! EA = L0x7fffffffbe60; Value = 0x9a956cbbb31fa973; PC = 0x555555578e32 *)
mov ymm13_0 L0x7fffffffbe60;
mov ymm13_1 L0x7fffffffbe68;
mov ymm13_2 L0x7fffffffbe70;
mov ymm13_3 L0x7fffffffbe78;
(* vpandn %ymm0,%ymm5,%ymm1                        #! PC = 0x555555578e3a *)
not ymm5_0n@uint64 ymm5_0;
and ymm1_0@uint64 ymm5_0n ymm0_0;
not ymm5_1n@uint64 ymm5_1;
and ymm1_1@uint64 ymm5_1n ymm0_1;
not ymm5_2n@uint64 ymm5_2;
and ymm1_2@uint64 ymm5_2n ymm0_2;
not ymm5_3n@uint64 ymm5_3;
and ymm1_3@uint64 ymm5_3n ymm0_3;
(* vpxor  %ymm12,%ymm14,%ymm14                     #! PC = 0x555555578e3e *)
xor ymm14_0@uint64 ymm14_0 ymm12_0;
xor ymm14_1@uint64 ymm14_1 ymm12_1;
xor ymm14_2@uint64 ymm14_2 ymm12_2;
xor ymm14_3@uint64 ymm14_3 ymm12_3;
(* vpxor  %ymm0,%ymm4,%ymm4                        #! PC = 0x555555578e43 *)
xor ymm4_0@uint64 ymm4_0 ymm0_0;
xor ymm4_1@uint64 ymm4_1 ymm0_1;
xor ymm4_2@uint64 ymm4_2 ymm0_2;
xor ymm4_3@uint64 ymm4_3 ymm0_3;
(* vpxor  %ymm11,%ymm1,%ymm11                      #! PC = 0x555555578e47 *)
xor ymm11_0@uint64 ymm1_0 ymm11_0;
xor ymm11_1@uint64 ymm1_1 ymm11_1;
xor ymm11_2@uint64 ymm1_2 ymm11_2;
xor ymm11_3@uint64 ymm1_3 ymm11_3;
(* vmovdqa -0x110(%rbp),%ymm1                      #! EA = L0x7fffffffbe40; Value = 0x12e5367ba3ec4777; PC = 0x555555578e4c *)
mov ymm1_0 L0x7fffffffbe40;
mov ymm1_1 L0x7fffffffbe48;
mov ymm1_2 L0x7fffffffbe50;
mov ymm1_3 L0x7fffffffbe58;
(* vpxor  -0x270(%rbp),%ymm1,%ymm1                 #! EA = L0x7fffffffbce0; Value = 0xa1d7456dd43b2098; PC = 0x555555578e54 *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbce0;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbce8;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbcf0;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbcf8;
(* vpxor  %ymm1,%ymm15,%ymm15                      #! PC = 0x555555578e5c *)
xor ymm15_0@uint64 ymm15_0 ymm1_0;
xor ymm15_1@uint64 ymm15_1 ymm1_1;
xor ymm15_2@uint64 ymm15_2 ymm1_2;
xor ymm15_3@uint64 ymm15_3 ymm1_3;
(* vpandn %ymm3,%ymm0,%ymm1                        #! PC = 0x555555578e60 *)
not ymm0_0n@uint64 ymm0_0;
and ymm1_0@uint64 ymm0_0n ymm3_0;
not ymm0_1n@uint64 ymm0_1;
and ymm1_1@uint64 ymm0_1n ymm3_1;
not ymm0_2n@uint64 ymm0_2;
and ymm1_2@uint64 ymm0_2n ymm3_2;
not ymm0_3n@uint64 ymm0_3;
and ymm1_3@uint64 ymm0_3n ymm3_3;
(* vpxor  -0x250(%rbp),%ymm13,%ymm0                #! EA = L0x7fffffffbd00; Value = 0x67a8fa7ccafb6fcc; PC = 0x555555578e64 *)
xor ymm0_0@uint64 ymm13_0 L0x7fffffffbd00;
xor ymm0_1@uint64 ymm13_1 L0x7fffffffbd08;
xor ymm0_2@uint64 ymm13_2 L0x7fffffffbd10;
xor ymm0_3@uint64 ymm13_3 L0x7fffffffbd18;
(* vpxor  %ymm5,%ymm1,%ymm1                        #! PC = 0x555555578e6c *)
xor ymm1_0@uint64 ymm1_0 ymm5_0;
xor ymm1_1@uint64 ymm1_1 ymm5_1;
xor ymm1_2@uint64 ymm1_2 ymm5_2;
xor ymm1_3@uint64 ymm1_3 ymm5_3;
(* vpsllq $0x1,%ymm14,%ymm5                        #! PC = 0x555555578e70 *)
shl ymm5_0 ymm14_0 0x1@uint64;
shl ymm5_1 ymm14_1 0x1@uint64;
shl ymm5_2 ymm14_2 0x1@uint64;
shl ymm5_3 ymm14_3 0x1@uint64;
(* vpxor  %ymm11,%ymm15,%ymm15                     #! PC = 0x555555578e76 *)
xor ymm15_0@uint64 ymm15_0 ymm11_0;
xor ymm15_1@uint64 ymm15_1 ymm11_1;
xor ymm15_2@uint64 ymm15_2 ymm11_2;
xor ymm15_3@uint64 ymm15_3 ymm11_3;
(* vpxor  %ymm1,%ymm10,%ymm12                      #! PC = 0x555555578e7b *)
xor ymm12_0@uint64 ymm10_0 ymm1_0;
xor ymm12_1@uint64 ymm10_1 ymm1_1;
xor ymm12_2@uint64 ymm10_2 ymm1_2;
xor ymm12_3@uint64 ymm10_3 ymm1_3;
(* vmovdqa %ymm1,-0x2b0(%rbp)                      #! EA = L0x7fffffffbca0; PC = 0x555555578e7f *)
mov L0x7fffffffbca0 ymm1_0;
mov L0x7fffffffbca8 ymm1_1;
mov L0x7fffffffbcb0 ymm1_2;
mov L0x7fffffffbcb8 ymm1_3;
(* vpsllq $0x1,%ymm15,%ymm3                        #! PC = 0x555555578e87 *)
shl ymm3_0 ymm15_0 0x1@uint64;
shl ymm3_1 ymm15_1 0x1@uint64;
shl ymm3_2 ymm15_2 0x1@uint64;
shl ymm3_3 ymm15_3 0x1@uint64;
(* vmovdqa -0x210(%rbp),%ymm1                      #! EA = L0x7fffffffbd40; Value = 0x6256066085a1c509; PC = 0x555555578e8d *)
mov ymm1_0 L0x7fffffffbd40;
mov ymm1_1 L0x7fffffffbd48;
mov ymm1_2 L0x7fffffffbd50;
mov ymm1_3 L0x7fffffffbd58;
(* vpxor  -0x130(%rbp),%ymm1,%ymm1                 #! EA = L0x7fffffffbe20; Value = 0x628b689b07200e95; PC = 0x555555578e95 *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbe20;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbe28;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbe30;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbe38;
(* vpxor  %ymm1,%ymm12,%ymm12                      #! PC = 0x555555578e9d *)
xor ymm12_0@uint64 ymm12_0 ymm1_0;
xor ymm12_1@uint64 ymm12_1 ymm1_1;
xor ymm12_2@uint64 ymm12_2 ymm1_2;
xor ymm12_3@uint64 ymm12_3 ymm1_3;
(* vpxor  -0x170(%rbp),%ymm4,%ymm1                 #! EA = L0x7fffffffbde0; Value = 0x321a2cfbb72874a4; PC = 0x555555578ea1 *)
xor ymm1_0@uint64 ymm4_0 L0x7fffffffbde0;
xor ymm1_1@uint64 ymm4_1 L0x7fffffffbde8;
xor ymm1_2@uint64 ymm4_2 L0x7fffffffbdf0;
xor ymm1_3@uint64 ymm4_3 L0x7fffffffbdf8;
(* vpxor  -0xb0(%rbp),%ymm12,%ymm12                #! EA = L0x7fffffffbea0; Value = 0x8099b9e1f6f6e869; PC = 0x555555578ea9 *)
xor ymm12_0@uint64 ymm12_0 L0x7fffffffbea0;
xor ymm12_1@uint64 ymm12_1 L0x7fffffffbea8;
xor ymm12_2@uint64 ymm12_2 L0x7fffffffbeb0;
xor ymm12_3@uint64 ymm12_3 L0x7fffffffbeb8;
(* vpxor  %ymm0,%ymm1,%ymm1                        #! PC = 0x555555578eb1 *)
xor ymm1_0@uint64 ymm1_0 ymm0_0;
xor ymm1_1@uint64 ymm1_1 ymm0_1;
xor ymm1_2@uint64 ymm1_2 ymm0_2;
xor ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3f,%ymm14,%ymm0                       #! PC = 0x555555578eb5 *)
shr ymm0_0 ymm14_0 0x3f@uint64;
shr ymm0_1 ymm14_1 0x3f@uint64;
shr ymm0_2 ymm14_2 0x3f@uint64;
shr ymm0_3 ymm14_3 0x3f@uint64;
(* vpxor  -0x50(%rbp),%ymm1,%ymm1                  #! EA = L0x7fffffffbf00; Value = 0x508c1b026dafdc95; PC = 0x555555578ebb *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbf00;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbf08;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbf10;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbf18;
(* vpor   %ymm0,%ymm5,%ymm5                        #! PC = 0x555555578ec0 *)
or ymm5_0@uint64 ymm5_0 ymm0_0;
or ymm5_1@uint64 ymm5_1 ymm0_1;
or ymm5_2@uint64 ymm5_2 ymm0_2;
or ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpsrlq $0x3f,%ymm15,%ymm0                       #! PC = 0x555555578ec4 *)
shr ymm0_0 ymm15_0 0x3f@uint64;
shr ymm0_1 ymm15_1 0x3f@uint64;
shr ymm0_2 ymm15_2 0x3f@uint64;
shr ymm0_3 ymm15_3 0x3f@uint64;
(* vpsrlq $0x3f,%ymm12,%ymm13                      #! PC = 0x555555578eca *)
shr ymm13_0 ymm12_0 0x3f@uint64;
shr ymm13_1 ymm12_1 0x3f@uint64;
shr ymm13_2 ymm12_2 0x3f@uint64;
shr ymm13_3 ymm12_3 0x3f@uint64;
(* vpor   %ymm0,%ymm3,%ymm3                        #! PC = 0x555555578ed0 *)
or ymm3_0@uint64 ymm3_0 ymm0_0;
or ymm3_1@uint64 ymm3_1 ymm0_1;
or ymm3_2@uint64 ymm3_2 ymm0_2;
or ymm3_3@uint64 ymm3_3 ymm0_3;
(* vpxor  %ymm1,%ymm5,%ymm5                        #! PC = 0x555555578ed4 *)
xor ymm5_0@uint64 ymm5_0 ymm1_0;
xor ymm5_1@uint64 ymm5_1 ymm1_1;
xor ymm5_2@uint64 ymm5_2 ymm1_2;
xor ymm5_3@uint64 ymm5_3 ymm1_3;
(* vpsllq $0x1,%ymm12,%ymm0                        #! PC = 0x555555578ed8 *)
shl ymm0_0 ymm12_0 0x1@uint64;
shl ymm0_1 ymm12_1 0x1@uint64;
shl ymm0_2 ymm12_2 0x1@uint64;
shl ymm0_3 ymm12_3 0x1@uint64;
(* vpxor  %ymm2,%ymm3,%ymm3                        #! PC = 0x555555578ede *)
xor ymm3_0@uint64 ymm3_0 ymm2_0;
xor ymm3_1@uint64 ymm3_1 ymm2_1;
xor ymm3_2@uint64 ymm3_2 ymm2_2;
xor ymm3_3@uint64 ymm3_3 ymm2_3;
(* vpxor  %ymm6,%ymm5,%ymm6                        #! PC = 0x555555578ee2 *)
xor ymm6_0@uint64 ymm5_0 ymm6_0;
xor ymm6_1@uint64 ymm5_1 ymm6_1;
xor ymm6_2@uint64 ymm5_2 ymm6_2;
xor ymm6_3@uint64 ymm5_3 ymm6_3;
(* vpor   %ymm13,%ymm0,%ymm0                       #! PC = 0x555555578ee6 *)
or ymm0_0@uint64 ymm0_0 ymm13_0;
or ymm0_1@uint64 ymm0_1 ymm13_1;
or ymm0_2@uint64 ymm0_2 ymm13_2;
or ymm0_3@uint64 ymm0_3 ymm13_3;
(* vpxor  %ymm8,%ymm3,%ymm8                        #! PC = 0x555555578eeb *)
xor ymm8_0@uint64 ymm3_0 ymm8_0;
xor ymm8_1@uint64 ymm3_1 ymm8_1;
xor ymm8_2@uint64 ymm3_2 ymm8_2;
xor ymm8_3@uint64 ymm3_3 ymm8_3;
(* vpxor  %ymm7,%ymm3,%ymm7                        #! PC = 0x555555578ef0 *)
xor ymm7_0@uint64 ymm3_0 ymm7_0;
xor ymm7_1@uint64 ymm3_1 ymm7_1;
xor ymm7_2@uint64 ymm3_2 ymm7_2;
xor ymm7_3@uint64 ymm3_3 ymm7_3;
(* vpxor  %ymm14,%ymm0,%ymm14                      #! PC = 0x555555578ef4 *)
xor ymm14_0@uint64 ymm0_0 ymm14_0;
xor ymm14_1@uint64 ymm0_1 ymm14_1;
xor ymm14_2@uint64 ymm0_2 ymm14_2;
xor ymm14_3@uint64 ymm0_3 ymm14_3;
(* vpsrlq $0x3f,%ymm1,%ymm0                        #! PC = 0x555555578ef9 *)
shr ymm0_0 ymm1_0 0x3f@uint64;
shr ymm0_1 ymm1_1 0x3f@uint64;
shr ymm0_2 ymm1_2 0x3f@uint64;
shr ymm0_3 ymm1_3 0x3f@uint64;
(* vpsllq $0x1,%ymm1,%ymm1                         #! PC = 0x555555578efe *)
shl ymm1_0 ymm1_0 0x1@uint64;
shl ymm1_1 ymm1_1 0x1@uint64;
shl ymm1_2 ymm1_2 0x1@uint64;
shl ymm1_3 ymm1_3 0x1@uint64;
(* vpxor  %ymm9,%ymm14,%ymm9                       #! PC = 0x555555578f03 *)
xor ymm9_0@uint64 ymm14_0 ymm9_0;
xor ymm9_1@uint64 ymm14_1 ymm9_1;
xor ymm9_2@uint64 ymm14_2 ymm9_2;
xor ymm9_3@uint64 ymm14_3 ymm9_3;
(* vpxor  %ymm11,%ymm14,%ymm11                     #! PC = 0x555555578f08 *)
xor ymm11_0@uint64 ymm14_0 ymm11_0;
xor ymm11_1@uint64 ymm14_1 ymm11_1;
xor ymm11_2@uint64 ymm14_2 ymm11_2;
xor ymm11_3@uint64 ymm14_3 ymm11_3;
(* vpor   %ymm0,%ymm1,%ymm1                        #! PC = 0x555555578f0d *)
or ymm1_0@uint64 ymm1_0 ymm0_0;
or ymm1_1@uint64 ymm1_1 ymm0_1;
or ymm1_2@uint64 ymm1_2 ymm0_2;
or ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3f,%ymm2,%ymm0                        #! PC = 0x555555578f11 *)
shr ymm0_0 ymm2_0 0x3f@uint64;
shr ymm0_1 ymm2_1 0x3f@uint64;
shr ymm0_2 ymm2_2 0x3f@uint64;
shr ymm0_3 ymm2_3 0x3f@uint64;
(* vpsllq $0x1,%ymm2,%ymm2                         #! PC = 0x555555578f16 *)
shl ymm2_0 ymm2_0 0x1@uint64;
shl ymm2_1 ymm2_1 0x1@uint64;
shl ymm2_2 ymm2_2 0x1@uint64;
shl ymm2_3 ymm2_3 0x1@uint64;
(* vpxor  %ymm15,%ymm1,%ymm15                      #! PC = 0x555555578f1b *)
xor ymm15_0@uint64 ymm1_0 ymm15_0;
xor ymm15_1@uint64 ymm1_1 ymm15_1;
xor ymm15_2@uint64 ymm1_2 ymm15_2;
xor ymm15_3@uint64 ymm1_3 ymm15_3;
(* vpxor  -0x90(%rbp),%ymm5,%ymm1                  #! EA = L0x7fffffffbec0; Value = 0xfbdf32a8511a215a; PC = 0x555555578f20 *)
xor ymm1_0@uint64 ymm5_0 L0x7fffffffbec0;
xor ymm1_1@uint64 ymm5_1 L0x7fffffffbec8;
xor ymm1_2@uint64 ymm5_2 L0x7fffffffbed0;
xor ymm1_3@uint64 ymm5_3 L0x7fffffffbed8;
(* vpor   %ymm0,%ymm2,%ymm2                        #! PC = 0x555555578f28 *)
or ymm2_0@uint64 ymm2_0 ymm0_0;
or ymm2_1@uint64 ymm2_1 ymm0_1;
or ymm2_2@uint64 ymm2_2 ymm0_2;
or ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x14,%ymm8,%ymm0                        #! PC = 0x555555578f2c *)
shr ymm0_0 ymm8_0 0x14@uint64;
shr ymm0_1 ymm8_1 0x14@uint64;
shr ymm0_2 ymm8_2 0x14@uint64;
shr ymm0_3 ymm8_3 0x14@uint64;
(* vpxor  %ymm10,%ymm15,%ymm10                     #! PC = 0x555555578f32 *)
xor ymm10_0@uint64 ymm15_0 ymm10_0;
xor ymm10_1@uint64 ymm15_1 ymm10_1;
xor ymm10_2@uint64 ymm15_2 ymm10_2;
xor ymm10_3@uint64 ymm15_3 ymm10_3;
(* vpsllq $0x2c,%ymm8,%ymm8                        #! PC = 0x555555578f37 *)
shl ymm8_0 ymm8_0 0x2c@uint64;
shl ymm8_1 ymm8_1 0x2c@uint64;
shl ymm8_2 ymm8_2 0x2c@uint64;
shl ymm8_3 ymm8_3 0x2c@uint64;
(* vpxor  %ymm12,%ymm2,%ymm12                      #! PC = 0x555555578f3d *)
xor ymm12_0@uint64 ymm2_0 ymm12_0;
xor ymm12_1@uint64 ymm2_1 ymm12_1;
xor ymm12_2@uint64 ymm2_2 ymm12_2;
xor ymm12_3@uint64 ymm2_3 ymm12_3;
(* vpor   %ymm0,%ymm8,%ymm8                        #! PC = 0x555555578f42 *)
or ymm8_0@uint64 ymm8_0 ymm0_0;
or ymm8_1@uint64 ymm8_1 ymm0_1;
or ymm8_2@uint64 ymm8_2 ymm0_2;
or ymm8_3@uint64 ymm8_3 ymm0_3;
(* vpsrlq $0x15,%ymm9,%ymm0                        #! PC = 0x555555578f46 *)
shr ymm0_0 ymm9_0 0x15@uint64;
shr ymm0_1 ymm9_1 0x15@uint64;
shr ymm0_2 ymm9_2 0x15@uint64;
shr ymm0_3 ymm9_3 0x15@uint64;
(* vpxor  %ymm4,%ymm12,%ymm4                       #! PC = 0x555555578f4c *)
xor ymm4_0@uint64 ymm12_0 ymm4_0;
xor ymm4_1@uint64 ymm12_1 ymm4_1;
xor ymm4_2@uint64 ymm12_2 ymm4_2;
xor ymm4_3@uint64 ymm12_3 ymm4_3;
(* vpsllq $0x2b,%ymm9,%ymm9                        #! PC = 0x555555578f50 *)
shl ymm9_0 ymm9_0 0x2b@uint64;
shl ymm9_1 ymm9_1 0x2b@uint64;
shl ymm9_2 ymm9_2 0x2b@uint64;
shl ymm9_3 ymm9_3 0x2b@uint64;
(* vpor   %ymm0,%ymm9,%ymm9                        #! PC = 0x555555578f56 *)
or ymm9_0@uint64 ymm9_0 ymm0_0;
or ymm9_1@uint64 ymm9_1 ymm0_1;
or ymm9_2@uint64 ymm9_2 ymm0_2;
or ymm9_3@uint64 ymm9_3 ymm0_3;
(* vmovq  %r10,%xmm0                               #! PC = 0x555555578f5a *)
mov xmm0_0 r10;
mov xmm0_1 0@uint64;
(* vpandn %ymm9,%ymm8,%ymm2                        #! PC = 0x555555578f5f *)
not ymm8_0n@uint64 ymm8_0;
and ymm2_0@uint64 ymm8_0n ymm9_0;
not ymm8_1n@uint64 ymm8_1;
and ymm2_1@uint64 ymm8_1n ymm9_1;
not ymm8_2n@uint64 ymm8_2;
and ymm2_2@uint64 ymm8_2n ymm9_2;
not ymm8_3n@uint64 ymm8_3;
and ymm2_3@uint64 ymm8_3n ymm9_3;
(* vpbroadcastq %xmm0,%ymm0                        #! PC = 0x555555578f64 *)
mov ymm0_0 xmm0_0;
mov ymm0_1 xmm0_0;
mov ymm0_2 xmm0_0;
mov ymm0_3 xmm0_0;
(* vpxor  %ymm2,%ymm0,%ymm0                        #! PC = 0x555555578f69 *)
xor ymm0_0@uint64 ymm0_0 ymm2_0;
xor ymm0_1@uint64 ymm0_1 ymm2_1;
xor ymm0_2@uint64 ymm0_2 ymm2_2;
xor ymm0_3@uint64 ymm0_3 ymm2_3;
(* vpxor  %ymm1,%ymm0,%ymm2                        #! PC = 0x555555578f6d *)
xor ymm2_0@uint64 ymm0_0 ymm1_0;
xor ymm2_1@uint64 ymm0_1 ymm1_1;
xor ymm2_2@uint64 ymm0_2 ymm1_2;
xor ymm2_3@uint64 ymm0_3 ymm1_3;
(* vpsrlq $0x2b,%ymm10,%ymm0                       #! PC = 0x555555578f71 *)
shr ymm0_0 ymm10_0 0x2b@uint64;
shr ymm0_1 ymm10_1 0x2b@uint64;
shr ymm0_2 ymm10_2 0x2b@uint64;
shr ymm0_3 ymm10_3 0x2b@uint64;
(* vpsllq $0x15,%ymm10,%ymm10                      #! PC = 0x555555578f77 *)
shl ymm10_0 ymm10_0 0x15@uint64;
shl ymm10_1 ymm10_1 0x15@uint64;
shl ymm10_2 ymm10_2 0x15@uint64;
shl ymm10_3 ymm10_3 0x15@uint64;
(* vmovdqa %ymm2,-0x90(%rbp)                       #! EA = L0x7fffffffbec0; PC = 0x555555578f7d *)
mov L0x7fffffffbec0 ymm2_0;
mov L0x7fffffffbec8 ymm2_1;
mov L0x7fffffffbed0 ymm2_2;
mov L0x7fffffffbed8 ymm2_3;
(* vpor   %ymm0,%ymm10,%ymm10                      #! PC = 0x555555578f85 *)
or ymm10_0@uint64 ymm10_0 ymm0_0;
or ymm10_1@uint64 ymm10_1 ymm0_1;
or ymm10_2@uint64 ymm10_2 ymm0_2;
or ymm10_3@uint64 ymm10_3 ymm0_3;
(* vpandn %ymm10,%ymm9,%ymm0                       #! PC = 0x555555578f89 *)
not ymm9_0n@uint64 ymm9_0;
and ymm0_0@uint64 ymm9_0n ymm10_0;
not ymm9_1n@uint64 ymm9_1;
and ymm0_1@uint64 ymm9_1n ymm10_1;
not ymm9_2n@uint64 ymm9_2;
and ymm0_2@uint64 ymm9_2n ymm10_2;
not ymm9_3n@uint64 ymm9_3;
and ymm0_3@uint64 ymm9_3n ymm10_3;
(* vpxor  %ymm8,%ymm0,%ymm0                        #! PC = 0x555555578f8e *)
xor ymm0_0@uint64 ymm0_0 ymm8_0;
xor ymm0_1@uint64 ymm0_1 ymm8_1;
xor ymm0_2@uint64 ymm0_2 ymm8_2;
xor ymm0_3@uint64 ymm0_3 ymm8_3;
(* vpandn %ymm8,%ymm1,%ymm8                        #! PC = 0x555555578f93 *)
not ymm1_0n@uint64 ymm1_0;
and ymm8_0@uint64 ymm1_0n ymm8_0;
not ymm1_1n@uint64 ymm1_1;
and ymm8_1@uint64 ymm1_1n ymm8_1;
not ymm1_2n@uint64 ymm1_2;
and ymm8_2@uint64 ymm1_2n ymm8_2;
not ymm1_3n@uint64 ymm1_3;
and ymm8_3@uint64 ymm1_3n ymm8_3;
(* vmovdqa %ymm0,-0x1f0(%rbp)                      #! EA = L0x7fffffffbd60; PC = 0x555555578f98 *)
mov L0x7fffffffbd60 ymm0_0;
mov L0x7fffffffbd68 ymm0_1;
mov L0x7fffffffbd70 ymm0_2;
mov L0x7fffffffbd78 ymm0_3;
(* vpsrlq $0x32,%ymm4,%ymm0                        #! PC = 0x555555578fa0 *)
shr ymm0_0 ymm4_0 0x32@uint64;
shr ymm0_1 ymm4_1 0x32@uint64;
shr ymm0_2 ymm4_2 0x32@uint64;
shr ymm0_3 ymm4_3 0x32@uint64;
(* vpsllq $0xe,%ymm4,%ymm4                         #! PC = 0x555555578fa5 *)
shl ymm4_0 ymm4_0 0xe@uint64;
shl ymm4_1 ymm4_1 0xe@uint64;
shl ymm4_2 ymm4_2 0xe@uint64;
shl ymm4_3 ymm4_3 0xe@uint64;
(* vpor   %ymm0,%ymm4,%ymm4                        #! PC = 0x555555578faa *)
or ymm4_0@uint64 ymm4_0 ymm0_0;
or ymm4_1@uint64 ymm4_1 ymm0_1;
or ymm4_2@uint64 ymm4_2 ymm0_2;
or ymm4_3@uint64 ymm4_3 ymm0_3;
(* vpandn %ymm4,%ymm10,%ymm0                       #! PC = 0x555555578fae *)
not ymm10_0n@uint64 ymm10_0;
and ymm0_0@uint64 ymm10_0n ymm4_0;
not ymm10_1n@uint64 ymm10_1;
and ymm0_1@uint64 ymm10_1n ymm4_1;
not ymm10_2n@uint64 ymm10_2;
and ymm0_2@uint64 ymm10_2n ymm4_2;
not ymm10_3n@uint64 ymm10_3;
and ymm0_3@uint64 ymm10_3n ymm4_3;
(* vpxor  %ymm4,%ymm8,%ymm8                        #! PC = 0x555555578fb2 *)
xor ymm8_0@uint64 ymm8_0 ymm4_0;
xor ymm8_1@uint64 ymm8_1 ymm4_1;
xor ymm8_2@uint64 ymm8_2 ymm4_2;
xor ymm8_3@uint64 ymm8_3 ymm4_3;
(* vpxor  %ymm9,%ymm0,%ymm9                        #! PC = 0x555555578fb6 *)
xor ymm9_0@uint64 ymm0_0 ymm9_0;
xor ymm9_1@uint64 ymm0_1 ymm9_1;
xor ymm9_2@uint64 ymm0_2 ymm9_2;
xor ymm9_3@uint64 ymm0_3 ymm9_3;
(* vpandn %ymm1,%ymm4,%ymm0                        #! PC = 0x555555578fbb *)
not ymm4_0n@uint64 ymm4_0;
and ymm0_0@uint64 ymm4_0n ymm1_0;
not ymm4_1n@uint64 ymm4_1;
and ymm0_1@uint64 ymm4_1n ymm1_1;
not ymm4_2n@uint64 ymm4_2;
and ymm0_2@uint64 ymm4_2n ymm1_2;
not ymm4_3n@uint64 ymm4_3;
and ymm0_3@uint64 ymm4_3n ymm1_3;
(* vmovdqa %ymm9,-0x310(%rbp)                      #! EA = L0x7fffffffbc40; PC = 0x555555578fbf *)
mov L0x7fffffffbc40 ymm9_0;
mov L0x7fffffffbc48 ymm9_1;
mov L0x7fffffffbc50 ymm9_2;
mov L0x7fffffffbc58 ymm9_3;
(* vpxor  %ymm10,%ymm0,%ymm9                       #! PC = 0x555555578fc7 *)
xor ymm9_0@uint64 ymm0_0 ymm10_0;
xor ymm9_1@uint64 ymm0_1 ymm10_1;
xor ymm9_2@uint64 ymm0_2 ymm10_2;
xor ymm9_3@uint64 ymm0_3 ymm10_3;
(* vmovdqa %ymm9,-0x150(%rbp)                      #! EA = L0x7fffffffbe00; PC = 0x555555578fcc *)
mov L0x7fffffffbe00 ymm9_0;
mov L0x7fffffffbe08 ymm9_1;
mov L0x7fffffffbe10 ymm9_2;
mov L0x7fffffffbe18 ymm9_3;
(* vmovdqa %ymm8,-0x2f0(%rbp)                      #! EA = L0x7fffffffbc60; PC = 0x555555578fd4 *)
mov L0x7fffffffbc60 ymm8_0;
mov L0x7fffffffbc68 ymm8_1;
mov L0x7fffffffbc70 ymm8_2;
mov L0x7fffffffbc78 ymm8_3;
(* vpxor  -0x130(%rbp),%ymm15,%ymm10               #! EA = L0x7fffffffbe20; Value = 0x628b689b07200e95; PC = 0x555555578fdc *)
xor ymm10_0@uint64 ymm15_0 L0x7fffffffbe20;
xor ymm10_1@uint64 ymm15_1 L0x7fffffffbe28;
xor ymm10_2@uint64 ymm15_2 L0x7fffffffbe30;
xor ymm10_3@uint64 ymm15_3 L0x7fffffffbe38;
(* vpxor  -0xb0(%rbp),%ymm15,%ymm9                 #! EA = L0x7fffffffbea0; Value = 0x8099b9e1f6f6e869; PC = 0x555555578fe4 *)
xor ymm9_0@uint64 ymm15_0 L0x7fffffffbea0;
xor ymm9_1@uint64 ymm15_1 L0x7fffffffbea8;
xor ymm9_2@uint64 ymm15_2 L0x7fffffffbeb0;
xor ymm9_3@uint64 ymm15_3 L0x7fffffffbeb8;
(* vpsrlq $0x24,%ymm10,%ymm0                       #! PC = 0x555555578fec *)
shr ymm0_0 ymm10_0 0x24@uint64;
shr ymm0_1 ymm10_1 0x24@uint64;
shr ymm0_2 ymm10_2 0x24@uint64;
shr ymm0_3 ymm10_3 0x24@uint64;
(* vpsllq $0x1c,%ymm10,%ymm1                       #! PC = 0x555555578ff2 *)
shl ymm1_0 ymm10_0 0x1c@uint64;
shl ymm1_1 ymm10_1 0x1c@uint64;
shl ymm1_2 ymm10_2 0x1c@uint64;
shl ymm1_3 ymm10_3 0x1c@uint64;
(* vpor   %ymm0,%ymm1,%ymm1                        #! PC = 0x555555578ff8 *)
or ymm1_0@uint64 ymm1_0 ymm0_0;
or ymm1_1@uint64 ymm1_1 ymm0_1;
or ymm1_2@uint64 ymm1_2 ymm0_2;
or ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpxor  -0xf0(%rbp),%ymm12,%ymm0                 #! EA = L0x7fffffffbe60; Value = 0x9a956cbbb31fa973; PC = 0x555555578ffc *)
xor ymm0_0@uint64 ymm12_0 L0x7fffffffbe60;
xor ymm0_1@uint64 ymm12_1 L0x7fffffffbe68;
xor ymm0_2@uint64 ymm12_2 L0x7fffffffbe70;
xor ymm0_3@uint64 ymm12_3 L0x7fffffffbe78;
(* vpsrlq $0x2c,%ymm0,%ymm2                        #! PC = 0x555555579004 *)
shr ymm2_0 ymm0_0 0x2c@uint64;
shr ymm2_1 ymm0_1 0x2c@uint64;
shr ymm2_2 ymm0_2 0x2c@uint64;
shr ymm2_3 ymm0_3 0x2c@uint64;
(* vpsllq $0x14,%ymm0,%ymm0                        #! PC = 0x555555579009 *)
shl ymm0_0 ymm0_0 0x14@uint64;
shl ymm0_1 ymm0_1 0x14@uint64;
shl ymm0_2 ymm0_2 0x14@uint64;
shl ymm0_3 ymm0_3 0x14@uint64;
(* vpor   %ymm2,%ymm0,%ymm0                        #! PC = 0x55555557900e *)
or ymm0_0@uint64 ymm0_0 ymm2_0;
or ymm0_1@uint64 ymm0_1 ymm2_1;
or ymm0_2@uint64 ymm0_2 ymm2_2;
or ymm0_3@uint64 ymm0_3 ymm2_3;
(* vpxor  -0xd0(%rbp),%ymm5,%ymm2                  #! EA = L0x7fffffffbe80; Value = 0x91282d2c07d3c3ea; PC = 0x555555579012 *)
xor ymm2_0@uint64 ymm5_0 L0x7fffffffbe80;
xor ymm2_1@uint64 ymm5_1 L0x7fffffffbe88;
xor ymm2_2@uint64 ymm5_2 L0x7fffffffbe90;
xor ymm2_3@uint64 ymm5_3 L0x7fffffffbe98;
(* vpsrlq $0x3d,%ymm2,%ymm4                        #! PC = 0x55555557901a *)
shr ymm4_0 ymm2_0 0x3d@uint64;
shr ymm4_1 ymm2_1 0x3d@uint64;
shr ymm4_2 ymm2_2 0x3d@uint64;
shr ymm4_3 ymm2_3 0x3d@uint64;
(* vpsllq $0x3,%ymm2,%ymm2                         #! PC = 0x55555557901f *)
shl ymm2_0 ymm2_0 0x3@uint64;
shl ymm2_1 ymm2_1 0x3@uint64;
shl ymm2_2 ymm2_2 0x3@uint64;
shl ymm2_3 ymm2_3 0x3@uint64;
(* vpor   %ymm4,%ymm2,%ymm2                        #! PC = 0x555555579024 *)
or ymm2_0@uint64 ymm2_0 ymm4_0;
or ymm2_1@uint64 ymm2_1 ymm4_1;
or ymm2_2@uint64 ymm2_2 ymm4_2;
or ymm2_3@uint64 ymm2_3 ymm4_3;
(* vpsrlq $0x13,%ymm7,%ymm4                        #! PC = 0x555555579028 *)
shr ymm4_0 ymm7_0 0x13@uint64;
shr ymm4_1 ymm7_1 0x13@uint64;
shr ymm4_2 ymm7_2 0x13@uint64;
shr ymm4_3 ymm7_3 0x13@uint64;
(* vpandn %ymm2,%ymm0,%ymm10                       #! PC = 0x55555557902d *)
not ymm0_0n@uint64 ymm0_0;
and ymm10_0@uint64 ymm0_0n ymm2_0;
not ymm0_1n@uint64 ymm0_1;
and ymm10_1@uint64 ymm0_1n ymm2_1;
not ymm0_2n@uint64 ymm0_2;
and ymm10_2@uint64 ymm0_2n ymm2_2;
not ymm0_3n@uint64 ymm0_3;
and ymm10_3@uint64 ymm0_3n ymm2_3;
(* vpsllq $0x2d,%ymm7,%ymm7                        #! PC = 0x555555579031 *)
shl ymm7_0 ymm7_0 0x2d@uint64;
shl ymm7_1 ymm7_1 0x2d@uint64;
shl ymm7_2 ymm7_2 0x2d@uint64;
shl ymm7_3 ymm7_3 0x2d@uint64;
(* vpxor  %ymm1,%ymm10,%ymm13                      #! PC = 0x555555579036 *)
xor ymm13_0@uint64 ymm10_0 ymm1_0;
xor ymm13_1@uint64 ymm10_1 ymm1_1;
xor ymm13_2@uint64 ymm10_2 ymm1_2;
xor ymm13_3@uint64 ymm10_3 ymm1_3;
(* vmovdqa %ymm13,-0x2d0(%rbp)                     #! EA = L0x7fffffffbc80; PC = 0x55555557903a *)
mov L0x7fffffffbc80 ymm13_0;
mov L0x7fffffffbc88 ymm13_1;
mov L0x7fffffffbc90 ymm13_2;
mov L0x7fffffffbc98 ymm13_3;
(* vpor   %ymm4,%ymm7,%ymm13                       #! PC = 0x555555579042 *)
or ymm13_0@uint64 ymm7_0 ymm4_0;
or ymm13_1@uint64 ymm7_1 ymm4_1;
or ymm13_2@uint64 ymm7_2 ymm4_2;
or ymm13_3@uint64 ymm7_3 ymm4_3;
(* vpsrlq $0x3,%ymm11,%ymm4                        #! PC = 0x555555579046 *)
shr ymm4_0 ymm11_0 0x3@uint64;
shr ymm4_1 ymm11_1 0x3@uint64;
shr ymm4_2 ymm11_2 0x3@uint64;
shr ymm4_3 ymm11_3 0x3@uint64;
(* vpsllq $0x3d,%ymm11,%ymm11                      #! PC = 0x55555557904c *)
shl ymm11_0 ymm11_0 0x3d@uint64;
shl ymm11_1 ymm11_1 0x3d@uint64;
shl ymm11_2 ymm11_2 0x3d@uint64;
shl ymm11_3 ymm11_3 0x3d@uint64;
(* vpandn %ymm13,%ymm2,%ymm8                       #! PC = 0x555555579052 *)
not ymm2_0n@uint64 ymm2_0;
and ymm8_0@uint64 ymm2_0n ymm13_0;
not ymm2_1n@uint64 ymm2_1;
and ymm8_1@uint64 ymm2_1n ymm13_1;
not ymm2_2n@uint64 ymm2_2;
and ymm8_2@uint64 ymm2_2n ymm13_2;
not ymm2_3n@uint64 ymm2_3;
and ymm8_3@uint64 ymm2_3n ymm13_3;
(* vpor   %ymm4,%ymm11,%ymm11                      #! PC = 0x555555579057 *)
or ymm11_0@uint64 ymm11_0 ymm4_0;
or ymm11_1@uint64 ymm11_1 ymm4_1;
or ymm11_2@uint64 ymm11_2 ymm4_2;
or ymm11_3@uint64 ymm11_3 ymm4_3;
(* vpxor  %ymm0,%ymm8,%ymm8                        #! PC = 0x55555557905b *)
xor ymm8_0@uint64 ymm8_0 ymm0_0;
xor ymm8_1@uint64 ymm8_1 ymm0_1;
xor ymm8_2@uint64 ymm8_2 ymm0_2;
xor ymm8_3@uint64 ymm8_3 ymm0_3;
(* vpandn %ymm11,%ymm13,%ymm4                      #! PC = 0x55555557905f *)
not ymm13_0n@uint64 ymm13_0;
and ymm4_0@uint64 ymm13_0n ymm11_0;
not ymm13_1n@uint64 ymm13_1;
and ymm4_1@uint64 ymm13_1n ymm11_1;
not ymm13_2n@uint64 ymm13_2;
and ymm4_2@uint64 ymm13_2n ymm11_2;
not ymm13_3n@uint64 ymm13_3;
and ymm4_3@uint64 ymm13_3n ymm11_3;
(* vpxor  %ymm2,%ymm4,%ymm7                        #! PC = 0x555555579064 *)
xor ymm7_0@uint64 ymm4_0 ymm2_0;
xor ymm7_1@uint64 ymm4_1 ymm2_1;
xor ymm7_2@uint64 ymm4_2 ymm2_2;
xor ymm7_3@uint64 ymm4_3 ymm2_3;
(* vpsrlq $0x27,%ymm9,%ymm4                        #! PC = 0x555555579068 *)
shr ymm4_0 ymm9_0 0x27@uint64;
shr ymm4_1 ymm9_1 0x27@uint64;
shr ymm4_2 ymm9_2 0x27@uint64;
shr ymm4_3 ymm9_3 0x27@uint64;
(* vpandn %ymm1,%ymm11,%ymm2                       #! PC = 0x55555557906e *)
not ymm11_0n@uint64 ymm11_0;
and ymm2_0@uint64 ymm11_0n ymm1_0;
not ymm11_1n@uint64 ymm11_1;
and ymm2_1@uint64 ymm11_1n ymm1_1;
not ymm11_2n@uint64 ymm11_2;
and ymm2_2@uint64 ymm11_2n ymm1_2;
not ymm11_3n@uint64 ymm11_3;
and ymm2_3@uint64 ymm11_3n ymm1_3;
(* vpandn %ymm0,%ymm1,%ymm1                        #! PC = 0x555555579072 *)
not ymm1_0n@uint64 ymm1_0;
and ymm1_0@uint64 ymm1_0n ymm0_0;
not ymm1_1n@uint64 ymm1_1;
and ymm1_1@uint64 ymm1_1n ymm0_1;
not ymm1_2n@uint64 ymm1_2;
and ymm1_2@uint64 ymm1_2n ymm0_2;
not ymm1_3n@uint64 ymm1_3;
and ymm1_3@uint64 ymm1_3n ymm0_3;
(* vpxor  %ymm13,%ymm2,%ymm13                      #! PC = 0x555555579076 *)
xor ymm13_0@uint64 ymm2_0 ymm13_0;
xor ymm13_1@uint64 ymm2_1 ymm13_1;
xor ymm13_2@uint64 ymm2_2 ymm13_2;
xor ymm13_3@uint64 ymm2_3 ymm13_3;
(* vpxor  -0x110(%rbp),%ymm14,%ymm2                #! EA = L0x7fffffffbe40; Value = 0x12e5367ba3ec4777; PC = 0x55555557907b *)
xor ymm2_0@uint64 ymm14_0 L0x7fffffffbe40;
xor ymm2_1@uint64 ymm14_1 L0x7fffffffbe48;
xor ymm2_2@uint64 ymm14_2 L0x7fffffffbe50;
xor ymm2_3@uint64 ymm14_3 L0x7fffffffbe58;
(* vmovdqa %ymm7,-0x1b0(%rbp)                      #! EA = L0x7fffffffbda0; PC = 0x555555579083 *)
mov L0x7fffffffbda0 ymm7_0;
mov L0x7fffffffbda8 ymm7_1;
mov L0x7fffffffbdb0 ymm7_2;
mov L0x7fffffffbdb8 ymm7_3;
(* vpxor  %ymm11,%ymm1,%ymm11                      #! PC = 0x55555557908b *)
xor ymm11_0@uint64 ymm1_0 ymm11_0;
xor ymm11_1@uint64 ymm1_1 ymm11_1;
xor ymm11_2@uint64 ymm1_2 ymm11_2;
xor ymm11_3@uint64 ymm1_3 ymm11_3;
(* vpxor  -0x290(%rbp),%ymm3,%ymm1                 #! EA = L0x7fffffffbcc0; Value = 0x077cc8c29c4070d1; PC = 0x555555579090 *)
xor ymm1_0@uint64 ymm3_0 L0x7fffffffbcc0;
xor ymm1_1@uint64 ymm3_1 L0x7fffffffbcc8;
xor ymm1_2@uint64 ymm3_2 L0x7fffffffbcd0;
xor ymm1_3@uint64 ymm3_3 L0x7fffffffbcd8;
(* vmovdqa %ymm13,-0x190(%rbp)                     #! EA = L0x7fffffffbdc0; PC = 0x555555579098 *)
mov L0x7fffffffbdc0 ymm13_0;
mov L0x7fffffffbdc8 ymm13_1;
mov L0x7fffffffbdd0 ymm13_2;
mov L0x7fffffffbdd8 ymm13_3;
(* vpsllq $0x19,%ymm9,%ymm9                        #! PC = 0x5555555790a0 *)
shl ymm9_0 ymm9_0 0x19@uint64;
shl ymm9_1 ymm9_1 0x19@uint64;
shl ymm9_2 ymm9_2 0x19@uint64;
shl ymm9_3 ymm9_3 0x19@uint64;
(* vmovdqa %ymm11,-0x130(%rbp)                     #! EA = L0x7fffffffbe20; PC = 0x5555555790a6 *)
mov L0x7fffffffbe20 ymm11_0;
mov L0x7fffffffbe28 ymm11_1;
mov L0x7fffffffbe30 ymm11_2;
mov L0x7fffffffbe38 ymm11_3;
(* vpsrlq $0x3f,%ymm1,%ymm0                        #! PC = 0x5555555790ae *)
shr ymm0_0 ymm1_0 0x3f@uint64;
shr ymm0_1 ymm1_1 0x3f@uint64;
shr ymm0_2 ymm1_2 0x3f@uint64;
shr ymm0_3 ymm1_3 0x3f@uint64;
(* vpsllq $0x1,%ymm1,%ymm1                         #! PC = 0x5555555790b3 *)
shl ymm1_0 ymm1_0 0x1@uint64;
shl ymm1_1 ymm1_1 0x1@uint64;
shl ymm1_2 ymm1_2 0x1@uint64;
shl ymm1_3 ymm1_3 0x1@uint64;
(* vpor   %ymm0,%ymm1,%ymm1                        #! PC = 0x5555555790b8 *)
or ymm1_0@uint64 ymm1_0 ymm0_0;
or ymm1_1@uint64 ymm1_1 ymm0_1;
or ymm1_2@uint64 ymm1_2 ymm0_2;
or ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3a,%ymm2,%ymm0                        #! PC = 0x5555555790bc *)
shr ymm0_0 ymm2_0 0x3a@uint64;
shr ymm0_1 ymm2_1 0x3a@uint64;
shr ymm0_2 ymm2_2 0x3a@uint64;
shr ymm0_3 ymm2_3 0x3a@uint64;
(* vpsllq $0x6,%ymm2,%ymm2                         #! PC = 0x5555555790c1 *)
shl ymm2_0 ymm2_0 0x6@uint64;
shl ymm2_1 ymm2_1 0x6@uint64;
shl ymm2_2 ymm2_2 0x6@uint64;
shl ymm2_3 ymm2_3 0x6@uint64;
(* vpor   %ymm0,%ymm2,%ymm2                        #! PC = 0x5555555790c6 *)
or ymm2_0@uint64 ymm2_0 ymm0_0;
or ymm2_1@uint64 ymm2_1 ymm0_1;
or ymm2_2@uint64 ymm2_2 ymm0_2;
or ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpor   %ymm4,%ymm9,%ymm0                        #! PC = 0x5555555790ca *)
or ymm0_0@uint64 ymm9_0 ymm4_0;
or ymm0_1@uint64 ymm9_1 ymm4_1;
or ymm0_2@uint64 ymm9_2 ymm4_2;
or ymm0_3@uint64 ymm9_3 ymm4_3;
(* vpandn %ymm0,%ymm2,%ymm4                        #! PC = 0x5555555790ce *)
not ymm2_0n@uint64 ymm2_0;
and ymm4_0@uint64 ymm2_0n ymm0_0;
not ymm2_1n@uint64 ymm2_1;
and ymm4_1@uint64 ymm2_1n ymm0_1;
not ymm2_2n@uint64 ymm2_2;
and ymm4_2@uint64 ymm2_2n ymm0_2;
not ymm2_3n@uint64 ymm2_3;
and ymm4_3@uint64 ymm2_3n ymm0_3;
(* vpxor  %ymm1,%ymm4,%ymm11                       #! PC = 0x5555555790d2 *)
xor ymm11_0@uint64 ymm4_0 ymm1_0;
xor ymm11_1@uint64 ymm4_1 ymm1_1;
xor ymm11_2@uint64 ymm4_2 ymm1_2;
xor ymm11_3@uint64 ymm4_3 ymm1_3;
(* vpxor  -0x50(%rbp),%ymm12,%ymm4                 #! EA = L0x7fffffffbf00; Value = 0x508c1b026dafdc95; PC = 0x5555555790d6 *)
xor ymm4_0@uint64 ymm12_0 L0x7fffffffbf00;
xor ymm4_1@uint64 ymm12_1 L0x7fffffffbf08;
xor ymm4_2@uint64 ymm12_2 L0x7fffffffbf10;
xor ymm4_3@uint64 ymm12_3 L0x7fffffffbf18;
(* vmovdqa %ymm11,%ymm13                           #! PC = 0x5555555790db *)
mov ymm13_0 ymm11_0;
mov ymm13_1 ymm11_1;
mov ymm13_2 ymm11_2;
mov ymm13_3 ymm11_3;
(* vpshufb 0x54e17(%rip),%ymm4,%ymm4        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x5555555790e0 *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm4_0, ymm4_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm4_2, ymm4_3, tmp_2, tmp_3);
mov ymm4_0 tmp_0;
mov ymm4_1 tmp_1;
mov ymm4_2 tmp_2;
mov ymm4_3 tmp_3;
(* vpandn %ymm4,%ymm0,%ymm7                        #! PC = 0x5555555790e9 *)
not ymm0_0n@uint64 ymm0_0;
and ymm7_0@uint64 ymm0_0n ymm4_0;
not ymm0_1n@uint64 ymm0_1;
and ymm7_1@uint64 ymm0_1n ymm4_1;
not ymm0_2n@uint64 ymm0_2;
and ymm7_2@uint64 ymm0_2n ymm4_2;
not ymm0_3n@uint64 ymm0_3;
and ymm7_3@uint64 ymm0_3n ymm4_3;
(* vpxor  %ymm2,%ymm7,%ymm7                        #! PC = 0x5555555790ed *)
xor ymm7_0@uint64 ymm7_0 ymm2_0;
xor ymm7_1@uint64 ymm7_1 ymm2_1;
xor ymm7_2@uint64 ymm7_2 ymm2_2;
xor ymm7_3@uint64 ymm7_3 ymm2_3;
(* vmovdqa %ymm7,-0x110(%rbp)                      #! EA = L0x7fffffffbe40; PC = 0x5555555790f1 *)
mov L0x7fffffffbe40 ymm7_0;
mov L0x7fffffffbe48 ymm7_1;
mov L0x7fffffffbe50 ymm7_2;
mov L0x7fffffffbe58 ymm7_3;
(* vpsrlq $0x2e,%ymm6,%ymm7                        #! PC = 0x5555555790f9 *)
shr ymm7_0 ymm6_0 0x2e@uint64;
shr ymm7_1 ymm6_1 0x2e@uint64;
shr ymm7_2 ymm6_2 0x2e@uint64;
shr ymm7_3 ymm6_3 0x2e@uint64;
(* vpsllq $0x12,%ymm6,%ymm6                        #! PC = 0x5555555790fe *)
shl ymm6_0 ymm6_0 0x12@uint64;
shl ymm6_1 ymm6_1 0x12@uint64;
shl ymm6_2 ymm6_2 0x12@uint64;
shl ymm6_3 ymm6_3 0x12@uint64;
(* vpor   %ymm7,%ymm6,%ymm11                       #! PC = 0x555555579103 *)
or ymm11_0@uint64 ymm6_0 ymm7_0;
or ymm11_1@uint64 ymm6_1 ymm7_1;
or ymm11_2@uint64 ymm6_2 ymm7_2;
or ymm11_3@uint64 ymm6_3 ymm7_3;
(* vpxor  -0x1d0(%rbp),%ymm3,%ymm7                 #! EA = L0x7fffffffbd80; Value = 0x55c1963ad365f063; PC = 0x555555579107 *)
xor ymm7_0@uint64 ymm3_0 L0x7fffffffbd80;
xor ymm7_1@uint64 ymm3_1 L0x7fffffffbd88;
xor ymm7_2@uint64 ymm3_2 L0x7fffffffbd90;
xor ymm7_3@uint64 ymm3_3 L0x7fffffffbd98;
(* vmovdqa %ymm13,-0x1d0(%rbp)                     #! EA = L0x7fffffffbd80; PC = 0x55555557910f *)
mov L0x7fffffffbd80 ymm13_0;
mov L0x7fffffffbd88 ymm13_1;
mov L0x7fffffffbd90 ymm13_2;
mov L0x7fffffffbd98 ymm13_3;
(* vpandn %ymm11,%ymm4,%ymm9                       #! PC = 0x555555579117 *)
not ymm4_0n@uint64 ymm4_0;
and ymm9_0@uint64 ymm4_0n ymm11_0;
not ymm4_1n@uint64 ymm4_1;
and ymm9_1@uint64 ymm4_1n ymm11_1;
not ymm4_2n@uint64 ymm4_2;
and ymm9_2@uint64 ymm4_2n ymm11_2;
not ymm4_3n@uint64 ymm4_3;
and ymm9_3@uint64 ymm4_3n ymm11_3;
(* vpxor  %ymm0,%ymm9,%ymm9                        #! PC = 0x55555557911c *)
xor ymm9_0@uint64 ymm9_0 ymm0_0;
xor ymm9_1@uint64 ymm9_1 ymm0_1;
xor ymm9_2@uint64 ymm9_2 ymm0_2;
xor ymm9_3@uint64 ymm9_3 ymm0_3;
(* vpandn %ymm1,%ymm11,%ymm0                       #! PC = 0x555555579120 *)
not ymm11_0n@uint64 ymm11_0;
and ymm0_0@uint64 ymm11_0n ymm1_0;
not ymm11_1n@uint64 ymm11_1;
and ymm0_1@uint64 ymm11_1n ymm1_1;
not ymm11_2n@uint64 ymm11_2;
and ymm0_2@uint64 ymm11_2n ymm1_2;
not ymm11_3n@uint64 ymm11_3;
and ymm0_3@uint64 ymm11_3n ymm1_3;
(* vpandn %ymm2,%ymm1,%ymm1                        #! PC = 0x555555579124 *)
not ymm1_0n@uint64 ymm1_0;
and ymm1_0@uint64 ymm1_0n ymm2_0;
not ymm1_1n@uint64 ymm1_1;
and ymm1_1@uint64 ymm1_1n ymm2_1;
not ymm1_2n@uint64 ymm1_2;
and ymm1_2@uint64 ymm1_2n ymm2_2;
not ymm1_3n@uint64 ymm1_3;
and ymm1_3@uint64 ymm1_3n ymm2_3;
(* vpxor  %ymm4,%ymm0,%ymm10                       #! PC = 0x555555579128 *)
xor ymm10_0@uint64 ymm0_0 ymm4_0;
xor ymm10_1@uint64 ymm0_1 ymm4_1;
xor ymm10_2@uint64 ymm0_2 ymm4_2;
xor ymm10_3@uint64 ymm0_3 ymm4_3;
(* vpxor  -0x250(%rbp),%ymm12,%ymm0                #! EA = L0x7fffffffbd00; Value = 0x67a8fa7ccafb6fcc; PC = 0x55555557912c *)
xor ymm0_0@uint64 ymm12_0 L0x7fffffffbd00;
xor ymm0_1@uint64 ymm12_1 L0x7fffffffbd08;
xor ymm0_2@uint64 ymm12_2 L0x7fffffffbd10;
xor ymm0_3@uint64 ymm12_3 L0x7fffffffbd18;
(* vpxor  %ymm11,%ymm1,%ymm11                      #! PC = 0x555555579134 *)
xor ymm11_0@uint64 ymm1_0 ymm11_0;
xor ymm11_1@uint64 ymm1_1 ymm11_1;
xor ymm11_2@uint64 ymm1_2 ymm11_2;
xor ymm11_3@uint64 ymm1_3 ymm11_3;
(* vmovdqa %ymm11,-0xd0(%rbp)                      #! EA = L0x7fffffffbe80; PC = 0x555555579139 *)
mov L0x7fffffffbe80 ymm11_0;
mov L0x7fffffffbe88 ymm11_1;
mov L0x7fffffffbe90 ymm11_2;
mov L0x7fffffffbe98 ymm11_3;
(* vpxor  -0x170(%rbp),%ymm12,%ymm12               #! EA = L0x7fffffffbde0; Value = 0x321a2cfbb72874a4; PC = 0x555555579141 *)
xor ymm12_0@uint64 ymm12_0 L0x7fffffffbde0;
xor ymm12_1@uint64 ymm12_1 L0x7fffffffbde8;
xor ymm12_2@uint64 ymm12_2 L0x7fffffffbdf0;
xor ymm12_3@uint64 ymm12_3 L0x7fffffffbdf8;
(* vpsrlq $0x25,%ymm0,%ymm1                        #! PC = 0x555555579149 *)
shr ymm1_0 ymm0_0 0x25@uint64;
shr ymm1_1 ymm0_1 0x25@uint64;
shr ymm1_2 ymm0_2 0x25@uint64;
shr ymm1_3 ymm0_3 0x25@uint64;
(* vpsllq $0x1b,%ymm0,%ymm0                        #! PC = 0x55555557914e *)
shl ymm0_0 ymm0_0 0x1b@uint64;
shl ymm0_1 ymm0_1 0x1b@uint64;
shl ymm0_2 ymm0_2 0x1b@uint64;
shl ymm0_3 ymm0_3 0x1b@uint64;
(* vmovdqa %ymm10,-0xf0(%rbp)                      #! EA = L0x7fffffffbe60; PC = 0x555555579153 *)
mov L0x7fffffffbe60 ymm10_0;
mov L0x7fffffffbe68 ymm10_1;
mov L0x7fffffffbe70 ymm10_2;
mov L0x7fffffffbe78 ymm10_3;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x55555557915b *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x230(%rbp),%ymm5,%ymm1                 #! EA = L0x7fffffffbd20; Value = 0x4bc8b925db11fc4e; PC = 0x55555557915f *)
xor ymm1_0@uint64 ymm5_0 L0x7fffffffbd20;
xor ymm1_1@uint64 ymm5_1 L0x7fffffffbd28;
xor ymm1_2@uint64 ymm5_2 L0x7fffffffbd30;
xor ymm1_3@uint64 ymm5_3 L0x7fffffffbd38;
(* vpxor  -0x350(%rbp),%ymm5,%ymm5                 #! EA = L0x7fffffffbc00; Value = 0xf35acbbfb66c85ca; PC = 0x555555579167 *)
xor ymm5_0@uint64 ymm5_0 L0x7fffffffbc00;
xor ymm5_1@uint64 ymm5_1 L0x7fffffffbc08;
xor ymm5_2@uint64 ymm5_2 L0x7fffffffbc10;
xor ymm5_3@uint64 ymm5_3 L0x7fffffffbc18;
(* vpsrlq $0x1c,%ymm1,%ymm2                        #! PC = 0x55555557916f *)
shr ymm2_0 ymm1_0 0x1c@uint64;
shr ymm2_1 ymm1_1 0x1c@uint64;
shr ymm2_2 ymm1_2 0x1c@uint64;
shr ymm2_3 ymm1_3 0x1c@uint64;
(* vpsllq $0x24,%ymm1,%ymm1                        #! PC = 0x555555579174 *)
shl ymm1_0 ymm1_0 0x24@uint64;
shl ymm1_1 ymm1_1 0x24@uint64;
shl ymm1_2 ymm1_2 0x24@uint64;
shl ymm1_3 ymm1_3 0x24@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x555555579179 *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpsrlq $0x36,%ymm7,%ymm2                        #! PC = 0x55555557917d *)
shr ymm2_0 ymm7_0 0x36@uint64;
shr ymm2_1 ymm7_1 0x36@uint64;
shr ymm2_2 ymm7_2 0x36@uint64;
shr ymm2_3 ymm7_3 0x36@uint64;
(* vpsllq $0xa,%ymm7,%ymm7                         #! PC = 0x555555579182 *)
shl ymm7_0 ymm7_0 0xa@uint64;
shl ymm7_1 ymm7_1 0xa@uint64;
shl ymm7_2 ymm7_2 0xa@uint64;
shl ymm7_3 ymm7_3 0xa@uint64;
(* vpor   %ymm2,%ymm7,%ymm11                       #! PC = 0x555555579187 *)
or ymm11_0@uint64 ymm7_0 ymm2_0;
or ymm11_1@uint64 ymm7_1 ymm2_1;
or ymm11_2@uint64 ymm7_2 ymm2_2;
or ymm11_3@uint64 ymm7_3 ymm2_3;
(* vpxor  -0x70(%rbp),%ymm14,%ymm2                 #! EA = L0x7fffffffbee0; Value = 0x84b0356a433bf9a8; PC = 0x55555557918b *)
xor ymm2_0@uint64 ymm14_0 L0x7fffffffbee0;
xor ymm2_1@uint64 ymm14_1 L0x7fffffffbee8;
xor ymm2_2@uint64 ymm14_2 L0x7fffffffbef0;
xor ymm2_3@uint64 ymm14_3 L0x7fffffffbef8;
(* vpxor  -0x270(%rbp),%ymm14,%ymm14               #! EA = L0x7fffffffbce0; Value = 0xa1d7456dd43b2098; PC = 0x555555579190 *)
xor ymm14_0@uint64 ymm14_0 L0x7fffffffbce0;
xor ymm14_1@uint64 ymm14_1 L0x7fffffffbce8;
xor ymm14_2@uint64 ymm14_2 L0x7fffffffbcf0;
xor ymm14_3@uint64 ymm14_3 L0x7fffffffbcf8;
(* vpandn %ymm11,%ymm1,%ymm4                       #! PC = 0x555555579198 *)
not ymm1_0n@uint64 ymm1_0;
and ymm4_0@uint64 ymm1_0n ymm11_0;
not ymm1_1n@uint64 ymm1_1;
and ymm4_1@uint64 ymm1_1n ymm11_1;
not ymm1_2n@uint64 ymm1_2;
and ymm4_2@uint64 ymm1_2n ymm11_2;
not ymm1_3n@uint64 ymm1_3;
and ymm4_3@uint64 ymm1_3n ymm11_3;
(* vpxor  %ymm0,%ymm4,%ymm10                       #! PC = 0x55555557919d *)
xor ymm10_0@uint64 ymm4_0 ymm0_0;
xor ymm10_1@uint64 ymm4_1 ymm0_1;
xor ymm10_2@uint64 ymm4_2 ymm0_2;
xor ymm10_3@uint64 ymm4_3 ymm0_3;
(* vpsrlq $0x31,%ymm2,%ymm4                        #! PC = 0x5555555791a1 *)
shr ymm4_0 ymm2_0 0x31@uint64;
shr ymm4_1 ymm2_1 0x31@uint64;
shr ymm4_2 ymm2_2 0x31@uint64;
shr ymm4_3 ymm2_3 0x31@uint64;
(* vpsllq $0xf,%ymm2,%ymm2                         #! PC = 0x5555555791a6 *)
shl ymm2_0 ymm2_0 0xf@uint64;
shl ymm2_1 ymm2_1 0xf@uint64;
shl ymm2_2 ymm2_2 0xf@uint64;
shl ymm2_3 ymm2_3 0xf@uint64;
(* vmovdqa %ymm10,-0xb0(%rbp)                      #! EA = L0x7fffffffbea0; PC = 0x5555555791ab *)
mov L0x7fffffffbea0 ymm10_0;
mov L0x7fffffffbea8 ymm10_1;
mov L0x7fffffffbeb0 ymm10_2;
mov L0x7fffffffbeb8 ymm10_3;
(* vpor   %ymm4,%ymm2,%ymm2                        #! PC = 0x5555555791b3 *)
or ymm2_0@uint64 ymm2_0 ymm4_0;
or ymm2_1@uint64 ymm2_1 ymm4_1;
or ymm2_2@uint64 ymm2_2 ymm4_2;
or ymm2_3@uint64 ymm2_3 ymm4_3;
(* vpxor  -0x2b0(%rbp),%ymm15,%ymm4                #! EA = L0x7fffffffbca0; Value = 0x6da5463417ce143f; PC = 0x5555555791b7 *)
xor ymm4_0@uint64 ymm15_0 L0x7fffffffbca0;
xor ymm4_1@uint64 ymm15_1 L0x7fffffffbca8;
xor ymm4_2@uint64 ymm15_2 L0x7fffffffbcb0;
xor ymm4_3@uint64 ymm15_3 L0x7fffffffbcb8;
(* vpxor  -0x210(%rbp),%ymm15,%ymm15               #! EA = L0x7fffffffbd40; Value = 0x6256066085a1c509; PC = 0x5555555791bf *)
xor ymm15_0@uint64 ymm15_0 L0x7fffffffbd40;
xor ymm15_1@uint64 ymm15_1 L0x7fffffffbd48;
xor ymm15_2@uint64 ymm15_2 L0x7fffffffbd50;
xor ymm15_3@uint64 ymm15_3 L0x7fffffffbd58;
(* vpandn %ymm2,%ymm11,%ymm7                       #! PC = 0x5555555791c7 *)
not ymm11_0n@uint64 ymm11_0;
and ymm7_0@uint64 ymm11_0n ymm2_0;
not ymm11_1n@uint64 ymm11_1;
and ymm7_1@uint64 ymm11_1n ymm2_1;
not ymm11_2n@uint64 ymm11_2;
and ymm7_2@uint64 ymm11_2n ymm2_2;
not ymm11_3n@uint64 ymm11_3;
and ymm7_3@uint64 ymm11_3n ymm2_3;
(* vpshufb 0x54d0c(%rip),%ymm4,%ymm4        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x5555555791cb *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm4_0, ymm4_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm4_2, ymm4_3, tmp_2, tmp_3);
mov ymm4_0 tmp_0;
mov ymm4_1 tmp_1;
mov ymm4_2 tmp_2;
mov ymm4_3 tmp_3;
(* vpxor  %ymm1,%ymm7,%ymm7                        #! PC = 0x5555555791d4 *)
xor ymm7_0@uint64 ymm7_0 ymm1_0;
xor ymm7_1@uint64 ymm7_1 ymm1_1;
xor ymm7_2@uint64 ymm7_2 ymm1_2;
xor ymm7_3@uint64 ymm7_3 ymm1_3;
(* vpandn %ymm4,%ymm2,%ymm6                        #! PC = 0x5555555791d8 *)
not ymm2_0n@uint64 ymm2_0;
and ymm6_0@uint64 ymm2_0n ymm4_0;
not ymm2_1n@uint64 ymm2_1;
and ymm6_1@uint64 ymm2_1n ymm4_1;
not ymm2_2n@uint64 ymm2_2;
and ymm6_2@uint64 ymm2_2n ymm4_2;
not ymm2_3n@uint64 ymm2_3;
and ymm6_3@uint64 ymm2_3n ymm4_3;
(* vpandn %ymm0,%ymm4,%ymm10                       #! PC = 0x5555555791dc *)
not ymm4_0n@uint64 ymm4_0;
and ymm10_0@uint64 ymm4_0n ymm0_0;
not ymm4_1n@uint64 ymm4_1;
and ymm10_1@uint64 ymm4_1n ymm0_1;
not ymm4_2n@uint64 ymm4_2;
and ymm10_2@uint64 ymm4_2n ymm0_2;
not ymm4_3n@uint64 ymm4_3;
and ymm10_3@uint64 ymm4_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x5555555791e0 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm11,%ymm6,%ymm6                       #! PC = 0x5555555791e4 *)
xor ymm6_0@uint64 ymm6_0 ymm11_0;
xor ymm6_1@uint64 ymm6_1 ymm11_1;
xor ymm6_2@uint64 ymm6_2 ymm11_2;
xor ymm6_3@uint64 ymm6_3 ymm11_3;
(* vpsrlq $0x19,%ymm12,%ymm1                       #! PC = 0x5555555791e9 *)
shr ymm1_0 ymm12_0 0x19@uint64;
shr ymm1_1 ymm12_1 0x19@uint64;
shr ymm1_2 ymm12_2 0x19@uint64;
shr ymm1_3 ymm12_3 0x19@uint64;
(* vpxor  %ymm2,%ymm10,%ymm10                      #! PC = 0x5555555791ef *)
xor ymm10_0@uint64 ymm10_0 ymm2_0;
xor ymm10_1@uint64 ymm10_1 ymm2_1;
xor ymm10_2@uint64 ymm10_2 ymm2_2;
xor ymm10_3@uint64 ymm10_3 ymm2_3;
(* vmovdqa %ymm6,-0x70(%rbp)                       #! EA = L0x7fffffffbee0; PC = 0x5555555791f3 *)
mov L0x7fffffffbee0 ymm6_0;
mov L0x7fffffffbee8 ymm6_1;
mov L0x7fffffffbef0 ymm6_2;
mov L0x7fffffffbef8 ymm6_3;
(* vpsllq $0x27,%ymm12,%ymm12                      #! PC = 0x5555555791f8 *)
shl ymm12_0 ymm12_0 0x27@uint64;
shl ymm12_1 ymm12_1 0x27@uint64;
shl ymm12_2 ymm12_2 0x27@uint64;
shl ymm12_3 ymm12_3 0x27@uint64;
(* vpxor  %ymm4,%ymm0,%ymm6                        #! PC = 0x5555555791fe *)
xor ymm6_0@uint64 ymm0_0 ymm4_0;
xor ymm6_1@uint64 ymm0_1 ymm4_1;
xor ymm6_2@uint64 ymm0_2 ymm4_2;
xor ymm6_3@uint64 ymm0_3 ymm4_3;
(* vpsrlq $0x9,%ymm15,%ymm0                        #! PC = 0x555555579202 *)
shr ymm0_0 ymm15_0 0x9@uint64;
shr ymm0_1 ymm15_1 0x9@uint64;
shr ymm0_2 ymm15_2 0x9@uint64;
shr ymm0_3 ymm15_3 0x9@uint64;
(* vpsllq $0x37,%ymm15,%ymm15                      #! PC = 0x555555579208 *)
shl ymm15_0 ymm15_0 0x37@uint64;
shl ymm15_1 ymm15_1 0x37@uint64;
shl ymm15_2 ymm15_2 0x37@uint64;
shl ymm15_3 ymm15_3 0x37@uint64;
(* vpor   %ymm1,%ymm12,%ymm12                      #! PC = 0x55555557920e *)
or ymm12_0@uint64 ymm12_0 ymm1_0;
or ymm12_1@uint64 ymm12_1 ymm1_1;
or ymm12_2@uint64 ymm12_2 ymm1_2;
or ymm12_3@uint64 ymm12_3 ymm1_3;
(* vmovdqa %ymm6,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x555555579212 *)
mov L0x7fffffffbf00 ymm6_0;
mov L0x7fffffffbf08 ymm6_1;
mov L0x7fffffffbf10 ymm6_2;
mov L0x7fffffffbf18 ymm6_3;
(* vpor   %ymm0,%ymm15,%ymm15                      #! PC = 0x555555579217 *)
or ymm15_0@uint64 ymm15_0 ymm0_0;
or ymm15_1@uint64 ymm15_1 ymm0_1;
or ymm15_2@uint64 ymm15_2 ymm0_2;
or ymm15_3@uint64 ymm15_3 ymm0_3;
(* vpsrlq $0x2,%ymm14,%ymm2                        #! PC = 0x55555557921b *)
shr ymm2_0 ymm14_0 0x2@uint64;
shr ymm2_1 ymm14_1 0x2@uint64;
shr ymm2_2 ymm14_2 0x2@uint64;
shr ymm2_3 ymm14_3 0x2@uint64;
(* vpxor  -0x2d0(%rbp),%ymm13,%ymm1                #! EA = L0x7fffffffbc80; Value = 0x8bf1357228f2736e; PC = 0x555555579221 *)
xor ymm1_0@uint64 ymm13_0 L0x7fffffffbc80;
xor ymm1_1@uint64 ymm13_1 L0x7fffffffbc88;
xor ymm1_2@uint64 ymm13_2 L0x7fffffffbc90;
xor ymm1_3@uint64 ymm13_3 L0x7fffffffbc98;
(* vpsllq $0x3e,%ymm14,%ymm14                      #! PC = 0x555555579229 *)
shl ymm14_0 ymm14_0 0x3e@uint64;
shl ymm14_1 ymm14_1 0x3e@uint64;
shl ymm14_2 ymm14_2 0x3e@uint64;
shl ymm14_3 ymm14_3 0x3e@uint64;
(* vpandn %ymm12,%ymm15,%ymm6                      #! PC = 0x55555557922f *)
not ymm15_0n@uint64 ymm15_0;
and ymm6_0@uint64 ymm15_0n ymm12_0;
not ymm15_1n@uint64 ymm15_1;
and ymm6_1@uint64 ymm15_1n ymm12_1;
not ymm15_2n@uint64 ymm15_2;
and ymm6_2@uint64 ymm15_2n ymm12_2;
not ymm15_3n@uint64 ymm15_3;
and ymm6_3@uint64 ymm15_3n ymm12_3;
(* vpor   %ymm2,%ymm14,%ymm4                       #! PC = 0x555555579234 *)
or ymm4_0@uint64 ymm14_0 ymm2_0;
or ymm4_1@uint64 ymm14_1 ymm2_1;
or ymm4_2@uint64 ymm14_2 ymm2_2;
or ymm4_3@uint64 ymm14_3 ymm2_3;
(* vpxor  %ymm4,%ymm6,%ymm6                        #! PC = 0x555555579238 *)
xor ymm6_0@uint64 ymm6_0 ymm4_0;
xor ymm6_1@uint64 ymm6_1 ymm4_1;
xor ymm6_2@uint64 ymm6_2 ymm4_2;
xor ymm6_3@uint64 ymm6_3 ymm4_3;
(* vpxor  -0xb0(%rbp),%ymm6,%ymm0                  #! EA = L0x7fffffffbea0; Value = 0x5ce481e8d139a791; PC = 0x55555557923c *)
xor ymm0_0@uint64 ymm6_0 L0x7fffffffbea0;
xor ymm0_1@uint64 ymm6_1 L0x7fffffffbea8;
xor ymm0_2@uint64 ymm6_2 L0x7fffffffbeb0;
xor ymm0_3@uint64 ymm6_3 L0x7fffffffbeb8;
(* vpxor  %ymm1,%ymm0,%ymm0                        #! PC = 0x555555579244 *)
xor ymm0_0@uint64 ymm0_0 ymm1_0;
xor ymm0_1@uint64 ymm0_1 ymm1_1;
xor ymm0_2@uint64 ymm0_2 ymm1_2;
xor ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpsrlq $0x17,%ymm5,%ymm1                        #! PC = 0x555555579248 *)
shr ymm1_0 ymm5_0 0x17@uint64;
shr ymm1_1 ymm5_1 0x17@uint64;
shr ymm1_2 ymm5_2 0x17@uint64;
shr ymm1_3 ymm5_3 0x17@uint64;
(* vpxor  -0x90(%rbp),%ymm0,%ymm0                  #! EA = L0x7fffffffbec0; Value = 0x599d6ef52fd9ae41; PC = 0x55555557924d *)
xor ymm0_0@uint64 ymm0_0 L0x7fffffffbec0;
xor ymm0_1@uint64 ymm0_1 L0x7fffffffbec8;
xor ymm0_2@uint64 ymm0_2 L0x7fffffffbed0;
xor ymm0_3@uint64 ymm0_3 L0x7fffffffbed8;
(* vpsllq $0x29,%ymm5,%ymm5                        #! PC = 0x555555579255 *)
shl ymm5_0 ymm5_0 0x29@uint64;
shl ymm5_1 ymm5_1 0x29@uint64;
shl ymm5_2 ymm5_2 0x29@uint64;
shl ymm5_3 ymm5_3 0x29@uint64;
(* vpxor  -0x110(%rbp),%ymm7,%ymm14                #! EA = L0x7fffffffbe40; Value = 0x720fede69ef73316; PC = 0x55555557925a *)
xor ymm14_0@uint64 ymm7_0 L0x7fffffffbe40;
xor ymm14_1@uint64 ymm7_1 L0x7fffffffbe48;
xor ymm14_2@uint64 ymm7_2 L0x7fffffffbe50;
xor ymm14_3@uint64 ymm7_3 L0x7fffffffbe58;
(* vpxor  -0x330(%rbp),%ymm3,%ymm3                 #! EA = L0x7fffffffbc20; Value = 0x87ccc9784dd9d4bf; PC = 0x555555579262 *)
xor ymm3_0@uint64 ymm3_0 L0x7fffffffbc20;
xor ymm3_1@uint64 ymm3_1 L0x7fffffffbc28;
xor ymm3_2@uint64 ymm3_2 L0x7fffffffbc30;
xor ymm3_3@uint64 ymm3_3 L0x7fffffffbc38;
(* vpor   %ymm1,%ymm5,%ymm11                       #! PC = 0x55555557926a *)
or ymm11_0@uint64 ymm5_0 ymm1_0;
or ymm11_1@uint64 ymm5_1 ymm1_1;
or ymm11_2@uint64 ymm5_2 ymm1_2;
or ymm11_3@uint64 ymm5_3 ymm1_3;
(* vpxor  -0x1f0(%rbp),%ymm8,%ymm5                 #! EA = L0x7fffffffbd60; Value = 0xbf34c0d029d604e5; PC = 0x55555557926e *)
xor ymm5_0@uint64 ymm8_0 L0x7fffffffbd60;
xor ymm5_1@uint64 ymm8_1 L0x7fffffffbd68;
xor ymm5_2@uint64 ymm8_2 L0x7fffffffbd70;
xor ymm5_3@uint64 ymm8_3 L0x7fffffffbd78;
(* vpxor  -0x70(%rbp),%ymm9,%ymm13                 #! EA = L0x7fffffffbee0; Value = 0x10ee357fe5674585; PC = 0x555555579276 *)
xor ymm13_0@uint64 ymm9_0 L0x7fffffffbee0;
xor ymm13_1@uint64 ymm9_1 L0x7fffffffbee8;
xor ymm13_2@uint64 ymm9_2 L0x7fffffffbef0;
xor ymm13_3@uint64 ymm9_3 L0x7fffffffbef8;
(* vpandn %ymm11,%ymm12,%ymm1                      #! PC = 0x55555557927b *)
not ymm12_0n@uint64 ymm12_0;
and ymm1_0@uint64 ymm12_0n ymm11_0;
not ymm12_1n@uint64 ymm12_1;
and ymm1_1@uint64 ymm12_1n ymm11_1;
not ymm12_2n@uint64 ymm12_2;
and ymm1_2@uint64 ymm12_2n ymm11_2;
not ymm12_3n@uint64 ymm12_3;
and ymm1_3@uint64 ymm12_3n ymm11_3;
(* vpxor  %ymm15,%ymm1,%ymm2                       #! PC = 0x555555579280 *)
xor ymm2_0@uint64 ymm1_0 ymm15_0;
xor ymm2_1@uint64 ymm1_1 ymm15_1;
xor ymm2_2@uint64 ymm1_2 ymm15_2;
xor ymm2_3@uint64 ymm1_3 ymm15_3;
(* vpsrlq $0x3e,%ymm3,%ymm1                        #! PC = 0x555555579285 *)
shr ymm1_0 ymm3_0 0x3e@uint64;
shr ymm1_1 ymm3_1 0x3e@uint64;
shr ymm1_2 ymm3_2 0x3e@uint64;
shr ymm1_3 ymm3_3 0x3e@uint64;
(* vpxor  %ymm5,%ymm14,%ymm14                      #! PC = 0x55555557928a *)
xor ymm14_0@uint64 ymm14_0 ymm5_0;
xor ymm14_1@uint64 ymm14_1 ymm5_1;
xor ymm14_2@uint64 ymm14_2 ymm5_2;
xor ymm14_3@uint64 ymm14_3 ymm5_3;
(* vmovdqa -0x1b0(%rbp),%ymm5                      #! EA = L0x7fffffffbda0; Value = 0xdf7213b7f0a141c9; PC = 0x55555557928e *)
mov ymm5_0 L0x7fffffffbda0;
mov ymm5_1 L0x7fffffffbda8;
mov ymm5_2 L0x7fffffffbdb0;
mov ymm5_3 L0x7fffffffbdb8;
(* vpsllq $0x2,%ymm3,%ymm3                         #! PC = 0x555555579296 *)
shl ymm3_0 ymm3_0 0x2@uint64;
shl ymm3_1 ymm3_1 0x2@uint64;
shl ymm3_2 ymm3_2 0x2@uint64;
shl ymm3_3 ymm3_3 0x2@uint64;
(* vpxor  %ymm2,%ymm14,%ymm14                      #! PC = 0x55555557929b *)
xor ymm14_0@uint64 ymm14_0 ymm2_0;
xor ymm14_1@uint64 ymm14_1 ymm2_1;
xor ymm14_2@uint64 ymm14_2 ymm2_2;
xor ymm14_3@uint64 ymm14_3 ymm2_3;
(* vmovdqa %ymm2,-0x350(%rbp)                      #! EA = L0x7fffffffbc00; PC = 0x55555557929f *)
mov L0x7fffffffbc00 ymm2_0;
mov L0x7fffffffbc08 ymm2_1;
mov L0x7fffffffbc10 ymm2_2;
mov L0x7fffffffbc18 ymm2_3;
(* vpor   %ymm1,%ymm3,%ymm2                        #! PC = 0x5555555792a7 *)
or ymm2_0@uint64 ymm3_0 ymm1_0;
or ymm2_1@uint64 ymm3_1 ymm1_1;
or ymm2_2@uint64 ymm3_2 ymm1_2;
or ymm2_3@uint64 ymm3_3 ymm1_3;
(* vmovdqa -0x190(%rbp),%ymm3                      #! EA = L0x7fffffffbdc0; Value = 0xb409a8ca23c781ef; PC = 0x5555555792ab *)
mov ymm3_0 L0x7fffffffbdc0;
mov ymm3_1 L0x7fffffffbdc8;
mov ymm3_2 L0x7fffffffbdd0;
mov ymm3_3 L0x7fffffffbdd8;
(* vpandn %ymm2,%ymm11,%ymm1                       #! PC = 0x5555555792b3 *)
not ymm11_0n@uint64 ymm11_0;
and ymm1_0@uint64 ymm11_0n ymm2_0;
not ymm11_1n@uint64 ymm11_1;
and ymm1_1@uint64 ymm11_1n ymm2_1;
not ymm11_2n@uint64 ymm11_2;
and ymm1_2@uint64 ymm11_2n ymm2_2;
not ymm11_3n@uint64 ymm11_3;
and ymm1_3@uint64 ymm11_3n ymm2_3;
(* vpxor  %ymm12,%ymm1,%ymm12                      #! PC = 0x5555555792b7 *)
xor ymm12_0@uint64 ymm1_0 ymm12_0;
xor ymm12_1@uint64 ymm1_1 ymm12_1;
xor ymm12_2@uint64 ymm1_2 ymm12_2;
xor ymm12_3@uint64 ymm1_3 ymm12_3;
(* vpxor  -0x310(%rbp),%ymm5,%ymm1                 #! EA = L0x7fffffffbc40; Value = 0x088d5a13bfc74406; PC = 0x5555555792bc *)
xor ymm1_0@uint64 ymm5_0 L0x7fffffffbc40;
xor ymm1_1@uint64 ymm5_1 L0x7fffffffbc48;
xor ymm1_2@uint64 ymm5_2 L0x7fffffffbc50;
xor ymm1_3@uint64 ymm5_3 L0x7fffffffbc58;
(* vpandn %ymm4,%ymm2,%ymm5                        #! PC = 0x5555555792c4 *)
not ymm2_0n@uint64 ymm2_0;
and ymm5_0@uint64 ymm2_0n ymm4_0;
not ymm2_1n@uint64 ymm2_1;
and ymm5_1@uint64 ymm2_1n ymm4_1;
not ymm2_2n@uint64 ymm2_2;
and ymm5_2@uint64 ymm2_2n ymm4_2;
not ymm2_3n@uint64 ymm2_3;
and ymm5_3@uint64 ymm2_3n ymm4_3;
(* vpxor  %ymm11,%ymm5,%ymm11                      #! PC = 0x5555555792c8 *)
xor ymm11_0@uint64 ymm5_0 ymm11_0;
xor ymm11_1@uint64 ymm5_1 ymm11_1;
xor ymm11_2@uint64 ymm5_2 ymm11_2;
xor ymm11_3@uint64 ymm5_3 ymm11_3;
(* vpxor  %ymm1,%ymm13,%ymm13                      #! PC = 0x5555555792cd *)
xor ymm13_0@uint64 ymm13_0 ymm1_0;
xor ymm13_1@uint64 ymm13_1 ymm1_1;
xor ymm13_2@uint64 ymm13_2 ymm1_2;
xor ymm13_3@uint64 ymm13_3 ymm1_3;
(* vpxor  -0x150(%rbp),%ymm3,%ymm1                 #! EA = L0x7fffffffbe00; Value = 0xfc80d28cc5cc542e; PC = 0x5555555792d1 *)
xor ymm1_0@uint64 ymm3_0 L0x7fffffffbe00;
xor ymm1_1@uint64 ymm3_1 L0x7fffffffbe08;
xor ymm1_2@uint64 ymm3_2 L0x7fffffffbe10;
xor ymm1_3@uint64 ymm3_3 L0x7fffffffbe18;
(* vpandn %ymm15,%ymm4,%ymm3                       #! PC = 0x5555555792d9 *)
not ymm4_0n@uint64 ymm4_0;
and ymm3_0@uint64 ymm4_0n ymm15_0;
not ymm4_1n@uint64 ymm4_1;
and ymm3_1@uint64 ymm4_1n ymm15_1;
not ymm4_2n@uint64 ymm4_2;
and ymm3_2@uint64 ymm4_2n ymm15_2;
not ymm4_3n@uint64 ymm4_3;
and ymm3_3@uint64 ymm4_3n ymm15_3;
(* vmovdqa %ymm11,-0x330(%rbp)                     #! EA = L0x7fffffffbc20; PC = 0x5555555792de *)
mov L0x7fffffffbc20 ymm11_0;
mov L0x7fffffffbc28 ymm11_1;
mov L0x7fffffffbc30 ymm11_2;
mov L0x7fffffffbc38 ymm11_3;
(* vpxor  %ymm11,%ymm10,%ymm11                     #! PC = 0x5555555792e6 *)
xor ymm11_0@uint64 ymm10_0 ymm11_0;
xor ymm11_1@uint64 ymm10_1 ymm11_1;
xor ymm11_2@uint64 ymm10_2 ymm11_2;
xor ymm11_3@uint64 ymm10_3 ymm11_3;
(* vmovdqa -0x130(%rbp),%ymm15                     #! EA = L0x7fffffffbe20; Value = 0xa627237129eee7a1; PC = 0x5555555792eb *)
mov ymm15_0 L0x7fffffffbe20;
mov ymm15_1 L0x7fffffffbe28;
mov ymm15_2 L0x7fffffffbe30;
mov ymm15_3 L0x7fffffffbe38;
(* vpxor  %ymm2,%ymm3,%ymm3                        #! PC = 0x5555555792f3 *)
xor ymm3_0@uint64 ymm3_0 ymm2_0;
xor ymm3_1@uint64 ymm3_1 ymm2_1;
xor ymm3_2@uint64 ymm3_2 ymm2_2;
xor ymm3_3@uint64 ymm3_3 ymm2_3;
(* vpxor  -0x2f0(%rbp),%ymm15,%ymm2                #! EA = L0x7fffffffbc60; Value = 0x3be9b37a46dfa823; PC = 0x5555555792f7 *)
xor ymm2_0@uint64 ymm15_0 L0x7fffffffbc60;
xor ymm2_1@uint64 ymm15_1 L0x7fffffffbc68;
xor ymm2_2@uint64 ymm15_2 L0x7fffffffbc70;
xor ymm2_3@uint64 ymm15_3 L0x7fffffffbc78;
(* vpxor  %ymm1,%ymm11,%ymm11                      #! PC = 0x5555555792ff *)
xor ymm11_0@uint64 ymm11_0 ymm1_0;
xor ymm11_1@uint64 ymm11_1 ymm1_1;
xor ymm11_2@uint64 ymm11_2 ymm1_2;
xor ymm11_3@uint64 ymm11_3 ymm1_3;
(* vpxor  -0xd0(%rbp),%ymm3,%ymm1                  #! EA = L0x7fffffffbe80; Value = 0x347937f914e65553; PC = 0x555555579303 *)
xor ymm1_0@uint64 ymm3_0 L0x7fffffffbe80;
xor ymm1_1@uint64 ymm3_1 L0x7fffffffbe88;
xor ymm1_2@uint64 ymm3_2 L0x7fffffffbe90;
xor ymm1_3@uint64 ymm3_3 L0x7fffffffbe98;
(* vpxor  %ymm12,%ymm13,%ymm13                     #! PC = 0x55555557930b *)
xor ymm13_0@uint64 ymm13_0 ymm12_0;
xor ymm13_1@uint64 ymm13_1 ymm12_1;
xor ymm13_2@uint64 ymm13_2 ymm12_2;
xor ymm13_3@uint64 ymm13_3 ymm12_3;
(* vpxor  -0xf0(%rbp),%ymm11,%ymm11                #! EA = L0x7fffffffbe60; Value = 0x5211152025d6cffa; PC = 0x555555579310 *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbe60;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbe68;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbe70;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbe78;
(* vpsllq $0x1,%ymm14,%ymm4                        #! PC = 0x555555579318 *)
shl ymm4_0 ymm14_0 0x1@uint64;
shl ymm4_1 ymm14_1 0x1@uint64;
shl ymm4_2 ymm14_2 0x1@uint64;
shl ymm4_3 ymm14_3 0x1@uint64;
(* vpxor  %ymm2,%ymm1,%ymm1                        #! PC = 0x55555557931e *)
xor ymm1_0@uint64 ymm1_0 ymm2_0;
xor ymm1_1@uint64 ymm1_1 ymm2_1;
xor ymm1_2@uint64 ymm1_2 ymm2_2;
xor ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpsrlq $0x3f,%ymm14,%ymm2                       #! PC = 0x555555579322 *)
shr ymm2_0 ymm14_0 0x3f@uint64;
shr ymm2_1 ymm14_1 0x3f@uint64;
shr ymm2_2 ymm14_2 0x3f@uint64;
shr ymm2_3 ymm14_3 0x3f@uint64;
(* vpxor  -0x50(%rbp),%ymm1,%ymm1                  #! EA = L0x7fffffffbf00; Value = 0xcd893f7adababc47; PC = 0x555555579328 *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbf00;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbf08;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbf10;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbf18;
(* vpsrlq $0x3f,%ymm13,%ymm5                       #! PC = 0x55555557932d *)
shr ymm5_0 ymm13_0 0x3f@uint64;
shr ymm5_1 ymm13_1 0x3f@uint64;
shr ymm5_2 ymm13_2 0x3f@uint64;
shr ymm5_3 ymm13_3 0x3f@uint64;
(* vpsrlq $0x3f,%ymm11,%ymm15                      #! PC = 0x555555579333 *)
shr ymm15_0 ymm11_0 0x3f@uint64;
shr ymm15_1 ymm11_1 0x3f@uint64;
shr ymm15_2 ymm11_2 0x3f@uint64;
shr ymm15_3 ymm11_3 0x3f@uint64;
(* vpor   %ymm2,%ymm4,%ymm4                        #! PC = 0x555555579339 *)
or ymm4_0@uint64 ymm4_0 ymm2_0;
or ymm4_1@uint64 ymm4_1 ymm2_1;
or ymm4_2@uint64 ymm4_2 ymm2_2;
or ymm4_3@uint64 ymm4_3 ymm2_3;
(* vpsllq $0x1,%ymm13,%ymm2                        #! PC = 0x55555557933d *)
shl ymm2_0 ymm13_0 0x1@uint64;
shl ymm2_1 ymm13_1 0x1@uint64;
shl ymm2_2 ymm13_2 0x1@uint64;
shl ymm2_3 ymm13_3 0x1@uint64;
(* vpxor  %ymm1,%ymm4,%ymm4                        #! PC = 0x555555579343 *)
xor ymm4_0@uint64 ymm4_0 ymm1_0;
xor ymm4_1@uint64 ymm4_1 ymm1_1;
xor ymm4_2@uint64 ymm4_2 ymm1_2;
xor ymm4_3@uint64 ymm4_3 ymm1_3;
(* vpor   %ymm5,%ymm2,%ymm2                        #! PC = 0x555555579347 *)
or ymm2_0@uint64 ymm2_0 ymm5_0;
or ymm2_1@uint64 ymm2_1 ymm5_1;
or ymm2_2@uint64 ymm2_2 ymm5_2;
or ymm2_3@uint64 ymm2_3 ymm5_3;
(* vpsllq $0x1,%ymm11,%ymm5                        #! PC = 0x55555557934b *)
shl ymm5_0 ymm11_0 0x1@uint64;
shl ymm5_1 ymm11_1 0x1@uint64;
shl ymm5_2 ymm11_2 0x1@uint64;
shl ymm5_3 ymm11_3 0x1@uint64;
(* vpxor  %ymm6,%ymm4,%ymm6                        #! PC = 0x555555579351 *)
xor ymm6_0@uint64 ymm4_0 ymm6_0;
xor ymm6_1@uint64 ymm4_1 ymm6_1;
xor ymm6_2@uint64 ymm4_2 ymm6_2;
xor ymm6_3@uint64 ymm4_3 ymm6_3;
(* vpor   %ymm15,%ymm5,%ymm5                       #! PC = 0x555555579355 *)
or ymm5_0@uint64 ymm5_0 ymm15_0;
or ymm5_1@uint64 ymm5_1 ymm15_1;
or ymm5_2@uint64 ymm5_2 ymm15_2;
or ymm5_3@uint64 ymm5_3 ymm15_3;
(* vpxor  %ymm0,%ymm2,%ymm2                        #! PC = 0x55555557935a *)
xor ymm2_0@uint64 ymm2_0 ymm0_0;
xor ymm2_1@uint64 ymm2_1 ymm0_1;
xor ymm2_2@uint64 ymm2_2 ymm0_2;
xor ymm2_3@uint64 ymm2_3 ymm0_3;
(* vmovq  %r11,%xmm15                              #! PC = 0x55555557935e *)
mov xmm15_0 r11;
mov xmm15_1 0@uint64;
(* vpxor  %ymm14,%ymm5,%ymm14                      #! PC = 0x555555579363 *)
xor ymm14_0@uint64 ymm5_0 ymm14_0;
xor ymm14_1@uint64 ymm5_1 ymm14_1;
xor ymm14_2@uint64 ymm5_2 ymm14_2;
xor ymm14_3@uint64 ymm5_3 ymm14_3;
(* vpsrlq $0x3f,%ymm1,%ymm5                        #! PC = 0x555555579368 *)
shr ymm5_0 ymm1_0 0x3f@uint64;
shr ymm5_1 ymm1_1 0x3f@uint64;
shr ymm5_2 ymm1_2 0x3f@uint64;
shr ymm5_3 ymm1_3 0x3f@uint64;
(* vpxor  %ymm8,%ymm2,%ymm8                        #! PC = 0x55555557936d *)
xor ymm8_0@uint64 ymm2_0 ymm8_0;
xor ymm8_1@uint64 ymm2_1 ymm8_1;
xor ymm8_2@uint64 ymm2_2 ymm8_2;
xor ymm8_3@uint64 ymm2_3 ymm8_3;
(* vpsllq $0x1,%ymm1,%ymm1                         #! PC = 0x555555579372 *)
shl ymm1_0 ymm1_0 0x1@uint64;
shl ymm1_1 ymm1_1 0x1@uint64;
shl ymm1_2 ymm1_2 0x1@uint64;
shl ymm1_3 ymm1_3 0x1@uint64;
(* vpxor  %ymm9,%ymm14,%ymm9                       #! PC = 0x555555579377 *)
xor ymm9_0@uint64 ymm14_0 ymm9_0;
xor ymm9_1@uint64 ymm14_1 ymm9_1;
xor ymm9_2@uint64 ymm14_2 ymm9_2;
xor ymm9_3@uint64 ymm14_3 ymm9_3;
(* vpxor  %ymm7,%ymm2,%ymm7                        #! PC = 0x55555557937c *)
xor ymm7_0@uint64 ymm2_0 ymm7_0;
xor ymm7_1@uint64 ymm2_1 ymm7_1;
xor ymm7_2@uint64 ymm2_2 ymm7_2;
xor ymm7_3@uint64 ymm2_3 ymm7_3;
(* vpor   %ymm5,%ymm1,%ymm1                        #! PC = 0x555555579380 *)
or ymm1_0@uint64 ymm1_0 ymm5_0;
or ymm1_1@uint64 ymm1_1 ymm5_1;
or ymm1_2@uint64 ymm1_2 ymm5_2;
or ymm1_3@uint64 ymm1_3 ymm5_3;
(* vpxor  %ymm12,%ymm14,%ymm12                     #! PC = 0x555555579384 *)
xor ymm12_0@uint64 ymm14_0 ymm12_0;
xor ymm12_1@uint64 ymm14_1 ymm12_1;
xor ymm12_2@uint64 ymm14_2 ymm12_2;
xor ymm12_3@uint64 ymm14_3 ymm12_3;
(* vpxor  %ymm13,%ymm1,%ymm13                      #! PC = 0x555555579389 *)
xor ymm13_0@uint64 ymm1_0 ymm13_0;
xor ymm13_1@uint64 ymm1_1 ymm13_1;
xor ymm13_2@uint64 ymm1_2 ymm13_2;
xor ymm13_3@uint64 ymm1_3 ymm13_3;
(* vpsrlq $0x3f,%ymm0,%ymm1                        #! PC = 0x55555557938e *)
shr ymm1_0 ymm0_0 0x3f@uint64;
shr ymm1_1 ymm0_1 0x3f@uint64;
shr ymm1_2 ymm0_2 0x3f@uint64;
shr ymm1_3 ymm0_3 0x3f@uint64;
(* vpsllq $0x1,%ymm0,%ymm0                         #! PC = 0x555555579393 *)
shl ymm0_0 ymm0_0 0x1@uint64;
shl ymm0_1 ymm0_1 0x1@uint64;
shl ymm0_2 ymm0_2 0x1@uint64;
shl ymm0_3 ymm0_3 0x1@uint64;
(* vpxor  %ymm10,%ymm13,%ymm10                     #! PC = 0x555555579398 *)
xor ymm10_0@uint64 ymm13_0 ymm10_0;
xor ymm10_1@uint64 ymm13_1 ymm10_1;
xor ymm10_2@uint64 ymm13_2 ymm10_2;
xor ymm10_3@uint64 ymm13_3 ymm10_3;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x55555557939d *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpsrlq $0x14,%ymm8,%ymm1                        #! PC = 0x5555555793a1 *)
shr ymm1_0 ymm8_0 0x14@uint64;
shr ymm1_1 ymm8_1 0x14@uint64;
shr ymm1_2 ymm8_2 0x14@uint64;
shr ymm1_3 ymm8_3 0x14@uint64;
(* vpsllq $0x2c,%ymm8,%ymm8                        #! PC = 0x5555555793a7 *)
shl ymm8_0 ymm8_0 0x2c@uint64;
shl ymm8_1 ymm8_1 0x2c@uint64;
shl ymm8_2 ymm8_2 0x2c@uint64;
shl ymm8_3 ymm8_3 0x2c@uint64;
(* vpxor  %ymm11,%ymm0,%ymm11                      #! PC = 0x5555555793ad *)
xor ymm11_0@uint64 ymm0_0 ymm11_0;
xor ymm11_1@uint64 ymm0_1 ymm11_1;
xor ymm11_2@uint64 ymm0_2 ymm11_2;
xor ymm11_3@uint64 ymm0_3 ymm11_3;
(* vpxor  -0x90(%rbp),%ymm4,%ymm0                  #! EA = L0x7fffffffbec0; Value = 0x599d6ef52fd9ae41; PC = 0x5555555793b2 *)
xor ymm0_0@uint64 ymm4_0 L0x7fffffffbec0;
xor ymm0_1@uint64 ymm4_1 L0x7fffffffbec8;
xor ymm0_2@uint64 ymm4_2 L0x7fffffffbed0;
xor ymm0_3@uint64 ymm4_3 L0x7fffffffbed8;
(* vpor   %ymm1,%ymm8,%ymm8                        #! PC = 0x5555555793ba *)
or ymm8_0@uint64 ymm8_0 ymm1_0;
or ymm8_1@uint64 ymm8_1 ymm1_1;
or ymm8_2@uint64 ymm8_2 ymm1_2;
or ymm8_3@uint64 ymm8_3 ymm1_3;
(* vpsrlq $0x15,%ymm9,%ymm1                        #! PC = 0x5555555793be *)
shr ymm1_0 ymm9_0 0x15@uint64;
shr ymm1_1 ymm9_1 0x15@uint64;
shr ymm1_2 ymm9_2 0x15@uint64;
shr ymm1_3 ymm9_3 0x15@uint64;
(* vpxor  %ymm3,%ymm11,%ymm3                       #! PC = 0x5555555793c4 *)
xor ymm3_0@uint64 ymm11_0 ymm3_0;
xor ymm3_1@uint64 ymm11_1 ymm3_1;
xor ymm3_2@uint64 ymm11_2 ymm3_2;
xor ymm3_3@uint64 ymm11_3 ymm3_3;
(* vpsllq $0x2b,%ymm9,%ymm9                        #! PC = 0x5555555793c8 *)
shl ymm9_0 ymm9_0 0x2b@uint64;
shl ymm9_1 ymm9_1 0x2b@uint64;
shl ymm9_2 ymm9_2 0x2b@uint64;
shl ymm9_3 ymm9_3 0x2b@uint64;
(* vpor   %ymm1,%ymm9,%ymm9                        #! PC = 0x5555555793ce *)
or ymm9_0@uint64 ymm9_0 ymm1_0;
or ymm9_1@uint64 ymm9_1 ymm1_1;
or ymm9_2@uint64 ymm9_2 ymm1_2;
or ymm9_3@uint64 ymm9_3 ymm1_3;
(* vpbroadcastq %xmm15,%ymm1                       #! PC = 0x5555555793d2 *)
mov ymm1_0 xmm15_0;
mov ymm1_1 xmm15_0;
mov ymm1_2 xmm15_0;
mov ymm1_3 xmm15_0;
(* vpandn %ymm9,%ymm8,%ymm5                        #! PC = 0x5555555793d7 *)
not ymm8_0n@uint64 ymm8_0;
and ymm5_0@uint64 ymm8_0n ymm9_0;
not ymm8_1n@uint64 ymm8_1;
and ymm5_1@uint64 ymm8_1n ymm9_1;
not ymm8_2n@uint64 ymm8_2;
and ymm5_2@uint64 ymm8_2n ymm9_2;
not ymm8_3n@uint64 ymm8_3;
and ymm5_3@uint64 ymm8_3n ymm9_3;
(* vpxor  %ymm5,%ymm1,%ymm1                        #! PC = 0x5555555793dc *)
xor ymm1_0@uint64 ymm1_0 ymm5_0;
xor ymm1_1@uint64 ymm1_1 ymm5_1;
xor ymm1_2@uint64 ymm1_2 ymm5_2;
xor ymm1_3@uint64 ymm1_3 ymm5_3;
(* vpxor  %ymm0,%ymm1,%ymm15                       #! PC = 0x5555555793e0 *)
xor ymm15_0@uint64 ymm1_0 ymm0_0;
xor ymm15_1@uint64 ymm1_1 ymm0_1;
xor ymm15_2@uint64 ymm1_2 ymm0_2;
xor ymm15_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x2b,%ymm10,%ymm1                       #! PC = 0x5555555793e4 *)
shr ymm1_0 ymm10_0 0x2b@uint64;
shr ymm1_1 ymm10_1 0x2b@uint64;
shr ymm1_2 ymm10_2 0x2b@uint64;
shr ymm1_3 ymm10_3 0x2b@uint64;
(* vpsllq $0x15,%ymm10,%ymm10                      #! PC = 0x5555555793ea *)
shl ymm10_0 ymm10_0 0x15@uint64;
shl ymm10_1 ymm10_1 0x15@uint64;
shl ymm10_2 ymm10_2 0x15@uint64;
shl ymm10_3 ymm10_3 0x15@uint64;
(* vmovdqa %ymm15,-0x90(%rbp)                      #! EA = L0x7fffffffbec0; PC = 0x5555555793f0 *)
mov L0x7fffffffbec0 ymm15_0;
mov L0x7fffffffbec8 ymm15_1;
mov L0x7fffffffbed0 ymm15_2;
mov L0x7fffffffbed8 ymm15_3;
(* vpor   %ymm1,%ymm10,%ymm10                      #! PC = 0x5555555793f8 *)
or ymm10_0@uint64 ymm10_0 ymm1_0;
or ymm10_1@uint64 ymm10_1 ymm1_1;
or ymm10_2@uint64 ymm10_2 ymm1_2;
or ymm10_3@uint64 ymm10_3 ymm1_3;
(* vpandn %ymm10,%ymm9,%ymm1                       #! PC = 0x5555555793fc *)
not ymm9_0n@uint64 ymm9_0;
and ymm1_0@uint64 ymm9_0n ymm10_0;
not ymm9_1n@uint64 ymm9_1;
and ymm1_1@uint64 ymm9_1n ymm10_1;
not ymm9_2n@uint64 ymm9_2;
and ymm1_2@uint64 ymm9_2n ymm10_2;
not ymm9_3n@uint64 ymm9_3;
and ymm1_3@uint64 ymm9_3n ymm10_3;
(* vpxor  %ymm8,%ymm1,%ymm1                        #! PC = 0x555555579401 *)
xor ymm1_0@uint64 ymm1_0 ymm8_0;
xor ymm1_1@uint64 ymm1_1 ymm8_1;
xor ymm1_2@uint64 ymm1_2 ymm8_2;
xor ymm1_3@uint64 ymm1_3 ymm8_3;
(* vpandn %ymm8,%ymm0,%ymm8                        #! PC = 0x555555579406 *)
not ymm0_0n@uint64 ymm0_0;
and ymm8_0@uint64 ymm0_0n ymm8_0;
not ymm0_1n@uint64 ymm0_1;
and ymm8_1@uint64 ymm0_1n ymm8_1;
not ymm0_2n@uint64 ymm0_2;
and ymm8_2@uint64 ymm0_2n ymm8_2;
not ymm0_3n@uint64 ymm0_3;
and ymm8_3@uint64 ymm0_3n ymm8_3;
(* vmovdqa %ymm1,-0x2b0(%rbp)                      #! EA = L0x7fffffffbca0; PC = 0x55555557940b *)
mov L0x7fffffffbca0 ymm1_0;
mov L0x7fffffffbca8 ymm1_1;
mov L0x7fffffffbcb0 ymm1_2;
mov L0x7fffffffbcb8 ymm1_3;
(* vpsrlq $0x32,%ymm3,%ymm1                        #! PC = 0x555555579413 *)
shr ymm1_0 ymm3_0 0x32@uint64;
shr ymm1_1 ymm3_1 0x32@uint64;
shr ymm1_2 ymm3_2 0x32@uint64;
shr ymm1_3 ymm3_3 0x32@uint64;
(* vpsllq $0xe,%ymm3,%ymm3                         #! PC = 0x555555579418 *)
shl ymm3_0 ymm3_0 0xe@uint64;
shl ymm3_1 ymm3_1 0xe@uint64;
shl ymm3_2 ymm3_2 0xe@uint64;
shl ymm3_3 ymm3_3 0xe@uint64;
(* vpor   %ymm1,%ymm3,%ymm3                        #! PC = 0x55555557941d *)
or ymm3_0@uint64 ymm3_0 ymm1_0;
or ymm3_1@uint64 ymm3_1 ymm1_1;
or ymm3_2@uint64 ymm3_2 ymm1_2;
or ymm3_3@uint64 ymm3_3 ymm1_3;
(* vpandn %ymm3,%ymm10,%ymm1                       #! PC = 0x555555579421 *)
not ymm10_0n@uint64 ymm10_0;
and ymm1_0@uint64 ymm10_0n ymm3_0;
not ymm10_1n@uint64 ymm10_1;
and ymm1_1@uint64 ymm10_1n ymm3_1;
not ymm10_2n@uint64 ymm10_2;
and ymm1_2@uint64 ymm10_2n ymm3_2;
not ymm10_3n@uint64 ymm10_3;
and ymm1_3@uint64 ymm10_3n ymm3_3;
(* vpxor  %ymm3,%ymm8,%ymm8                        #! PC = 0x555555579425 *)
xor ymm8_0@uint64 ymm8_0 ymm3_0;
xor ymm8_1@uint64 ymm8_1 ymm3_1;
xor ymm8_2@uint64 ymm8_2 ymm3_2;
xor ymm8_3@uint64 ymm8_3 ymm3_3;
(* vpxor  %ymm9,%ymm1,%ymm9                        #! PC = 0x555555579429 *)
xor ymm9_0@uint64 ymm1_0 ymm9_0;
xor ymm9_1@uint64 ymm1_1 ymm9_1;
xor ymm9_2@uint64 ymm1_2 ymm9_2;
xor ymm9_3@uint64 ymm1_3 ymm9_3;
(* vpandn %ymm0,%ymm3,%ymm1                        #! PC = 0x55555557942e *)
not ymm3_0n@uint64 ymm3_0;
and ymm1_0@uint64 ymm3_0n ymm0_0;
not ymm3_1n@uint64 ymm3_1;
and ymm1_1@uint64 ymm3_1n ymm0_1;
not ymm3_2n@uint64 ymm3_2;
and ymm1_2@uint64 ymm3_2n ymm0_2;
not ymm3_3n@uint64 ymm3_3;
and ymm1_3@uint64 ymm3_3n ymm0_3;
(* vmovdqa %ymm8,-0x270(%rbp)                      #! EA = L0x7fffffffbce0; PC = 0x555555579432 *)
mov L0x7fffffffbce0 ymm8_0;
mov L0x7fffffffbce8 ymm8_1;
mov L0x7fffffffbcf0 ymm8_2;
mov L0x7fffffffbcf8 ymm8_3;
(* vmovdqa %ymm9,-0x290(%rbp)                      #! EA = L0x7fffffffbcc0; PC = 0x55555557943a *)
mov L0x7fffffffbcc0 ymm9_0;
mov L0x7fffffffbcc8 ymm9_1;
mov L0x7fffffffbcd0 ymm9_2;
mov L0x7fffffffbcd8 ymm9_3;
(* vpxor  %ymm10,%ymm1,%ymm9                       #! PC = 0x555555579442 *)
xor ymm9_0@uint64 ymm1_0 ymm10_0;
xor ymm9_1@uint64 ymm1_1 ymm10_1;
xor ymm9_2@uint64 ymm1_2 ymm10_2;
xor ymm9_3@uint64 ymm1_3 ymm10_3;
(* vpxor  -0x150(%rbp),%ymm13,%ymm10               #! EA = L0x7fffffffbe00; Value = 0xfc80d28cc5cc542e; PC = 0x555555579447 *)
xor ymm10_0@uint64 ymm13_0 L0x7fffffffbe00;
xor ymm10_1@uint64 ymm13_1 L0x7fffffffbe08;
xor ymm10_2@uint64 ymm13_2 L0x7fffffffbe10;
xor ymm10_3@uint64 ymm13_3 L0x7fffffffbe18;
(* vmovdqa %ymm9,-0x170(%rbp)                      #! EA = L0x7fffffffbde0; PC = 0x55555557944f *)
mov L0x7fffffffbde0 ymm9_0;
mov L0x7fffffffbde8 ymm9_1;
mov L0x7fffffffbdf0 ymm9_2;
mov L0x7fffffffbdf8 ymm9_3;
(* vpxor  -0xf0(%rbp),%ymm13,%ymm9                 #! EA = L0x7fffffffbe60; Value = 0x5211152025d6cffa; PC = 0x555555579457 *)
xor ymm9_0@uint64 ymm13_0 L0x7fffffffbe60;
xor ymm9_1@uint64 ymm13_1 L0x7fffffffbe68;
xor ymm9_2@uint64 ymm13_2 L0x7fffffffbe70;
xor ymm9_3@uint64 ymm13_3 L0x7fffffffbe78;
(* vpsrlq $0x24,%ymm10,%ymm1                       #! PC = 0x55555557945f *)
shr ymm1_0 ymm10_0 0x24@uint64;
shr ymm1_1 ymm10_1 0x24@uint64;
shr ymm1_2 ymm10_2 0x24@uint64;
shr ymm1_3 ymm10_3 0x24@uint64;
(* vpsllq $0x1c,%ymm10,%ymm0                       #! PC = 0x555555579465 *)
shl ymm0_0 ymm10_0 0x1c@uint64;
shl ymm0_1 ymm10_1 0x1c@uint64;
shl ymm0_2 ymm10_2 0x1c@uint64;
shl ymm0_3 ymm10_3 0x1c@uint64;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x55555557946b *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x130(%rbp),%ymm11,%ymm1                #! EA = L0x7fffffffbe20; Value = 0xa627237129eee7a1; PC = 0x55555557946f *)
xor ymm1_0@uint64 ymm11_0 L0x7fffffffbe20;
xor ymm1_1@uint64 ymm11_1 L0x7fffffffbe28;
xor ymm1_2@uint64 ymm11_2 L0x7fffffffbe30;
xor ymm1_3@uint64 ymm11_3 L0x7fffffffbe38;
(* vpsrlq $0x2c,%ymm1,%ymm3                        #! PC = 0x555555579477 *)
shr ymm3_0 ymm1_0 0x2c@uint64;
shr ymm3_1 ymm1_1 0x2c@uint64;
shr ymm3_2 ymm1_2 0x2c@uint64;
shr ymm3_3 ymm1_3 0x2c@uint64;
(* vpsllq $0x14,%ymm1,%ymm1                        #! PC = 0x55555557947c *)
shl ymm1_0 ymm1_0 0x14@uint64;
shl ymm1_1 ymm1_1 0x14@uint64;
shl ymm1_2 ymm1_2 0x14@uint64;
shl ymm1_3 ymm1_3 0x14@uint64;
(* vpor   %ymm3,%ymm1,%ymm1                        #! PC = 0x555555579481 *)
or ymm1_0@uint64 ymm1_0 ymm3_0;
or ymm1_1@uint64 ymm1_1 ymm3_1;
or ymm1_2@uint64 ymm1_2 ymm3_2;
or ymm1_3@uint64 ymm1_3 ymm3_3;
(* vpxor  -0x1d0(%rbp),%ymm4,%ymm3                 #! EA = L0x7fffffffbd80; Value = 0x62e2cafa15d2e5b2; PC = 0x555555579485 *)
xor ymm3_0@uint64 ymm4_0 L0x7fffffffbd80;
xor ymm3_1@uint64 ymm4_1 L0x7fffffffbd88;
xor ymm3_2@uint64 ymm4_2 L0x7fffffffbd90;
xor ymm3_3@uint64 ymm4_3 L0x7fffffffbd98;
(* vpsrlq $0x3d,%ymm3,%ymm5                        #! PC = 0x55555557948d *)
shr ymm5_0 ymm3_0 0x3d@uint64;
shr ymm5_1 ymm3_1 0x3d@uint64;
shr ymm5_2 ymm3_2 0x3d@uint64;
shr ymm5_3 ymm3_3 0x3d@uint64;
(* vpsllq $0x3,%ymm3,%ymm3                         #! PC = 0x555555579492 *)
shl ymm3_0 ymm3_0 0x3@uint64;
shl ymm3_1 ymm3_1 0x3@uint64;
shl ymm3_2 ymm3_2 0x3@uint64;
shl ymm3_3 ymm3_3 0x3@uint64;
(* vpor   %ymm5,%ymm3,%ymm3                        #! PC = 0x555555579497 *)
or ymm3_0@uint64 ymm3_0 ymm5_0;
or ymm3_1@uint64 ymm3_1 ymm5_1;
or ymm3_2@uint64 ymm3_2 ymm5_2;
or ymm3_3@uint64 ymm3_3 ymm5_3;
(* vpsrlq $0x13,%ymm7,%ymm5                        #! PC = 0x55555557949b *)
shr ymm5_0 ymm7_0 0x13@uint64;
shr ymm5_1 ymm7_1 0x13@uint64;
shr ymm5_2 ymm7_2 0x13@uint64;
shr ymm5_3 ymm7_3 0x13@uint64;
(* vpsllq $0x2d,%ymm7,%ymm7                        #! PC = 0x5555555794a0 *)
shl ymm7_0 ymm7_0 0x2d@uint64;
shl ymm7_1 ymm7_1 0x2d@uint64;
shl ymm7_2 ymm7_2 0x2d@uint64;
shl ymm7_3 ymm7_3 0x2d@uint64;
(* vpandn %ymm3,%ymm1,%ymm10                       #! PC = 0x5555555794a5 *)
not ymm1_0n@uint64 ymm1_0;
and ymm10_0@uint64 ymm1_0n ymm3_0;
not ymm1_1n@uint64 ymm1_1;
and ymm10_1@uint64 ymm1_1n ymm3_1;
not ymm1_2n@uint64 ymm1_2;
and ymm10_2@uint64 ymm1_2n ymm3_2;
not ymm1_3n@uint64 ymm1_3;
and ymm10_3@uint64 ymm1_3n ymm3_3;
(* vpor   %ymm5,%ymm7,%ymm15                       #! PC = 0x5555555794a9 *)
or ymm15_0@uint64 ymm7_0 ymm5_0;
or ymm15_1@uint64 ymm7_1 ymm5_1;
or ymm15_2@uint64 ymm7_2 ymm5_2;
or ymm15_3@uint64 ymm7_3 ymm5_3;
(* vpsrlq $0x3,%ymm12,%ymm5                        #! PC = 0x5555555794ad *)
shr ymm5_0 ymm12_0 0x3@uint64;
shr ymm5_1 ymm12_1 0x3@uint64;
shr ymm5_2 ymm12_2 0x3@uint64;
shr ymm5_3 ymm12_3 0x3@uint64;
(* vpxor  %ymm0,%ymm10,%ymm8                       #! PC = 0x5555555794b3 *)
xor ymm8_0@uint64 ymm10_0 ymm0_0;
xor ymm8_1@uint64 ymm10_1 ymm0_1;
xor ymm8_2@uint64 ymm10_2 ymm0_2;
xor ymm8_3@uint64 ymm10_3 ymm0_3;
(* vpsllq $0x3d,%ymm12,%ymm12                      #! PC = 0x5555555794b7 *)
shl ymm12_0 ymm12_0 0x3d@uint64;
shl ymm12_1 ymm12_1 0x3d@uint64;
shl ymm12_2 ymm12_2 0x3d@uint64;
shl ymm12_3 ymm12_3 0x3d@uint64;
(* vmovdqa %ymm8,-0x210(%rbp)                      #! EA = L0x7fffffffbd40; PC = 0x5555555794bd *)
mov L0x7fffffffbd40 ymm8_0;
mov L0x7fffffffbd48 ymm8_1;
mov L0x7fffffffbd50 ymm8_2;
mov L0x7fffffffbd58 ymm8_3;
(* vpandn %ymm15,%ymm3,%ymm8                       #! PC = 0x5555555794c5 *)
not ymm3_0n@uint64 ymm3_0;
and ymm8_0@uint64 ymm3_0n ymm15_0;
not ymm3_1n@uint64 ymm3_1;
and ymm8_1@uint64 ymm3_1n ymm15_1;
not ymm3_2n@uint64 ymm3_2;
and ymm8_2@uint64 ymm3_2n ymm15_2;
not ymm3_3n@uint64 ymm3_3;
and ymm8_3@uint64 ymm3_3n ymm15_3;
(* vpor   %ymm5,%ymm12,%ymm12                      #! PC = 0x5555555794ca *)
or ymm12_0@uint64 ymm12_0 ymm5_0;
or ymm12_1@uint64 ymm12_1 ymm5_1;
or ymm12_2@uint64 ymm12_2 ymm5_2;
or ymm12_3@uint64 ymm12_3 ymm5_3;
(* vpxor  %ymm1,%ymm8,%ymm8                        #! PC = 0x5555555794ce *)
xor ymm8_0@uint64 ymm8_0 ymm1_0;
xor ymm8_1@uint64 ymm8_1 ymm1_1;
xor ymm8_2@uint64 ymm8_2 ymm1_2;
xor ymm8_3@uint64 ymm8_3 ymm1_3;
(* vpandn %ymm12,%ymm15,%ymm5                      #! PC = 0x5555555794d2 *)
not ymm15_0n@uint64 ymm15_0;
and ymm5_0@uint64 ymm15_0n ymm12_0;
not ymm15_1n@uint64 ymm15_1;
and ymm5_1@uint64 ymm15_1n ymm12_1;
not ymm15_2n@uint64 ymm15_2;
and ymm5_2@uint64 ymm15_2n ymm12_2;
not ymm15_3n@uint64 ymm15_3;
and ymm5_3@uint64 ymm15_3n ymm12_3;
(* vpxor  %ymm3,%ymm5,%ymm7                        #! PC = 0x5555555794d7 *)
xor ymm7_0@uint64 ymm5_0 ymm3_0;
xor ymm7_1@uint64 ymm5_1 ymm3_1;
xor ymm7_2@uint64 ymm5_2 ymm3_2;
xor ymm7_3@uint64 ymm5_3 ymm3_3;
(* vpsrlq $0x27,%ymm9,%ymm5                        #! PC = 0x5555555794db *)
shr ymm5_0 ymm9_0 0x27@uint64;
shr ymm5_1 ymm9_1 0x27@uint64;
shr ymm5_2 ymm9_2 0x27@uint64;
shr ymm5_3 ymm9_3 0x27@uint64;
(* vpandn %ymm0,%ymm12,%ymm3                       #! PC = 0x5555555794e1 *)
not ymm12_0n@uint64 ymm12_0;
and ymm3_0@uint64 ymm12_0n ymm0_0;
not ymm12_1n@uint64 ymm12_1;
and ymm3_1@uint64 ymm12_1n ymm0_1;
not ymm12_2n@uint64 ymm12_2;
and ymm3_2@uint64 ymm12_2n ymm0_2;
not ymm12_3n@uint64 ymm12_3;
and ymm3_3@uint64 ymm12_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x5555555794e5 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm15,%ymm3,%ymm10                      #! PC = 0x5555555794e9 *)
xor ymm10_0@uint64 ymm3_0 ymm15_0;
xor ymm10_1@uint64 ymm3_1 ymm15_1;
xor ymm10_2@uint64 ymm3_2 ymm15_2;
xor ymm10_3@uint64 ymm3_3 ymm15_3;
(* vmovdqa %ymm7,-0x1d0(%rbp)                      #! EA = L0x7fffffffbd80; PC = 0x5555555794ee *)
mov L0x7fffffffbd80 ymm7_0;
mov L0x7fffffffbd88 ymm7_1;
mov L0x7fffffffbd90 ymm7_2;
mov L0x7fffffffbd98 ymm7_3;
(* vpxor  %ymm12,%ymm0,%ymm12                      #! PC = 0x5555555794f6 *)
xor ymm12_0@uint64 ymm0_0 ymm12_0;
xor ymm12_1@uint64 ymm0_1 ymm12_1;
xor ymm12_2@uint64 ymm0_2 ymm12_2;
xor ymm12_3@uint64 ymm0_3 ymm12_3;
(* vpxor  -0x1f0(%rbp),%ymm2,%ymm0                 #! EA = L0x7fffffffbd60; Value = 0xbf34c0d029d604e5; PC = 0x5555555794fb *)
xor ymm0_0@uint64 ymm2_0 L0x7fffffffbd60;
xor ymm0_1@uint64 ymm2_1 L0x7fffffffbd68;
xor ymm0_2@uint64 ymm2_2 L0x7fffffffbd70;
xor ymm0_3@uint64 ymm2_3 L0x7fffffffbd78;
(* vmovdqa %ymm10,-0x250(%rbp)                     #! EA = L0x7fffffffbd00; PC = 0x555555579503 *)
mov L0x7fffffffbd00 ymm10_0;
mov L0x7fffffffbd08 ymm10_1;
mov L0x7fffffffbd10 ymm10_2;
mov L0x7fffffffbd18 ymm10_3;
(* vpsllq $0x19,%ymm9,%ymm9                        #! PC = 0x55555557950b *)
shl ymm9_0 ymm9_0 0x19@uint64;
shl ymm9_1 ymm9_1 0x19@uint64;
shl ymm9_2 ymm9_2 0x19@uint64;
shl ymm9_3 ymm9_3 0x19@uint64;
(* vmovdqa %ymm12,-0x150(%rbp)                     #! EA = L0x7fffffffbe00; PC = 0x555555579511 *)
mov L0x7fffffffbe00 ymm12_0;
mov L0x7fffffffbe08 ymm12_1;
mov L0x7fffffffbe10 ymm12_2;
mov L0x7fffffffbe18 ymm12_3;
(* vpsrlq $0x3f,%ymm0,%ymm1                        #! PC = 0x555555579519 *)
shr ymm1_0 ymm0_0 0x3f@uint64;
shr ymm1_1 ymm0_1 0x3f@uint64;
shr ymm1_2 ymm0_2 0x3f@uint64;
shr ymm1_3 ymm0_3 0x3f@uint64;
(* vpsllq $0x1,%ymm0,%ymm0                         #! PC = 0x55555557951e *)
shl ymm0_0 ymm0_0 0x1@uint64;
shl ymm0_1 ymm0_1 0x1@uint64;
shl ymm0_2 ymm0_2 0x1@uint64;
shl ymm0_3 ymm0_3 0x1@uint64;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x555555579523 *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x1b0(%rbp),%ymm14,%ymm1                #! EA = L0x7fffffffbda0; Value = 0xdf7213b7f0a141c9; PC = 0x555555579527 *)
xor ymm1_0@uint64 ymm14_0 L0x7fffffffbda0;
xor ymm1_1@uint64 ymm14_1 L0x7fffffffbda8;
xor ymm1_2@uint64 ymm14_2 L0x7fffffffbdb0;
xor ymm1_3@uint64 ymm14_3 L0x7fffffffbdb8;
(* vpsrlq $0x3a,%ymm1,%ymm3                        #! PC = 0x55555557952f *)
shr ymm3_0 ymm1_0 0x3a@uint64;
shr ymm3_1 ymm1_1 0x3a@uint64;
shr ymm3_2 ymm1_2 0x3a@uint64;
shr ymm3_3 ymm1_3 0x3a@uint64;
(* vpsllq $0x6,%ymm1,%ymm1                         #! PC = 0x555555579534 *)
shl ymm1_0 ymm1_0 0x6@uint64;
shl ymm1_1 ymm1_1 0x6@uint64;
shl ymm1_2 ymm1_2 0x6@uint64;
shl ymm1_3 ymm1_3 0x6@uint64;
(* vpor   %ymm3,%ymm1,%ymm1                        #! PC = 0x555555579539 *)
or ymm1_0@uint64 ymm1_0 ymm3_0;
or ymm1_1@uint64 ymm1_1 ymm3_1;
or ymm1_2@uint64 ymm1_2 ymm3_2;
or ymm1_3@uint64 ymm1_3 ymm3_3;
(* vpor   %ymm5,%ymm9,%ymm3                        #! PC = 0x55555557953d *)
or ymm3_0@uint64 ymm9_0 ymm5_0;
or ymm3_1@uint64 ymm9_1 ymm5_1;
or ymm3_2@uint64 ymm9_2 ymm5_2;
or ymm3_3@uint64 ymm9_3 ymm5_3;
(* vpandn %ymm3,%ymm1,%ymm5                        #! PC = 0x555555579541 *)
not ymm1_0n@uint64 ymm1_0;
and ymm5_0@uint64 ymm1_0n ymm3_0;
not ymm1_1n@uint64 ymm1_1;
and ymm5_1@uint64 ymm1_1n ymm3_1;
not ymm1_2n@uint64 ymm1_2;
and ymm5_2@uint64 ymm1_2n ymm3_2;
not ymm1_3n@uint64 ymm1_3;
and ymm5_3@uint64 ymm1_3n ymm3_3;
(* vpxor  %ymm0,%ymm5,%ymm12                       #! PC = 0x555555579545 *)
xor ymm12_0@uint64 ymm5_0 ymm0_0;
xor ymm12_1@uint64 ymm5_1 ymm0_1;
xor ymm12_2@uint64 ymm5_2 ymm0_2;
xor ymm12_3@uint64 ymm5_3 ymm0_3;
(* vmovdqa %ymm12,-0x130(%rbp)                     #! EA = L0x7fffffffbe20; PC = 0x555555579549 *)
mov L0x7fffffffbe20 ymm12_0;
mov L0x7fffffffbe28 ymm12_1;
mov L0x7fffffffbe30 ymm12_2;
mov L0x7fffffffbe38 ymm12_3;
(* vpxor  -0x50(%rbp),%ymm11,%ymm5                 #! EA = L0x7fffffffbf00; Value = 0xcd893f7adababc47; PC = 0x555555579551 *)
xor ymm5_0@uint64 ymm11_0 L0x7fffffffbf00;
xor ymm5_1@uint64 ymm11_1 L0x7fffffffbf08;
xor ymm5_2@uint64 ymm11_2 L0x7fffffffbf10;
xor ymm5_3@uint64 ymm11_3 L0x7fffffffbf18;
(* vpshufb 0x549a1(%rip),%ymm5,%ymm5        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x555555579556 *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm5_0, ymm5_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm5_2, ymm5_3, tmp_2, tmp_3);
mov ymm5_0 tmp_0;
mov ymm5_1 tmp_1;
mov ymm5_2 tmp_2;
mov ymm5_3 tmp_3;
(* vpandn %ymm5,%ymm3,%ymm7                        #! PC = 0x55555557955f *)
not ymm3_0n@uint64 ymm3_0;
and ymm7_0@uint64 ymm3_0n ymm5_0;
not ymm3_1n@uint64 ymm3_1;
and ymm7_1@uint64 ymm3_1n ymm5_1;
not ymm3_2n@uint64 ymm3_2;
and ymm7_2@uint64 ymm3_2n ymm5_2;
not ymm3_3n@uint64 ymm3_3;
and ymm7_3@uint64 ymm3_3n ymm5_3;
(* vpxor  %ymm1,%ymm7,%ymm15                       #! PC = 0x555555579563 *)
xor ymm15_0@uint64 ymm7_0 ymm1_0;
xor ymm15_1@uint64 ymm7_1 ymm1_1;
xor ymm15_2@uint64 ymm7_2 ymm1_2;
xor ymm15_3@uint64 ymm7_3 ymm1_3;
(* vpsrlq $0x2e,%ymm6,%ymm7                        #! PC = 0x555555579567 *)
shr ymm7_0 ymm6_0 0x2e@uint64;
shr ymm7_1 ymm6_1 0x2e@uint64;
shr ymm7_2 ymm6_2 0x2e@uint64;
shr ymm7_3 ymm6_3 0x2e@uint64;
(* vpsllq $0x12,%ymm6,%ymm6                        #! PC = 0x55555557956c *)
shl ymm6_0 ymm6_0 0x12@uint64;
shl ymm6_1 ymm6_1 0x12@uint64;
shl ymm6_2 ymm6_2 0x12@uint64;
shl ymm6_3 ymm6_3 0x12@uint64;
(* vmovdqa %ymm15,-0x1f0(%rbp)                     #! EA = L0x7fffffffbd60; PC = 0x555555579571 *)
mov L0x7fffffffbd60 ymm15_0;
mov L0x7fffffffbd68 ymm15_1;
mov L0x7fffffffbd70 ymm15_2;
mov L0x7fffffffbd78 ymm15_3;
(* vpor   %ymm7,%ymm6,%ymm12                       #! PC = 0x555555579579 *)
or ymm12_0@uint64 ymm6_0 ymm7_0;
or ymm12_1@uint64 ymm6_1 ymm7_1;
or ymm12_2@uint64 ymm6_2 ymm7_2;
or ymm12_3@uint64 ymm6_3 ymm7_3;
(* vpxor  -0x110(%rbp),%ymm2,%ymm7                 #! EA = L0x7fffffffbe40; Value = 0x720fede69ef73316; PC = 0x55555557957d *)
xor ymm7_0@uint64 ymm2_0 L0x7fffffffbe40;
xor ymm7_1@uint64 ymm2_1 L0x7fffffffbe48;
xor ymm7_2@uint64 ymm2_2 L0x7fffffffbe50;
xor ymm7_3@uint64 ymm2_3 L0x7fffffffbe58;
(* vpandn %ymm12,%ymm5,%ymm9                       #! PC = 0x555555579585 *)
not ymm5_0n@uint64 ymm5_0;
and ymm9_0@uint64 ymm5_0n ymm12_0;
not ymm5_1n@uint64 ymm5_1;
and ymm9_1@uint64 ymm5_1n ymm12_1;
not ymm5_2n@uint64 ymm5_2;
and ymm9_2@uint64 ymm5_2n ymm12_2;
not ymm5_3n@uint64 ymm5_3;
and ymm9_3@uint64 ymm5_3n ymm12_3;
(* vpxor  %ymm3,%ymm9,%ymm9                        #! PC = 0x55555557958a *)
xor ymm9_0@uint64 ymm9_0 ymm3_0;
xor ymm9_1@uint64 ymm9_1 ymm3_1;
xor ymm9_2@uint64 ymm9_2 ymm3_2;
xor ymm9_3@uint64 ymm9_3 ymm3_3;
(* vpandn %ymm0,%ymm12,%ymm3                       #! PC = 0x55555557958e *)
not ymm12_0n@uint64 ymm12_0;
and ymm3_0@uint64 ymm12_0n ymm0_0;
not ymm12_1n@uint64 ymm12_1;
and ymm3_1@uint64 ymm12_1n ymm0_1;
not ymm12_2n@uint64 ymm12_2;
and ymm3_2@uint64 ymm12_2n ymm0_2;
not ymm12_3n@uint64 ymm12_3;
and ymm3_3@uint64 ymm12_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x555555579592 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  -0x2f0(%rbp),%ymm11,%ymm1                #! EA = L0x7fffffffbc60; Value = 0x3be9b37a46dfa823; PC = 0x555555579596 *)
xor ymm1_0@uint64 ymm11_0 L0x7fffffffbc60;
xor ymm1_1@uint64 ymm11_1 L0x7fffffffbc68;
xor ymm1_2@uint64 ymm11_2 L0x7fffffffbc70;
xor ymm1_3@uint64 ymm11_3 L0x7fffffffbc78;
(* vpxor  %ymm12,%ymm0,%ymm0                       #! PC = 0x55555557959e *)
xor ymm0_0@uint64 ymm0_0 ymm12_0;
xor ymm0_1@uint64 ymm0_1 ymm12_1;
xor ymm0_2@uint64 ymm0_2 ymm12_2;
xor ymm0_3@uint64 ymm0_3 ymm12_3;
(* vpxor  %ymm5,%ymm3,%ymm10                       #! PC = 0x5555555795a3 *)
xor ymm10_0@uint64 ymm3_0 ymm5_0;
xor ymm10_1@uint64 ymm3_1 ymm5_1;
xor ymm10_2@uint64 ymm3_2 ymm5_2;
xor ymm10_3@uint64 ymm3_3 ymm5_3;
(* vmovdqa %ymm0,-0x230(%rbp)                      #! EA = L0x7fffffffbd20; PC = 0x5555555795a7 *)
mov L0x7fffffffbd20 ymm0_0;
mov L0x7fffffffbd28 ymm0_1;
mov L0x7fffffffbd30 ymm0_2;
mov L0x7fffffffbd38 ymm0_3;
(* vpsrlq $0x36,%ymm7,%ymm12                       #! PC = 0x5555555795af *)
shr ymm12_0 ymm7_0 0x36@uint64;
shr ymm12_1 ymm7_1 0x36@uint64;
shr ymm12_2 ymm7_2 0x36@uint64;
shr ymm12_3 ymm7_3 0x36@uint64;
(* vpsllq $0xa,%ymm7,%ymm7                         #! PC = 0x5555555795b4 *)
shl ymm7_0 ymm7_0 0xa@uint64;
shl ymm7_1 ymm7_1 0xa@uint64;
shl ymm7_2 ymm7_2 0xa@uint64;
shl ymm7_3 ymm7_3 0xa@uint64;
(* vpxor  -0xd0(%rbp),%ymm11,%ymm11                #! EA = L0x7fffffffbe80; Value = 0x347937f914e65553; PC = 0x5555555795b9 *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbe80;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbe88;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbe90;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbe98;
(* vpsrlq $0x25,%ymm1,%ymm3                        #! PC = 0x5555555795c1 *)
shr ymm3_0 ymm1_0 0x25@uint64;
shr ymm3_1 ymm1_1 0x25@uint64;
shr ymm3_2 ymm1_2 0x25@uint64;
shr ymm3_3 ymm1_3 0x25@uint64;
(* vpsllq $0x1b,%ymm1,%ymm0                        #! PC = 0x5555555795c6 *)
shl ymm0_0 ymm1_0 0x1b@uint64;
shl ymm0_1 ymm1_1 0x1b@uint64;
shl ymm0_2 ymm1_2 0x1b@uint64;
shl ymm0_3 ymm1_3 0x1b@uint64;
(* vpxor  -0x2d0(%rbp),%ymm4,%ymm1                 #! EA = L0x7fffffffbc80; Value = 0x8bf1357228f2736e; PC = 0x5555555795cb *)
xor ymm1_0@uint64 ymm4_0 L0x7fffffffbc80;
xor ymm1_1@uint64 ymm4_1 L0x7fffffffbc88;
xor ymm1_2@uint64 ymm4_2 L0x7fffffffbc90;
xor ymm1_3@uint64 ymm4_3 L0x7fffffffbc98;
(* vmovdqa %ymm10,-0x1b0(%rbp)                     #! EA = L0x7fffffffbda0; PC = 0x5555555795d3 *)
mov L0x7fffffffbda0 ymm10_0;
mov L0x7fffffffbda8 ymm10_1;
mov L0x7fffffffbdb0 ymm10_2;
mov L0x7fffffffbdb8 ymm10_3;
(* vpor   %ymm7,%ymm12,%ymm12                      #! PC = 0x5555555795db *)
or ymm12_0@uint64 ymm12_0 ymm7_0;
or ymm12_1@uint64 ymm12_1 ymm7_1;
or ymm12_2@uint64 ymm12_2 ymm7_2;
or ymm12_3@uint64 ymm12_3 ymm7_3;
(* vpor   %ymm3,%ymm0,%ymm0                        #! PC = 0x5555555795df *)
or ymm0_0@uint64 ymm0_0 ymm3_0;
or ymm0_1@uint64 ymm0_1 ymm3_1;
or ymm0_2@uint64 ymm0_2 ymm3_2;
or ymm0_3@uint64 ymm0_3 ymm3_3;
(* vpxor  -0xb0(%rbp),%ymm4,%ymm4                  #! EA = L0x7fffffffbea0; Value = 0x5ce481e8d139a791; PC = 0x5555555795e3 *)
xor ymm4_0@uint64 ymm4_0 L0x7fffffffbea0;
xor ymm4_1@uint64 ymm4_1 L0x7fffffffbea8;
xor ymm4_2@uint64 ymm4_2 L0x7fffffffbeb0;
xor ymm4_3@uint64 ymm4_3 L0x7fffffffbeb8;
(* vpsrlq $0x1c,%ymm1,%ymm10                       #! PC = 0x5555555795eb *)
shr ymm10_0 ymm1_0 0x1c@uint64;
shr ymm10_1 ymm1_1 0x1c@uint64;
shr ymm10_2 ymm1_2 0x1c@uint64;
shr ymm10_3 ymm1_3 0x1c@uint64;
(* vpsllq $0x24,%ymm1,%ymm1                        #! PC = 0x5555555795f0 *)
shl ymm1_0 ymm1_0 0x24@uint64;
shl ymm1_1 ymm1_1 0x24@uint64;
shl ymm1_2 ymm1_2 0x24@uint64;
shl ymm1_3 ymm1_3 0x24@uint64;
(* vpor   %ymm1,%ymm10,%ymm10                      #! PC = 0x5555555795f5 *)
or ymm10_0@uint64 ymm10_0 ymm1_0;
or ymm10_1@uint64 ymm10_1 ymm1_1;
or ymm10_2@uint64 ymm10_2 ymm1_2;
or ymm10_3@uint64 ymm10_3 ymm1_3;
(* vpxor  -0x330(%rbp),%ymm13,%ymm1                #! EA = L0x7fffffffbc20; Value = 0x7c5fb2e3e371aef9; PC = 0x5555555795f9 *)
xor ymm1_0@uint64 ymm13_0 L0x7fffffffbc20;
xor ymm1_1@uint64 ymm13_1 L0x7fffffffbc28;
xor ymm1_2@uint64 ymm13_2 L0x7fffffffbc30;
xor ymm1_3@uint64 ymm13_3 L0x7fffffffbc38;
(* vpxor  -0x190(%rbp),%ymm13,%ymm13               #! EA = L0x7fffffffbdc0; Value = 0xb409a8ca23c781ef; PC = 0x555555579601 *)
xor ymm13_0@uint64 ymm13_0 L0x7fffffffbdc0;
xor ymm13_1@uint64 ymm13_1 L0x7fffffffbdc8;
xor ymm13_2@uint64 ymm13_2 L0x7fffffffbdd0;
xor ymm13_3@uint64 ymm13_3 L0x7fffffffbdd8;
(* vpandn %ymm12,%ymm10,%ymm3                      #! PC = 0x555555579609 *)
not ymm10_0n@uint64 ymm10_0;
and ymm3_0@uint64 ymm10_0n ymm12_0;
not ymm10_1n@uint64 ymm10_1;
and ymm3_1@uint64 ymm10_1n ymm12_1;
not ymm10_2n@uint64 ymm10_2;
and ymm3_2@uint64 ymm10_2n ymm12_2;
not ymm10_3n@uint64 ymm10_3;
and ymm3_3@uint64 ymm10_3n ymm12_3;
(* vpshufb 0x548c9(%rip),%ymm1,%ymm1        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x55555557960e *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm1_0, ymm1_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm1_2, ymm1_3, tmp_2, tmp_3);
mov ymm1_0 tmp_0;
mov ymm1_1 tmp_1;
mov ymm1_2 tmp_2;
mov ymm1_3 tmp_3;
(* vpxor  %ymm0,%ymm3,%ymm6                        #! PC = 0x555555579617 *)
xor ymm6_0@uint64 ymm3_0 ymm0_0;
xor ymm6_1@uint64 ymm3_1 ymm0_1;
xor ymm6_2@uint64 ymm3_2 ymm0_2;
xor ymm6_3@uint64 ymm3_3 ymm0_3;
(* vmovdqa %ymm6,%ymm15                            #! PC = 0x55555557961b *)
mov ymm15_0 ymm6_0;
mov ymm15_1 ymm6_1;
mov ymm15_2 ymm6_2;
mov ymm15_3 ymm6_3;
(* vpxor  -0x70(%rbp),%ymm14,%ymm6                 #! EA = L0x7fffffffbee0; Value = 0x10ee357fe5674585; PC = 0x55555557961f *)
xor ymm6_0@uint64 ymm14_0 L0x7fffffffbee0;
xor ymm6_1@uint64 ymm14_1 L0x7fffffffbee8;
xor ymm6_2@uint64 ymm14_2 L0x7fffffffbef0;
xor ymm6_3@uint64 ymm14_3 L0x7fffffffbef8;
(* vpxor  -0x310(%rbp),%ymm14,%ymm14               #! EA = L0x7fffffffbc40; Value = 0x088d5a13bfc74406; PC = 0x555555579624 *)
xor ymm14_0@uint64 ymm14_0 L0x7fffffffbc40;
xor ymm14_1@uint64 ymm14_1 L0x7fffffffbc48;
xor ymm14_2@uint64 ymm14_2 L0x7fffffffbc50;
xor ymm14_3@uint64 ymm14_3 L0x7fffffffbc58;
(* vmovdqa %ymm15,-0x2d0(%rbp)                     #! EA = L0x7fffffffbc80; PC = 0x55555557962c *)
mov L0x7fffffffbc80 ymm15_0;
mov L0x7fffffffbc88 ymm15_1;
mov L0x7fffffffbc90 ymm15_2;
mov L0x7fffffffbc98 ymm15_3;
(* vpsrlq $0x31,%ymm6,%ymm7                        #! PC = 0x555555579634 *)
shr ymm7_0 ymm6_0 0x31@uint64;
shr ymm7_1 ymm6_1 0x31@uint64;
shr ymm7_2 ymm6_2 0x31@uint64;
shr ymm7_3 ymm6_3 0x31@uint64;
(* vpsllq $0xf,%ymm6,%ymm6                         #! PC = 0x555555579639 *)
shl ymm6_0 ymm6_0 0xf@uint64;
shl ymm6_1 ymm6_1 0xf@uint64;
shl ymm6_2 ymm6_2 0xf@uint64;
shl ymm6_3 ymm6_3 0xf@uint64;
(* vpor   %ymm6,%ymm7,%ymm3                        #! PC = 0x55555557963e *)
or ymm3_0@uint64 ymm7_0 ymm6_0;
or ymm3_1@uint64 ymm7_1 ymm6_1;
or ymm3_2@uint64 ymm7_2 ymm6_2;
or ymm3_3@uint64 ymm7_3 ymm6_3;
(* vpandn %ymm1,%ymm3,%ymm5                        #! PC = 0x555555579642 *)
not ymm3_0n@uint64 ymm3_0;
and ymm5_0@uint64 ymm3_0n ymm1_0;
not ymm3_1n@uint64 ymm3_1;
and ymm5_1@uint64 ymm3_1n ymm1_1;
not ymm3_2n@uint64 ymm3_2;
and ymm5_2@uint64 ymm3_2n ymm1_2;
not ymm3_3n@uint64 ymm3_3;
and ymm5_3@uint64 ymm3_3n ymm1_3;
(* vpandn %ymm3,%ymm12,%ymm6                       #! PC = 0x555555579646 *)
not ymm12_0n@uint64 ymm12_0;
and ymm6_0@uint64 ymm12_0n ymm3_0;
not ymm12_1n@uint64 ymm12_1;
and ymm6_1@uint64 ymm12_1n ymm3_1;
not ymm12_2n@uint64 ymm12_2;
and ymm6_2@uint64 ymm12_2n ymm3_2;
not ymm12_3n@uint64 ymm12_3;
and ymm6_3@uint64 ymm12_3n ymm3_3;
(* vpxor  %ymm12,%ymm5,%ymm7                       #! PC = 0x55555557964a *)
xor ymm7_0@uint64 ymm5_0 ymm12_0;
xor ymm7_1@uint64 ymm5_1 ymm12_1;
xor ymm7_2@uint64 ymm5_2 ymm12_2;
xor ymm7_3@uint64 ymm5_3 ymm12_3;
(* vpsrlq $0x19,%ymm11,%ymm5                       #! PC = 0x55555557964f *)
shr ymm5_0 ymm11_0 0x19@uint64;
shr ymm5_1 ymm11_1 0x19@uint64;
shr ymm5_2 ymm11_2 0x19@uint64;
shr ymm5_3 ymm11_3 0x19@uint64;
(* vpxor  %ymm10,%ymm6,%ymm6                       #! PC = 0x555555579655 *)
xor ymm6_0@uint64 ymm6_0 ymm10_0;
xor ymm6_1@uint64 ymm6_1 ymm10_1;
xor ymm6_2@uint64 ymm6_2 ymm10_2;
xor ymm6_3@uint64 ymm6_3 ymm10_3;
(* vmovdqa %ymm7,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x55555557965a *)
mov L0x7fffffffbf00 ymm7_0;
mov L0x7fffffffbf08 ymm7_1;
mov L0x7fffffffbf10 ymm7_2;
mov L0x7fffffffbf18 ymm7_3;
(* vpandn %ymm0,%ymm1,%ymm7                        #! PC = 0x55555557965f *)
not ymm1_0n@uint64 ymm1_0;
and ymm7_0@uint64 ymm1_0n ymm0_0;
not ymm1_1n@uint64 ymm1_1;
and ymm7_1@uint64 ymm1_1n ymm0_1;
not ymm1_2n@uint64 ymm1_2;
and ymm7_2@uint64 ymm1_2n ymm0_2;
not ymm1_3n@uint64 ymm1_3;
and ymm7_3@uint64 ymm1_3n ymm0_3;
(* vpandn %ymm10,%ymm0,%ymm0                       #! PC = 0x555555579663 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm10_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm10_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm10_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm10_3;
(* vpxor  %ymm1,%ymm0,%ymm1                        #! PC = 0x555555579668 *)
xor ymm1_0@uint64 ymm0_0 ymm1_0;
xor ymm1_1@uint64 ymm0_1 ymm1_1;
xor ymm1_2@uint64 ymm0_2 ymm1_2;
xor ymm1_3@uint64 ymm0_3 ymm1_3;
(* vpsllq $0x27,%ymm11,%ymm11                      #! PC = 0x55555557966c *)
shl ymm11_0 ymm11_0 0x27@uint64;
shl ymm11_1 ymm11_1 0x27@uint64;
shl ymm11_2 ymm11_2 0x27@uint64;
shl ymm11_3 ymm11_3 0x27@uint64;
(* vpxor  %ymm3,%ymm7,%ymm7                        #! PC = 0x555555579672 *)
xor ymm7_0@uint64 ymm7_0 ymm3_0;
xor ymm7_1@uint64 ymm7_1 ymm3_1;
xor ymm7_2@uint64 ymm7_2 ymm3_2;
xor ymm7_3@uint64 ymm7_3 ymm3_3;
(* vpsrlq $0x9,%ymm13,%ymm0                        #! PC = 0x555555579676 *)
shr ymm0_0 ymm13_0 0x9@uint64;
shr ymm0_1 ymm13_1 0x9@uint64;
shr ymm0_2 ymm13_2 0x9@uint64;
shr ymm0_3 ymm13_3 0x9@uint64;
(* vpsllq $0x37,%ymm13,%ymm13                      #! PC = 0x55555557967c *)
shl ymm13_0 ymm13_0 0x37@uint64;
shl ymm13_1 ymm13_1 0x37@uint64;
shl ymm13_2 ymm13_2 0x37@uint64;
shl ymm13_3 ymm13_3 0x37@uint64;
(* vpor   %ymm11,%ymm5,%ymm11                      #! PC = 0x555555579682 *)
or ymm11_0@uint64 ymm5_0 ymm11_0;
or ymm11_1@uint64 ymm5_1 ymm11_1;
or ymm11_2@uint64 ymm5_2 ymm11_2;
or ymm11_3@uint64 ymm5_3 ymm11_3;
(* vmovdqa %ymm1,-0x70(%rbp)                       #! EA = L0x7fffffffbee0; PC = 0x555555579687 *)
mov L0x7fffffffbee0 ymm1_0;
mov L0x7fffffffbee8 ymm1_1;
mov L0x7fffffffbef0 ymm1_2;
mov L0x7fffffffbef8 ymm1_3;
(* vpor   %ymm13,%ymm0,%ymm13                      #! PC = 0x55555557968c *)
or ymm13_0@uint64 ymm0_0 ymm13_0;
or ymm13_1@uint64 ymm0_1 ymm13_1;
or ymm13_2@uint64 ymm0_2 ymm13_2;
or ymm13_3@uint64 ymm0_3 ymm13_3;
(* vpsrlq $0x2,%ymm14,%ymm3                        #! PC = 0x555555579691 *)
shr ymm3_0 ymm14_0 0x2@uint64;
shr ymm3_1 ymm14_1 0x2@uint64;
shr ymm3_2 ymm14_2 0x2@uint64;
shr ymm3_3 ymm14_3 0x2@uint64;
(* vpxor  -0x350(%rbp),%ymm2,%ymm1                 #! EA = L0x7fffffffbc00; Value = 0x63076cb5f51fcd4b; PC = 0x555555579697 *)
xor ymm1_0@uint64 ymm2_0 L0x7fffffffbc00;
xor ymm1_1@uint64 ymm2_1 L0x7fffffffbc08;
xor ymm1_2@uint64 ymm2_2 L0x7fffffffbc10;
xor ymm1_3@uint64 ymm2_3 L0x7fffffffbc18;
(* vpsllq $0x3e,%ymm14,%ymm14                      #! PC = 0x55555557969f *)
shl ymm14_0 ymm14_0 0x3e@uint64;
shl ymm14_1 ymm14_1 0x3e@uint64;
shl ymm14_2 ymm14_2 0x3e@uint64;
shl ymm14_3 ymm14_3 0x3e@uint64;
(* vpandn %ymm11,%ymm13,%ymm5                      #! PC = 0x5555555796a5 *)
not ymm13_0n@uint64 ymm13_0;
and ymm5_0@uint64 ymm13_0n ymm11_0;
not ymm13_1n@uint64 ymm13_1;
and ymm5_1@uint64 ymm13_1n ymm11_1;
not ymm13_2n@uint64 ymm13_2;
and ymm5_2@uint64 ymm13_2n ymm11_2;
not ymm13_3n@uint64 ymm13_3;
and ymm5_3@uint64 ymm13_3n ymm11_3;
(* vpor   %ymm14,%ymm3,%ymm3                       #! PC = 0x5555555796aa *)
or ymm3_0@uint64 ymm3_0 ymm14_0;
or ymm3_1@uint64 ymm3_1 ymm14_1;
or ymm3_2@uint64 ymm3_2 ymm14_2;
or ymm3_3@uint64 ymm3_3 ymm14_3;
(* vpsllq $0x2,%ymm1,%ymm2                         #! PC = 0x5555555796af *)
shl ymm2_0 ymm1_0 0x2@uint64;
shl ymm2_1 ymm1_1 0x2@uint64;
shl ymm2_2 ymm1_2 0x2@uint64;
shl ymm2_3 ymm1_3 0x2@uint64;
(* vpxor  %ymm3,%ymm5,%ymm5                        #! PC = 0x5555555796b4 *)
xor ymm5_0@uint64 ymm5_0 ymm3_0;
xor ymm5_1@uint64 ymm5_1 ymm3_1;
xor ymm5_2@uint64 ymm5_2 ymm3_2;
xor ymm5_3@uint64 ymm5_3 ymm3_3;
(* vpxor  %ymm15,%ymm5,%ymm10                      #! PC = 0x5555555796b8 *)
xor ymm10_0@uint64 ymm5_0 ymm15_0;
xor ymm10_1@uint64 ymm5_1 ymm15_1;
xor ymm10_2@uint64 ymm5_2 ymm15_2;
xor ymm10_3@uint64 ymm5_3 ymm15_3;
(* vmovdqa -0x130(%rbp),%ymm15                     #! EA = L0x7fffffffbe20; Value = 0x632b5576588a026d; PC = 0x5555555796bd *)
mov ymm15_0 L0x7fffffffbe20;
mov ymm15_1 L0x7fffffffbe28;
mov ymm15_2 L0x7fffffffbe30;
mov ymm15_3 L0x7fffffffbe38;
(* vpxor  -0x210(%rbp),%ymm15,%ymm0                #! EA = L0x7fffffffbd40; Value = 0x689cbc39b01c0cb9; PC = 0x5555555796c5 *)
xor ymm0_0@uint64 ymm15_0 L0x7fffffffbd40;
xor ymm0_1@uint64 ymm15_1 L0x7fffffffbd48;
xor ymm0_2@uint64 ymm15_2 L0x7fffffffbd50;
xor ymm0_3@uint64 ymm15_3 L0x7fffffffbd58;
(* vpxor  -0x50(%rbp),%ymm9,%ymm15                 #! EA = L0x7fffffffbf00; Value = 0xabfe34cd40bb334a; PC = 0x5555555796cd *)
xor ymm15_0@uint64 ymm9_0 L0x7fffffffbf00;
xor ymm15_1@uint64 ymm9_1 L0x7fffffffbf08;
xor ymm15_2@uint64 ymm9_2 L0x7fffffffbf10;
xor ymm15_3@uint64 ymm9_3 L0x7fffffffbf18;
(* vpxor  %ymm0,%ymm10,%ymm10                      #! PC = 0x5555555796d2 *)
xor ymm10_0@uint64 ymm10_0 ymm0_0;
xor ymm10_1@uint64 ymm10_1 ymm0_1;
xor ymm10_2@uint64 ymm10_2 ymm0_2;
xor ymm10_3@uint64 ymm10_3 ymm0_3;
(* vpsrlq $0x17,%ymm4,%ymm0                        #! PC = 0x5555555796d6 *)
shr ymm0_0 ymm4_0 0x17@uint64;
shr ymm0_1 ymm4_1 0x17@uint64;
shr ymm0_2 ymm4_2 0x17@uint64;
shr ymm0_3 ymm4_3 0x17@uint64;
(* vpxor  -0x90(%rbp),%ymm10,%ymm10                #! EA = L0x7fffffffbec0; Value = 0x1a1ad63ea6de94cd; PC = 0x5555555796db *)
xor ymm10_0@uint64 ymm10_0 L0x7fffffffbec0;
xor ymm10_1@uint64 ymm10_1 L0x7fffffffbec8;
xor ymm10_2@uint64 ymm10_2 L0x7fffffffbed0;
xor ymm10_3@uint64 ymm10_3 L0x7fffffffbed8;
(* vpsllq $0x29,%ymm4,%ymm4                        #! PC = 0x5555555796e3 *)
shl ymm4_0 ymm4_0 0x29@uint64;
shl ymm4_1 ymm4_1 0x29@uint64;
shl ymm4_2 ymm4_2 0x29@uint64;
shl ymm4_3 ymm4_3 0x29@uint64;
(* vpor   %ymm4,%ymm0,%ymm4                        #! PC = 0x5555555796e8 *)
or ymm4_0@uint64 ymm0_0 ymm4_0;
or ymm4_1@uint64 ymm0_1 ymm4_1;
or ymm4_2@uint64 ymm0_2 ymm4_2;
or ymm4_3@uint64 ymm0_3 ymm4_3;
(* vpandn %ymm4,%ymm11,%ymm0                       #! PC = 0x5555555796ec *)
not ymm11_0n@uint64 ymm11_0;
and ymm0_0@uint64 ymm11_0n ymm4_0;
not ymm11_1n@uint64 ymm11_1;
and ymm0_1@uint64 ymm11_1n ymm4_1;
not ymm11_2n@uint64 ymm11_2;
and ymm0_2@uint64 ymm11_2n ymm4_2;
not ymm11_3n@uint64 ymm11_3;
and ymm0_3@uint64 ymm11_3n ymm4_3;
(* vpxor  %ymm13,%ymm0,%ymm14                      #! PC = 0x5555555796f0 *)
xor ymm14_0@uint64 ymm0_0 ymm13_0;
xor ymm14_1@uint64 ymm0_1 ymm13_1;
xor ymm14_2@uint64 ymm0_2 ymm13_2;
xor ymm14_3@uint64 ymm0_3 ymm13_3;
(* vpxor  -0x2b0(%rbp),%ymm8,%ymm0                 #! EA = L0x7fffffffbca0; Value = 0x76f7142d14da9bfc; PC = 0x5555555796f5 *)
xor ymm0_0@uint64 ymm8_0 L0x7fffffffbca0;
xor ymm0_1@uint64 ymm8_1 L0x7fffffffbca8;
xor ymm0_2@uint64 ymm8_2 L0x7fffffffbcb0;
xor ymm0_3@uint64 ymm8_3 L0x7fffffffbcb8;
(* vmovdqa %ymm14,%ymm12                           #! PC = 0x5555555796fd *)
mov ymm12_0 ymm14_0;
mov ymm12_1 ymm14_1;
mov ymm12_2 ymm14_2;
mov ymm12_3 ymm14_3;
(* vpxor  -0x1f0(%rbp),%ymm6,%ymm14                #! EA = L0x7fffffffbd60; Value = 0x2b12597baf864e64; PC = 0x555555579702 *)
xor ymm14_0@uint64 ymm6_0 L0x7fffffffbd60;
xor ymm14_1@uint64 ymm6_1 L0x7fffffffbd68;
xor ymm14_2@uint64 ymm6_2 L0x7fffffffbd70;
xor ymm14_3@uint64 ymm6_3 L0x7fffffffbd78;
(* vmovdqa %ymm12,-0x2f0(%rbp)                     #! EA = L0x7fffffffbc60; PC = 0x55555557970a *)
mov L0x7fffffffbc60 ymm12_0;
mov L0x7fffffffbc68 ymm12_1;
mov L0x7fffffffbc70 ymm12_2;
mov L0x7fffffffbc78 ymm12_3;
(* vpxor  %ymm0,%ymm14,%ymm14                      #! PC = 0x555555579712 *)
xor ymm14_0@uint64 ymm14_0 ymm0_0;
xor ymm14_1@uint64 ymm14_1 ymm0_1;
xor ymm14_2@uint64 ymm14_2 ymm0_2;
xor ymm14_3@uint64 ymm14_3 ymm0_3;
(* vpsrlq $0x3e,%ymm1,%ymm0                        #! PC = 0x555555579716 *)
shr ymm0_0 ymm1_0 0x3e@uint64;
shr ymm0_1 ymm1_1 0x3e@uint64;
shr ymm0_2 ymm1_2 0x3e@uint64;
shr ymm0_3 ymm1_3 0x3e@uint64;
(* vpor   %ymm2,%ymm0,%ymm0                        #! PC = 0x55555557971b *)
or ymm0_0@uint64 ymm0_0 ymm2_0;
or ymm0_1@uint64 ymm0_1 ymm2_1;
or ymm0_2@uint64 ymm0_2 ymm2_2;
or ymm0_3@uint64 ymm0_3 ymm2_3;
(* vpxor  %ymm12,%ymm14,%ymm14                     #! PC = 0x55555557971f *)
xor ymm14_0@uint64 ymm14_0 ymm12_0;
xor ymm14_1@uint64 ymm14_1 ymm12_1;
xor ymm14_2@uint64 ymm14_2 ymm12_2;
xor ymm14_3@uint64 ymm14_3 ymm12_3;
(* vmovdqa -0x1d0(%rbp),%ymm12                     #! EA = L0x7fffffffbd80; Value = 0x8b60d78abfae9861; PC = 0x555555579724 *)
mov ymm12_0 L0x7fffffffbd80;
mov ymm12_1 L0x7fffffffbd88;
mov ymm12_2 L0x7fffffffbd90;
mov ymm12_3 L0x7fffffffbd98;
(* vpandn %ymm13,%ymm3,%ymm2                       #! PC = 0x55555557972c *)
not ymm3_0n@uint64 ymm3_0;
and ymm2_0@uint64 ymm3_0n ymm13_0;
not ymm3_1n@uint64 ymm3_1;
and ymm2_1@uint64 ymm3_1n ymm13_1;
not ymm3_2n@uint64 ymm3_2;
and ymm2_2@uint64 ymm3_2n ymm13_2;
not ymm3_3n@uint64 ymm3_3;
and ymm2_3@uint64 ymm3_3n ymm13_3;
(* vpandn %ymm0,%ymm4,%ymm1                        #! PC = 0x555555579731 *)
not ymm4_0n@uint64 ymm4_0;
and ymm1_0@uint64 ymm4_0n ymm0_0;
not ymm4_1n@uint64 ymm4_1;
and ymm1_1@uint64 ymm4_1n ymm0_1;
not ymm4_2n@uint64 ymm4_2;
and ymm1_2@uint64 ymm4_2n ymm0_2;
not ymm4_3n@uint64 ymm4_3;
and ymm1_3@uint64 ymm4_3n ymm0_3;
(* vpxor  %ymm0,%ymm2,%ymm2                        #! PC = 0x555555579735 *)
xor ymm2_0@uint64 ymm2_0 ymm0_0;
xor ymm2_1@uint64 ymm2_1 ymm0_1;
xor ymm2_2@uint64 ymm2_2 ymm0_2;
xor ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpxor  %ymm11,%ymm1,%ymm11                      #! PC = 0x555555579739 *)
xor ymm11_0@uint64 ymm1_0 ymm11_0;
xor ymm11_1@uint64 ymm1_1 ymm11_1;
xor ymm11_2@uint64 ymm1_2 ymm11_2;
xor ymm11_3@uint64 ymm1_3 ymm11_3;
(* vpxor  -0x290(%rbp),%ymm12,%ymm1                #! EA = L0x7fffffffbcc0; Value = 0xa59d6cd06eb2ac2e; PC = 0x55555557973e *)
xor ymm1_0@uint64 ymm12_0 L0x7fffffffbcc0;
xor ymm1_1@uint64 ymm12_1 L0x7fffffffbcc8;
xor ymm1_2@uint64 ymm12_2 L0x7fffffffbcd0;
xor ymm1_3@uint64 ymm12_3 L0x7fffffffbcd8;
(* vpxor  %ymm1,%ymm15,%ymm15                      #! PC = 0x555555579746 *)
xor ymm15_0@uint64 ymm15_0 ymm1_0;
xor ymm15_1@uint64 ymm15_1 ymm1_1;
xor ymm15_2@uint64 ymm15_2 ymm1_2;
xor ymm15_3@uint64 ymm15_3 ymm1_3;
(* vpandn %ymm3,%ymm0,%ymm1                        #! PC = 0x55555557974a *)
not ymm0_0n@uint64 ymm0_0;
and ymm1_0@uint64 ymm0_0n ymm3_0;
not ymm0_1n@uint64 ymm0_1;
and ymm1_1@uint64 ymm0_1n ymm3_1;
not ymm0_2n@uint64 ymm0_2;
and ymm1_2@uint64 ymm0_2n ymm3_2;
not ymm0_3n@uint64 ymm0_3;
and ymm1_3@uint64 ymm0_3n ymm3_3;
(* vpxor  %ymm4,%ymm1,%ymm4                        #! PC = 0x55555557974e *)
xor ymm4_0@uint64 ymm1_0 ymm4_0;
xor ymm4_1@uint64 ymm1_1 ymm4_1;
xor ymm4_2@uint64 ymm1_2 ymm4_2;
xor ymm4_3@uint64 ymm1_3 ymm4_3;
(* vmovdqa -0x250(%rbp),%ymm1                      #! EA = L0x7fffffffbd00; Value = 0x35f1f6fe72e0a82f; PC = 0x555555579752 *)
mov ymm1_0 L0x7fffffffbd00;
mov ymm1_1 L0x7fffffffbd08;
mov ymm1_2 L0x7fffffffbd10;
mov ymm1_3 L0x7fffffffbd18;
(* vpxor  %ymm11,%ymm15,%ymm15                     #! PC = 0x55555557975a *)
xor ymm15_0@uint64 ymm15_0 ymm11_0;
xor ymm15_1@uint64 ymm15_1 ymm11_1;
xor ymm15_2@uint64 ymm15_2 ymm11_2;
xor ymm15_3@uint64 ymm15_3 ymm11_3;
(* vpxor  -0x170(%rbp),%ymm1,%ymm1                 #! EA = L0x7fffffffbde0; Value = 0xe3b85e2e2fccb614; PC = 0x55555557975f *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbde0;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbde8;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbdf0;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbdf8;
(* vpxor  %ymm4,%ymm7,%ymm12                       #! PC = 0x555555579767 *)
xor ymm12_0@uint64 ymm7_0 ymm4_0;
xor ymm12_1@uint64 ymm7_1 ymm4_1;
xor ymm12_2@uint64 ymm7_2 ymm4_2;
xor ymm12_3@uint64 ymm7_3 ymm4_3;
(* vmovdqa %ymm4,-0x310(%rbp)                      #! EA = L0x7fffffffbc40; PC = 0x55555557976b *)
mov L0x7fffffffbc40 ymm4_0;
mov L0x7fffffffbc48 ymm4_1;
mov L0x7fffffffbc50 ymm4_2;
mov L0x7fffffffbc58 ymm4_3;
(* vpsrlq $0x3f,%ymm14,%ymm3                       #! PC = 0x555555579773 *)
shr ymm3_0 ymm14_0 0x3f@uint64;
shr ymm3_1 ymm14_1 0x3f@uint64;
shr ymm3_2 ymm14_2 0x3f@uint64;
shr ymm3_3 ymm14_3 0x3f@uint64;
(* vpxor  %ymm1,%ymm12,%ymm12                      #! PC = 0x555555579779 *)
xor ymm12_0@uint64 ymm12_0 ymm1_0;
xor ymm12_1@uint64 ymm12_1 ymm1_1;
xor ymm12_2@uint64 ymm12_2 ymm1_2;
xor ymm12_3@uint64 ymm12_3 ymm1_3;
(* vpxor  -0x1b0(%rbp),%ymm12,%ymm12               #! EA = L0x7fffffffbda0; Value = 0x6c5d7fd109030bd9; PC = 0x55555557977d *)
xor ymm12_0@uint64 ymm12_0 L0x7fffffffbda0;
xor ymm12_1@uint64 ymm12_1 L0x7fffffffbda8;
xor ymm12_2@uint64 ymm12_2 L0x7fffffffbdb0;
xor ymm12_3@uint64 ymm12_3 L0x7fffffffbdb8;
(* vpxor  -0x230(%rbp),%ymm2,%ymm4                 #! EA = L0x7fffffffbd20; Value = 0x05ec69b93ee6cf97; PC = 0x555555579785 *)
xor ymm4_0@uint64 ymm2_0 L0x7fffffffbd20;
xor ymm4_1@uint64 ymm2_1 L0x7fffffffbd28;
xor ymm4_2@uint64 ymm2_2 L0x7fffffffbd30;
xor ymm4_3@uint64 ymm2_3 L0x7fffffffbd38;
(* vmovdqa -0x150(%rbp),%ymm0                      #! EA = L0x7fffffffbe00; Value = 0x8f4eeff944e0c2d4; PC = 0x55555557978d *)
mov ymm0_0 L0x7fffffffbe00;
mov ymm0_1 L0x7fffffffbe08;
mov ymm0_2 L0x7fffffffbe10;
mov ymm0_3 L0x7fffffffbe18;
(* vpxor  -0x270(%rbp),%ymm0,%ymm0                 #! EA = L0x7fffffffbce0; Value = 0xc36e991c1a383054; PC = 0x555555579795 *)
xor ymm0_0@uint64 ymm0_0 L0x7fffffffbce0;
xor ymm0_1@uint64 ymm0_1 L0x7fffffffbce8;
xor ymm0_2@uint64 ymm0_2 L0x7fffffffbcf0;
xor ymm0_3@uint64 ymm0_3 L0x7fffffffbcf8;
(* vpsrlq $0x3f,%ymm15,%ymm1                       #! PC = 0x55555557979d *)
shr ymm1_0 ymm15_0 0x3f@uint64;
shr ymm1_1 ymm15_1 0x3f@uint64;
shr ymm1_2 ymm15_2 0x3f@uint64;
shr ymm1_3 ymm15_3 0x3f@uint64;
(* vpsllq $0x1,%ymm12,%ymm13                       #! PC = 0x5555555797a3 *)
shl ymm13_0 ymm12_0 0x1@uint64;
shl ymm13_1 ymm12_1 0x1@uint64;
shl ymm13_2 ymm12_2 0x1@uint64;
shl ymm13_3 ymm12_3 0x1@uint64;
(* vpxor  %ymm0,%ymm4,%ymm4                        #! PC = 0x5555555797a9 *)
xor ymm4_0@uint64 ymm4_0 ymm0_0;
xor ymm4_1@uint64 ymm4_1 ymm0_1;
xor ymm4_2@uint64 ymm4_2 ymm0_2;
xor ymm4_3@uint64 ymm4_3 ymm0_3;
(* vpsllq $0x1,%ymm14,%ymm0                        #! PC = 0x5555555797ad *)
shl ymm0_0 ymm14_0 0x1@uint64;
shl ymm0_1 ymm14_1 0x1@uint64;
shl ymm0_2 ymm14_2 0x1@uint64;
shl ymm0_3 ymm14_3 0x1@uint64;
(* vpxor  -0x70(%rbp),%ymm4,%ymm4                  #! EA = L0x7fffffffbee0; Value = 0x58a7ac24ec3ffe99; PC = 0x5555555797b3 *)
xor ymm4_0@uint64 ymm4_0 L0x7fffffffbee0;
xor ymm4_1@uint64 ymm4_1 L0x7fffffffbee8;
xor ymm4_2@uint64 ymm4_2 L0x7fffffffbef0;
xor ymm4_3@uint64 ymm4_3 L0x7fffffffbef8;
(* vpor   %ymm0,%ymm3,%ymm3                        #! PC = 0x5555555797b8 *)
or ymm3_0@uint64 ymm3_0 ymm0_0;
or ymm3_1@uint64 ymm3_1 ymm0_1;
or ymm3_2@uint64 ymm3_2 ymm0_2;
or ymm3_3@uint64 ymm3_3 ymm0_3;
(* vpsllq $0x1,%ymm15,%ymm0                        #! PC = 0x5555555797bc *)
shl ymm0_0 ymm15_0 0x1@uint64;
shl ymm0_1 ymm15_1 0x1@uint64;
shl ymm0_2 ymm15_2 0x1@uint64;
shl ymm0_3 ymm15_3 0x1@uint64;
(* vpor   %ymm0,%ymm1,%ymm1                        #! PC = 0x5555555797c2 *)
or ymm1_0@uint64 ymm1_0 ymm0_0;
or ymm1_1@uint64 ymm1_1 ymm0_1;
or ymm1_2@uint64 ymm1_2 ymm0_2;
or ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x3f,%ymm12,%ymm0                       #! PC = 0x5555555797c6 *)
shr ymm0_0 ymm12_0 0x3f@uint64;
shr ymm0_1 ymm12_1 0x3f@uint64;
shr ymm0_2 ymm12_2 0x3f@uint64;
shr ymm0_3 ymm12_3 0x3f@uint64;
(* vpxor  %ymm4,%ymm3,%ymm3                        #! PC = 0x5555555797cc *)
xor ymm3_0@uint64 ymm3_0 ymm4_0;
xor ymm3_1@uint64 ymm3_1 ymm4_1;
xor ymm3_2@uint64 ymm3_2 ymm4_2;
xor ymm3_3@uint64 ymm3_3 ymm4_3;
(* vpor   %ymm13,%ymm0,%ymm0                       #! PC = 0x5555555797d0 *)
or ymm0_0@uint64 ymm0_0 ymm13_0;
or ymm0_1@uint64 ymm0_1 ymm13_1;
or ymm0_2@uint64 ymm0_2 ymm13_2;
or ymm0_3@uint64 ymm0_3 ymm13_3;
(* vpxor  %ymm10,%ymm1,%ymm1                       #! PC = 0x5555555797d5 *)
xor ymm1_0@uint64 ymm1_0 ymm10_0;
xor ymm1_1@uint64 ymm1_1 ymm10_1;
xor ymm1_2@uint64 ymm1_2 ymm10_2;
xor ymm1_3@uint64 ymm1_3 ymm10_3;
(* vpxor  %ymm3,%ymm5,%ymm5                        #! PC = 0x5555555797da *)
xor ymm5_0@uint64 ymm5_0 ymm3_0;
xor ymm5_1@uint64 ymm5_1 ymm3_1;
xor ymm5_2@uint64 ymm5_2 ymm3_2;
xor ymm5_3@uint64 ymm5_3 ymm3_3;
(* vpxor  %ymm14,%ymm0,%ymm14                      #! PC = 0x5555555797de *)
xor ymm14_0@uint64 ymm0_0 ymm14_0;
xor ymm14_1@uint64 ymm0_1 ymm14_1;
xor ymm14_2@uint64 ymm0_2 ymm14_2;
xor ymm14_3@uint64 ymm0_3 ymm14_3;
(* vpsrlq $0x3f,%ymm4,%ymm0                        #! PC = 0x5555555797e3 *)
shr ymm0_0 ymm4_0 0x3f@uint64;
shr ymm0_1 ymm4_1 0x3f@uint64;
shr ymm0_2 ymm4_2 0x3f@uint64;
shr ymm0_3 ymm4_3 0x3f@uint64;
(* vpxor  %ymm8,%ymm1,%ymm8                        #! PC = 0x5555555797e8 *)
xor ymm8_0@uint64 ymm1_0 ymm8_0;
xor ymm8_1@uint64 ymm1_1 ymm8_1;
xor ymm8_2@uint64 ymm1_2 ymm8_2;
xor ymm8_3@uint64 ymm1_3 ymm8_3;
(* vpsllq $0x1,%ymm4,%ymm4                         #! PC = 0x5555555797ed *)
shl ymm4_0 ymm4_0 0x1@uint64;
shl ymm4_1 ymm4_1 0x1@uint64;
shl ymm4_2 ymm4_2 0x1@uint64;
shl ymm4_3 ymm4_3 0x1@uint64;
(* vpxor  %ymm9,%ymm14,%ymm9                       #! PC = 0x5555555797f2 *)
xor ymm9_0@uint64 ymm14_0 ymm9_0;
xor ymm9_1@uint64 ymm14_1 ymm9_1;
xor ymm9_2@uint64 ymm14_2 ymm9_2;
xor ymm9_3@uint64 ymm14_3 ymm9_3;
(* vpxor  %ymm1,%ymm6,%ymm6                        #! PC = 0x5555555797f7 *)
xor ymm6_0@uint64 ymm6_0 ymm1_0;
xor ymm6_1@uint64 ymm6_1 ymm1_1;
xor ymm6_2@uint64 ymm6_2 ymm1_2;
xor ymm6_3@uint64 ymm6_3 ymm1_3;
(* vpor   %ymm4,%ymm0,%ymm4                        #! PC = 0x5555555797fb *)
or ymm4_0@uint64 ymm0_0 ymm4_0;
or ymm4_1@uint64 ymm0_1 ymm4_1;
or ymm4_2@uint64 ymm0_2 ymm4_2;
or ymm4_3@uint64 ymm0_3 ymm4_3;
(* vpsrlq $0x3f,%ymm10,%ymm0                       #! PC = 0x5555555797ff *)
shr ymm0_0 ymm10_0 0x3f@uint64;
shr ymm0_1 ymm10_1 0x3f@uint64;
shr ymm0_2 ymm10_2 0x3f@uint64;
shr ymm0_3 ymm10_3 0x3f@uint64;
(* vpxor  %ymm14,%ymm11,%ymm11                     #! PC = 0x555555579805 *)
xor ymm11_0@uint64 ymm11_0 ymm14_0;
xor ymm11_1@uint64 ymm11_1 ymm14_1;
xor ymm11_2@uint64 ymm11_2 ymm14_2;
xor ymm11_3@uint64 ymm11_3 ymm14_3;
(* vpxor  %ymm15,%ymm4,%ymm15                      #! PC = 0x55555557980a *)
xor ymm15_0@uint64 ymm4_0 ymm15_0;
xor ymm15_1@uint64 ymm4_1 ymm15_1;
xor ymm15_2@uint64 ymm4_2 ymm15_2;
xor ymm15_3@uint64 ymm4_3 ymm15_3;
(* vpsllq $0x1,%ymm10,%ymm10                       #! PC = 0x55555557980f *)
shl ymm10_0 ymm10_0 0x1@uint64;
shl ymm10_1 ymm10_1 0x1@uint64;
shl ymm10_2 ymm10_2 0x1@uint64;
shl ymm10_3 ymm10_3 0x1@uint64;
(* vpsrlq $0x14,%ymm8,%ymm4                        #! PC = 0x555555579815 *)
shr ymm4_0 ymm8_0 0x14@uint64;
shr ymm4_1 ymm8_1 0x14@uint64;
shr ymm4_2 ymm8_2 0x14@uint64;
shr ymm4_3 ymm8_3 0x14@uint64;
(* vpsllq $0x2c,%ymm8,%ymm8                        #! PC = 0x55555557981b *)
shl ymm8_0 ymm8_0 0x2c@uint64;
shl ymm8_1 ymm8_1 0x2c@uint64;
shl ymm8_2 ymm8_2 0x2c@uint64;
shl ymm8_3 ymm8_3 0x2c@uint64;
(* vpor   %ymm10,%ymm0,%ymm10                      #! PC = 0x555555579821 *)
or ymm10_0@uint64 ymm0_0 ymm10_0;
or ymm10_1@uint64 ymm0_1 ymm10_1;
or ymm10_2@uint64 ymm0_2 ymm10_2;
or ymm10_3@uint64 ymm0_3 ymm10_3;
(* vpor   %ymm8,%ymm4,%ymm8                        #! PC = 0x555555579826 *)
or ymm8_0@uint64 ymm4_0 ymm8_0;
or ymm8_1@uint64 ymm4_1 ymm8_1;
or ymm8_2@uint64 ymm4_2 ymm8_2;
or ymm8_3@uint64 ymm4_3 ymm8_3;
(* vpxor  %ymm12,%ymm10,%ymm12                     #! PC = 0x55555557982b *)
xor ymm12_0@uint64 ymm10_0 ymm12_0;
xor ymm12_1@uint64 ymm10_1 ymm12_1;
xor ymm12_2@uint64 ymm10_2 ymm12_2;
xor ymm12_3@uint64 ymm10_3 ymm12_3;
(* vmovq  %rbx,%xmm10                              #! PC = 0x555555579830 *)
mov xmm10_0 rbx;
mov xmm10_1 0@uint64;
(* vpsrlq $0x15,%ymm9,%ymm4                        #! PC = 0x555555579835 *)
shr ymm4_0 ymm9_0 0x15@uint64;
shr ymm4_1 ymm9_1 0x15@uint64;
shr ymm4_2 ymm9_2 0x15@uint64;
shr ymm4_3 ymm9_3 0x15@uint64;
(* vpsllq $0x2b,%ymm9,%ymm9                        #! PC = 0x55555557983b *)
shl ymm9_0 ymm9_0 0x2b@uint64;
shl ymm9_1 ymm9_1 0x2b@uint64;
shl ymm9_2 ymm9_2 0x2b@uint64;
shl ymm9_3 ymm9_3 0x2b@uint64;
(* vpxor  %ymm15,%ymm7,%ymm7                       #! PC = 0x555555579841 *)
xor ymm7_0@uint64 ymm7_0 ymm15_0;
xor ymm7_1@uint64 ymm7_1 ymm15_1;
xor ymm7_2@uint64 ymm7_2 ymm15_2;
xor ymm7_3@uint64 ymm7_3 ymm15_3;
(* vpor   %ymm9,%ymm4,%ymm9                        #! PC = 0x555555579846 *)
or ymm9_0@uint64 ymm4_0 ymm9_0;
or ymm9_1@uint64 ymm4_1 ymm9_1;
or ymm9_2@uint64 ymm4_2 ymm9_2;
or ymm9_3@uint64 ymm4_3 ymm9_3;
(* vpbroadcastq %xmm10,%ymm10                      #! PC = 0x55555557984b *)
mov ymm10_0 xmm10_0;
mov ymm10_1 xmm10_0;
mov ymm10_2 xmm10_0;
mov ymm10_3 xmm10_0;
(* vpxor  %ymm12,%ymm2,%ymm2                       #! PC = 0x555555579850 *)
xor ymm2_0@uint64 ymm2_0 ymm12_0;
xor ymm2_1@uint64 ymm2_1 ymm12_1;
xor ymm2_2@uint64 ymm2_2 ymm12_2;
xor ymm2_3@uint64 ymm2_3 ymm12_3;
(* vpandn %ymm9,%ymm8,%ymm4                        #! PC = 0x555555579855 *)
not ymm8_0n@uint64 ymm8_0;
and ymm4_0@uint64 ymm8_0n ymm9_0;
not ymm8_1n@uint64 ymm8_1;
and ymm4_1@uint64 ymm8_1n ymm9_1;
not ymm8_2n@uint64 ymm8_2;
and ymm4_2@uint64 ymm8_2n ymm9_2;
not ymm8_3n@uint64 ymm8_3;
and ymm4_3@uint64 ymm8_3n ymm9_3;
(* vpxor  -0x90(%rbp),%ymm3,%ymm0                  #! EA = L0x7fffffffbec0; Value = 0x1a1ad63ea6de94cd; PC = 0x55555557985a *)
xor ymm0_0@uint64 ymm3_0 L0x7fffffffbec0;
xor ymm0_1@uint64 ymm3_1 L0x7fffffffbec8;
xor ymm0_2@uint64 ymm3_2 L0x7fffffffbed0;
xor ymm0_3@uint64 ymm3_3 L0x7fffffffbed8;
(* vpxor  %ymm10,%ymm4,%ymm10                      #! PC = 0x555555579862 *)
xor ymm10_0@uint64 ymm4_0 ymm10_0;
xor ymm10_1@uint64 ymm4_1 ymm10_1;
xor ymm10_2@uint64 ymm4_2 ymm10_2;
xor ymm10_3@uint64 ymm4_3 ymm10_3;
(* vpsrlq $0x2b,%ymm7,%ymm4                        #! PC = 0x555555579867 *)
shr ymm4_0 ymm7_0 0x2b@uint64;
shr ymm4_1 ymm7_1 0x2b@uint64;
shr ymm4_2 ymm7_2 0x2b@uint64;
shr ymm4_3 ymm7_3 0x2b@uint64;
(* vpsllq $0x15,%ymm7,%ymm7                        #! PC = 0x55555557986c *)
shl ymm7_0 ymm7_0 0x15@uint64;
shl ymm7_1 ymm7_1 0x15@uint64;
shl ymm7_2 ymm7_2 0x15@uint64;
shl ymm7_3 ymm7_3 0x15@uint64;
(* vpxor  %ymm0,%ymm10,%ymm10                      #! PC = 0x555555579871 *)
xor ymm10_0@uint64 ymm10_0 ymm0_0;
xor ymm10_1@uint64 ymm10_1 ymm0_1;
xor ymm10_2@uint64 ymm10_2 ymm0_2;
xor ymm10_3@uint64 ymm10_3 ymm0_3;
(* vpor   %ymm7,%ymm4,%ymm7                        #! PC = 0x555555579875 *)
or ymm7_0@uint64 ymm4_0 ymm7_0;
or ymm7_1@uint64 ymm4_1 ymm7_1;
or ymm7_2@uint64 ymm4_2 ymm7_2;
or ymm7_3@uint64 ymm4_3 ymm7_3;
(* vmovdqa %ymm10,-0x90(%rbp)                      #! EA = L0x7fffffffbec0; PC = 0x555555579879 *)
mov L0x7fffffffbec0 ymm10_0;
mov L0x7fffffffbec8 ymm10_1;
mov L0x7fffffffbed0 ymm10_2;
mov L0x7fffffffbed8 ymm10_3;
(* vpandn %ymm7,%ymm9,%ymm4                        #! PC = 0x555555579881 *)
not ymm9_0n@uint64 ymm9_0;
and ymm4_0@uint64 ymm9_0n ymm7_0;
not ymm9_1n@uint64 ymm9_1;
and ymm4_1@uint64 ymm9_1n ymm7_1;
not ymm9_2n@uint64 ymm9_2;
and ymm4_2@uint64 ymm9_2n ymm7_2;
not ymm9_3n@uint64 ymm9_3;
and ymm4_3@uint64 ymm9_3n ymm7_3;
(* vpxor  %ymm8,%ymm4,%ymm13                       #! PC = 0x555555579885 *)
xor ymm13_0@uint64 ymm4_0 ymm8_0;
xor ymm13_1@uint64 ymm4_1 ymm8_1;
xor ymm13_2@uint64 ymm4_2 ymm8_2;
xor ymm13_3@uint64 ymm4_3 ymm8_3;
(* vpsrlq $0x32,%ymm2,%ymm4                        #! PC = 0x55555557988a *)
shr ymm4_0 ymm2_0 0x32@uint64;
shr ymm4_1 ymm2_1 0x32@uint64;
shr ymm4_2 ymm2_2 0x32@uint64;
shr ymm4_3 ymm2_3 0x32@uint64;
(* vpsllq $0xe,%ymm2,%ymm2                         #! PC = 0x55555557988f *)
shl ymm2_0 ymm2_0 0xe@uint64;
shl ymm2_1 ymm2_1 0xe@uint64;
shl ymm2_2 ymm2_2 0xe@uint64;
shl ymm2_3 ymm2_3 0xe@uint64;
(* vmovdqa %ymm13,-0xb0(%rbp)                      #! EA = L0x7fffffffbea0; PC = 0x555555579894 *)
mov L0x7fffffffbea0 ymm13_0;
mov L0x7fffffffbea8 ymm13_1;
mov L0x7fffffffbeb0 ymm13_2;
mov L0x7fffffffbeb8 ymm13_3;
(* vpsrlq $0x13,%ymm6,%ymm13                       #! PC = 0x55555557989c *)
shr ymm13_0 ymm6_0 0x13@uint64;
shr ymm13_1 ymm6_1 0x13@uint64;
shr ymm13_2 ymm6_2 0x13@uint64;
shr ymm13_3 ymm6_3 0x13@uint64;
(* vpor   %ymm2,%ymm4,%ymm2                        #! PC = 0x5555555798a1 *)
or ymm2_0@uint64 ymm4_0 ymm2_0;
or ymm2_1@uint64 ymm4_1 ymm2_1;
or ymm2_2@uint64 ymm4_2 ymm2_2;
or ymm2_3@uint64 ymm4_3 ymm2_3;
(* vpsllq $0x2d,%ymm6,%ymm6                        #! PC = 0x5555555798a5 *)
shl ymm6_0 ymm6_0 0x2d@uint64;
shl ymm6_1 ymm6_1 0x2d@uint64;
shl ymm6_2 ymm6_2 0x2d@uint64;
shl ymm6_3 ymm6_3 0x2d@uint64;
(* vpandn %ymm2,%ymm7,%ymm4                        #! PC = 0x5555555798aa *)
not ymm7_0n@uint64 ymm7_0;
and ymm4_0@uint64 ymm7_0n ymm2_0;
not ymm7_1n@uint64 ymm7_1;
and ymm4_1@uint64 ymm7_1n ymm2_1;
not ymm7_2n@uint64 ymm7_2;
and ymm4_2@uint64 ymm7_2n ymm2_2;
not ymm7_3n@uint64 ymm7_3;
and ymm4_3@uint64 ymm7_3n ymm2_3;
(* vpor   %ymm6,%ymm13,%ymm13                      #! PC = 0x5555555798ae *)
or ymm13_0@uint64 ymm13_0 ymm6_0;
or ymm13_1@uint64 ymm13_1 ymm6_1;
or ymm13_2@uint64 ymm13_2 ymm6_2;
or ymm13_3@uint64 ymm13_3 ymm6_3;
(* vpxor  %ymm9,%ymm4,%ymm9                        #! PC = 0x5555555798b2 *)
xor ymm9_0@uint64 ymm4_0 ymm9_0;
xor ymm9_1@uint64 ymm4_1 ymm9_1;
xor ymm9_2@uint64 ymm4_2 ymm9_2;
xor ymm9_3@uint64 ymm4_3 ymm9_3;
(* vpandn %ymm0,%ymm2,%ymm4                        #! PC = 0x5555555798b7 *)
not ymm2_0n@uint64 ymm2_0;
and ymm4_0@uint64 ymm2_0n ymm0_0;
not ymm2_1n@uint64 ymm2_1;
and ymm4_1@uint64 ymm2_1n ymm0_1;
not ymm2_2n@uint64 ymm2_2;
and ymm4_2@uint64 ymm2_2n ymm0_2;
not ymm2_3n@uint64 ymm2_3;
and ymm4_3@uint64 ymm2_3n ymm0_3;
(* vpandn %ymm8,%ymm0,%ymm0                        #! PC = 0x5555555798bb *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm8_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm8_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm8_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm8_3;
(* vpxor  %ymm2,%ymm0,%ymm8                        #! PC = 0x5555555798c0 *)
xor ymm8_0@uint64 ymm0_0 ymm2_0;
xor ymm8_1@uint64 ymm0_1 ymm2_1;
xor ymm8_2@uint64 ymm0_2 ymm2_2;
xor ymm8_3@uint64 ymm0_3 ymm2_3;
(* vpxor  -0x170(%rbp),%ymm15,%ymm0                #! EA = L0x7fffffffbde0; Value = 0xe3b85e2e2fccb614; PC = 0x5555555798c4 *)
xor ymm0_0@uint64 ymm15_0 L0x7fffffffbde0;
xor ymm0_1@uint64 ymm15_1 L0x7fffffffbde8;
xor ymm0_2@uint64 ymm15_2 L0x7fffffffbdf0;
xor ymm0_3@uint64 ymm15_3 L0x7fffffffbdf8;
(* vpxor  %ymm7,%ymm4,%ymm7                        #! PC = 0x5555555798cc *)
xor ymm7_0@uint64 ymm4_0 ymm7_0;
xor ymm7_1@uint64 ymm4_1 ymm7_1;
xor ymm7_2@uint64 ymm4_2 ymm7_2;
xor ymm7_3@uint64 ymm4_3 ymm7_3;
(* vmovdqa %ymm9,-0xd0(%rbp)                       #! EA = L0x7fffffffbe80; PC = 0x5555555798d0 *)
mov L0x7fffffffbe80 ymm9_0;
mov L0x7fffffffbe88 ymm9_1;
mov L0x7fffffffbe90 ymm9_2;
mov L0x7fffffffbe98 ymm9_3;
(* vpxor  -0x150(%rbp),%ymm12,%ymm2                #! EA = L0x7fffffffbe00; Value = 0x8f4eeff944e0c2d4; PC = 0x5555555798d8 *)
xor ymm2_0@uint64 ymm12_0 L0x7fffffffbe00;
xor ymm2_1@uint64 ymm12_1 L0x7fffffffbe08;
xor ymm2_2@uint64 ymm12_2 L0x7fffffffbe10;
xor ymm2_3@uint64 ymm12_3 L0x7fffffffbe18;
(* vpxor  -0x130(%rbp),%ymm3,%ymm4                 #! EA = L0x7fffffffbe20; Value = 0x632b5576588a026d; PC = 0x5555555798e0 *)
xor ymm4_0@uint64 ymm3_0 L0x7fffffffbe20;
xor ymm4_1@uint64 ymm3_1 L0x7fffffffbe28;
xor ymm4_2@uint64 ymm3_2 L0x7fffffffbe30;
xor ymm4_3@uint64 ymm3_3 L0x7fffffffbe38;
(* vmovdqa %ymm7,-0xf0(%rbp)                       #! EA = L0x7fffffffbe60; PC = 0x5555555798e8 *)
mov L0x7fffffffbe60 ymm7_0;
mov L0x7fffffffbe68 ymm7_1;
mov L0x7fffffffbe70 ymm7_2;
mov L0x7fffffffbe78 ymm7_3;
(* vpsrlq $0x24,%ymm0,%ymm10                       #! PC = 0x5555555798f0 *)
shr ymm10_0 ymm0_0 0x24@uint64;
shr ymm10_1 ymm0_1 0x24@uint64;
shr ymm10_2 ymm0_2 0x24@uint64;
shr ymm10_3 ymm0_3 0x24@uint64;
(* vpsllq $0x1c,%ymm0,%ymm0                        #! PC = 0x5555555798f5 *)
shl ymm0_0 ymm0_0 0x1c@uint64;
shl ymm0_1 ymm0_1 0x1c@uint64;
shl ymm0_2 ymm0_2 0x1c@uint64;
shl ymm0_3 ymm0_3 0x1c@uint64;
(* vmovdqa %ymm8,-0x110(%rbp)                      #! EA = L0x7fffffffbe40; PC = 0x5555555798fa *)
mov L0x7fffffffbe40 ymm8_0;
mov L0x7fffffffbe48 ymm8_1;
mov L0x7fffffffbe50 ymm8_2;
mov L0x7fffffffbe58 ymm8_3;
(* vpor   %ymm0,%ymm10,%ymm10                      #! PC = 0x555555579902 *)
or ymm10_0@uint64 ymm10_0 ymm0_0;
or ymm10_1@uint64 ymm10_1 ymm0_1;
or ymm10_2@uint64 ymm10_2 ymm0_2;
or ymm10_3@uint64 ymm10_3 ymm0_3;
(* vpsrlq $0x2c,%ymm2,%ymm0                        #! PC = 0x555555579906 *)
shr ymm0_0 ymm2_0 0x2c@uint64;
shr ymm0_1 ymm2_1 0x2c@uint64;
shr ymm0_2 ymm2_2 0x2c@uint64;
shr ymm0_3 ymm2_3 0x2c@uint64;
(* vpsllq $0x14,%ymm2,%ymm2                        #! PC = 0x55555557990b *)
shl ymm2_0 ymm2_0 0x14@uint64;
shl ymm2_1 ymm2_1 0x14@uint64;
shl ymm2_2 ymm2_2 0x14@uint64;
shl ymm2_3 ymm2_3 0x14@uint64;
(* vpor   %ymm2,%ymm0,%ymm0                        #! PC = 0x555555579910 *)
or ymm0_0@uint64 ymm0_0 ymm2_0;
or ymm0_1@uint64 ymm0_1 ymm2_1;
or ymm0_2@uint64 ymm0_2 ymm2_2;
or ymm0_3@uint64 ymm0_3 ymm2_3;
(* vpsrlq $0x3d,%ymm4,%ymm2                        #! PC = 0x555555579914 *)
shr ymm2_0 ymm4_0 0x3d@uint64;
shr ymm2_1 ymm4_1 0x3d@uint64;
shr ymm2_2 ymm4_2 0x3d@uint64;
shr ymm2_3 ymm4_3 0x3d@uint64;
(* vpsllq $0x3,%ymm4,%ymm4                         #! PC = 0x555555579919 *)
shl ymm4_0 ymm4_0 0x3@uint64;
shl ymm4_1 ymm4_1 0x3@uint64;
shl ymm4_2 ymm4_2 0x3@uint64;
shl ymm4_3 ymm4_3 0x3@uint64;
(* vpor   %ymm4,%ymm2,%ymm2                        #! PC = 0x55555557991e *)
or ymm2_0@uint64 ymm2_0 ymm4_0;
or ymm2_1@uint64 ymm2_1 ymm4_1;
or ymm2_2@uint64 ymm2_2 ymm4_2;
or ymm2_3@uint64 ymm2_3 ymm4_3;
(* vpsrlq $0x3,%ymm11,%ymm4                        #! PC = 0x555555579922 *)
shr ymm4_0 ymm11_0 0x3@uint64;
shr ymm4_1 ymm11_1 0x3@uint64;
shr ymm4_2 ymm11_2 0x3@uint64;
shr ymm4_3 ymm11_3 0x3@uint64;
(* vpsllq $0x3d,%ymm11,%ymm11                      #! PC = 0x555555579928 *)
shl ymm11_0 ymm11_0 0x3d@uint64;
shl ymm11_1 ymm11_1 0x3d@uint64;
shl ymm11_2 ymm11_2 0x3d@uint64;
shl ymm11_3 ymm11_3 0x3d@uint64;
(* vpandn %ymm2,%ymm0,%ymm9                        #! PC = 0x55555557992e *)
not ymm0_0n@uint64 ymm0_0;
and ymm9_0@uint64 ymm0_0n ymm2_0;
not ymm0_1n@uint64 ymm0_1;
and ymm9_1@uint64 ymm0_1n ymm2_1;
not ymm0_2n@uint64 ymm0_2;
and ymm9_2@uint64 ymm0_2n ymm2_2;
not ymm0_3n@uint64 ymm0_3;
and ymm9_3@uint64 ymm0_3n ymm2_3;
(* vpor   %ymm11,%ymm4,%ymm11                      #! PC = 0x555555579932 *)
or ymm11_0@uint64 ymm4_0 ymm11_0;
or ymm11_1@uint64 ymm4_1 ymm11_1;
or ymm11_2@uint64 ymm4_2 ymm11_2;
or ymm11_3@uint64 ymm4_3 ymm11_3;
(* vpxor  %ymm10,%ymm9,%ymm7                       #! PC = 0x555555579937 *)
xor ymm7_0@uint64 ymm9_0 ymm10_0;
xor ymm7_1@uint64 ymm9_1 ymm10_1;
xor ymm7_2@uint64 ymm9_2 ymm10_2;
xor ymm7_3@uint64 ymm9_3 ymm10_3;
(* vpandn %ymm11,%ymm13,%ymm4                      #! PC = 0x55555557993c *)
not ymm13_0n@uint64 ymm13_0;
and ymm4_0@uint64 ymm13_0n ymm11_0;
not ymm13_1n@uint64 ymm13_1;
and ymm4_1@uint64 ymm13_1n ymm11_1;
not ymm13_2n@uint64 ymm13_2;
and ymm4_2@uint64 ymm13_2n ymm11_2;
not ymm13_3n@uint64 ymm13_3;
and ymm4_3@uint64 ymm13_3n ymm11_3;
(* vmovdqa %ymm7,-0x130(%rbp)                      #! EA = L0x7fffffffbe20; PC = 0x555555579941 *)
mov L0x7fffffffbe20 ymm7_0;
mov L0x7fffffffbe28 ymm7_1;
mov L0x7fffffffbe30 ymm7_2;
mov L0x7fffffffbe38 ymm7_3;
(* vpandn %ymm13,%ymm2,%ymm7                       #! PC = 0x555555579949 *)
not ymm2_0n@uint64 ymm2_0;
and ymm7_0@uint64 ymm2_0n ymm13_0;
not ymm2_1n@uint64 ymm2_1;
and ymm7_1@uint64 ymm2_1n ymm13_1;
not ymm2_2n@uint64 ymm2_2;
and ymm7_2@uint64 ymm2_2n ymm13_2;
not ymm2_3n@uint64 ymm2_3;
and ymm7_3@uint64 ymm2_3n ymm13_3;
(* vpxor  %ymm2,%ymm4,%ymm8                        #! PC = 0x55555557994e *)
xor ymm8_0@uint64 ymm4_0 ymm2_0;
xor ymm8_1@uint64 ymm4_1 ymm2_1;
xor ymm8_2@uint64 ymm4_2 ymm2_2;
xor ymm8_3@uint64 ymm4_3 ymm2_3;
(* vpandn %ymm10,%ymm11,%ymm2                      #! PC = 0x555555579952 *)
not ymm11_0n@uint64 ymm11_0;
and ymm2_0@uint64 ymm11_0n ymm10_0;
not ymm11_1n@uint64 ymm11_1;
and ymm2_1@uint64 ymm11_1n ymm10_1;
not ymm11_2n@uint64 ymm11_2;
and ymm2_2@uint64 ymm11_2n ymm10_2;
not ymm11_3n@uint64 ymm11_3;
and ymm2_3@uint64 ymm11_3n ymm10_3;
(* vpxor  -0x1d0(%rbp),%ymm14,%ymm4                #! EA = L0x7fffffffbd80; Value = 0x8b60d78abfae9861; PC = 0x555555579957 *)
xor ymm4_0@uint64 ymm14_0 L0x7fffffffbd80;
xor ymm4_1@uint64 ymm14_1 L0x7fffffffbd88;
xor ymm4_2@uint64 ymm14_2 L0x7fffffffbd90;
xor ymm4_3@uint64 ymm14_3 L0x7fffffffbd98;
(* vpxor  %ymm13,%ymm2,%ymm9                       #! PC = 0x55555557995f *)
xor ymm9_0@uint64 ymm2_0 ymm13_0;
xor ymm9_1@uint64 ymm2_1 ymm13_1;
xor ymm9_2@uint64 ymm2_2 ymm13_2;
xor ymm9_3@uint64 ymm2_3 ymm13_3;
(* vpxor  %ymm0,%ymm7,%ymm7                        #! PC = 0x555555579964 *)
xor ymm7_0@uint64 ymm7_0 ymm0_0;
xor ymm7_1@uint64 ymm7_1 ymm0_1;
xor ymm7_2@uint64 ymm7_2 ymm0_2;
xor ymm7_3@uint64 ymm7_3 ymm0_3;
(* vpxor  -0x2b0(%rbp),%ymm1,%ymm2                 #! EA = L0x7fffffffbca0; Value = 0x76f7142d14da9bfc; PC = 0x555555579968 *)
xor ymm2_0@uint64 ymm1_0 L0x7fffffffbca0;
xor ymm2_1@uint64 ymm1_1 L0x7fffffffbca8;
xor ymm2_2@uint64 ymm1_2 L0x7fffffffbcb0;
xor ymm2_3@uint64 ymm1_3 L0x7fffffffbcb8;
(* vmovdqa %ymm8,-0x150(%rbp)                      #! EA = L0x7fffffffbe00; PC = 0x555555579970 *)
mov L0x7fffffffbe00 ymm8_0;
mov L0x7fffffffbe08 ymm8_1;
mov L0x7fffffffbe10 ymm8_2;
mov L0x7fffffffbe18 ymm8_3;
(* vpandn %ymm0,%ymm10,%ymm10                      #! PC = 0x555555579978 *)
not ymm10_0n@uint64 ymm10_0;
and ymm10_0@uint64 ymm10_0n ymm0_0;
not ymm10_1n@uint64 ymm10_1;
and ymm10_1@uint64 ymm10_1n ymm0_1;
not ymm10_2n@uint64 ymm10_2;
and ymm10_2@uint64 ymm10_2n ymm0_2;
not ymm10_3n@uint64 ymm10_3;
and ymm10_3@uint64 ymm10_3n ymm0_3;
(* vmovdqa %ymm9,-0x170(%rbp)                      #! EA = L0x7fffffffbde0; PC = 0x55555557997c *)
mov L0x7fffffffbde0 ymm9_0;
mov L0x7fffffffbde8 ymm9_1;
mov L0x7fffffffbdf0 ymm9_2;
mov L0x7fffffffbdf8 ymm9_3;
(* vpsrlq $0x3f,%ymm2,%ymm0                        #! PC = 0x555555579984 *)
shr ymm0_0 ymm2_0 0x3f@uint64;
shr ymm0_1 ymm2_1 0x3f@uint64;
shr ymm0_2 ymm2_2 0x3f@uint64;
shr ymm0_3 ymm2_3 0x3f@uint64;
(* vpsllq $0x1,%ymm2,%ymm2                         #! PC = 0x555555579989 *)
shl ymm2_0 ymm2_0 0x1@uint64;
shl ymm2_1 ymm2_1 0x1@uint64;
shl ymm2_2 ymm2_2 0x1@uint64;
shl ymm2_3 ymm2_3 0x1@uint64;
(* vpxor  %ymm11,%ymm10,%ymm11                     #! PC = 0x55555557998e *)
xor ymm11_0@uint64 ymm10_0 ymm11_0;
xor ymm11_1@uint64 ymm10_1 ymm11_1;
xor ymm11_2@uint64 ymm10_2 ymm11_2;
xor ymm11_3@uint64 ymm10_3 ymm11_3;
(* vpor   %ymm2,%ymm0,%ymm0                        #! PC = 0x555555579993 *)
or ymm0_0@uint64 ymm0_0 ymm2_0;
or ymm0_1@uint64 ymm0_1 ymm2_1;
or ymm0_2@uint64 ymm0_2 ymm2_2;
or ymm0_3@uint64 ymm0_3 ymm2_3;
(* vpsrlq $0x3a,%ymm4,%ymm2                        #! PC = 0x555555579997 *)
shr ymm2_0 ymm4_0 0x3a@uint64;
shr ymm2_1 ymm4_1 0x3a@uint64;
shr ymm2_2 ymm4_2 0x3a@uint64;
shr ymm2_3 ymm4_3 0x3a@uint64;
(* vmovdqa %ymm11,-0x190(%rbp)                     #! EA = L0x7fffffffbdc0; PC = 0x55555557999c *)
mov L0x7fffffffbdc0 ymm11_0;
mov L0x7fffffffbdc8 ymm11_1;
mov L0x7fffffffbdd0 ymm11_2;
mov L0x7fffffffbdd8 ymm11_3;
(* vpsllq $0x6,%ymm4,%ymm4                         #! PC = 0x5555555799a4 *)
shl ymm4_0 ymm4_0 0x6@uint64;
shl ymm4_1 ymm4_1 0x6@uint64;
shl ymm4_2 ymm4_2 0x6@uint64;
shl ymm4_3 ymm4_3 0x6@uint64;
(* vpor   %ymm4,%ymm2,%ymm2                        #! PC = 0x5555555799a9 *)
or ymm2_0@uint64 ymm2_0 ymm4_0;
or ymm2_1@uint64 ymm2_1 ymm4_1;
or ymm2_2@uint64 ymm2_2 ymm4_2;
or ymm2_3@uint64 ymm2_3 ymm4_3;
(* vpxor  -0x1b0(%rbp),%ymm15,%ymm4                #! EA = L0x7fffffffbda0; Value = 0x6c5d7fd109030bd9; PC = 0x5555555799ad *)
xor ymm4_0@uint64 ymm15_0 L0x7fffffffbda0;
xor ymm4_1@uint64 ymm15_1 L0x7fffffffbda8;
xor ymm4_2@uint64 ymm15_2 L0x7fffffffbdb0;
xor ymm4_3@uint64 ymm15_3 L0x7fffffffbdb8;
(* vpsrlq $0x27,%ymm4,%ymm8                        #! PC = 0x5555555799b5 *)
shr ymm8_0 ymm4_0 0x27@uint64;
shr ymm8_1 ymm4_1 0x27@uint64;
shr ymm8_2 ymm4_2 0x27@uint64;
shr ymm8_3 ymm4_3 0x27@uint64;
(* vpsllq $0x19,%ymm4,%ymm6                        #! PC = 0x5555555799ba *)
shl ymm6_0 ymm4_0 0x19@uint64;
shl ymm6_1 ymm4_1 0x19@uint64;
shl ymm6_2 ymm4_2 0x19@uint64;
shl ymm6_3 ymm4_3 0x19@uint64;
(* vpor   %ymm6,%ymm8,%ymm6                        #! PC = 0x5555555799bf *)
or ymm6_0@uint64 ymm8_0 ymm6_0;
or ymm6_1@uint64 ymm8_1 ymm6_1;
or ymm6_2@uint64 ymm8_2 ymm6_2;
or ymm6_3@uint64 ymm8_3 ymm6_3;
(* vpandn %ymm6,%ymm2,%ymm4                        #! PC = 0x5555555799c3 *)
not ymm2_0n@uint64 ymm2_0;
and ymm4_0@uint64 ymm2_0n ymm6_0;
not ymm2_1n@uint64 ymm2_1;
and ymm4_1@uint64 ymm2_1n ymm6_1;
not ymm2_2n@uint64 ymm2_2;
and ymm4_2@uint64 ymm2_2n ymm6_2;
not ymm2_3n@uint64 ymm2_3;
and ymm4_3@uint64 ymm2_3n ymm6_3;
(* vpxor  %ymm0,%ymm4,%ymm11                       #! PC = 0x5555555799c7 *)
xor ymm11_0@uint64 ymm4_0 ymm0_0;
xor ymm11_1@uint64 ymm4_1 ymm0_1;
xor ymm11_2@uint64 ymm4_2 ymm0_2;
xor ymm11_3@uint64 ymm4_3 ymm0_3;
(* vpxor  -0x70(%rbp),%ymm12,%ymm4                 #! EA = L0x7fffffffbee0; Value = 0x58a7ac24ec3ffe99; PC = 0x5555555799cb *)
xor ymm4_0@uint64 ymm12_0 L0x7fffffffbee0;
xor ymm4_1@uint64 ymm12_1 L0x7fffffffbee8;
xor ymm4_2@uint64 ymm12_2 L0x7fffffffbef0;
xor ymm4_3@uint64 ymm12_3 L0x7fffffffbef8;
(* vmovdqa %ymm11,%ymm13                           #! PC = 0x5555555799d0 *)
mov ymm13_0 ymm11_0;
mov ymm13_1 ymm11_1;
mov ymm13_2 ymm11_2;
mov ymm13_3 ymm11_3;
(* vpsrlq $0x2e,%ymm5,%ymm11                       #! PC = 0x5555555799d5 *)
shr ymm11_0 ymm5_0 0x2e@uint64;
shr ymm11_1 ymm5_1 0x2e@uint64;
shr ymm11_2 ymm5_2 0x2e@uint64;
shr ymm11_3 ymm5_3 0x2e@uint64;
(* vpshufb 0x5451d(%rip),%ymm4,%ymm4        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x5555555799da *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm4_0, ymm4_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm4_2, ymm4_3, tmp_2, tmp_3);
mov ymm4_0 tmp_0;
mov ymm4_1 tmp_1;
mov ymm4_2 tmp_2;
mov ymm4_3 tmp_3;
(* vpsllq $0x12,%ymm5,%ymm5                        #! PC = 0x5555555799e3 *)
shl ymm5_0 ymm5_0 0x12@uint64;
shl ymm5_1 ymm5_1 0x12@uint64;
shl ymm5_2 ymm5_2 0x12@uint64;
shl ymm5_3 ymm5_3 0x12@uint64;
(* vpandn %ymm4,%ymm6,%ymm10                       #! PC = 0x5555555799e8 *)
not ymm6_0n@uint64 ymm6_0;
and ymm10_0@uint64 ymm6_0n ymm4_0;
not ymm6_1n@uint64 ymm6_1;
and ymm10_1@uint64 ymm6_1n ymm4_1;
not ymm6_2n@uint64 ymm6_2;
and ymm10_2@uint64 ymm6_2n ymm4_2;
not ymm6_3n@uint64 ymm6_3;
and ymm10_3@uint64 ymm6_3n ymm4_3;
(* vpor   %ymm5,%ymm11,%ymm5                       #! PC = 0x5555555799ec *)
or ymm5_0@uint64 ymm11_0 ymm5_0;
or ymm5_1@uint64 ymm11_1 ymm5_1;
or ymm5_2@uint64 ymm11_2 ymm5_2;
or ymm5_3@uint64 ymm11_3 ymm5_3;
(* vpxor  %ymm2,%ymm10,%ymm8                       #! PC = 0x5555555799f0 *)
xor ymm8_0@uint64 ymm10_0 ymm2_0;
xor ymm8_1@uint64 ymm10_1 ymm2_1;
xor ymm8_2@uint64 ymm10_2 ymm2_2;
xor ymm8_3@uint64 ymm10_3 ymm2_3;
(* vmovdqa %ymm8,-0x70(%rbp)                       #! EA = L0x7fffffffbee0; PC = 0x5555555799f4 *)
mov L0x7fffffffbee0 ymm8_0;
mov L0x7fffffffbee8 ymm8_1;
mov L0x7fffffffbef0 ymm8_2;
mov L0x7fffffffbef8 ymm8_3;
(* vpandn %ymm5,%ymm4,%ymm8                        #! PC = 0x5555555799f9 *)
not ymm4_0n@uint64 ymm4_0;
and ymm8_0@uint64 ymm4_0n ymm5_0;
not ymm4_1n@uint64 ymm4_1;
and ymm8_1@uint64 ymm4_1n ymm5_1;
not ymm4_2n@uint64 ymm4_2;
and ymm8_2@uint64 ymm4_2n ymm5_2;
not ymm4_3n@uint64 ymm4_3;
and ymm8_3@uint64 ymm4_3n ymm5_3;
(* vpxor  %ymm6,%ymm8,%ymm8                        #! PC = 0x5555555799fd *)
xor ymm8_0@uint64 ymm8_0 ymm6_0;
xor ymm8_1@uint64 ymm8_1 ymm6_1;
xor ymm8_2@uint64 ymm8_2 ymm6_2;
xor ymm8_3@uint64 ymm8_3 ymm6_3;
(* vpandn %ymm0,%ymm5,%ymm6                        #! PC = 0x555555579a01 *)
not ymm5_0n@uint64 ymm5_0;
and ymm6_0@uint64 ymm5_0n ymm0_0;
not ymm5_1n@uint64 ymm5_1;
and ymm6_1@uint64 ymm5_1n ymm0_1;
not ymm5_2n@uint64 ymm5_2;
and ymm6_2@uint64 ymm5_2n ymm0_2;
not ymm5_3n@uint64 ymm5_3;
and ymm6_3@uint64 ymm5_3n ymm0_3;
(* vpandn %ymm2,%ymm0,%ymm0                        #! PC = 0x555555579a05 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm2_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm2_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm2_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm2_3;
(* vpxor  %ymm5,%ymm0,%ymm5                        #! PC = 0x555555579a09 *)
xor ymm5_0@uint64 ymm0_0 ymm5_0;
xor ymm5_1@uint64 ymm0_1 ymm5_1;
xor ymm5_2@uint64 ymm0_2 ymm5_2;
xor ymm5_3@uint64 ymm0_3 ymm5_3;
(* vpxor  -0x270(%rbp),%ymm12,%ymm0                #! EA = L0x7fffffffbce0; Value = 0xc36e991c1a383054; PC = 0x555555579a0d *)
xor ymm0_0@uint64 ymm12_0 L0x7fffffffbce0;
xor ymm0_1@uint64 ymm12_1 L0x7fffffffbce8;
xor ymm0_2@uint64 ymm12_2 L0x7fffffffbcf0;
xor ymm0_3@uint64 ymm12_3 L0x7fffffffbcf8;
(* vpxor  %ymm4,%ymm6,%ymm11                       #! PC = 0x555555579a15 *)
xor ymm11_0@uint64 ymm6_0 ymm4_0;
xor ymm11_1@uint64 ymm6_1 ymm4_1;
xor ymm11_2@uint64 ymm6_2 ymm4_2;
xor ymm11_3@uint64 ymm6_3 ymm4_3;
(* vmovdqa %ymm11,-0x1b0(%rbp)                     #! EA = L0x7fffffffbda0; PC = 0x555555579a19 *)
mov L0x7fffffffbda0 ymm11_0;
mov L0x7fffffffbda8 ymm11_1;
mov L0x7fffffffbdb0 ymm11_2;
mov L0x7fffffffbdb8 ymm11_3;
(* vpsrlq $0x25,%ymm0,%ymm9                        #! PC = 0x555555579a21 *)
shr ymm9_0 ymm0_0 0x25@uint64;
shr ymm9_1 ymm0_1 0x25@uint64;
shr ymm9_2 ymm0_2 0x25@uint64;
shr ymm9_3 ymm0_3 0x25@uint64;
(* vpsllq $0x1b,%ymm0,%ymm0                        #! PC = 0x555555579a26 *)
shl ymm0_0 ymm0_0 0x1b@uint64;
shl ymm0_1 ymm0_1 0x1b@uint64;
shl ymm0_2 ymm0_2 0x1b@uint64;
shl ymm0_3 ymm0_3 0x1b@uint64;
(* vmovdqa %ymm5,-0x1d0(%rbp)                      #! EA = L0x7fffffffbd80; PC = 0x555555579a2b *)
mov L0x7fffffffbd80 ymm5_0;
mov L0x7fffffffbd88 ymm5_1;
mov L0x7fffffffbd90 ymm5_2;
mov L0x7fffffffbd98 ymm5_3;
(* vpxor  -0x50(%rbp),%ymm14,%ymm5                 #! EA = L0x7fffffffbf00; Value = 0xabfe34cd40bb334a; PC = 0x555555579a33 *)
xor ymm5_0@uint64 ymm14_0 L0x7fffffffbf00;
xor ymm5_1@uint64 ymm14_1 L0x7fffffffbf08;
xor ymm5_2@uint64 ymm14_2 L0x7fffffffbf10;
xor ymm5_3@uint64 ymm14_3 L0x7fffffffbf18;
(* vpor   %ymm0,%ymm9,%ymm9                        #! PC = 0x555555579a38 *)
or ymm9_0@uint64 ymm9_0 ymm0_0;
or ymm9_1@uint64 ymm9_1 ymm0_1;
or ymm9_2@uint64 ymm9_2 ymm0_2;
or ymm9_3@uint64 ymm9_3 ymm0_3;
(* vpxor  -0x210(%rbp),%ymm3,%ymm0                 #! EA = L0x7fffffffbd40; Value = 0x689cbc39b01c0cb9; PC = 0x555555579a3c *)
xor ymm0_0@uint64 ymm3_0 L0x7fffffffbd40;
xor ymm0_1@uint64 ymm3_1 L0x7fffffffbd48;
xor ymm0_2@uint64 ymm3_2 L0x7fffffffbd50;
xor ymm0_3@uint64 ymm3_3 L0x7fffffffbd58;
(* vpsrlq $0x31,%ymm5,%ymm6                        #! PC = 0x555555579a44 *)
shr ymm6_0 ymm5_0 0x31@uint64;
shr ymm6_1 ymm5_1 0x31@uint64;
shr ymm6_2 ymm5_2 0x31@uint64;
shr ymm6_3 ymm5_3 0x31@uint64;
(* vpsllq $0xf,%ymm5,%ymm5                         #! PC = 0x555555579a49 *)
shl ymm5_0 ymm5_0 0xf@uint64;
shl ymm5_1 ymm5_1 0xf@uint64;
shl ymm5_2 ymm5_2 0xf@uint64;
shl ymm5_3 ymm5_3 0xf@uint64;
(* vpsrlq $0x1c,%ymm0,%ymm10                       #! PC = 0x555555579a4e *)
shr ymm10_0 ymm0_0 0x1c@uint64;
shr ymm10_1 ymm0_1 0x1c@uint64;
shr ymm10_2 ymm0_2 0x1c@uint64;
shr ymm10_3 ymm0_3 0x1c@uint64;
(* vpsllq $0x24,%ymm0,%ymm0                        #! PC = 0x555555579a53 *)
shl ymm0_0 ymm0_0 0x24@uint64;
shl ymm0_1 ymm0_1 0x24@uint64;
shl ymm0_2 ymm0_2 0x24@uint64;
shl ymm0_3 ymm0_3 0x24@uint64;
(* vpor   %ymm0,%ymm10,%ymm10                      #! PC = 0x555555579a58 *)
or ymm10_0@uint64 ymm10_0 ymm0_0;
or ymm10_1@uint64 ymm10_1 ymm0_1;
or ymm10_2@uint64 ymm10_2 ymm0_2;
or ymm10_3@uint64 ymm10_3 ymm0_3;
(* vpxor  -0x1f0(%rbp),%ymm1,%ymm0                 #! EA = L0x7fffffffbd60; Value = 0x2b12597baf864e64; PC = 0x555555579a5c *)
xor ymm0_0@uint64 ymm1_0 L0x7fffffffbd60;
xor ymm0_1@uint64 ymm1_1 L0x7fffffffbd68;
xor ymm0_2@uint64 ymm1_2 L0x7fffffffbd70;
xor ymm0_3@uint64 ymm1_3 L0x7fffffffbd78;
(* vpsrlq $0x36,%ymm0,%ymm11                       #! PC = 0x555555579a64 *)
shr ymm11_0 ymm0_0 0x36@uint64;
shr ymm11_1 ymm0_1 0x36@uint64;
shr ymm11_2 ymm0_2 0x36@uint64;
shr ymm11_3 ymm0_3 0x36@uint64;
(* vpsllq $0xa,%ymm0,%ymm0                         #! PC = 0x555555579a69 *)
shl ymm0_0 ymm0_0 0xa@uint64;
shl ymm0_1 ymm0_1 0xa@uint64;
shl ymm0_2 ymm0_2 0xa@uint64;
shl ymm0_3 ymm0_3 0xa@uint64;
(* vpor   %ymm0,%ymm11,%ymm11                      #! PC = 0x555555579a6e *)
or ymm11_0@uint64 ymm11_0 ymm0_0;
or ymm11_1@uint64 ymm11_1 ymm0_1;
or ymm11_2@uint64 ymm11_2 ymm0_2;
or ymm11_3@uint64 ymm11_3 ymm0_3;
(* vpandn %ymm11,%ymm10,%ymm2                      #! PC = 0x555555579a72 *)
not ymm10_0n@uint64 ymm10_0;
and ymm2_0@uint64 ymm10_0n ymm11_0;
not ymm10_1n@uint64 ymm10_1;
and ymm2_1@uint64 ymm10_1n ymm11_1;
not ymm10_2n@uint64 ymm10_2;
and ymm2_2@uint64 ymm10_2n ymm11_2;
not ymm10_3n@uint64 ymm10_3;
and ymm2_3@uint64 ymm10_3n ymm11_3;
(* vpxor  %ymm9,%ymm2,%ymm0                        #! PC = 0x555555579a77 *)
xor ymm0_0@uint64 ymm2_0 ymm9_0;
xor ymm0_1@uint64 ymm2_1 ymm9_1;
xor ymm0_2@uint64 ymm2_2 ymm9_2;
xor ymm0_3@uint64 ymm2_3 ymm9_3;
(* vpor   %ymm5,%ymm6,%ymm2                        #! PC = 0x555555579a7c *)
or ymm2_0@uint64 ymm6_0 ymm5_0;
or ymm2_1@uint64 ymm6_1 ymm5_1;
or ymm2_2@uint64 ymm6_2 ymm5_2;
or ymm2_3@uint64 ymm6_3 ymm5_3;
(* vmovdqa %ymm0,-0x1f0(%rbp)                      #! EA = L0x7fffffffbd60; PC = 0x555555579a80 *)
mov L0x7fffffffbd60 ymm0_0;
mov L0x7fffffffbd68 ymm0_1;
mov L0x7fffffffbd70 ymm0_2;
mov L0x7fffffffbd78 ymm0_3;
(* vpxor  -0x310(%rbp),%ymm15,%ymm0                #! EA = L0x7fffffffbc40; Value = 0x5decaa3cdbd5c36e; PC = 0x555555579a88 *)
xor ymm0_0@uint64 ymm15_0 L0x7fffffffbc40;
xor ymm0_1@uint64 ymm15_1 L0x7fffffffbc48;
xor ymm0_2@uint64 ymm15_2 L0x7fffffffbc50;
xor ymm0_3@uint64 ymm15_3 L0x7fffffffbc58;
(* vpandn %ymm2,%ymm11,%ymm5                       #! PC = 0x555555579a90 *)
not ymm11_0n@uint64 ymm11_0;
and ymm5_0@uint64 ymm11_0n ymm2_0;
not ymm11_1n@uint64 ymm11_1;
and ymm5_1@uint64 ymm11_1n ymm2_1;
not ymm11_2n@uint64 ymm11_2;
and ymm5_2@uint64 ymm11_2n ymm2_2;
not ymm11_3n@uint64 ymm11_3;
and ymm5_3@uint64 ymm11_3n ymm2_3;
(* vpxor  %ymm10,%ymm5,%ymm5                       #! PC = 0x555555579a94 *)
xor ymm5_0@uint64 ymm5_0 ymm10_0;
xor ymm5_1@uint64 ymm5_1 ymm10_1;
xor ymm5_2@uint64 ymm5_2 ymm10_2;
xor ymm5_3@uint64 ymm5_3 ymm10_3;
(* vpshufb 0x5443e(%rip),%ymm0,%ymm0        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x555555579a99 *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm0_0, ymm0_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm0_2, ymm0_3, tmp_2, tmp_3);
mov ymm0_0 tmp_0;
mov ymm0_1 tmp_1;
mov ymm0_2 tmp_2;
mov ymm0_3 tmp_3;
(* vpandn %ymm0,%ymm2,%ymm4                        #! PC = 0x555555579aa2 *)
not ymm2_0n@uint64 ymm2_0;
and ymm4_0@uint64 ymm2_0n ymm0_0;
not ymm2_1n@uint64 ymm2_1;
and ymm4_1@uint64 ymm2_1n ymm0_1;
not ymm2_2n@uint64 ymm2_2;
and ymm4_2@uint64 ymm2_2n ymm0_2;
not ymm2_3n@uint64 ymm2_3;
and ymm4_3@uint64 ymm2_3n ymm0_3;
(* vpandn %ymm9,%ymm0,%ymm6                        #! PC = 0x555555579aa6 *)
not ymm0_0n@uint64 ymm0_0;
and ymm6_0@uint64 ymm0_0n ymm9_0;
not ymm0_1n@uint64 ymm0_1;
and ymm6_1@uint64 ymm0_1n ymm9_1;
not ymm0_2n@uint64 ymm0_2;
and ymm6_2@uint64 ymm0_2n ymm9_2;
not ymm0_3n@uint64 ymm0_3;
and ymm6_3@uint64 ymm0_3n ymm9_3;
(* vpandn %ymm10,%ymm9,%ymm9                       #! PC = 0x555555579aab *)
not ymm9_0n@uint64 ymm9_0;
and ymm9_0@uint64 ymm9_0n ymm10_0;
not ymm9_1n@uint64 ymm9_1;
and ymm9_1@uint64 ymm9_1n ymm10_1;
not ymm9_2n@uint64 ymm9_2;
and ymm9_2@uint64 ymm9_2n ymm10_2;
not ymm9_3n@uint64 ymm9_3;
and ymm9_3@uint64 ymm9_3n ymm10_3;
(* vpxor  %ymm11,%ymm4,%ymm4                       #! PC = 0x555555579ab0 *)
xor ymm4_0@uint64 ymm4_0 ymm11_0;
xor ymm4_1@uint64 ymm4_1 ymm11_1;
xor ymm4_2@uint64 ymm4_2 ymm11_2;
xor ymm4_3@uint64 ymm4_3 ymm11_3;
(* vpxor  %ymm2,%ymm6,%ymm6                        #! PC = 0x555555579ab5 *)
xor ymm6_0@uint64 ymm6_0 ymm2_0;
xor ymm6_1@uint64 ymm6_1 ymm2_1;
xor ymm6_2@uint64 ymm6_2 ymm2_2;
xor ymm6_3@uint64 ymm6_3 ymm2_3;
(* vmovdqa %ymm4,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x555555579ab9 *)
mov L0x7fffffffbf00 ymm4_0;
mov L0x7fffffffbf08 ymm4_1;
mov L0x7fffffffbf10 ymm4_2;
mov L0x7fffffffbf18 ymm4_3;
(* vpxor  -0x250(%rbp),%ymm15,%ymm15               #! EA = L0x7fffffffbd00; Value = 0x35f1f6fe72e0a82f; PC = 0x555555579abe *)
xor ymm15_0@uint64 ymm15_0 L0x7fffffffbd00;
xor ymm15_1@uint64 ymm15_1 L0x7fffffffbd08;
xor ymm15_2@uint64 ymm15_2 L0x7fffffffbd10;
xor ymm15_3@uint64 ymm15_3 L0x7fffffffbd18;
(* vpxor  -0x230(%rbp),%ymm12,%ymm12               #! EA = L0x7fffffffbd20; Value = 0x05ec69b93ee6cf97; PC = 0x555555579ac6 *)
xor ymm12_0@uint64 ymm12_0 L0x7fffffffbd20;
xor ymm12_1@uint64 ymm12_1 L0x7fffffffbd28;
xor ymm12_2@uint64 ymm12_2 L0x7fffffffbd30;
xor ymm12_3@uint64 ymm12_3 L0x7fffffffbd38;
(* vpxor  %ymm0,%ymm9,%ymm4                        #! PC = 0x555555579ace *)
xor ymm4_0@uint64 ymm9_0 ymm0_0;
xor ymm4_1@uint64 ymm9_1 ymm0_1;
xor ymm4_2@uint64 ymm9_2 ymm0_2;
xor ymm4_3@uint64 ymm9_3 ymm0_3;
(* vpxor  -0x290(%rbp),%ymm14,%ymm14               #! EA = L0x7fffffffbcc0; Value = 0xa59d6cd06eb2ac2e; PC = 0x555555579ad2 *)
xor ymm14_0@uint64 ymm14_0 L0x7fffffffbcc0;
xor ymm14_1@uint64 ymm14_1 L0x7fffffffbcc8;
xor ymm14_2@uint64 ymm14_2 L0x7fffffffbcd0;
xor ymm14_3@uint64 ymm14_3 L0x7fffffffbcd8;
(* vpxor  -0x2d0(%rbp),%ymm3,%ymm3                 #! EA = L0x7fffffffbc80; Value = 0x1852407bda60f6dc; PC = 0x555555579ada *)
xor ymm3_0@uint64 ymm3_0 L0x7fffffffbc80;
xor ymm3_1@uint64 ymm3_1 L0x7fffffffbc88;
xor ymm3_2@uint64 ymm3_2 L0x7fffffffbc90;
xor ymm3_3@uint64 ymm3_3 L0x7fffffffbc98;
(* vmovdqa %ymm4,-0x210(%rbp)                      #! EA = L0x7fffffffbd40; PC = 0x555555579ae2 *)
mov L0x7fffffffbd40 ymm4_0;
mov L0x7fffffffbd48 ymm4_1;
mov L0x7fffffffbd50 ymm4_2;
mov L0x7fffffffbd58 ymm4_3;
(* vpsrlq $0x9,%ymm15,%ymm0                        #! PC = 0x555555579aea *)
shr ymm0_0 ymm15_0 0x9@uint64;
shr ymm0_1 ymm15_1 0x9@uint64;
shr ymm0_2 ymm15_2 0x9@uint64;
shr ymm0_3 ymm15_3 0x9@uint64;
(* vpsrlq $0x19,%ymm12,%ymm4                       #! PC = 0x555555579af0 *)
shr ymm4_0 ymm12_0 0x19@uint64;
shr ymm4_1 ymm12_1 0x19@uint64;
shr ymm4_2 ymm12_2 0x19@uint64;
shr ymm4_3 ymm12_3 0x19@uint64;
(* vmovdqa %ymm13,-0x2b0(%rbp)                     #! EA = L0x7fffffffbca0; PC = 0x555555579af6 *)
mov L0x7fffffffbca0 ymm13_0;
mov L0x7fffffffbca8 ymm13_1;
mov L0x7fffffffbcb0 ymm13_2;
mov L0x7fffffffbcb8 ymm13_3;
(* vpsllq $0x37,%ymm15,%ymm15                      #! PC = 0x555555579afe *)
shl ymm15_0 ymm15_0 0x37@uint64;
shl ymm15_1 ymm15_1 0x37@uint64;
shl ymm15_2 ymm15_2 0x37@uint64;
shl ymm15_3 ymm15_3 0x37@uint64;
(* vpsllq $0x27,%ymm12,%ymm12                      #! PC = 0x555555579b04 *)
shl ymm12_0 ymm12_0 0x27@uint64;
shl ymm12_1 ymm12_1 0x27@uint64;
shl ymm12_2 ymm12_2 0x27@uint64;
shl ymm12_3 ymm12_3 0x27@uint64;
(* vpor   %ymm15,%ymm0,%ymm15                      #! PC = 0x555555579b0a *)
or ymm15_0@uint64 ymm0_0 ymm15_0;
or ymm15_1@uint64 ymm0_1 ymm15_1;
or ymm15_2@uint64 ymm0_2 ymm15_2;
or ymm15_3@uint64 ymm0_3 ymm15_3;
(* vpsrlq $0x2,%ymm14,%ymm2                        #! PC = 0x555555579b0f *)
shr ymm2_0 ymm14_0 0x2@uint64;
shr ymm2_1 ymm14_1 0x2@uint64;
shr ymm2_2 ymm14_2 0x2@uint64;
shr ymm2_3 ymm14_3 0x2@uint64;
(* vpor   %ymm12,%ymm4,%ymm12                      #! PC = 0x555555579b15 *)
or ymm12_0@uint64 ymm4_0 ymm12_0;
or ymm12_1@uint64 ymm4_1 ymm12_1;
or ymm12_2@uint64 ymm4_2 ymm12_2;
or ymm12_3@uint64 ymm4_3 ymm12_3;
(* vpsllq $0x3e,%ymm14,%ymm14                      #! PC = 0x555555579b1a *)
shl ymm14_0 ymm14_0 0x3e@uint64;
shl ymm14_1 ymm14_1 0x3e@uint64;
shl ymm14_2 ymm14_2 0x3e@uint64;
shl ymm14_3 ymm14_3 0x3e@uint64;
(* vpsrlq $0x17,%ymm3,%ymm11                       #! PC = 0x555555579b20 *)
shr ymm11_0 ymm3_0 0x17@uint64;
shr ymm11_1 ymm3_1 0x17@uint64;
shr ymm11_2 ymm3_2 0x17@uint64;
shr ymm11_3 ymm3_3 0x17@uint64;
(* vpandn %ymm12,%ymm15,%ymm4                      #! PC = 0x555555579b25 *)
not ymm15_0n@uint64 ymm15_0;
and ymm4_0@uint64 ymm15_0n ymm12_0;
not ymm15_1n@uint64 ymm15_1;
and ymm4_1@uint64 ymm15_1n ymm12_1;
not ymm15_2n@uint64 ymm15_2;
and ymm4_2@uint64 ymm15_2n ymm12_2;
not ymm15_3n@uint64 ymm15_3;
and ymm4_3@uint64 ymm15_3n ymm12_3;
(* vpsllq $0x29,%ymm3,%ymm3                        #! PC = 0x555555579b2a *)
shl ymm3_0 ymm3_0 0x29@uint64;
shl ymm3_1 ymm3_1 0x29@uint64;
shl ymm3_2 ymm3_2 0x29@uint64;
shl ymm3_3 ymm3_3 0x29@uint64;
(* vpor   %ymm14,%ymm2,%ymm2                       #! PC = 0x555555579b2f *)
or ymm2_0@uint64 ymm2_0 ymm14_0;
or ymm2_1@uint64 ymm2_1 ymm14_1;
or ymm2_2@uint64 ymm2_2 ymm14_2;
or ymm2_3@uint64 ymm2_3 ymm14_3;
(* vpxor  -0x70(%rbp),%ymm5,%ymm14                 #! EA = L0x7fffffffbee0; Value = 0xedb9f9068bf73fef; PC = 0x555555579b34 *)
xor ymm14_0@uint64 ymm5_0 L0x7fffffffbee0;
xor ymm14_1@uint64 ymm5_1 L0x7fffffffbee8;
xor ymm14_2@uint64 ymm5_2 L0x7fffffffbef0;
xor ymm14_3@uint64 ymm5_3 L0x7fffffffbef8;
(* vpor   %ymm3,%ymm11,%ymm11                      #! PC = 0x555555579b39 *)
or ymm11_0@uint64 ymm11_0 ymm3_0;
or ymm11_1@uint64 ymm11_1 ymm3_1;
or ymm11_2@uint64 ymm11_2 ymm3_2;
or ymm11_3@uint64 ymm11_3 ymm3_3;
(* vpxor  -0xb0(%rbp),%ymm7,%ymm3                  #! EA = L0x7fffffffbea0; Value = 0x8e72c3b2db12a66f; PC = 0x555555579b3d *)
xor ymm3_0@uint64 ymm7_0 L0x7fffffffbea0;
xor ymm3_1@uint64 ymm7_1 L0x7fffffffbea8;
xor ymm3_2@uint64 ymm7_2 L0x7fffffffbeb0;
xor ymm3_3@uint64 ymm7_3 L0x7fffffffbeb8;
(* vpxor  -0x130(%rbp),%ymm13,%ymm0                #! EA = L0x7fffffffbe20; Value = 0x76c4ebd02adac11a; PC = 0x555555579b45 *)
xor ymm0_0@uint64 ymm13_0 L0x7fffffffbe20;
xor ymm0_1@uint64 ymm13_1 L0x7fffffffbe28;
xor ymm0_2@uint64 ymm13_2 L0x7fffffffbe30;
xor ymm0_3@uint64 ymm13_3 L0x7fffffffbe38;
(* vpxor  %ymm2,%ymm4,%ymm4                        #! PC = 0x555555579b4d *)
xor ymm4_0@uint64 ymm4_0 ymm2_0;
xor ymm4_1@uint64 ymm4_1 ymm2_1;
xor ymm4_2@uint64 ymm4_2 ymm2_2;
xor ymm4_3@uint64 ymm4_3 ymm2_3;
(* vpxor  -0x1f0(%rbp),%ymm4,%ymm9                 #! EA = L0x7fffffffbd60; Value = 0x96cf2f872c22d061; PC = 0x555555579b51 *)
xor ymm9_0@uint64 ymm4_0 L0x7fffffffbd60;
xor ymm9_1@uint64 ymm4_1 L0x7fffffffbd68;
xor ymm9_2@uint64 ymm4_2 L0x7fffffffbd70;
xor ymm9_3@uint64 ymm4_3 L0x7fffffffbd78;
(* vpxor  %ymm3,%ymm14,%ymm14                      #! PC = 0x555555579b59 *)
xor ymm14_0@uint64 ymm14_0 ymm3_0;
xor ymm14_1@uint64 ymm14_1 ymm3_1;
xor ymm14_2@uint64 ymm14_2 ymm3_2;
xor ymm14_3@uint64 ymm14_3 ymm3_3;
(* vpxor  -0x2f0(%rbp),%ymm1,%ymm3                 #! EA = L0x7fffffffbc60; Value = 0x21b79a41e0bc679b; PC = 0x555555579b5d *)
xor ymm3_0@uint64 ymm1_0 L0x7fffffffbc60;
xor ymm3_1@uint64 ymm1_1 L0x7fffffffbc68;
xor ymm3_2@uint64 ymm1_2 L0x7fffffffbc70;
xor ymm3_3@uint64 ymm1_3 L0x7fffffffbc78;
(* vpxor  %ymm0,%ymm9,%ymm9                        #! PC = 0x555555579b65 *)
xor ymm9_0@uint64 ymm9_0 ymm0_0;
xor ymm9_1@uint64 ymm9_1 ymm0_1;
xor ymm9_2@uint64 ymm9_2 ymm0_2;
xor ymm9_3@uint64 ymm9_3 ymm0_3;
(* vpandn %ymm11,%ymm12,%ymm0                      #! PC = 0x555555579b69 *)
not ymm12_0n@uint64 ymm12_0;
and ymm0_0@uint64 ymm12_0n ymm11_0;
not ymm12_1n@uint64 ymm12_1;
and ymm0_1@uint64 ymm12_1n ymm11_1;
not ymm12_2n@uint64 ymm12_2;
and ymm0_2@uint64 ymm12_2n ymm11_2;
not ymm12_3n@uint64 ymm12_3;
and ymm0_3@uint64 ymm12_3n ymm11_3;
(* vpxor  -0x90(%rbp),%ymm9,%ymm9                  #! EA = L0x7fffffffbec0; Value = 0xcb967722d7e71945; PC = 0x555555579b6e *)
xor ymm9_0@uint64 ymm9_0 L0x7fffffffbec0;
xor ymm9_1@uint64 ymm9_1 L0x7fffffffbec8;
xor ymm9_2@uint64 ymm9_2 L0x7fffffffbed0;
xor ymm9_3@uint64 ymm9_3 L0x7fffffffbed8;
(* vpxor  %ymm15,%ymm0,%ymm13                      #! PC = 0x555555579b76 *)
xor ymm13_0@uint64 ymm0_0 ymm15_0;
xor ymm13_1@uint64 ymm0_1 ymm15_1;
xor ymm13_2@uint64 ymm0_2 ymm15_2;
xor ymm13_3@uint64 ymm0_3 ymm15_3;
(* vpsllq $0x2,%ymm3,%ymm1                         #! PC = 0x555555579b7b *)
shl ymm1_0 ymm3_0 0x2@uint64;
shl ymm1_1 ymm3_1 0x2@uint64;
shl ymm1_2 ymm3_2 0x2@uint64;
shl ymm1_3 ymm3_3 0x2@uint64;
(* vpsrlq $0x3e,%ymm3,%ymm0                        #! PC = 0x555555579b80 *)
shr ymm0_0 ymm3_0 0x3e@uint64;
shr ymm0_1 ymm3_1 0x3e@uint64;
shr ymm0_2 ymm3_2 0x3e@uint64;
shr ymm0_3 ymm3_3 0x3e@uint64;
(* vpxor  %ymm13,%ymm14,%ymm14                     #! PC = 0x555555579b85 *)
xor ymm14_0@uint64 ymm14_0 ymm13_0;
xor ymm14_1@uint64 ymm14_1 ymm13_1;
xor ymm14_2@uint64 ymm14_2 ymm13_2;
xor ymm14_3@uint64 ymm14_3 ymm13_3;
(* vmovdqa %ymm13,-0x310(%rbp)                     #! EA = L0x7fffffffbc40; PC = 0x555555579b8a *)
mov L0x7fffffffbc40 ymm13_0;
mov L0x7fffffffbc48 ymm13_1;
mov L0x7fffffffbc50 ymm13_2;
mov L0x7fffffffbc58 ymm13_3;
(* vpxor  -0x50(%rbp),%ymm8,%ymm13                 #! EA = L0x7fffffffbf00; Value = 0xced2c9104a8bc9a6; PC = 0x555555579b92 *)
xor ymm13_0@uint64 ymm8_0 L0x7fffffffbf00;
xor ymm13_1@uint64 ymm8_1 L0x7fffffffbf08;
xor ymm13_2@uint64 ymm8_2 L0x7fffffffbf10;
xor ymm13_3@uint64 ymm8_3 L0x7fffffffbf18;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x555555579b97 *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpandn %ymm0,%ymm11,%ymm1                       #! PC = 0x555555579b9b *)
not ymm11_0n@uint64 ymm11_0;
and ymm1_0@uint64 ymm11_0n ymm0_0;
not ymm11_1n@uint64 ymm11_1;
and ymm1_1@uint64 ymm11_1n ymm0_1;
not ymm11_2n@uint64 ymm11_2;
and ymm1_2@uint64 ymm11_2n ymm0_2;
not ymm11_3n@uint64 ymm11_3;
and ymm1_3@uint64 ymm11_3n ymm0_3;
(* vpandn %ymm2,%ymm0,%ymm3                        #! PC = 0x555555579b9f *)
not ymm0_0n@uint64 ymm0_0;
and ymm3_0@uint64 ymm0_0n ymm2_0;
not ymm0_1n@uint64 ymm0_1;
and ymm3_1@uint64 ymm0_1n ymm2_1;
not ymm0_2n@uint64 ymm0_2;
and ymm3_2@uint64 ymm0_2n ymm2_2;
not ymm0_3n@uint64 ymm0_3;
and ymm3_3@uint64 ymm0_3n ymm2_3;
(* vpxor  %ymm12,%ymm1,%ymm12                      #! PC = 0x555555579ba3 *)
xor ymm12_0@uint64 ymm1_0 ymm12_0;
xor ymm12_1@uint64 ymm1_1 ymm12_1;
xor ymm12_2@uint64 ymm1_2 ymm12_2;
xor ymm12_3@uint64 ymm1_3 ymm12_3;
(* vmovdqa -0x150(%rbp),%ymm1                      #! EA = L0x7fffffffbe00; Value = 0x093ca1652def39a7; PC = 0x555555579ba8 *)
mov ymm1_0 L0x7fffffffbe00;
mov ymm1_1 L0x7fffffffbe08;
mov ymm1_2 L0x7fffffffbe10;
mov ymm1_3 L0x7fffffffbe18;
(* vpxor  -0xd0(%rbp),%ymm1,%ymm1                  #! EA = L0x7fffffffbe80; Value = 0xeff0aac9862e6863; PC = 0x555555579bb0 *)
xor ymm1_0@uint64 ymm1_0 L0x7fffffffbe80;
xor ymm1_1@uint64 ymm1_1 L0x7fffffffbe88;
xor ymm1_2@uint64 ymm1_2 L0x7fffffffbe90;
xor ymm1_3@uint64 ymm1_3 L0x7fffffffbe98;
(* vpxor  %ymm1,%ymm13,%ymm13                      #! PC = 0x555555579bb8 *)
xor ymm13_0@uint64 ymm13_0 ymm1_0;
xor ymm13_1@uint64 ymm13_1 ymm1_1;
xor ymm13_2@uint64 ymm13_2 ymm1_2;
xor ymm13_3@uint64 ymm13_3 ymm1_3;
(* vpxor  %ymm11,%ymm3,%ymm1                       #! PC = 0x555555579bbc *)
xor ymm1_0@uint64 ymm3_0 ymm11_0;
xor ymm1_1@uint64 ymm3_1 ymm11_1;
xor ymm1_2@uint64 ymm3_2 ymm11_2;
xor ymm1_3@uint64 ymm3_3 ymm11_3;
(* vmovdqa -0x170(%rbp),%ymm3                      #! EA = L0x7fffffffbde0; Value = 0xafbd5eafaa200412; PC = 0x555555579bc1 *)
mov ymm3_0 L0x7fffffffbde0;
mov ymm3_1 L0x7fffffffbde8;
mov ymm3_2 L0x7fffffffbdf0;
mov ymm3_3 L0x7fffffffbdf8;
(* vpxor  %ymm1,%ymm6,%ymm11                       #! PC = 0x555555579bc9 *)
xor ymm11_0@uint64 ymm6_0 ymm1_0;
xor ymm11_1@uint64 ymm6_1 ymm1_1;
xor ymm11_2@uint64 ymm6_2 ymm1_2;
xor ymm11_3@uint64 ymm6_3 ymm1_3;
(* vmovdqa %ymm1,-0x2f0(%rbp)                      #! EA = L0x7fffffffbc60; PC = 0x555555579bcd *)
mov L0x7fffffffbc60 ymm1_0;
mov L0x7fffffffbc68 ymm1_1;
mov L0x7fffffffbc70 ymm1_2;
mov L0x7fffffffbc78 ymm1_3;
(* vpxor  %ymm12,%ymm13,%ymm13                     #! PC = 0x555555579bd5 *)
xor ymm13_0@uint64 ymm13_0 ymm12_0;
xor ymm13_1@uint64 ymm13_1 ymm12_1;
xor ymm13_2@uint64 ymm13_2 ymm12_2;
xor ymm13_3@uint64 ymm13_3 ymm12_3;
(* vpxor  -0xf0(%rbp),%ymm3,%ymm1                  #! EA = L0x7fffffffbe60; Value = 0xcbcfe6fa8917f50f; PC = 0x555555579bda *)
xor ymm1_0@uint64 ymm3_0 L0x7fffffffbe60;
xor ymm1_1@uint64 ymm3_1 L0x7fffffffbe68;
xor ymm1_2@uint64 ymm3_2 L0x7fffffffbe70;
xor ymm1_3@uint64 ymm3_3 L0x7fffffffbe78;
(* vpsllq $0x1,%ymm13,%ymm3                        #! PC = 0x555555579be2 *)
shl ymm3_0 ymm13_0 0x1@uint64;
shl ymm3_1 ymm13_1 0x1@uint64;
shl ymm3_2 ymm13_2 0x1@uint64;
shl ymm3_3 ymm13_3 0x1@uint64;
(* vpxor  %ymm1,%ymm11,%ymm11                      #! PC = 0x555555579be8 *)
xor ymm11_0@uint64 ymm11_0 ymm1_0;
xor ymm11_1@uint64 ymm11_1 ymm1_1;
xor ymm11_2@uint64 ymm11_2 ymm1_2;
xor ymm11_3@uint64 ymm11_3 ymm1_3;
(* vpandn %ymm15,%ymm2,%ymm1                       #! PC = 0x555555579bec *)
not ymm2_0n@uint64 ymm2_0;
and ymm1_0@uint64 ymm2_0n ymm15_0;
not ymm2_1n@uint64 ymm2_1;
and ymm1_1@uint64 ymm2_1n ymm15_1;
not ymm2_2n@uint64 ymm2_2;
and ymm1_2@uint64 ymm2_2n ymm15_2;
not ymm2_3n@uint64 ymm2_3;
and ymm1_3@uint64 ymm2_3n ymm15_3;
(* vmovdqa -0x190(%rbp),%ymm15                     #! EA = L0x7fffffffbdc0; Value = 0x3cb75a7d2f1410e1; PC = 0x555555579bf1 *)
mov ymm15_0 L0x7fffffffbdc0;
mov ymm15_1 L0x7fffffffbdc8;
mov ymm15_2 L0x7fffffffbdd0;
mov ymm15_3 L0x7fffffffbdd8;
(* vpxor  -0x1b0(%rbp),%ymm11,%ymm11               #! EA = L0x7fffffffbda0; Value = 0xdf8594cb00c06448; PC = 0x555555579bf9 *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbda0;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbda8;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbdb0;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbdb8;
(* vpxor  %ymm0,%ymm1,%ymm1                        #! PC = 0x555555579c01 *)
xor ymm1_0@uint64 ymm1_0 ymm0_0;
xor ymm1_1@uint64 ymm1_1 ymm0_1;
xor ymm1_2@uint64 ymm1_2 ymm0_2;
xor ymm1_3@uint64 ymm1_3 ymm0_3;
(* vpxor  -0x110(%rbp),%ymm15,%ymm0                #! EA = L0x7fffffffbe40; Value = 0xf0388aed43035577; PC = 0x555555579c05 *)
xor ymm0_0@uint64 ymm15_0 L0x7fffffffbe40;
xor ymm0_1@uint64 ymm15_1 L0x7fffffffbe48;
xor ymm0_2@uint64 ymm15_2 L0x7fffffffbe50;
xor ymm0_3@uint64 ymm15_3 L0x7fffffffbe58;
(* vpxor  -0x1d0(%rbp),%ymm1,%ymm10                #! EA = L0x7fffffffbd80; Value = 0xb22a75014d2fbb08; PC = 0x555555579c0d *)
xor ymm10_0@uint64 ymm1_0 L0x7fffffffbd80;
xor ymm10_1@uint64 ymm1_1 L0x7fffffffbd88;
xor ymm10_2@uint64 ymm1_2 L0x7fffffffbd90;
xor ymm10_3@uint64 ymm1_3 L0x7fffffffbd98;
(* vpsrlq $0x3f,%ymm14,%ymm2                       #! PC = 0x555555579c15 *)
shr ymm2_0 ymm14_0 0x3f@uint64;
shr ymm2_1 ymm14_1 0x3f@uint64;
shr ymm2_2 ymm14_2 0x3f@uint64;
shr ymm2_3 ymm14_3 0x3f@uint64;
(* vpsllq $0x1,%ymm11,%ymm15                       #! PC = 0x555555579c1b *)
shl ymm15_0 ymm11_0 0x1@uint64;
shl ymm15_1 ymm11_1 0x1@uint64;
shl ymm15_2 ymm11_2 0x1@uint64;
shl ymm15_3 ymm11_3 0x1@uint64;
(* vpxor  %ymm0,%ymm10,%ymm10                      #! PC = 0x555555579c21 *)
xor ymm10_0@uint64 ymm10_0 ymm0_0;
xor ymm10_1@uint64 ymm10_1 ymm0_1;
xor ymm10_2@uint64 ymm10_2 ymm0_2;
xor ymm10_3@uint64 ymm10_3 ymm0_3;
(* vpsllq $0x1,%ymm14,%ymm0                        #! PC = 0x555555579c25 *)
shl ymm0_0 ymm14_0 0x1@uint64;
shl ymm0_1 ymm14_1 0x1@uint64;
shl ymm0_2 ymm14_2 0x1@uint64;
shl ymm0_3 ymm14_3 0x1@uint64;
(* vpxor  -0x210(%rbp),%ymm10,%ymm10               #! EA = L0x7fffffffbd40; Value = 0x6a81ca651589b15e; PC = 0x555555579c2b *)
xor ymm10_0@uint64 ymm10_0 L0x7fffffffbd40;
xor ymm10_1@uint64 ymm10_1 L0x7fffffffbd48;
xor ymm10_2@uint64 ymm10_2 L0x7fffffffbd50;
xor ymm10_3@uint64 ymm10_3 L0x7fffffffbd58;
(* vpor   %ymm0,%ymm2,%ymm2                        #! PC = 0x555555579c33 *)
or ymm2_0@uint64 ymm2_0 ymm0_0;
or ymm2_1@uint64 ymm2_1 ymm0_1;
or ymm2_2@uint64 ymm2_2 ymm0_2;
or ymm2_3@uint64 ymm2_3 ymm0_3;
(* vpsrlq $0x3f,%ymm13,%ymm0                       #! PC = 0x555555579c37 *)
shr ymm0_0 ymm13_0 0x3f@uint64;
shr ymm0_1 ymm13_1 0x3f@uint64;
shr ymm0_2 ymm13_2 0x3f@uint64;
shr ymm0_3 ymm13_3 0x3f@uint64;
(* vpor   %ymm3,%ymm0,%ymm0                        #! PC = 0x555555579c3d *)
or ymm0_0@uint64 ymm0_0 ymm3_0;
or ymm0_1@uint64 ymm0_1 ymm3_1;
or ymm0_2@uint64 ymm0_2 ymm3_2;
or ymm0_3@uint64 ymm0_3 ymm3_3;
(* vpsrlq $0x3f,%ymm11,%ymm3                       #! PC = 0x555555579c41 *)
shr ymm3_0 ymm11_0 0x3f@uint64;
shr ymm3_1 ymm11_1 0x3f@uint64;
shr ymm3_2 ymm11_2 0x3f@uint64;
shr ymm3_3 ymm11_3 0x3f@uint64;
(* vpxor  %ymm10,%ymm2,%ymm2                       #! PC = 0x555555579c47 *)
xor ymm2_0@uint64 ymm2_0 ymm10_0;
xor ymm2_1@uint64 ymm2_1 ymm10_1;
xor ymm2_2@uint64 ymm2_2 ymm10_2;
xor ymm2_3@uint64 ymm2_3 ymm10_3;
(* vpor   %ymm15,%ymm3,%ymm3                       #! PC = 0x555555579c4c *)
or ymm3_0@uint64 ymm3_0 ymm15_0;
or ymm3_1@uint64 ymm3_1 ymm15_1;
or ymm3_2@uint64 ymm3_2 ymm15_2;
or ymm3_3@uint64 ymm3_3 ymm15_3;
(* vpxor  %ymm9,%ymm0,%ymm0                        #! PC = 0x555555579c51 *)
xor ymm0_0@uint64 ymm0_0 ymm9_0;
xor ymm0_1@uint64 ymm0_1 ymm9_1;
xor ymm0_2@uint64 ymm0_2 ymm9_2;
xor ymm0_3@uint64 ymm0_3 ymm9_3;
(* vpxor  %ymm2,%ymm4,%ymm4                        #! PC = 0x555555579c56 *)
xor ymm4_0@uint64 ymm4_0 ymm2_0;
xor ymm4_1@uint64 ymm4_1 ymm2_1;
xor ymm4_2@uint64 ymm4_2 ymm2_2;
xor ymm4_3@uint64 ymm4_3 ymm2_3;
(* vpxor  %ymm14,%ymm3,%ymm14                      #! PC = 0x555555579c5a *)
xor ymm14_0@uint64 ymm3_0 ymm14_0;
xor ymm14_1@uint64 ymm3_1 ymm14_1;
xor ymm14_2@uint64 ymm3_2 ymm14_2;
xor ymm14_3@uint64 ymm3_3 ymm14_3;
(* vpsrlq $0x3f,%ymm10,%ymm3                       #! PC = 0x555555579c5f *)
shr ymm3_0 ymm10_0 0x3f@uint64;
shr ymm3_1 ymm10_1 0x3f@uint64;
shr ymm3_2 ymm10_2 0x3f@uint64;
shr ymm3_3 ymm10_3 0x3f@uint64;
(* vpxor  %ymm0,%ymm7,%ymm7                        #! PC = 0x555555579c65 *)
xor ymm7_0@uint64 ymm7_0 ymm0_0;
xor ymm7_1@uint64 ymm7_1 ymm0_1;
xor ymm7_2@uint64 ymm7_2 ymm0_2;
xor ymm7_3@uint64 ymm7_3 ymm0_3;
(* vpsllq $0x1,%ymm10,%ymm10                       #! PC = 0x555555579c69 *)
shl ymm10_0 ymm10_0 0x1@uint64;
shl ymm10_1 ymm10_1 0x1@uint64;
shl ymm10_2 ymm10_2 0x1@uint64;
shl ymm10_3 ymm10_3 0x1@uint64;
(* vpxor  %ymm14,%ymm8,%ymm8                       #! PC = 0x555555579c6f *)
xor ymm8_0@uint64 ymm8_0 ymm14_0;
xor ymm8_1@uint64 ymm8_1 ymm14_1;
xor ymm8_2@uint64 ymm8_2 ymm14_2;
xor ymm8_3@uint64 ymm8_3 ymm14_3;
(* vpxor  %ymm0,%ymm5,%ymm5                        #! PC = 0x555555579c74 *)
xor ymm5_0@uint64 ymm5_0 ymm0_0;
xor ymm5_1@uint64 ymm5_1 ymm0_1;
xor ymm5_2@uint64 ymm5_2 ymm0_2;
xor ymm5_3@uint64 ymm5_3 ymm0_3;
(* vpor   %ymm10,%ymm3,%ymm10                      #! PC = 0x555555579c78 *)
or ymm10_0@uint64 ymm3_0 ymm10_0;
or ymm10_1@uint64 ymm3_1 ymm10_1;
or ymm10_2@uint64 ymm3_2 ymm10_2;
or ymm10_3@uint64 ymm3_3 ymm10_3;
(* vpsrlq $0x3f,%ymm9,%ymm3                        #! PC = 0x555555579c7d *)
shr ymm3_0 ymm9_0 0x3f@uint64;
shr ymm3_1 ymm9_1 0x3f@uint64;
shr ymm3_2 ymm9_2 0x3f@uint64;
shr ymm3_3 ymm9_3 0x3f@uint64;
(* vpxor  %ymm14,%ymm12,%ymm12                     #! PC = 0x555555579c83 *)
xor ymm12_0@uint64 ymm12_0 ymm14_0;
xor ymm12_1@uint64 ymm12_1 ymm14_1;
xor ymm12_2@uint64 ymm12_2 ymm14_2;
xor ymm12_3@uint64 ymm12_3 ymm14_3;
(* vpsllq $0x1,%ymm9,%ymm9                         #! PC = 0x555555579c88 *)
shl ymm9_0 ymm9_0 0x1@uint64;
shl ymm9_1 ymm9_1 0x1@uint64;
shl ymm9_2 ymm9_2 0x1@uint64;
shl ymm9_3 ymm9_3 0x1@uint64;
(* vpxor  %ymm13,%ymm10,%ymm13                     #! PC = 0x555555579c8e *)
xor ymm13_0@uint64 ymm10_0 ymm13_0;
xor ymm13_1@uint64 ymm10_1 ymm13_1;
xor ymm13_2@uint64 ymm10_2 ymm13_2;
xor ymm13_3@uint64 ymm10_3 ymm13_3;
(* vpxor  -0x90(%rbp),%ymm2,%ymm10                 #! EA = L0x7fffffffbec0; Value = 0xcb967722d7e71945; PC = 0x555555579c93 *)
xor ymm10_0@uint64 ymm2_0 L0x7fffffffbec0;
xor ymm10_1@uint64 ymm2_1 L0x7fffffffbec8;
xor ymm10_2@uint64 ymm2_2 L0x7fffffffbed0;
xor ymm10_3@uint64 ymm2_3 L0x7fffffffbed8;
(* vpor   %ymm9,%ymm3,%ymm9                        #! PC = 0x555555579c9b *)
or ymm9_0@uint64 ymm3_0 ymm9_0;
or ymm9_1@uint64 ymm3_1 ymm9_1;
or ymm9_2@uint64 ymm3_2 ymm9_2;
or ymm9_3@uint64 ymm3_3 ymm9_3;
(* vpsrlq $0x14,%ymm7,%ymm3                        #! PC = 0x555555579ca0 *)
shr ymm3_0 ymm7_0 0x14@uint64;
shr ymm3_1 ymm7_1 0x14@uint64;
shr ymm3_2 ymm7_2 0x14@uint64;
shr ymm3_3 ymm7_3 0x14@uint64;
(* vpxor  %ymm13,%ymm6,%ymm6                       #! PC = 0x555555579ca5 *)
xor ymm6_0@uint64 ymm6_0 ymm13_0;
xor ymm6_1@uint64 ymm6_1 ymm13_1;
xor ymm6_2@uint64 ymm6_2 ymm13_2;
xor ymm6_3@uint64 ymm6_3 ymm13_3;
(* vpsllq $0x2c,%ymm7,%ymm7                        #! PC = 0x555555579caa *)
shl ymm7_0 ymm7_0 0x2c@uint64;
shl ymm7_1 ymm7_1 0x2c@uint64;
shl ymm7_2 ymm7_2 0x2c@uint64;
shl ymm7_3 ymm7_3 0x2c@uint64;
(* vpxor  %ymm11,%ymm9,%ymm11                      #! PC = 0x555555579caf *)
xor ymm11_0@uint64 ymm9_0 ymm11_0;
xor ymm11_1@uint64 ymm9_1 ymm11_1;
xor ymm11_2@uint64 ymm9_2 ymm11_2;
xor ymm11_3@uint64 ymm9_3 ymm11_3;
(* vmovq  %r12,%xmm9                               #! PC = 0x555555579cb4 *)
mov xmm9_0 r12;
mov xmm9_1 0@uint64;
(* vpor   %ymm7,%ymm3,%ymm7                        #! PC = 0x555555579cb9 *)
or ymm7_0@uint64 ymm3_0 ymm7_0;
or ymm7_1@uint64 ymm3_1 ymm7_1;
or ymm7_2@uint64 ymm3_2 ymm7_2;
or ymm7_3@uint64 ymm3_3 ymm7_3;
(* vpsrlq $0x15,%ymm8,%ymm3                        #! PC = 0x555555579cbd *)
shr ymm3_0 ymm8_0 0x15@uint64;
shr ymm3_1 ymm8_1 0x15@uint64;
shr ymm3_2 ymm8_2 0x15@uint64;
shr ymm3_3 ymm8_3 0x15@uint64;
(* vpbroadcastq %xmm9,%ymm9                        #! PC = 0x555555579cc3 *)
mov ymm9_0 xmm9_0;
mov ymm9_1 xmm9_0;
mov ymm9_2 xmm9_0;
mov ymm9_3 xmm9_0;
(* vpsllq $0x2b,%ymm8,%ymm8                        #! PC = 0x555555579cc8 *)
shl ymm8_0 ymm8_0 0x2b@uint64;
shl ymm8_1 ymm8_1 0x2b@uint64;
shl ymm8_2 ymm8_2 0x2b@uint64;
shl ymm8_3 ymm8_3 0x2b@uint64;
(* vpxor  %ymm11,%ymm1,%ymm1                       #! PC = 0x555555579cce *)
xor ymm1_0@uint64 ymm1_0 ymm11_0;
xor ymm1_1@uint64 ymm1_1 ymm11_1;
xor ymm1_2@uint64 ymm1_2 ymm11_2;
xor ymm1_3@uint64 ymm1_3 ymm11_3;
(* vpor   %ymm8,%ymm3,%ymm8                        #! PC = 0x555555579cd3 *)
or ymm8_0@uint64 ymm3_0 ymm8_0;
or ymm8_1@uint64 ymm3_1 ymm8_1;
or ymm8_2@uint64 ymm3_2 ymm8_2;
or ymm8_3@uint64 ymm3_3 ymm8_3;
(* vpandn %ymm8,%ymm7,%ymm3                        #! PC = 0x555555579cd8 *)
not ymm7_0n@uint64 ymm7_0;
and ymm3_0@uint64 ymm7_0n ymm8_0;
not ymm7_1n@uint64 ymm7_1;
and ymm3_1@uint64 ymm7_1n ymm8_1;
not ymm7_2n@uint64 ymm7_2;
and ymm3_2@uint64 ymm7_2n ymm8_2;
not ymm7_3n@uint64 ymm7_3;
and ymm3_3@uint64 ymm7_3n ymm8_3;
(* vpxor  %ymm9,%ymm3,%ymm9                        #! PC = 0x555555579cdd *)
xor ymm9_0@uint64 ymm3_0 ymm9_0;
xor ymm9_1@uint64 ymm3_1 ymm9_1;
xor ymm9_2@uint64 ymm3_2 ymm9_2;
xor ymm9_3@uint64 ymm3_3 ymm9_3;
(* vpsrlq $0x2b,%ymm6,%ymm3                        #! PC = 0x555555579ce2 *)
shr ymm3_0 ymm6_0 0x2b@uint64;
shr ymm3_1 ymm6_1 0x2b@uint64;
shr ymm3_2 ymm6_2 0x2b@uint64;
shr ymm3_3 ymm6_3 0x2b@uint64;
(* vpsllq $0x15,%ymm6,%ymm6                        #! PC = 0x555555579ce7 *)
shl ymm6_0 ymm6_0 0x15@uint64;
shl ymm6_1 ymm6_1 0x15@uint64;
shl ymm6_2 ymm6_2 0x15@uint64;
shl ymm6_3 ymm6_3 0x15@uint64;
(* vpxor  %ymm10,%ymm9,%ymm9                       #! PC = 0x555555579cec *)
xor ymm9_0@uint64 ymm9_0 ymm10_0;
xor ymm9_1@uint64 ymm9_1 ymm10_1;
xor ymm9_2@uint64 ymm9_2 ymm10_2;
xor ymm9_3@uint64 ymm9_3 ymm10_3;
(* vpor   %ymm6,%ymm3,%ymm6                        #! PC = 0x555555579cf1 *)
or ymm6_0@uint64 ymm3_0 ymm6_0;
or ymm6_1@uint64 ymm3_1 ymm6_1;
or ymm6_2@uint64 ymm3_2 ymm6_2;
or ymm6_3@uint64 ymm3_3 ymm6_3;
(* vmovdqa %ymm9,-0x90(%rbp)                       #! EA = L0x7fffffffbec0; PC = 0x555555579cf5 *)
mov L0x7fffffffbec0 ymm9_0;
mov L0x7fffffffbec8 ymm9_1;
mov L0x7fffffffbed0 ymm9_2;
mov L0x7fffffffbed8 ymm9_3;
(* vpandn %ymm6,%ymm8,%ymm3                        #! PC = 0x555555579cfd *)
not ymm8_0n@uint64 ymm8_0;
and ymm3_0@uint64 ymm8_0n ymm6_0;
not ymm8_1n@uint64 ymm8_1;
and ymm3_1@uint64 ymm8_1n ymm6_1;
not ymm8_2n@uint64 ymm8_2;
and ymm3_2@uint64 ymm8_2n ymm6_2;
not ymm8_3n@uint64 ymm8_3;
and ymm3_3@uint64 ymm8_3n ymm6_3;
(* vpxor  %ymm7,%ymm3,%ymm9                        #! PC = 0x555555579d01 *)
xor ymm9_0@uint64 ymm3_0 ymm7_0;
xor ymm9_1@uint64 ymm3_1 ymm7_1;
xor ymm9_2@uint64 ymm3_2 ymm7_2;
xor ymm9_3@uint64 ymm3_3 ymm7_3;
(* vpsrlq $0x32,%ymm1,%ymm3                        #! PC = 0x555555579d05 *)
shr ymm3_0 ymm1_0 0x32@uint64;
shr ymm3_1 ymm1_1 0x32@uint64;
shr ymm3_2 ymm1_2 0x32@uint64;
shr ymm3_3 ymm1_3 0x32@uint64;
(* vpsllq $0xe,%ymm1,%ymm1                         #! PC = 0x555555579d0a *)
shl ymm1_0 ymm1_0 0xe@uint64;
shl ymm1_1 ymm1_1 0xe@uint64;
shl ymm1_2 ymm1_2 0xe@uint64;
shl ymm1_3 ymm1_3 0xe@uint64;
(* vmovdqa %ymm9,-0x230(%rbp)                      #! EA = L0x7fffffffbd20; PC = 0x555555579d0f *)
mov L0x7fffffffbd20 ymm9_0;
mov L0x7fffffffbd28 ymm9_1;
mov L0x7fffffffbd30 ymm9_2;
mov L0x7fffffffbd38 ymm9_3;
(* vpor   %ymm1,%ymm3,%ymm1                        #! PC = 0x555555579d17 *)
or ymm1_0@uint64 ymm3_0 ymm1_0;
or ymm1_1@uint64 ymm3_1 ymm1_1;
or ymm1_2@uint64 ymm3_2 ymm1_2;
or ymm1_3@uint64 ymm3_3 ymm1_3;
(* vpandn %ymm1,%ymm6,%ymm3                        #! PC = 0x555555579d1b *)
not ymm6_0n@uint64 ymm6_0;
and ymm3_0@uint64 ymm6_0n ymm1_0;
not ymm6_1n@uint64 ymm6_1;
and ymm3_1@uint64 ymm6_1n ymm1_1;
not ymm6_2n@uint64 ymm6_2;
and ymm3_2@uint64 ymm6_2n ymm1_2;
not ymm6_3n@uint64 ymm6_3;
and ymm3_3@uint64 ymm6_3n ymm1_3;
(* vpxor  %ymm8,%ymm3,%ymm8                        #! PC = 0x555555579d1f *)
xor ymm8_0@uint64 ymm3_0 ymm8_0;
xor ymm8_1@uint64 ymm3_1 ymm8_1;
xor ymm8_2@uint64 ymm3_2 ymm8_2;
xor ymm8_3@uint64 ymm3_3 ymm8_3;
(* vpandn %ymm10,%ymm1,%ymm3                       #! PC = 0x555555579d24 *)
not ymm1_0n@uint64 ymm1_0;
and ymm3_0@uint64 ymm1_0n ymm10_0;
not ymm1_1n@uint64 ymm1_1;
and ymm3_1@uint64 ymm1_1n ymm10_1;
not ymm1_2n@uint64 ymm1_2;
and ymm3_2@uint64 ymm1_2n ymm10_2;
not ymm1_3n@uint64 ymm1_3;
and ymm3_3@uint64 ymm1_3n ymm10_3;
(* vpandn %ymm7,%ymm10,%ymm10                      #! PC = 0x555555579d29 *)
not ymm10_0n@uint64 ymm10_0;
and ymm10_0@uint64 ymm10_0n ymm7_0;
not ymm10_1n@uint64 ymm10_1;
and ymm10_1@uint64 ymm10_1n ymm7_1;
not ymm10_2n@uint64 ymm10_2;
and ymm10_2@uint64 ymm10_2n ymm7_2;
not ymm10_3n@uint64 ymm10_3;
and ymm10_3@uint64 ymm10_3n ymm7_3;
(* vpxor  %ymm1,%ymm10,%ymm7                       #! PC = 0x555555579d2d *)
xor ymm7_0@uint64 ymm10_0 ymm1_0;
xor ymm7_1@uint64 ymm10_1 ymm1_1;
xor ymm7_2@uint64 ymm10_2 ymm1_2;
xor ymm7_3@uint64 ymm10_3 ymm1_3;
(* vpxor  -0xf0(%rbp),%ymm13,%ymm1                 #! EA = L0x7fffffffbe60; Value = 0xcbcfe6fa8917f50f; PC = 0x555555579d31 *)
xor ymm1_0@uint64 ymm13_0 L0x7fffffffbe60;
xor ymm1_1@uint64 ymm13_1 L0x7fffffffbe68;
xor ymm1_2@uint64 ymm13_2 L0x7fffffffbe70;
xor ymm1_3@uint64 ymm13_3 L0x7fffffffbe78;
(* vmovdqa %ymm8,-0x250(%rbp)                      #! EA = L0x7fffffffbd00; PC = 0x555555579d39 *)
mov L0x7fffffffbd00 ymm8_0;
mov L0x7fffffffbd08 ymm8_1;
mov L0x7fffffffbd10 ymm8_2;
mov L0x7fffffffbd18 ymm8_3;
(* vpxor  %ymm6,%ymm3,%ymm8                        #! PC = 0x555555579d41 *)
xor ymm8_0@uint64 ymm3_0 ymm6_0;
xor ymm8_1@uint64 ymm3_1 ymm6_1;
xor ymm8_2@uint64 ymm3_2 ymm6_2;
xor ymm8_3@uint64 ymm3_3 ymm6_3;
(* vpxor  -0x190(%rbp),%ymm11,%ymm3                #! EA = L0x7fffffffbdc0; Value = 0x3cb75a7d2f1410e1; PC = 0x555555579d45 *)
xor ymm3_0@uint64 ymm11_0 L0x7fffffffbdc0;
xor ymm3_1@uint64 ymm11_1 L0x7fffffffbdc8;
xor ymm3_2@uint64 ymm11_2 L0x7fffffffbdd0;
xor ymm3_3@uint64 ymm11_3 L0x7fffffffbdd8;
(* vmovdqa %ymm8,-0x270(%rbp)                      #! EA = L0x7fffffffbce0; PC = 0x555555579d4d *)
mov L0x7fffffffbce0 ymm8_0;
mov L0x7fffffffbce8 ymm8_1;
mov L0x7fffffffbcf0 ymm8_2;
mov L0x7fffffffbcf8 ymm8_3;
(* vmovdqa %ymm7,-0x290(%rbp)                      #! EA = L0x7fffffffbcc0; PC = 0x555555579d55 *)
mov L0x7fffffffbcc0 ymm7_0;
mov L0x7fffffffbcc8 ymm7_1;
mov L0x7fffffffbcd0 ymm7_2;
mov L0x7fffffffbcd8 ymm7_3;
(* vpxor  -0x2b0(%rbp),%ymm2,%ymm6                 #! EA = L0x7fffffffbca0; Value = 0x5bbcb96cfede2e86; PC = 0x555555579d5d *)
xor ymm6_0@uint64 ymm2_0 L0x7fffffffbca0;
xor ymm6_1@uint64 ymm2_1 L0x7fffffffbca8;
xor ymm6_2@uint64 ymm2_2 L0x7fffffffbcb0;
xor ymm6_3@uint64 ymm2_3 L0x7fffffffbcb8;
(* vpsrlq $0x24,%ymm1,%ymm7                        #! PC = 0x555555579d65 *)
shr ymm7_0 ymm1_0 0x24@uint64;
shr ymm7_1 ymm1_1 0x24@uint64;
shr ymm7_2 ymm1_2 0x24@uint64;
shr ymm7_3 ymm1_3 0x24@uint64;
(* vpsllq $0x1c,%ymm1,%ymm1                        #! PC = 0x555555579d6a *)
shl ymm1_0 ymm1_0 0x1c@uint64;
shl ymm1_1 ymm1_1 0x1c@uint64;
shl ymm1_2 ymm1_2 0x1c@uint64;
shl ymm1_3 ymm1_3 0x1c@uint64;
(* vpor   %ymm1,%ymm7,%ymm7                        #! PC = 0x555555579d6f *)
or ymm7_0@uint64 ymm7_0 ymm1_0;
or ymm7_1@uint64 ymm7_1 ymm1_1;
or ymm7_2@uint64 ymm7_2 ymm1_2;
or ymm7_3@uint64 ymm7_3 ymm1_3;
(* vpsrlq $0x2c,%ymm3,%ymm1                        #! PC = 0x555555579d73 *)
shr ymm1_0 ymm3_0 0x2c@uint64;
shr ymm1_1 ymm3_1 0x2c@uint64;
shr ymm1_2 ymm3_2 0x2c@uint64;
shr ymm1_3 ymm3_3 0x2c@uint64;
(* vpsllq $0x14,%ymm3,%ymm3                        #! PC = 0x555555579d78 *)
shl ymm3_0 ymm3_0 0x14@uint64;
shl ymm3_1 ymm3_1 0x14@uint64;
shl ymm3_2 ymm3_2 0x14@uint64;
shl ymm3_3 ymm3_3 0x14@uint64;
(* vpor   %ymm3,%ymm1,%ymm1                        #! PC = 0x555555579d7d *)
or ymm1_0@uint64 ymm1_0 ymm3_0;
or ymm1_1@uint64 ymm1_1 ymm3_1;
or ymm1_2@uint64 ymm1_2 ymm3_2;
or ymm1_3@uint64 ymm1_3 ymm3_3;
(* vpsrlq $0x3d,%ymm6,%ymm3                        #! PC = 0x555555579d81 *)
shr ymm3_0 ymm6_0 0x3d@uint64;
shr ymm3_1 ymm6_1 0x3d@uint64;
shr ymm3_2 ymm6_2 0x3d@uint64;
shr ymm3_3 ymm6_3 0x3d@uint64;
(* vpsllq $0x3,%ymm6,%ymm6                         #! PC = 0x555555579d86 *)
shl ymm6_0 ymm6_0 0x3@uint64;
shl ymm6_1 ymm6_1 0x3@uint64;
shl ymm6_2 ymm6_2 0x3@uint64;
shl ymm6_3 ymm6_3 0x3@uint64;
(* vpor   %ymm6,%ymm3,%ymm3                        #! PC = 0x555555579d8b *)
or ymm3_0@uint64 ymm3_0 ymm6_0;
or ymm3_1@uint64 ymm3_1 ymm6_1;
or ymm3_2@uint64 ymm3_2 ymm6_2;
or ymm3_3@uint64 ymm3_3 ymm6_3;
(* vpandn %ymm3,%ymm1,%ymm6                        #! PC = 0x555555579d8f *)
not ymm1_0n@uint64 ymm1_0;
and ymm6_0@uint64 ymm1_0n ymm3_0;
not ymm1_1n@uint64 ymm1_1;
and ymm6_1@uint64 ymm1_1n ymm3_1;
not ymm1_2n@uint64 ymm1_2;
and ymm6_2@uint64 ymm1_2n ymm3_2;
not ymm1_3n@uint64 ymm1_3;
and ymm6_3@uint64 ymm1_3n ymm3_3;
(* vpxor  %ymm7,%ymm6,%ymm15                       #! PC = 0x555555579d93 *)
xor ymm15_0@uint64 ymm6_0 ymm7_0;
xor ymm15_1@uint64 ymm6_1 ymm7_1;
xor ymm15_2@uint64 ymm6_2 ymm7_2;
xor ymm15_3@uint64 ymm6_3 ymm7_3;
(* vmovdqa %ymm15,-0xf0(%rbp)                      #! EA = L0x7fffffffbe60; PC = 0x555555579d97 *)
mov L0x7fffffffbe60 ymm15_0;
mov L0x7fffffffbe68 ymm15_1;
mov L0x7fffffffbe70 ymm15_2;
mov L0x7fffffffbe78 ymm15_3;
(* vpsrlq $0x13,%ymm5,%ymm15                       #! PC = 0x555555579d9f *)
shr ymm15_0 ymm5_0 0x13@uint64;
shr ymm15_1 ymm5_1 0x13@uint64;
shr ymm15_2 ymm5_2 0x13@uint64;
shr ymm15_3 ymm5_3 0x13@uint64;
(* vpsllq $0x2d,%ymm5,%ymm5                        #! PC = 0x555555579da4 *)
shl ymm5_0 ymm5_0 0x2d@uint64;
shl ymm5_1 ymm5_1 0x2d@uint64;
shl ymm5_2 ymm5_2 0x2d@uint64;
shl ymm5_3 ymm5_3 0x2d@uint64;
(* vpor   %ymm5,%ymm15,%ymm15                      #! PC = 0x555555579da9 *)
or ymm15_0@uint64 ymm15_0 ymm5_0;
or ymm15_1@uint64 ymm15_1 ymm5_1;
or ymm15_2@uint64 ymm15_2 ymm5_2;
or ymm15_3@uint64 ymm15_3 ymm5_3;
(* vpsrlq $0x3,%ymm12,%ymm5                        #! PC = 0x555555579dad *)
shr ymm5_0 ymm12_0 0x3@uint64;
shr ymm5_1 ymm12_1 0x3@uint64;
shr ymm5_2 ymm12_2 0x3@uint64;
shr ymm5_3 ymm12_3 0x3@uint64;
(* vpsllq $0x3d,%ymm12,%ymm12                      #! PC = 0x555555579db3 *)
shl ymm12_0 ymm12_0 0x3d@uint64;
shl ymm12_1 ymm12_1 0x3d@uint64;
shl ymm12_2 ymm12_2 0x3d@uint64;
shl ymm12_3 ymm12_3 0x3d@uint64;
(* vpandn %ymm15,%ymm3,%ymm8                       #! PC = 0x555555579db9 *)
not ymm3_0n@uint64 ymm3_0;
and ymm8_0@uint64 ymm3_0n ymm15_0;
not ymm3_1n@uint64 ymm3_1;
and ymm8_1@uint64 ymm3_1n ymm15_1;
not ymm3_2n@uint64 ymm3_2;
and ymm8_2@uint64 ymm3_2n ymm15_2;
not ymm3_3n@uint64 ymm3_3;
and ymm8_3@uint64 ymm3_3n ymm15_3;
(* vpor   %ymm12,%ymm5,%ymm12                      #! PC = 0x555555579dbe *)
or ymm12_0@uint64 ymm5_0 ymm12_0;
or ymm12_1@uint64 ymm5_1 ymm12_1;
or ymm12_2@uint64 ymm5_2 ymm12_2;
or ymm12_3@uint64 ymm5_3 ymm12_3;
(* vpxor  %ymm1,%ymm8,%ymm8                        #! PC = 0x555555579dc3 *)
xor ymm8_0@uint64 ymm8_0 ymm1_0;
xor ymm8_1@uint64 ymm8_1 ymm1_1;
xor ymm8_2@uint64 ymm8_2 ymm1_2;
xor ymm8_3@uint64 ymm8_3 ymm1_3;
(* vpandn %ymm12,%ymm15,%ymm5                      #! PC = 0x555555579dc7 *)
not ymm15_0n@uint64 ymm15_0;
and ymm5_0@uint64 ymm15_0n ymm12_0;
not ymm15_1n@uint64 ymm15_1;
and ymm5_1@uint64 ymm15_1n ymm12_1;
not ymm15_2n@uint64 ymm15_2;
and ymm5_2@uint64 ymm15_2n ymm12_2;
not ymm15_3n@uint64 ymm15_3;
and ymm5_3@uint64 ymm15_3n ymm12_3;
(* vpxor  %ymm3,%ymm5,%ymm5                        #! PC = 0x555555579dcc *)
xor ymm5_0@uint64 ymm5_0 ymm3_0;
xor ymm5_1@uint64 ymm5_1 ymm3_1;
xor ymm5_2@uint64 ymm5_2 ymm3_2;
xor ymm5_3@uint64 ymm5_3 ymm3_3;
(* vpandn %ymm7,%ymm12,%ymm3                       #! PC = 0x555555579dd0 *)
not ymm12_0n@uint64 ymm12_0;
and ymm3_0@uint64 ymm12_0n ymm7_0;
not ymm12_1n@uint64 ymm12_1;
and ymm3_1@uint64 ymm12_1n ymm7_1;
not ymm12_2n@uint64 ymm12_2;
and ymm3_2@uint64 ymm12_2n ymm7_2;
not ymm12_3n@uint64 ymm12_3;
and ymm3_3@uint64 ymm12_3n ymm7_3;
(* vpandn %ymm1,%ymm7,%ymm7                        #! PC = 0x555555579dd4 *)
not ymm7_0n@uint64 ymm7_0;
and ymm7_0@uint64 ymm7_0n ymm1_0;
not ymm7_1n@uint64 ymm7_1;
and ymm7_1@uint64 ymm7_1n ymm1_1;
not ymm7_2n@uint64 ymm7_2;
and ymm7_2@uint64 ymm7_2n ymm1_2;
not ymm7_3n@uint64 ymm7_3;
and ymm7_3@uint64 ymm7_3n ymm1_3;
(* vpxor  %ymm15,%ymm3,%ymm15                      #! PC = 0x555555579dd8 *)
xor ymm15_0@uint64 ymm3_0 ymm15_0;
xor ymm15_1@uint64 ymm3_1 ymm15_1;
xor ymm15_2@uint64 ymm3_2 ymm15_2;
xor ymm15_3@uint64 ymm3_3 ymm15_3;
(* vpxor  -0xb0(%rbp),%ymm0,%ymm3                  #! EA = L0x7fffffffbea0; Value = 0x8e72c3b2db12a66f; PC = 0x555555579ddd *)
xor ymm3_0@uint64 ymm0_0 L0x7fffffffbea0;
xor ymm3_1@uint64 ymm0_1 L0x7fffffffbea8;
xor ymm3_2@uint64 ymm0_2 L0x7fffffffbeb0;
xor ymm3_3@uint64 ymm0_3 L0x7fffffffbeb8;
(* vmovdqa %ymm5,-0x190(%rbp)                      #! EA = L0x7fffffffbdc0; PC = 0x555555579de5 *)
mov L0x7fffffffbdc0 ymm5_0;
mov L0x7fffffffbdc8 ymm5_1;
mov L0x7fffffffbdd0 ymm5_2;
mov L0x7fffffffbdd8 ymm5_3;
(* vpxor  %ymm12,%ymm7,%ymm12                      #! PC = 0x555555579ded *)
xor ymm12_0@uint64 ymm7_0 ymm12_0;
xor ymm12_1@uint64 ymm7_1 ymm12_1;
xor ymm12_2@uint64 ymm7_2 ymm12_2;
xor ymm12_3@uint64 ymm7_3 ymm12_3;
(* vpxor  -0x150(%rbp),%ymm14,%ymm5                #! EA = L0x7fffffffbe00; Value = 0x093ca1652def39a7; PC = 0x555555579df2 *)
xor ymm5_0@uint64 ymm14_0 L0x7fffffffbe00;
xor ymm5_1@uint64 ymm14_1 L0x7fffffffbe08;
xor ymm5_2@uint64 ymm14_2 L0x7fffffffbe10;
xor ymm5_3@uint64 ymm14_3 L0x7fffffffbe18;
(* vmovdqa %ymm12,-0x2d0(%rbp)                     #! EA = L0x7fffffffbc80; PC = 0x555555579dfa *)
mov L0x7fffffffbc80 ymm12_0;
mov L0x7fffffffbc88 ymm12_1;
mov L0x7fffffffbc90 ymm12_2;
mov L0x7fffffffbc98 ymm12_3;
(* vpsrlq $0x2e,%ymm4,%ymm7                        #! PC = 0x555555579e02 *)
shr ymm7_0 ymm4_0 0x2e@uint64;
shr ymm7_1 ymm4_1 0x2e@uint64;
shr ymm7_2 ymm4_2 0x2e@uint64;
shr ymm7_3 ymm4_3 0x2e@uint64;
(* vpsrlq $0x3f,%ymm3,%ymm1                        #! PC = 0x555555579e07 *)
shr ymm1_0 ymm3_0 0x3f@uint64;
shr ymm1_1 ymm3_1 0x3f@uint64;
shr ymm1_2 ymm3_2 0x3f@uint64;
shr ymm1_3 ymm3_3 0x3f@uint64;
(* vpsllq $0x1,%ymm3,%ymm3                         #! PC = 0x555555579e0c *)
shl ymm3_0 ymm3_0 0x1@uint64;
shl ymm3_1 ymm3_1 0x1@uint64;
shl ymm3_2 ymm3_2 0x1@uint64;
shl ymm3_3 ymm3_3 0x1@uint64;
(* vmovdqa %ymm15,-0x2b0(%rbp)                     #! EA = L0x7fffffffbca0; PC = 0x555555579e11 *)
mov L0x7fffffffbca0 ymm15_0;
mov L0x7fffffffbca8 ymm15_1;
mov L0x7fffffffbcb0 ymm15_2;
mov L0x7fffffffbcb8 ymm15_3;
(* vpor   %ymm3,%ymm1,%ymm1                        #! PC = 0x555555579e19 *)
or ymm1_0@uint64 ymm1_0 ymm3_0;
or ymm1_1@uint64 ymm1_1 ymm3_1;
or ymm1_2@uint64 ymm1_2 ymm3_2;
or ymm1_3@uint64 ymm1_3 ymm3_3;
(* vpsrlq $0x3a,%ymm5,%ymm3                        #! PC = 0x555555579e1d *)
shr ymm3_0 ymm5_0 0x3a@uint64;
shr ymm3_1 ymm5_1 0x3a@uint64;
shr ymm3_2 ymm5_2 0x3a@uint64;
shr ymm3_3 ymm5_3 0x3a@uint64;
(* vpsllq $0x6,%ymm5,%ymm5                         #! PC = 0x555555579e22 *)
shl ymm5_0 ymm5_0 0x6@uint64;
shl ymm5_1 ymm5_1 0x6@uint64;
shl ymm5_2 ymm5_2 0x6@uint64;
shl ymm5_3 ymm5_3 0x6@uint64;
(* vpsllq $0x12,%ymm4,%ymm4                        #! PC = 0x555555579e27 *)
shl ymm4_0 ymm4_0 0x12@uint64;
shl ymm4_1 ymm4_1 0x12@uint64;
shl ymm4_2 ymm4_2 0x12@uint64;
shl ymm4_3 ymm4_3 0x12@uint64;
(* vpor   %ymm5,%ymm3,%ymm3                        #! PC = 0x555555579e2c *)
or ymm3_0@uint64 ymm3_0 ymm5_0;
or ymm3_1@uint64 ymm3_1 ymm5_1;
or ymm3_2@uint64 ymm3_2 ymm5_2;
or ymm3_3@uint64 ymm3_3 ymm5_3;
(* vpxor  -0x1b0(%rbp),%ymm13,%ymm5                #! EA = L0x7fffffffbda0; Value = 0xdf8594cb00c06448; PC = 0x555555579e30 *)
xor ymm5_0@uint64 ymm13_0 L0x7fffffffbda0;
xor ymm5_1@uint64 ymm13_1 L0x7fffffffbda8;
xor ymm5_2@uint64 ymm13_2 L0x7fffffffbdb0;
xor ymm5_3@uint64 ymm13_3 L0x7fffffffbdb8;
(* vpor   %ymm4,%ymm7,%ymm4                        #! PC = 0x555555579e38 *)
or ymm4_0@uint64 ymm7_0 ymm4_0;
or ymm4_1@uint64 ymm7_1 ymm4_1;
or ymm4_2@uint64 ymm7_2 ymm4_2;
or ymm4_3@uint64 ymm7_3 ymm4_3;
(* vpsrlq $0x27,%ymm5,%ymm12                       #! PC = 0x555555579e3c *)
shr ymm12_0 ymm5_0 0x27@uint64;
shr ymm12_1 ymm5_1 0x27@uint64;
shr ymm12_2 ymm5_2 0x27@uint64;
shr ymm12_3 ymm5_3 0x27@uint64;
(* vpsllq $0x19,%ymm5,%ymm6                        #! PC = 0x555555579e41 *)
shl ymm6_0 ymm5_0 0x19@uint64;
shl ymm6_1 ymm5_1 0x19@uint64;
shl ymm6_2 ymm5_2 0x19@uint64;
shl ymm6_3 ymm5_3 0x19@uint64;
(* vpor   %ymm6,%ymm12,%ymm6                       #! PC = 0x555555579e46 *)
or ymm6_0@uint64 ymm12_0 ymm6_0;
or ymm6_1@uint64 ymm12_1 ymm6_1;
or ymm6_2@uint64 ymm12_2 ymm6_2;
or ymm6_3@uint64 ymm12_3 ymm6_3;
(* vpandn %ymm6,%ymm3,%ymm5                        #! PC = 0x555555579e4a *)
not ymm3_0n@uint64 ymm3_0;
and ymm5_0@uint64 ymm3_0n ymm6_0;
not ymm3_1n@uint64 ymm3_1;
and ymm5_1@uint64 ymm3_1n ymm6_1;
not ymm3_2n@uint64 ymm3_2;
and ymm5_2@uint64 ymm3_2n ymm6_2;
not ymm3_3n@uint64 ymm3_3;
and ymm5_3@uint64 ymm3_3n ymm6_3;
(* vpxor  %ymm1,%ymm5,%ymm12                       #! PC = 0x555555579e4e *)
xor ymm12_0@uint64 ymm5_0 ymm1_0;
xor ymm12_1@uint64 ymm5_1 ymm1_1;
xor ymm12_2@uint64 ymm5_2 ymm1_2;
xor ymm12_3@uint64 ymm5_3 ymm1_3;
(* vpxor  -0x210(%rbp),%ymm11,%ymm5                #! EA = L0x7fffffffbd40; Value = 0x6a81ca651589b15e; PC = 0x555555579e52 *)
xor ymm5_0@uint64 ymm11_0 L0x7fffffffbd40;
xor ymm5_1@uint64 ymm11_1 L0x7fffffffbd48;
xor ymm5_2@uint64 ymm11_2 L0x7fffffffbd50;
xor ymm5_3@uint64 ymm11_3 L0x7fffffffbd58;
(* vmovdqa %ymm12,-0xb0(%rbp)                      #! EA = L0x7fffffffbea0; PC = 0x555555579e5a *)
mov L0x7fffffffbea0 ymm12_0;
mov L0x7fffffffbea8 ymm12_1;
mov L0x7fffffffbeb0 ymm12_2;
mov L0x7fffffffbeb8 ymm12_3;
(* vpshufb 0x54095(%rip),%ymm5,%ymm5        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x555555579e62 *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm5_0, ymm5_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm5_2, ymm5_3, tmp_2, tmp_3);
mov ymm5_0 tmp_0;
mov ymm5_1 tmp_1;
mov ymm5_2 tmp_2;
mov ymm5_3 tmp_3;
(* vpandn %ymm4,%ymm5,%ymm12                       #! PC = 0x555555579e6b *)
not ymm5_0n@uint64 ymm5_0;
and ymm12_0@uint64 ymm5_0n ymm4_0;
not ymm5_1n@uint64 ymm5_1;
and ymm12_1@uint64 ymm5_1n ymm4_1;
not ymm5_2n@uint64 ymm5_2;
and ymm12_2@uint64 ymm5_2n ymm4_2;
not ymm5_3n@uint64 ymm5_3;
and ymm12_3@uint64 ymm5_3n ymm4_3;
(* vpandn %ymm5,%ymm6,%ymm9                        #! PC = 0x555555579e6f *)
not ymm6_0n@uint64 ymm6_0;
and ymm9_0@uint64 ymm6_0n ymm5_0;
not ymm6_1n@uint64 ymm6_1;
and ymm9_1@uint64 ymm6_1n ymm5_1;
not ymm6_2n@uint64 ymm6_2;
and ymm9_2@uint64 ymm6_2n ymm5_2;
not ymm6_3n@uint64 ymm6_3;
and ymm9_3@uint64 ymm6_3n ymm5_3;
(* vpxor  %ymm6,%ymm12,%ymm12                      #! PC = 0x555555579e73 *)
xor ymm12_0@uint64 ymm12_0 ymm6_0;
xor ymm12_1@uint64 ymm12_1 ymm6_1;
xor ymm12_2@uint64 ymm12_2 ymm6_2;
xor ymm12_3@uint64 ymm12_3 ymm6_3;
(* vpandn %ymm1,%ymm4,%ymm6                        #! PC = 0x555555579e77 *)
not ymm4_0n@uint64 ymm4_0;
and ymm6_0@uint64 ymm4_0n ymm1_0;
not ymm4_1n@uint64 ymm4_1;
and ymm6_1@uint64 ymm4_1n ymm1_1;
not ymm4_2n@uint64 ymm4_2;
and ymm6_2@uint64 ymm4_2n ymm1_2;
not ymm4_3n@uint64 ymm4_3;
and ymm6_3@uint64 ymm4_3n ymm1_3;
(* vpandn %ymm3,%ymm1,%ymm1                        #! PC = 0x555555579e7b *)
not ymm1_0n@uint64 ymm1_0;
and ymm1_0@uint64 ymm1_0n ymm3_0;
not ymm1_1n@uint64 ymm1_1;
and ymm1_1@uint64 ymm1_1n ymm3_1;
not ymm1_2n@uint64 ymm1_2;
and ymm1_2@uint64 ymm1_2n ymm3_2;
not ymm1_3n@uint64 ymm1_3;
and ymm1_3@uint64 ymm1_3n ymm3_3;
(* vpxor  %ymm5,%ymm6,%ymm6                        #! PC = 0x555555579e7f *)
xor ymm6_0@uint64 ymm6_0 ymm5_0;
xor ymm6_1@uint64 ymm6_1 ymm5_1;
xor ymm6_2@uint64 ymm6_2 ymm5_2;
xor ymm6_3@uint64 ymm6_3 ymm5_3;
(* vpxor  %ymm4,%ymm1,%ymm5                        #! PC = 0x555555579e83 *)
xor ymm5_0@uint64 ymm1_0 ymm4_0;
xor ymm5_1@uint64 ymm1_1 ymm4_1;
xor ymm5_2@uint64 ymm1_2 ymm4_2;
xor ymm5_3@uint64 ymm1_3 ymm4_3;
(* vpxor  -0x110(%rbp),%ymm11,%ymm1                #! EA = L0x7fffffffbe40; Value = 0xf0388aed43035577; PC = 0x555555579e87 *)
xor ymm1_0@uint64 ymm11_0 L0x7fffffffbe40;
xor ymm1_1@uint64 ymm11_1 L0x7fffffffbe48;
xor ymm1_2@uint64 ymm11_2 L0x7fffffffbe50;
xor ymm1_3@uint64 ymm11_3 L0x7fffffffbe58;
(* vmovdqa %ymm5,-0x210(%rbp)                      #! EA = L0x7fffffffbd40; PC = 0x555555579e8f *)
mov L0x7fffffffbd40 ymm5_0;
mov L0x7fffffffbd48 ymm5_1;
mov L0x7fffffffbd50 ymm5_2;
mov L0x7fffffffbd58 ymm5_3;
(* vpxor  %ymm3,%ymm9,%ymm15                       #! PC = 0x555555579e97 *)
xor ymm15_0@uint64 ymm9_0 ymm3_0;
xor ymm15_1@uint64 ymm9_1 ymm3_1;
xor ymm15_2@uint64 ymm9_2 ymm3_2;
xor ymm15_3@uint64 ymm9_3 ymm3_3;
(* vpxor  -0x1d0(%rbp),%ymm11,%ymm11               #! EA = L0x7fffffffbd80; Value = 0xb22a75014d2fbb08; PC = 0x555555579e9b *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbd80;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbd88;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbd90;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbd98;
(* vpsrlq $0x25,%ymm1,%ymm5                        #! PC = 0x555555579ea3 *)
shr ymm5_0 ymm1_0 0x25@uint64;
shr ymm5_1 ymm1_1 0x25@uint64;
shr ymm5_2 ymm1_2 0x25@uint64;
shr ymm5_3 ymm1_3 0x25@uint64;
(* vpsllq $0x1b,%ymm1,%ymm1                        #! PC = 0x555555579ea8 *)
shl ymm1_0 ymm1_0 0x1b@uint64;
shl ymm1_1 ymm1_1 0x1b@uint64;
shl ymm1_2 ymm1_2 0x1b@uint64;
shl ymm1_3 ymm1_3 0x1b@uint64;
(* vmovdqa %ymm6,-0x1b0(%rbp)                      #! EA = L0x7fffffffbda0; PC = 0x555555579ead *)
mov L0x7fffffffbda0 ymm6_0;
mov L0x7fffffffbda8 ymm6_1;
mov L0x7fffffffbdb0 ymm6_2;
mov L0x7fffffffbdb8 ymm6_3;
(* vpor   %ymm1,%ymm5,%ymm5                        #! PC = 0x555555579eb5 *)
or ymm5_0@uint64 ymm5_0 ymm1_0;
or ymm5_1@uint64 ymm5_1 ymm1_1;
or ymm5_2@uint64 ymm5_2 ymm1_2;
or ymm5_3@uint64 ymm5_3 ymm1_3;
(* vpxor  -0x130(%rbp),%ymm2,%ymm1                 #! EA = L0x7fffffffbe20; Value = 0x76c4ebd02adac11a; PC = 0x555555579eb9 *)
xor ymm1_0@uint64 ymm2_0 L0x7fffffffbe20;
xor ymm1_1@uint64 ymm2_1 L0x7fffffffbe28;
xor ymm1_2@uint64 ymm2_2 L0x7fffffffbe30;
xor ymm1_3@uint64 ymm2_3 L0x7fffffffbe38;
(* vmovdqa %ymm15,-0x150(%rbp)                     #! EA = L0x7fffffffbe00; PC = 0x555555579ec1 *)
mov L0x7fffffffbe00 ymm15_0;
mov L0x7fffffffbe08 ymm15_1;
mov L0x7fffffffbe10 ymm15_2;
mov L0x7fffffffbe18 ymm15_3;
(* vpxor  -0x1f0(%rbp),%ymm2,%ymm2                 #! EA = L0x7fffffffbd60; Value = 0x96cf2f872c22d061; PC = 0x555555579ec9 *)
xor ymm2_0@uint64 ymm2_0 L0x7fffffffbd60;
xor ymm2_1@uint64 ymm2_1 L0x7fffffffbd68;
xor ymm2_2@uint64 ymm2_2 L0x7fffffffbd70;
xor ymm2_3@uint64 ymm2_3 L0x7fffffffbd78;
(* vpsrlq $0x1c,%ymm1,%ymm9                        #! PC = 0x555555579ed1 *)
shr ymm9_0 ymm1_0 0x1c@uint64;
shr ymm9_1 ymm1_1 0x1c@uint64;
shr ymm9_2 ymm1_2 0x1c@uint64;
shr ymm9_3 ymm1_3 0x1c@uint64;
(* vpsllq $0x24,%ymm1,%ymm1                        #! PC = 0x555555579ed6 *)
shl ymm1_0 ymm1_0 0x24@uint64;
shl ymm1_1 ymm1_1 0x24@uint64;
shl ymm1_2 ymm1_2 0x24@uint64;
shl ymm1_3 ymm1_3 0x24@uint64;
(* vpor   %ymm1,%ymm9,%ymm9                        #! PC = 0x555555579edb *)
or ymm9_0@uint64 ymm9_0 ymm1_0;
or ymm9_1@uint64 ymm9_1 ymm1_1;
or ymm9_2@uint64 ymm9_2 ymm1_2;
or ymm9_3@uint64 ymm9_3 ymm1_3;
(* vpxor  -0x70(%rbp),%ymm0,%ymm1                  #! EA = L0x7fffffffbee0; Value = 0xedb9f9068bf73fef; PC = 0x555555579edf *)
xor ymm1_0@uint64 ymm0_0 L0x7fffffffbee0;
xor ymm1_1@uint64 ymm0_1 L0x7fffffffbee8;
xor ymm1_2@uint64 ymm0_2 L0x7fffffffbef0;
xor ymm1_3@uint64 ymm0_3 L0x7fffffffbef8;
(* vpsrlq $0x36,%ymm1,%ymm10                       #! PC = 0x555555579ee4 *)
shr ymm10_0 ymm1_0 0x36@uint64;
shr ymm10_1 ymm1_1 0x36@uint64;
shr ymm10_2 ymm1_2 0x36@uint64;
shr ymm10_3 ymm1_3 0x36@uint64;
(* vpsllq $0xa,%ymm1,%ymm1                         #! PC = 0x555555579ee9 *)
shl ymm1_0 ymm1_0 0xa@uint64;
shl ymm1_1 ymm1_1 0xa@uint64;
shl ymm1_2 ymm1_2 0xa@uint64;
shl ymm1_3 ymm1_3 0xa@uint64;
(* vpor   %ymm1,%ymm10,%ymm10                      #! PC = 0x555555579eee *)
or ymm10_0@uint64 ymm10_0 ymm1_0;
or ymm10_1@uint64 ymm10_1 ymm1_1;
or ymm10_2@uint64 ymm10_2 ymm1_2;
or ymm10_3@uint64 ymm10_3 ymm1_3;
(* vpxor  -0x50(%rbp),%ymm14,%ymm1                 #! EA = L0x7fffffffbf00; Value = 0xced2c9104a8bc9a6; PC = 0x555555579ef2 *)
xor ymm1_0@uint64 ymm14_0 L0x7fffffffbf00;
xor ymm1_1@uint64 ymm14_1 L0x7fffffffbf08;
xor ymm1_2@uint64 ymm14_2 L0x7fffffffbf10;
xor ymm1_3@uint64 ymm14_3 L0x7fffffffbf18;
(* vpxor  -0xd0(%rbp),%ymm14,%ymm14                #! EA = L0x7fffffffbe80; Value = 0xeff0aac9862e6863; PC = 0x555555579ef7 *)
xor ymm14_0@uint64 ymm14_0 L0x7fffffffbe80;
xor ymm14_1@uint64 ymm14_1 L0x7fffffffbe88;
xor ymm14_2@uint64 ymm14_2 L0x7fffffffbe90;
xor ymm14_3@uint64 ymm14_3 L0x7fffffffbe98;
(* vpandn %ymm10,%ymm9,%ymm3                       #! PC = 0x555555579eff *)
not ymm9_0n@uint64 ymm9_0;
and ymm3_0@uint64 ymm9_0n ymm10_0;
not ymm9_1n@uint64 ymm9_1;
and ymm3_1@uint64 ymm9_1n ymm10_1;
not ymm9_2n@uint64 ymm9_2;
and ymm3_2@uint64 ymm9_2n ymm10_2;
not ymm9_3n@uint64 ymm9_3;
and ymm3_3@uint64 ymm9_3n ymm10_3;
(* vpsrlq $0x31,%ymm1,%ymm7                        #! PC = 0x555555579f04 *)
shr ymm7_0 ymm1_0 0x31@uint64;
shr ymm7_1 ymm1_1 0x31@uint64;
shr ymm7_2 ymm1_2 0x31@uint64;
shr ymm7_3 ymm1_3 0x31@uint64;
(* vpsllq $0xf,%ymm1,%ymm4                         #! PC = 0x555555579f09 *)
shl ymm4_0 ymm1_0 0xf@uint64;
shl ymm4_1 ymm1_1 0xf@uint64;
shl ymm4_2 ymm1_2 0xf@uint64;
shl ymm4_3 ymm1_3 0xf@uint64;
(* vpxor  %ymm5,%ymm3,%ymm6                        #! PC = 0x555555579f0e *)
xor ymm6_0@uint64 ymm3_0 ymm5_0;
xor ymm6_1@uint64 ymm3_1 ymm5_1;
xor ymm6_2@uint64 ymm3_2 ymm5_2;
xor ymm6_3@uint64 ymm3_3 ymm5_3;
(* vpxor  -0x2f0(%rbp),%ymm13,%ymm1                #! EA = L0x7fffffffbc60; Value = 0xcb631a23adca9452; PC = 0x555555579f12 *)
xor ymm1_0@uint64 ymm13_0 L0x7fffffffbc60;
xor ymm1_1@uint64 ymm13_1 L0x7fffffffbc68;
xor ymm1_2@uint64 ymm13_2 L0x7fffffffbc70;
xor ymm1_3@uint64 ymm13_3 L0x7fffffffbc78;
(* vpor   %ymm4,%ymm7,%ymm4                        #! PC = 0x555555579f1a *)
or ymm4_0@uint64 ymm7_0 ymm4_0;
or ymm4_1@uint64 ymm7_1 ymm4_1;
or ymm4_2@uint64 ymm7_2 ymm4_2;
or ymm4_3@uint64 ymm7_3 ymm4_3;
(* vmovdqa %ymm6,%ymm15                            #! PC = 0x555555579f1e *)
mov ymm15_0 ymm6_0;
mov ymm15_1 ymm6_1;
mov ymm15_2 ymm6_2;
mov ymm15_3 ymm6_3;
(* vpxor  -0x170(%rbp),%ymm13,%ymm13               #! EA = L0x7fffffffbde0; Value = 0xafbd5eafaa200412; PC = 0x555555579f22 *)
xor ymm13_0@uint64 ymm13_0 L0x7fffffffbde0;
xor ymm13_1@uint64 ymm13_1 L0x7fffffffbde8;
xor ymm13_2@uint64 ymm13_2 L0x7fffffffbdf0;
xor ymm13_3@uint64 ymm13_3 L0x7fffffffbdf8;
(* vpandn %ymm4,%ymm10,%ymm6                       #! PC = 0x555555579f2a *)
not ymm10_0n@uint64 ymm10_0;
and ymm6_0@uint64 ymm10_0n ymm4_0;
not ymm10_1n@uint64 ymm10_1;
and ymm6_1@uint64 ymm10_1n ymm4_1;
not ymm10_2n@uint64 ymm10_2;
and ymm6_2@uint64 ymm10_2n ymm4_2;
not ymm10_3n@uint64 ymm10_3;
and ymm6_3@uint64 ymm10_3n ymm4_3;
(* vmovdqa %ymm15,-0x110(%rbp)                     #! EA = L0x7fffffffbe40; PC = 0x555555579f2e *)
mov L0x7fffffffbe40 ymm15_0;
mov L0x7fffffffbe48 ymm15_1;
mov L0x7fffffffbe50 ymm15_2;
mov L0x7fffffffbe58 ymm15_3;
(* vpshufb 0x53fa1(%rip),%ymm1,%ymm1        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x555555579f36 *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm1_0, ymm1_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm1_2, ymm1_3, tmp_2, tmp_3);
mov ymm1_0 tmp_0;
mov ymm1_1 tmp_1;
mov ymm1_2 tmp_2;
mov ymm1_3 tmp_3;
(* vpxor  %ymm9,%ymm6,%ymm6                        #! PC = 0x555555579f3f *)
xor ymm6_0@uint64 ymm6_0 ymm9_0;
xor ymm6_1@uint64 ymm6_1 ymm9_1;
xor ymm6_2@uint64 ymm6_2 ymm9_2;
xor ymm6_3@uint64 ymm6_3 ymm9_3;
(* vpandn %ymm1,%ymm4,%ymm3                        #! PC = 0x555555579f44 *)
not ymm4_0n@uint64 ymm4_0;
and ymm3_0@uint64 ymm4_0n ymm1_0;
not ymm4_1n@uint64 ymm4_1;
and ymm3_1@uint64 ymm4_1n ymm1_1;
not ymm4_2n@uint64 ymm4_2;
and ymm3_2@uint64 ymm4_2n ymm1_2;
not ymm4_3n@uint64 ymm4_3;
and ymm3_3@uint64 ymm4_3n ymm1_3;
(* vpxor  %ymm10,%ymm3,%ymm7                       #! PC = 0x555555579f48 *)
xor ymm7_0@uint64 ymm3_0 ymm10_0;
xor ymm7_1@uint64 ymm3_1 ymm10_1;
xor ymm7_2@uint64 ymm3_2 ymm10_2;
xor ymm7_3@uint64 ymm3_3 ymm10_3;
(* vpsrlq $0x2,%ymm14,%ymm3                        #! PC = 0x555555579f4d *)
shr ymm3_0 ymm14_0 0x2@uint64;
shr ymm3_1 ymm14_1 0x2@uint64;
shr ymm3_2 ymm14_2 0x2@uint64;
shr ymm3_3 ymm14_3 0x2@uint64;
(* vmovdqa %ymm7,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x555555579f53 *)
mov L0x7fffffffbf00 ymm7_0;
mov L0x7fffffffbf08 ymm7_1;
mov L0x7fffffffbf10 ymm7_2;
mov L0x7fffffffbf18 ymm7_3;
(* vpandn %ymm5,%ymm1,%ymm7                        #! PC = 0x555555579f58 *)
not ymm1_0n@uint64 ymm1_0;
and ymm7_0@uint64 ymm1_0n ymm5_0;
not ymm1_1n@uint64 ymm1_1;
and ymm7_1@uint64 ymm1_1n ymm5_1;
not ymm1_2n@uint64 ymm1_2;
and ymm7_2@uint64 ymm1_2n ymm5_2;
not ymm1_3n@uint64 ymm1_3;
and ymm7_3@uint64 ymm1_3n ymm5_3;
(* vpandn %ymm9,%ymm5,%ymm5                        #! PC = 0x555555579f5c *)
not ymm5_0n@uint64 ymm5_0;
and ymm5_0@uint64 ymm5_0n ymm9_0;
not ymm5_1n@uint64 ymm5_1;
and ymm5_1@uint64 ymm5_1n ymm9_1;
not ymm5_2n@uint64 ymm5_2;
and ymm5_2@uint64 ymm5_2n ymm9_2;
not ymm5_3n@uint64 ymm5_3;
and ymm5_3@uint64 ymm5_3n ymm9_3;
(* vpxor  %ymm1,%ymm5,%ymm1                        #! PC = 0x555555579f61 *)
xor ymm1_0@uint64 ymm5_0 ymm1_0;
xor ymm1_1@uint64 ymm5_1 ymm1_1;
xor ymm1_2@uint64 ymm5_2 ymm1_2;
xor ymm1_3@uint64 ymm5_3 ymm1_3;
(* vpsllq $0x3e,%ymm14,%ymm14                      #! PC = 0x555555579f65 *)
shl ymm14_0 ymm14_0 0x3e@uint64;
shl ymm14_1 ymm14_1 0x3e@uint64;
shl ymm14_2 ymm14_2 0x3e@uint64;
shl ymm14_3 ymm14_3 0x3e@uint64;
(* vpxor  %ymm4,%ymm7,%ymm7                        #! PC = 0x555555579f6b *)
xor ymm7_0@uint64 ymm7_0 ymm4_0;
xor ymm7_1@uint64 ymm7_1 ymm4_1;
xor ymm7_2@uint64 ymm7_2 ymm4_2;
xor ymm7_3@uint64 ymm7_3 ymm4_3;
(* vmovdqa -0xb0(%rbp),%ymm4                       #! EA = L0x7fffffffbea0; Value = 0xf129d553258ac6c9; PC = 0x555555579f6f *)
mov ymm4_0 L0x7fffffffbea0;
mov ymm4_1 L0x7fffffffbea8;
mov ymm4_2 L0x7fffffffbeb0;
mov ymm4_3 L0x7fffffffbeb8;
(* vmovdqa %ymm1,-0x70(%rbp)                       #! EA = L0x7fffffffbee0; PC = 0x555555579f77 *)
mov L0x7fffffffbee0 ymm1_0;
mov L0x7fffffffbee8 ymm1_1;
mov L0x7fffffffbef0 ymm1_2;
mov L0x7fffffffbef8 ymm1_3;
(* vpsrlq $0x9,%ymm13,%ymm1                        #! PC = 0x555555579f7c *)
shr ymm1_0 ymm13_0 0x9@uint64;
shr ymm1_1 ymm13_1 0x9@uint64;
shr ymm1_2 ymm13_2 0x9@uint64;
shr ymm1_3 ymm13_3 0x9@uint64;
(* vpsllq $0x37,%ymm13,%ymm13                      #! PC = 0x555555579f82 *)
shl ymm13_0 ymm13_0 0x37@uint64;
shl ymm13_1 ymm13_1 0x37@uint64;
shl ymm13_2 ymm13_2 0x37@uint64;
shl ymm13_3 ymm13_3 0x37@uint64;
(* vpor   %ymm14,%ymm3,%ymm3                       #! PC = 0x555555579f88 *)
or ymm3_0@uint64 ymm3_0 ymm14_0;
or ymm3_1@uint64 ymm3_1 ymm14_1;
or ymm3_2@uint64 ymm3_2 ymm14_2;
or ymm3_3@uint64 ymm3_3 ymm14_3;
(* vpor   %ymm13,%ymm1,%ymm13                      #! PC = 0x555555579f8d *)
or ymm13_0@uint64 ymm1_0 ymm13_0;
or ymm13_1@uint64 ymm1_1 ymm13_1;
or ymm13_2@uint64 ymm1_2 ymm13_2;
or ymm13_3@uint64 ymm1_3 ymm13_3;
(* vpsrlq $0x19,%ymm11,%ymm1                       #! PC = 0x555555579f92 *)
shr ymm1_0 ymm11_0 0x19@uint64;
shr ymm1_1 ymm11_1 0x19@uint64;
shr ymm1_2 ymm11_2 0x19@uint64;
shr ymm1_3 ymm11_3 0x19@uint64;
(* vpsllq $0x27,%ymm11,%ymm11                      #! PC = 0x555555579f98 *)
shl ymm11_0 ymm11_0 0x27@uint64;
shl ymm11_1 ymm11_1 0x27@uint64;
shl ymm11_2 ymm11_2 0x27@uint64;
shl ymm11_3 ymm11_3 0x27@uint64;
(* vpor   %ymm11,%ymm1,%ymm11                      #! PC = 0x555555579f9e *)
or ymm11_0@uint64 ymm1_0 ymm11_0;
or ymm11_1@uint64 ymm1_1 ymm11_1;
or ymm11_2@uint64 ymm1_2 ymm11_2;
or ymm11_3@uint64 ymm1_3 ymm11_3;
(* vpxor  -0xf0(%rbp),%ymm4,%ymm1                  #! EA = L0x7fffffffbe60; Value = 0x3329889348e3611e; PC = 0x555555579fa3 *)
xor ymm1_0@uint64 ymm4_0 L0x7fffffffbe60;
xor ymm1_1@uint64 ymm4_1 L0x7fffffffbe68;
xor ymm1_2@uint64 ymm4_2 L0x7fffffffbe70;
xor ymm1_3@uint64 ymm4_3 L0x7fffffffbe78;
(* vpxor  -0x230(%rbp),%ymm8,%ymm4                 #! EA = L0x7fffffffbd20; Value = 0x3e2c73cc1842b6ee; PC = 0x555555579fab *)
xor ymm4_0@uint64 ymm8_0 L0x7fffffffbd20;
xor ymm4_1@uint64 ymm8_1 L0x7fffffffbd28;
xor ymm4_2@uint64 ymm8_2 L0x7fffffffbd30;
xor ymm4_3@uint64 ymm8_3 L0x7fffffffbd38;
(* vpandn %ymm11,%ymm13,%ymm5                      #! PC = 0x555555579fb3 *)
not ymm13_0n@uint64 ymm13_0;
and ymm5_0@uint64 ymm13_0n ymm11_0;
not ymm13_1n@uint64 ymm13_1;
and ymm5_1@uint64 ymm13_1n ymm11_1;
not ymm13_2n@uint64 ymm13_2;
and ymm5_2@uint64 ymm13_2n ymm11_2;
not ymm13_3n@uint64 ymm13_3;
and ymm5_3@uint64 ymm13_3n ymm11_3;
(* vpxor  %ymm3,%ymm5,%ymm5                        #! PC = 0x555555579fb8 *)
xor ymm5_0@uint64 ymm5_0 ymm3_0;
xor ymm5_1@uint64 ymm5_1 ymm3_1;
xor ymm5_2@uint64 ymm5_2 ymm3_2;
xor ymm5_3@uint64 ymm5_3 ymm3_3;
(* vpxor  %ymm15,%ymm5,%ymm9                       #! PC = 0x555555579fbc *)
xor ymm9_0@uint64 ymm5_0 ymm15_0;
xor ymm9_1@uint64 ymm5_1 ymm15_1;
xor ymm9_2@uint64 ymm5_2 ymm15_2;
xor ymm9_3@uint64 ymm5_3 ymm15_3;
(* vpxor  %ymm1,%ymm9,%ymm9                        #! PC = 0x555555579fc1 *)
xor ymm9_0@uint64 ymm9_0 ymm1_0;
xor ymm9_1@uint64 ymm9_1 ymm1_1;
xor ymm9_2@uint64 ymm9_2 ymm1_2;
xor ymm9_3@uint64 ymm9_3 ymm1_3;
(* vpsrlq $0x17,%ymm2,%ymm1                        #! PC = 0x555555579fc5 *)
shr ymm1_0 ymm2_0 0x17@uint64;
shr ymm1_1 ymm2_1 0x17@uint64;
shr ymm1_2 ymm2_2 0x17@uint64;
shr ymm1_3 ymm2_3 0x17@uint64;
(* vpxor  -0x90(%rbp),%ymm9,%ymm9                  #! EA = L0x7fffffffbec0; Value = 0x1dd2da9769da011f; PC = 0x555555579fca *)
xor ymm9_0@uint64 ymm9_0 L0x7fffffffbec0;
xor ymm9_1@uint64 ymm9_1 L0x7fffffffbec8;
xor ymm9_2@uint64 ymm9_2 L0x7fffffffbed0;
xor ymm9_3@uint64 ymm9_3 L0x7fffffffbed8;
(* vpsllq $0x29,%ymm2,%ymm2                        #! PC = 0x555555579fd2 *)
shl ymm2_0 ymm2_0 0x29@uint64;
shl ymm2_1 ymm2_1 0x29@uint64;
shl ymm2_2 ymm2_2 0x29@uint64;
shl ymm2_3 ymm2_3 0x29@uint64;
(* vpor   %ymm2,%ymm1,%ymm2                        #! PC = 0x555555579fd7 *)
or ymm2_0@uint64 ymm1_0 ymm2_0;
or ymm2_1@uint64 ymm1_1 ymm2_1;
or ymm2_2@uint64 ymm1_2 ymm2_2;
or ymm2_3@uint64 ymm1_3 ymm2_3;
(* vpandn %ymm2,%ymm11,%ymm1                       #! PC = 0x555555579fdb *)
not ymm11_0n@uint64 ymm11_0;
and ymm1_0@uint64 ymm11_0n ymm2_0;
not ymm11_1n@uint64 ymm11_1;
and ymm1_1@uint64 ymm11_1n ymm2_1;
not ymm11_2n@uint64 ymm11_2;
and ymm1_2@uint64 ymm11_2n ymm2_2;
not ymm11_3n@uint64 ymm11_3;
and ymm1_3@uint64 ymm11_3n ymm2_3;
(* vpxor  %ymm13,%ymm1,%ymm14                      #! PC = 0x555555579fdf *)
xor ymm14_0@uint64 ymm1_0 ymm13_0;
xor ymm14_1@uint64 ymm1_1 ymm13_1;
xor ymm14_2@uint64 ymm1_2 ymm13_2;
xor ymm14_3@uint64 ymm1_3 ymm13_3;
(* vmovdqa %ymm14,%ymm10                           #! PC = 0x555555579fe4 *)
mov ymm10_0 ymm14_0;
mov ymm10_1 ymm14_1;
mov ymm10_2 ymm14_2;
mov ymm10_3 ymm14_3;
(* vpxor  -0x150(%rbp),%ymm6,%ymm14                #! EA = L0x7fffffffbe00; Value = 0x2ade13fd70a45485; PC = 0x555555579fe9 *)
xor ymm14_0@uint64 ymm6_0 L0x7fffffffbe00;
xor ymm14_1@uint64 ymm6_1 L0x7fffffffbe08;
xor ymm14_2@uint64 ymm6_2 L0x7fffffffbe10;
xor ymm14_3@uint64 ymm6_3 L0x7fffffffbe18;
(* vmovdqa %ymm10,-0x130(%rbp)                     #! EA = L0x7fffffffbe20; PC = 0x555555579ff1 *)
mov L0x7fffffffbe20 ymm10_0;
mov L0x7fffffffbe28 ymm10_1;
mov L0x7fffffffbe30 ymm10_2;
mov L0x7fffffffbe38 ymm10_3;
(* vpxor  -0x310(%rbp),%ymm0,%ymm1                 #! EA = L0x7fffffffbc40; Value = 0xf79babacba12d714; PC = 0x555555579ff9 *)
xor ymm1_0@uint64 ymm0_0 L0x7fffffffbc40;
xor ymm1_1@uint64 ymm0_1 L0x7fffffffbc48;
xor ymm1_2@uint64 ymm0_2 L0x7fffffffbc50;
xor ymm1_3@uint64 ymm0_3 L0x7fffffffbc58;
(* vpxor  -0x50(%rbp),%ymm12,%ymm15                #! EA = L0x7fffffffbf00; Value = 0x7e78ea0caeaf0b0f; PC = 0x55555557a001 *)
xor ymm15_0@uint64 ymm12_0 L0x7fffffffbf00;
xor ymm15_1@uint64 ymm12_1 L0x7fffffffbf08;
xor ymm15_2@uint64 ymm12_2 L0x7fffffffbf10;
xor ymm15_3@uint64 ymm12_3 L0x7fffffffbf18;
(* vpxor  %ymm4,%ymm14,%ymm14                      #! PC = 0x55555557a006 *)
xor ymm14_0@uint64 ymm14_0 ymm4_0;
xor ymm14_1@uint64 ymm14_1 ymm4_1;
xor ymm14_2@uint64 ymm14_2 ymm4_2;
xor ymm14_3@uint64 ymm14_3 ymm4_3;
(* vpsllq $0x2,%ymm1,%ymm0                         #! PC = 0x55555557a00a *)
shl ymm0_0 ymm1_0 0x2@uint64;
shl ymm0_1 ymm1_1 0x2@uint64;
shl ymm0_2 ymm1_2 0x2@uint64;
shl ymm0_3 ymm1_3 0x2@uint64;
(* vpsrlq $0x3e,%ymm1,%ymm4                        #! PC = 0x55555557a00f *)
shr ymm4_0 ymm1_0 0x3e@uint64;
shr ymm4_1 ymm1_1 0x3e@uint64;
shr ymm4_2 ymm1_2 0x3e@uint64;
shr ymm4_3 ymm1_3 0x3e@uint64;
(* vpxor  %ymm10,%ymm14,%ymm14                     #! PC = 0x55555557a014 *)
xor ymm14_0@uint64 ymm14_0 ymm10_0;
xor ymm14_1@uint64 ymm14_1 ymm10_1;
xor ymm14_2@uint64 ymm14_2 ymm10_2;
xor ymm14_3@uint64 ymm14_3 ymm10_3;
(* vmovdqa -0x2b0(%rbp),%ymm10                     #! EA = L0x7fffffffbca0; Value = 0x9b74448952711318; PC = 0x55555557a019 *)
mov ymm10_0 L0x7fffffffbca0;
mov ymm10_1 L0x7fffffffbca8;
mov ymm10_2 L0x7fffffffbcb0;
mov ymm10_3 L0x7fffffffbcb8;
(* vpor   %ymm0,%ymm4,%ymm4                        #! PC = 0x55555557a021 *)
or ymm4_0@uint64 ymm4_0 ymm0_0;
or ymm4_1@uint64 ymm4_1 ymm0_1;
or ymm4_2@uint64 ymm4_2 ymm0_2;
or ymm4_3@uint64 ymm4_3 ymm0_3;
(* vpandn %ymm4,%ymm2,%ymm1                        #! PC = 0x55555557a025 *)
not ymm2_0n@uint64 ymm2_0;
and ymm1_0@uint64 ymm2_0n ymm4_0;
not ymm2_1n@uint64 ymm2_1;
and ymm1_1@uint64 ymm2_1n ymm4_1;
not ymm2_2n@uint64 ymm2_2;
and ymm1_2@uint64 ymm2_2n ymm4_2;
not ymm2_3n@uint64 ymm2_3;
and ymm1_3@uint64 ymm2_3n ymm4_3;
(* vpxor  %ymm11,%ymm1,%ymm1                       #! PC = 0x55555557a029 *)
xor ymm1_0@uint64 ymm1_0 ymm11_0;
xor ymm1_1@uint64 ymm1_1 ymm11_1;
xor ymm1_2@uint64 ymm1_2 ymm11_2;
xor ymm1_3@uint64 ymm1_3 ymm11_3;
(* vmovdqa -0x190(%rbp),%ymm11                     #! EA = L0x7fffffffbdc0; Value = 0x274f2636a613dacd; PC = 0x55555557a02e *)
mov ymm11_0 L0x7fffffffbdc0;
mov ymm11_1 L0x7fffffffbdc8;
mov ymm11_2 L0x7fffffffbdd0;
mov ymm11_3 L0x7fffffffbdd8;
(* vpxor  -0x250(%rbp),%ymm11,%ymm0                #! EA = L0x7fffffffbd00; Value = 0x11acb1eb1af15940; PC = 0x55555557a036 *)
xor ymm0_0@uint64 ymm11_0 L0x7fffffffbd00;
xor ymm0_1@uint64 ymm11_1 L0x7fffffffbd08;
xor ymm0_2@uint64 ymm11_2 L0x7fffffffbd10;
xor ymm0_3@uint64 ymm11_3 L0x7fffffffbd18;
(* vpxor  %ymm0,%ymm15,%ymm15                      #! PC = 0x55555557a03e *)
xor ymm15_0@uint64 ymm15_0 ymm0_0;
xor ymm15_1@uint64 ymm15_1 ymm0_1;
xor ymm15_2@uint64 ymm15_2 ymm0_2;
xor ymm15_3@uint64 ymm15_3 ymm0_3;
(* vpandn %ymm3,%ymm4,%ymm0                        #! PC = 0x55555557a042 *)
not ymm4_0n@uint64 ymm4_0;
and ymm0_0@uint64 ymm4_0n ymm3_0;
not ymm4_1n@uint64 ymm4_1;
and ymm0_1@uint64 ymm4_1n ymm3_1;
not ymm4_2n@uint64 ymm4_2;
and ymm0_2@uint64 ymm4_2n ymm3_2;
not ymm4_3n@uint64 ymm4_3;
and ymm0_3@uint64 ymm4_3n ymm3_3;
(* vpxor  %ymm2,%ymm0,%ymm2                        #! PC = 0x55555557a046 *)
xor ymm2_0@uint64 ymm0_0 ymm2_0;
xor ymm2_1@uint64 ymm0_1 ymm2_1;
xor ymm2_2@uint64 ymm0_2 ymm2_2;
xor ymm2_3@uint64 ymm0_3 ymm2_3;
(* vpxor  -0x270(%rbp),%ymm10,%ymm0                #! EA = L0x7fffffffbce0; Value = 0xdacf89ea70fddb42; PC = 0x55555557a04a *)
xor ymm0_0@uint64 ymm10_0 L0x7fffffffbce0;
xor ymm0_1@uint64 ymm10_1 L0x7fffffffbce8;
xor ymm0_2@uint64 ymm10_2 L0x7fffffffbcf0;
xor ymm0_3@uint64 ymm10_3 L0x7fffffffbcf8;
(* vpxor  %ymm1,%ymm15,%ymm15                      #! PC = 0x55555557a052 *)
xor ymm15_0@uint64 ymm15_0 ymm1_0;
xor ymm15_1@uint64 ymm15_1 ymm1_1;
xor ymm15_2@uint64 ymm15_2 ymm1_2;
xor ymm15_3@uint64 ymm15_3 ymm1_3;
(* vpxor  %ymm2,%ymm7,%ymm11                       #! PC = 0x55555557a056 *)
xor ymm11_0@uint64 ymm7_0 ymm2_0;
xor ymm11_1@uint64 ymm7_1 ymm2_1;
xor ymm11_2@uint64 ymm7_2 ymm2_2;
xor ymm11_3@uint64 ymm7_3 ymm2_3;
(* vmovdqa %ymm2,-0x1f0(%rbp)                      #! EA = L0x7fffffffbd60; PC = 0x55555557a05a *)
mov L0x7fffffffbd60 ymm2_0;
mov L0x7fffffffbd68 ymm2_1;
mov L0x7fffffffbd70 ymm2_2;
mov L0x7fffffffbd78 ymm2_3;
(* vpxor  %ymm0,%ymm11,%ymm11                      #! PC = 0x55555557a062 *)
xor ymm11_0@uint64 ymm11_0 ymm0_0;
xor ymm11_1@uint64 ymm11_1 ymm0_1;
xor ymm11_2@uint64 ymm11_2 ymm0_2;
xor ymm11_3@uint64 ymm11_3 ymm0_3;
(* vpandn %ymm13,%ymm3,%ymm0                       #! PC = 0x55555557a066 *)
not ymm3_0n@uint64 ymm3_0;
and ymm0_0@uint64 ymm3_0n ymm13_0;
not ymm3_1n@uint64 ymm3_1;
and ymm0_1@uint64 ymm3_1n ymm13_1;
not ymm3_2n@uint64 ymm3_2;
and ymm0_2@uint64 ymm3_2n ymm13_2;
not ymm3_3n@uint64 ymm3_3;
and ymm0_3@uint64 ymm3_3n ymm13_3;
(* vmovdqa -0x2d0(%rbp),%ymm13                     #! EA = L0x7fffffffbc80; Value = 0x841de7cdc8f8ba3d; PC = 0x55555557a06b *)
mov ymm13_0 L0x7fffffffbc80;
mov ymm13_1 L0x7fffffffbc88;
mov ymm13_2 L0x7fffffffbc90;
mov ymm13_3 L0x7fffffffbc98;
(* vpxor  -0x290(%rbp),%ymm13,%ymm2                #! EA = L0x7fffffffbcc0; Value = 0x76cd1ada7bddb391; PC = 0x55555557a073 *)
xor ymm2_0@uint64 ymm13_0 L0x7fffffffbcc0;
xor ymm2_1@uint64 ymm13_1 L0x7fffffffbcc8;
xor ymm2_2@uint64 ymm13_2 L0x7fffffffbcd0;
xor ymm2_3@uint64 ymm13_3 L0x7fffffffbcd8;
(* vpxor  %ymm4,%ymm0,%ymm0                        #! PC = 0x55555557a07b *)
xor ymm0_0@uint64 ymm0_0 ymm4_0;
xor ymm0_1@uint64 ymm0_1 ymm4_1;
xor ymm0_2@uint64 ymm0_2 ymm4_2;
xor ymm0_3@uint64 ymm0_3 ymm4_3;
(* vpxor  -0x210(%rbp),%ymm0,%ymm10                #! EA = L0x7fffffffbd40; Value = 0xbfc5ae08ed82cf6b; PC = 0x55555557a07f *)
xor ymm10_0@uint64 ymm0_0 L0x7fffffffbd40;
xor ymm10_1@uint64 ymm0_1 L0x7fffffffbd48;
xor ymm10_2@uint64 ymm0_2 L0x7fffffffbd50;
xor ymm10_3@uint64 ymm0_3 L0x7fffffffbd58;
(* vpxor  -0x1b0(%rbp),%ymm11,%ymm11               #! EA = L0x7fffffffbda0; Value = 0xdef7504ff47bd92e; PC = 0x55555557a087 *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbda0;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbda8;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbdb0;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbdb8;
(* vpsrlq $0x3f,%ymm14,%ymm3                       #! PC = 0x55555557a08f *)
shr ymm3_0 ymm14_0 0x3f@uint64;
shr ymm3_1 ymm14_1 0x3f@uint64;
shr ymm3_2 ymm14_2 0x3f@uint64;
shr ymm3_3 ymm14_3 0x3f@uint64;
(* vpsllq $0x1,%ymm15,%ymm4                        #! PC = 0x55555557a095 *)
shl ymm4_0 ymm15_0 0x1@uint64;
shl ymm4_1 ymm15_1 0x1@uint64;
shl ymm4_2 ymm15_2 0x1@uint64;
shl ymm4_3 ymm15_3 0x1@uint64;
(* vpxor  %ymm2,%ymm10,%ymm10                      #! PC = 0x55555557a09b *)
xor ymm10_0@uint64 ymm10_0 ymm2_0;
xor ymm10_1@uint64 ymm10_1 ymm2_1;
xor ymm10_2@uint64 ymm10_2 ymm2_2;
xor ymm10_3@uint64 ymm10_3 ymm2_3;
(* vpsllq $0x1,%ymm14,%ymm2                        #! PC = 0x55555557a09f *)
shl ymm2_0 ymm14_0 0x1@uint64;
shl ymm2_1 ymm14_1 0x1@uint64;
shl ymm2_2 ymm14_2 0x1@uint64;
shl ymm2_3 ymm14_3 0x1@uint64;
(* vpxor  -0x70(%rbp),%ymm10,%ymm10                #! EA = L0x7fffffffbee0; Value = 0x5c0794ebaa3e4dfd; PC = 0x55555557a0a5 *)
xor ymm10_0@uint64 ymm10_0 L0x7fffffffbee0;
xor ymm10_1@uint64 ymm10_1 L0x7fffffffbee8;
xor ymm10_2@uint64 ymm10_2 L0x7fffffffbef0;
xor ymm10_3@uint64 ymm10_3 L0x7fffffffbef8;
(* vpor   %ymm2,%ymm3,%ymm3                        #! PC = 0x55555557a0aa *)
or ymm3_0@uint64 ymm3_0 ymm2_0;
or ymm3_1@uint64 ymm3_1 ymm2_1;
or ymm3_2@uint64 ymm3_2 ymm2_2;
or ymm3_3@uint64 ymm3_3 ymm2_3;
(* vpsrlq $0x3f,%ymm15,%ymm2                       #! PC = 0x55555557a0ae *)
shr ymm2_0 ymm15_0 0x3f@uint64;
shr ymm2_1 ymm15_1 0x3f@uint64;
shr ymm2_2 ymm15_2 0x3f@uint64;
shr ymm2_3 ymm15_3 0x3f@uint64;
(* vpsllq $0x1,%ymm11,%ymm13                       #! PC = 0x55555557a0b4 *)
shl ymm13_0 ymm11_0 0x1@uint64;
shl ymm13_1 ymm11_1 0x1@uint64;
shl ymm13_2 ymm11_2 0x1@uint64;
shl ymm13_3 ymm11_3 0x1@uint64;
(* vpor   %ymm4,%ymm2,%ymm2                        #! PC = 0x55555557a0ba *)
or ymm2_0@uint64 ymm2_0 ymm4_0;
or ymm2_1@uint64 ymm2_1 ymm4_1;
or ymm2_2@uint64 ymm2_2 ymm4_2;
or ymm2_3@uint64 ymm2_3 ymm4_3;
(* vpxor  %ymm10,%ymm3,%ymm3                       #! PC = 0x55555557a0be *)
xor ymm3_0@uint64 ymm3_0 ymm10_0;
xor ymm3_1@uint64 ymm3_1 ymm10_1;
xor ymm3_2@uint64 ymm3_2 ymm10_2;
xor ymm3_3@uint64 ymm3_3 ymm10_3;
(* vpsrlq $0x3f,%ymm11,%ymm4                       #! PC = 0x55555557a0c3 *)
shr ymm4_0 ymm11_0 0x3f@uint64;
shr ymm4_1 ymm11_1 0x3f@uint64;
shr ymm4_2 ymm11_2 0x3f@uint64;
shr ymm4_3 ymm11_3 0x3f@uint64;
(* vpxor  %ymm9,%ymm2,%ymm2                        #! PC = 0x55555557a0c9 *)
xor ymm2_0@uint64 ymm2_0 ymm9_0;
xor ymm2_1@uint64 ymm2_1 ymm9_1;
xor ymm2_2@uint64 ymm2_2 ymm9_2;
xor ymm2_3@uint64 ymm2_3 ymm9_3;
(* vpxor  %ymm3,%ymm5,%ymm5                        #! PC = 0x55555557a0ce *)
xor ymm5_0@uint64 ymm5_0 ymm3_0;
xor ymm5_1@uint64 ymm5_1 ymm3_1;
xor ymm5_2@uint64 ymm5_2 ymm3_2;
xor ymm5_3@uint64 ymm5_3 ymm3_3;
(* vpor   %ymm13,%ymm4,%ymm4                       #! PC = 0x55555557a0d2 *)
or ymm4_0@uint64 ymm4_0 ymm13_0;
or ymm4_1@uint64 ymm4_1 ymm13_1;
or ymm4_2@uint64 ymm4_2 ymm13_2;
or ymm4_3@uint64 ymm4_3 ymm13_3;
(* vpxor  %ymm2,%ymm8,%ymm8                        #! PC = 0x55555557a0d7 *)
xor ymm8_0@uint64 ymm8_0 ymm2_0;
xor ymm8_1@uint64 ymm8_1 ymm2_1;
xor ymm8_2@uint64 ymm8_2 ymm2_2;
xor ymm8_3@uint64 ymm8_3 ymm2_3;
(* vmovq  %r13,%xmm13                              #! PC = 0x55555557a0db *)
mov xmm13_0 r13;
mov xmm13_1 0@uint64;
(* vpxor  %ymm14,%ymm4,%ymm14                      #! PC = 0x55555557a0e0 *)
xor ymm14_0@uint64 ymm4_0 ymm14_0;
xor ymm14_1@uint64 ymm4_1 ymm14_1;
xor ymm14_2@uint64 ymm4_2 ymm14_2;
xor ymm14_3@uint64 ymm4_3 ymm14_3;
(* vpsrlq $0x3f,%ymm10,%ymm4                       #! PC = 0x55555557a0e5 *)
shr ymm4_0 ymm10_0 0x3f@uint64;
shr ymm4_1 ymm10_1 0x3f@uint64;
shr ymm4_2 ymm10_2 0x3f@uint64;
shr ymm4_3 ymm10_3 0x3f@uint64;
(* vpxor  %ymm2,%ymm6,%ymm6                        #! PC = 0x55555557a0eb *)
xor ymm6_0@uint64 ymm6_0 ymm2_0;
xor ymm6_1@uint64 ymm6_1 ymm2_1;
xor ymm6_2@uint64 ymm6_2 ymm2_2;
xor ymm6_3@uint64 ymm6_3 ymm2_3;
(* vpsllq $0x1,%ymm10,%ymm10                       #! PC = 0x55555557a0ef *)
shl ymm10_0 ymm10_0 0x1@uint64;
shl ymm10_1 ymm10_1 0x1@uint64;
shl ymm10_2 ymm10_2 0x1@uint64;
shl ymm10_3 ymm10_3 0x1@uint64;
(* vpxor  %ymm14,%ymm12,%ymm12                     #! PC = 0x55555557a0f5 *)
xor ymm12_0@uint64 ymm12_0 ymm14_0;
xor ymm12_1@uint64 ymm12_1 ymm14_1;
xor ymm12_2@uint64 ymm12_2 ymm14_2;
xor ymm12_3@uint64 ymm12_3 ymm14_3;
(* vpxor  %ymm14,%ymm1,%ymm1                       #! PC = 0x55555557a0fa *)
xor ymm1_0@uint64 ymm1_0 ymm14_0;
xor ymm1_1@uint64 ymm1_1 ymm14_1;
xor ymm1_2@uint64 ymm1_2 ymm14_2;
xor ymm1_3@uint64 ymm1_3 ymm14_3;
(* vpor   %ymm10,%ymm4,%ymm10                      #! PC = 0x55555557a0ff *)
or ymm10_0@uint64 ymm4_0 ymm10_0;
or ymm10_1@uint64 ymm4_1 ymm10_1;
or ymm10_2@uint64 ymm4_2 ymm10_2;
or ymm10_3@uint64 ymm4_3 ymm10_3;
(* vpsrlq $0x3f,%ymm9,%ymm4                        #! PC = 0x55555557a104 *)
shr ymm4_0 ymm9_0 0x3f@uint64;
shr ymm4_1 ymm9_1 0x3f@uint64;
shr ymm4_2 ymm9_2 0x3f@uint64;
shr ymm4_3 ymm9_3 0x3f@uint64;
(* vpsllq $0x1,%ymm9,%ymm9                         #! PC = 0x55555557a10a *)
shl ymm9_0 ymm9_0 0x1@uint64;
shl ymm9_1 ymm9_1 0x1@uint64;
shl ymm9_2 ymm9_2 0x1@uint64;
shl ymm9_3 ymm9_3 0x1@uint64;
(* vpxor  %ymm15,%ymm10,%ymm15                     #! PC = 0x55555557a110 *)
xor ymm15_0@uint64 ymm10_0 ymm15_0;
xor ymm15_1@uint64 ymm10_1 ymm15_1;
xor ymm15_2@uint64 ymm10_2 ymm15_2;
xor ymm15_3@uint64 ymm10_3 ymm15_3;
(* vpbroadcastq %xmm13,%ymm10                      #! PC = 0x55555557a115 *)
mov ymm10_0 xmm13_0;
mov ymm10_1 xmm13_0;
mov ymm10_2 xmm13_0;
mov ymm10_3 xmm13_0;
(* vpor   %ymm9,%ymm4,%ymm9                        #! PC = 0x55555557a11a *)
or ymm9_0@uint64 ymm4_0 ymm9_0;
or ymm9_1@uint64 ymm4_1 ymm9_1;
or ymm9_2@uint64 ymm4_2 ymm9_2;
or ymm9_3@uint64 ymm4_3 ymm9_3;
(* vpsrlq $0x14,%ymm8,%ymm4                        #! PC = 0x55555557a11f *)
shr ymm4_0 ymm8_0 0x14@uint64;
shr ymm4_1 ymm8_1 0x14@uint64;
shr ymm4_2 ymm8_2 0x14@uint64;
shr ymm4_3 ymm8_3 0x14@uint64;
(* vpxor  %ymm15,%ymm7,%ymm7                       #! PC = 0x55555557a125 *)
xor ymm7_0@uint64 ymm7_0 ymm15_0;
xor ymm7_1@uint64 ymm7_1 ymm15_1;
xor ymm7_2@uint64 ymm7_2 ymm15_2;
xor ymm7_3@uint64 ymm7_3 ymm15_3;
(* vpsllq $0x2c,%ymm8,%ymm8                        #! PC = 0x55555557a12a *)
shl ymm8_0 ymm8_0 0x2c@uint64;
shl ymm8_1 ymm8_1 0x2c@uint64;
shl ymm8_2 ymm8_2 0x2c@uint64;
shl ymm8_3 ymm8_3 0x2c@uint64;
(* vpxor  %ymm11,%ymm9,%ymm11                      #! PC = 0x55555557a130 *)
xor ymm11_0@uint64 ymm9_0 ymm11_0;
xor ymm11_1@uint64 ymm9_1 ymm11_1;
xor ymm11_2@uint64 ymm9_2 ymm11_2;
xor ymm11_3@uint64 ymm9_3 ymm11_3;
(* vpxor  -0x90(%rbp),%ymm3,%ymm9                  #! EA = L0x7fffffffbec0; Value = 0x1dd2da9769da011f; PC = 0x55555557a135 *)
xor ymm9_0@uint64 ymm3_0 L0x7fffffffbec0;
xor ymm9_1@uint64 ymm3_1 L0x7fffffffbec8;
xor ymm9_2@uint64 ymm3_2 L0x7fffffffbed0;
xor ymm9_3@uint64 ymm3_3 L0x7fffffffbed8;
(* vpor   %ymm8,%ymm4,%ymm8                        #! PC = 0x55555557a13d *)
or ymm8_0@uint64 ymm4_0 ymm8_0;
or ymm8_1@uint64 ymm4_1 ymm8_1;
or ymm8_2@uint64 ymm4_2 ymm8_2;
or ymm8_3@uint64 ymm4_3 ymm8_3;
(* vpsrlq $0x15,%ymm12,%ymm4                       #! PC = 0x55555557a142 *)
shr ymm4_0 ymm12_0 0x15@uint64;
shr ymm4_1 ymm12_1 0x15@uint64;
shr ymm4_2 ymm12_2 0x15@uint64;
shr ymm4_3 ymm12_3 0x15@uint64;
(* vpxor  %ymm11,%ymm0,%ymm0                       #! PC = 0x55555557a148 *)
xor ymm0_0@uint64 ymm0_0 ymm11_0;
xor ymm0_1@uint64 ymm0_1 ymm11_1;
xor ymm0_2@uint64 ymm0_2 ymm11_2;
xor ymm0_3@uint64 ymm0_3 ymm11_3;
(* vpsllq $0x2b,%ymm12,%ymm12                      #! PC = 0x55555557a14d *)
shl ymm12_0 ymm12_0 0x2b@uint64;
shl ymm12_1 ymm12_1 0x2b@uint64;
shl ymm12_2 ymm12_2 0x2b@uint64;
shl ymm12_3 ymm12_3 0x2b@uint64;
(* vpor   %ymm12,%ymm4,%ymm12                      #! PC = 0x55555557a153 *)
or ymm12_0@uint64 ymm4_0 ymm12_0;
or ymm12_1@uint64 ymm4_1 ymm12_1;
or ymm12_2@uint64 ymm4_2 ymm12_2;
or ymm12_3@uint64 ymm4_3 ymm12_3;
(* vpandn %ymm12,%ymm8,%ymm4                       #! PC = 0x55555557a158 *)
not ymm8_0n@uint64 ymm8_0;
and ymm4_0@uint64 ymm8_0n ymm12_0;
not ymm8_1n@uint64 ymm8_1;
and ymm4_1@uint64 ymm8_1n ymm12_1;
not ymm8_2n@uint64 ymm8_2;
and ymm4_2@uint64 ymm8_2n ymm12_2;
not ymm8_3n@uint64 ymm8_3;
and ymm4_3@uint64 ymm8_3n ymm12_3;
(* vpxor  %ymm10,%ymm4,%ymm4                       #! PC = 0x55555557a15d *)
xor ymm4_0@uint64 ymm4_0 ymm10_0;
xor ymm4_1@uint64 ymm4_1 ymm10_1;
xor ymm4_2@uint64 ymm4_2 ymm10_2;
xor ymm4_3@uint64 ymm4_3 ymm10_3;
(* vpsrlq $0x13,%ymm6,%ymm10                       #! PC = 0x55555557a162 *)
shr ymm10_0 ymm6_0 0x13@uint64;
shr ymm10_1 ymm6_1 0x13@uint64;
shr ymm10_2 ymm6_2 0x13@uint64;
shr ymm10_3 ymm6_3 0x13@uint64;
(* vpxor  %ymm9,%ymm4,%ymm13                       #! PC = 0x55555557a167 *)
xor ymm13_0@uint64 ymm4_0 ymm9_0;
xor ymm13_1@uint64 ymm4_1 ymm9_1;
xor ymm13_2@uint64 ymm4_2 ymm9_2;
xor ymm13_3@uint64 ymm4_3 ymm9_3;
(* vpsrlq $0x2b,%ymm7,%ymm4                        #! PC = 0x55555557a16c *)
shr ymm4_0 ymm7_0 0x2b@uint64;
shr ymm4_1 ymm7_1 0x2b@uint64;
shr ymm4_2 ymm7_2 0x2b@uint64;
shr ymm4_3 ymm7_3 0x2b@uint64;
(* vpsllq $0x15,%ymm7,%ymm7                        #! PC = 0x55555557a171 *)
shl ymm7_0 ymm7_0 0x15@uint64;
shl ymm7_1 ymm7_1 0x15@uint64;
shl ymm7_2 ymm7_2 0x15@uint64;
shl ymm7_3 ymm7_3 0x15@uint64;
(* vmovdqa %ymm13,-0x90(%rbp)                      #! EA = L0x7fffffffbec0; PC = 0x55555557a176 *)
mov L0x7fffffffbec0 ymm13_0;
mov L0x7fffffffbec8 ymm13_1;
mov L0x7fffffffbed0 ymm13_2;
mov L0x7fffffffbed8 ymm13_3;
(* vpsllq $0x2d,%ymm6,%ymm6                        #! PC = 0x55555557a17e *)
shl ymm6_0 ymm6_0 0x2d@uint64;
shl ymm6_1 ymm6_1 0x2d@uint64;
shl ymm6_2 ymm6_2 0x2d@uint64;
shl ymm6_3 ymm6_3 0x2d@uint64;
(* vpor   %ymm7,%ymm4,%ymm7                        #! PC = 0x55555557a183 *)
or ymm7_0@uint64 ymm4_0 ymm7_0;
or ymm7_1@uint64 ymm4_1 ymm7_1;
or ymm7_2@uint64 ymm4_2 ymm7_2;
or ymm7_3@uint64 ymm4_3 ymm7_3;
(* vpor   %ymm6,%ymm10,%ymm6                       #! PC = 0x55555557a187 *)
or ymm6_0@uint64 ymm10_0 ymm6_0;
or ymm6_1@uint64 ymm10_1 ymm6_1;
or ymm6_2@uint64 ymm10_2 ymm6_2;
or ymm6_3@uint64 ymm10_3 ymm6_3;
(* vpandn %ymm7,%ymm12,%ymm4                       #! PC = 0x55555557a18b *)
not ymm12_0n@uint64 ymm12_0;
and ymm4_0@uint64 ymm12_0n ymm7_0;
not ymm12_1n@uint64 ymm12_1;
and ymm4_1@uint64 ymm12_1n ymm7_1;
not ymm12_2n@uint64 ymm12_2;
and ymm4_2@uint64 ymm12_2n ymm7_2;
not ymm12_3n@uint64 ymm12_3;
and ymm4_3@uint64 ymm12_3n ymm7_3;
(* vpxor  %ymm8,%ymm4,%ymm13                       #! PC = 0x55555557a18f *)
xor ymm13_0@uint64 ymm4_0 ymm8_0;
xor ymm13_1@uint64 ymm4_1 ymm8_1;
xor ymm13_2@uint64 ymm4_2 ymm8_2;
xor ymm13_3@uint64 ymm4_3 ymm8_3;
(* vpsrlq $0x32,%ymm0,%ymm4                        #! PC = 0x55555557a194 *)
shr ymm4_0 ymm0_0 0x32@uint64;
shr ymm4_1 ymm0_1 0x32@uint64;
shr ymm4_2 ymm0_2 0x32@uint64;
shr ymm4_3 ymm0_3 0x32@uint64;
(* vpsllq $0xe,%ymm0,%ymm0                         #! PC = 0x55555557a199 *)
shl ymm0_0 ymm0_0 0xe@uint64;
shl ymm0_1 ymm0_1 0xe@uint64;
shl ymm0_2 ymm0_2 0xe@uint64;
shl ymm0_3 ymm0_3 0xe@uint64;
(* vmovdqa %ymm13,-0xd0(%rbp)                      #! EA = L0x7fffffffbe80; PC = 0x55555557a19e *)
mov L0x7fffffffbe80 ymm13_0;
mov L0x7fffffffbe88 ymm13_1;
mov L0x7fffffffbe90 ymm13_2;
mov L0x7fffffffbe98 ymm13_3;
(* vpor   %ymm0,%ymm4,%ymm0                        #! PC = 0x55555557a1a6 *)
or ymm0_0@uint64 ymm4_0 ymm0_0;
or ymm0_1@uint64 ymm4_1 ymm0_1;
or ymm0_2@uint64 ymm4_2 ymm0_2;
or ymm0_3@uint64 ymm4_3 ymm0_3;
(* vpandn %ymm0,%ymm7,%ymm4                        #! PC = 0x55555557a1aa *)
not ymm7_0n@uint64 ymm7_0;
and ymm4_0@uint64 ymm7_0n ymm0_0;
not ymm7_1n@uint64 ymm7_1;
and ymm4_1@uint64 ymm7_1n ymm0_1;
not ymm7_2n@uint64 ymm7_2;
and ymm4_2@uint64 ymm7_2n ymm0_2;
not ymm7_3n@uint64 ymm7_3;
and ymm4_3@uint64 ymm7_3n ymm0_3;
(* vpxor  %ymm12,%ymm4,%ymm12                      #! PC = 0x55555557a1ae *)
xor ymm12_0@uint64 ymm4_0 ymm12_0;
xor ymm12_1@uint64 ymm4_1 ymm12_1;
xor ymm12_2@uint64 ymm4_2 ymm12_2;
xor ymm12_3@uint64 ymm4_3 ymm12_3;
(* vpandn %ymm9,%ymm0,%ymm4                        #! PC = 0x55555557a1b3 *)
not ymm0_0n@uint64 ymm0_0;
and ymm4_0@uint64 ymm0_0n ymm9_0;
not ymm0_1n@uint64 ymm0_1;
and ymm4_1@uint64 ymm0_1n ymm9_1;
not ymm0_2n@uint64 ymm0_2;
and ymm4_2@uint64 ymm0_2n ymm9_2;
not ymm0_3n@uint64 ymm0_3;
and ymm4_3@uint64 ymm0_3n ymm9_3;
(* vpandn %ymm8,%ymm9,%ymm9                        #! PC = 0x55555557a1b8 *)
not ymm9_0n@uint64 ymm9_0;
and ymm9_0@uint64 ymm9_0n ymm8_0;
not ymm9_1n@uint64 ymm9_1;
and ymm9_1@uint64 ymm9_1n ymm8_1;
not ymm9_2n@uint64 ymm9_2;
and ymm9_2@uint64 ymm9_2n ymm8_2;
not ymm9_3n@uint64 ymm9_3;
and ymm9_3@uint64 ymm9_3n ymm8_3;
(* vpxor  %ymm0,%ymm9,%ymm8                        #! PC = 0x55555557a1bd *)
xor ymm8_0@uint64 ymm9_0 ymm0_0;
xor ymm8_1@uint64 ymm9_1 ymm0_1;
xor ymm8_2@uint64 ymm9_2 ymm0_2;
xor ymm8_3@uint64 ymm9_3 ymm0_3;
(* vmovdqa %ymm12,-0x2f0(%rbp)                     #! EA = L0x7fffffffbc60; PC = 0x55555557a1c1 *)
mov L0x7fffffffbc60 ymm12_0;
mov L0x7fffffffbc68 ymm12_1;
mov L0x7fffffffbc70 ymm12_2;
mov L0x7fffffffbc78 ymm12_3;
(* vpxor  %ymm7,%ymm4,%ymm12                       #! PC = 0x55555557a1c9 *)
xor ymm12_0@uint64 ymm4_0 ymm7_0;
xor ymm12_1@uint64 ymm4_1 ymm7_1;
xor ymm12_2@uint64 ymm4_2 ymm7_2;
xor ymm12_3@uint64 ymm4_3 ymm7_3;
(* vpxor  -0x270(%rbp),%ymm15,%ymm4                #! EA = L0x7fffffffbce0; Value = 0xdacf89ea70fddb42; PC = 0x55555557a1cd *)
xor ymm4_0@uint64 ymm15_0 L0x7fffffffbce0;
xor ymm4_1@uint64 ymm15_1 L0x7fffffffbce8;
xor ymm4_2@uint64 ymm15_2 L0x7fffffffbcf0;
xor ymm4_3@uint64 ymm15_3 L0x7fffffffbcf8;
(* vmovdqa %ymm12,-0x170(%rbp)                     #! EA = L0x7fffffffbde0; PC = 0x55555557a1d5 *)
mov L0x7fffffffbde0 ymm12_0;
mov L0x7fffffffbde8 ymm12_1;
mov L0x7fffffffbdf0 ymm12_2;
mov L0x7fffffffbdf8 ymm12_3;
(* vpxor  -0x2d0(%rbp),%ymm11,%ymm12               #! EA = L0x7fffffffbc80; Value = 0x841de7cdc8f8ba3d; PC = 0x55555557a1dd *)
xor ymm12_0@uint64 ymm11_0 L0x7fffffffbc80;
xor ymm12_1@uint64 ymm11_1 L0x7fffffffbc88;
xor ymm12_2@uint64 ymm11_2 L0x7fffffffbc90;
xor ymm12_3@uint64 ymm11_3 L0x7fffffffbc98;
(* vmovdqa %ymm8,-0x1d0(%rbp)                      #! EA = L0x7fffffffbd80; PC = 0x55555557a1e5 *)
mov L0x7fffffffbd80 ymm8_0;
mov L0x7fffffffbd88 ymm8_1;
mov L0x7fffffffbd90 ymm8_2;
mov L0x7fffffffbd98 ymm8_3;
(* vpxor  -0xb0(%rbp),%ymm3,%ymm8                  #! EA = L0x7fffffffbea0; Value = 0xf129d553258ac6c9; PC = 0x55555557a1ed *)
xor ymm8_0@uint64 ymm3_0 L0x7fffffffbea0;
xor ymm8_1@uint64 ymm3_1 L0x7fffffffbea8;
xor ymm8_2@uint64 ymm3_2 L0x7fffffffbeb0;
xor ymm8_3@uint64 ymm3_3 L0x7fffffffbeb8;
(* vpsrlq $0x24,%ymm4,%ymm0                        #! PC = 0x55555557a1f5 *)
shr ymm0_0 ymm4_0 0x24@uint64;
shr ymm0_1 ymm4_1 0x24@uint64;
shr ymm0_2 ymm4_2 0x24@uint64;
shr ymm0_3 ymm4_3 0x24@uint64;
(* vpsllq $0x1c,%ymm4,%ymm4                        #! PC = 0x55555557a1fa *)
shl ymm4_0 ymm4_0 0x1c@uint64;
shl ymm4_1 ymm4_1 0x1c@uint64;
shl ymm4_2 ymm4_2 0x1c@uint64;
shl ymm4_3 ymm4_3 0x1c@uint64;
(* vpor   %ymm4,%ymm0,%ymm0                        #! PC = 0x55555557a1ff *)
or ymm0_0@uint64 ymm0_0 ymm4_0;
or ymm0_1@uint64 ymm0_1 ymm4_1;
or ymm0_2@uint64 ymm0_2 ymm4_2;
or ymm0_3@uint64 ymm0_3 ymm4_3;
(* vpsrlq $0x3d,%ymm8,%ymm7                        #! PC = 0x55555557a203 *)
shr ymm7_0 ymm8_0 0x3d@uint64;
shr ymm7_1 ymm8_1 0x3d@uint64;
shr ymm7_2 ymm8_2 0x3d@uint64;
shr ymm7_3 ymm8_3 0x3d@uint64;
(* vpsrlq $0x2c,%ymm12,%ymm4                       #! PC = 0x55555557a209 *)
shr ymm4_0 ymm12_0 0x2c@uint64;
shr ymm4_1 ymm12_1 0x2c@uint64;
shr ymm4_2 ymm12_2 0x2c@uint64;
shr ymm4_3 ymm12_3 0x2c@uint64;
(* vpsllq $0x3,%ymm8,%ymm8                         #! PC = 0x55555557a20f *)
shl ymm8_0 ymm8_0 0x3@uint64;
shl ymm8_1 ymm8_1 0x3@uint64;
shl ymm8_2 ymm8_2 0x3@uint64;
shl ymm8_3 ymm8_3 0x3@uint64;
(* vpsllq $0x14,%ymm12,%ymm12                      #! PC = 0x55555557a215 *)
shl ymm12_0 ymm12_0 0x14@uint64;
shl ymm12_1 ymm12_1 0x14@uint64;
shl ymm12_2 ymm12_2 0x14@uint64;
shl ymm12_3 ymm12_3 0x14@uint64;
(* vpor   %ymm8,%ymm7,%ymm7                        #! PC = 0x55555557a21b *)
or ymm7_0@uint64 ymm7_0 ymm8_0;
or ymm7_1@uint64 ymm7_1 ymm8_1;
or ymm7_2@uint64 ymm7_2 ymm8_2;
or ymm7_3@uint64 ymm7_3 ymm8_3;
(* vpor   %ymm12,%ymm4,%ymm4                       #! PC = 0x55555557a220 *)
or ymm4_0@uint64 ymm4_0 ymm12_0;
or ymm4_1@uint64 ymm4_1 ymm12_1;
or ymm4_2@uint64 ymm4_2 ymm12_2;
or ymm4_3@uint64 ymm4_3 ymm12_3;
(* vpandn %ymm7,%ymm4,%ymm8                        #! PC = 0x55555557a225 *)
not ymm4_0n@uint64 ymm4_0;
and ymm8_0@uint64 ymm4_0n ymm7_0;
not ymm4_1n@uint64 ymm4_1;
and ymm8_1@uint64 ymm4_1n ymm7_1;
not ymm4_2n@uint64 ymm4_2;
and ymm8_2@uint64 ymm4_2n ymm7_2;
not ymm4_3n@uint64 ymm4_3;
and ymm8_3@uint64 ymm4_3n ymm7_3;
(* vpxor  %ymm0,%ymm8,%ymm9                        #! PC = 0x55555557a229 *)
xor ymm9_0@uint64 ymm8_0 ymm0_0;
xor ymm9_1@uint64 ymm8_1 ymm0_1;
xor ymm9_2@uint64 ymm8_2 ymm0_2;
xor ymm9_3@uint64 ymm8_3 ymm0_3;
(* vpandn %ymm6,%ymm7,%ymm8                        #! PC = 0x55555557a22d *)
not ymm7_0n@uint64 ymm7_0;
and ymm8_0@uint64 ymm7_0n ymm6_0;
not ymm7_1n@uint64 ymm7_1;
and ymm8_1@uint64 ymm7_1n ymm6_1;
not ymm7_2n@uint64 ymm7_2;
and ymm8_2@uint64 ymm7_2n ymm6_2;
not ymm7_3n@uint64 ymm7_3;
and ymm8_3@uint64 ymm7_3n ymm6_3;
(* vmovdqa %ymm9,%ymm13                            #! PC = 0x55555557a231 *)
mov ymm13_0 ymm9_0;
mov ymm13_1 ymm9_1;
mov ymm13_2 ymm9_2;
mov ymm13_3 ymm9_3;
(* vpsrlq $0x3,%ymm1,%ymm9                         #! PC = 0x55555557a236 *)
shr ymm9_0 ymm1_0 0x3@uint64;
shr ymm9_1 ymm1_1 0x3@uint64;
shr ymm9_2 ymm1_2 0x3@uint64;
shr ymm9_3 ymm1_3 0x3@uint64;
(* vpxor  %ymm4,%ymm8,%ymm8                        #! PC = 0x55555557a23b *)
xor ymm8_0@uint64 ymm8_0 ymm4_0;
xor ymm8_1@uint64 ymm8_1 ymm4_1;
xor ymm8_2@uint64 ymm8_2 ymm4_2;
xor ymm8_3@uint64 ymm8_3 ymm4_3;
(* vpsllq $0x3d,%ymm1,%ymm1                        #! PC = 0x55555557a23f *)
shl ymm1_0 ymm1_0 0x3d@uint64;
shl ymm1_1 ymm1_1 0x3d@uint64;
shl ymm1_2 ymm1_2 0x3d@uint64;
shl ymm1_3 ymm1_3 0x3d@uint64;
(* vpor   %ymm1,%ymm9,%ymm1                        #! PC = 0x55555557a244 *)
or ymm1_0@uint64 ymm9_0 ymm1_0;
or ymm1_1@uint64 ymm9_1 ymm1_1;
or ymm1_2@uint64 ymm9_2 ymm1_2;
or ymm1_3@uint64 ymm9_3 ymm1_3;
(* vpandn %ymm1,%ymm6,%ymm9                        #! PC = 0x55555557a248 *)
not ymm6_0n@uint64 ymm6_0;
and ymm9_0@uint64 ymm6_0n ymm1_0;
not ymm6_1n@uint64 ymm6_1;
and ymm9_1@uint64 ymm6_1n ymm1_1;
not ymm6_2n@uint64 ymm6_2;
and ymm9_2@uint64 ymm6_2n ymm1_2;
not ymm6_3n@uint64 ymm6_3;
and ymm9_3@uint64 ymm6_3n ymm1_3;
(* vpandn %ymm0,%ymm1,%ymm10                       #! PC = 0x55555557a24c *)
not ymm1_0n@uint64 ymm1_0;
and ymm10_0@uint64 ymm1_0n ymm0_0;
not ymm1_1n@uint64 ymm1_1;
and ymm10_1@uint64 ymm1_1n ymm0_1;
not ymm1_2n@uint64 ymm1_2;
and ymm10_2@uint64 ymm1_2n ymm0_2;
not ymm1_3n@uint64 ymm1_3;
and ymm10_3@uint64 ymm1_3n ymm0_3;
(* vpandn %ymm4,%ymm0,%ymm0                        #! PC = 0x55555557a250 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm4_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm4_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm4_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm4_3;
(* vpxor  %ymm7,%ymm9,%ymm9                        #! PC = 0x55555557a254 *)
xor ymm9_0@uint64 ymm9_0 ymm7_0;
xor ymm9_1@uint64 ymm9_1 ymm7_1;
xor ymm9_2@uint64 ymm9_2 ymm7_2;
xor ymm9_3@uint64 ymm9_3 ymm7_3;
(* vpxor  -0x190(%rbp),%ymm14,%ymm4                #! EA = L0x7fffffffbdc0; Value = 0x274f2636a613dacd; PC = 0x55555557a258 *)
xor ymm4_0@uint64 ymm14_0 L0x7fffffffbdc0;
xor ymm4_1@uint64 ymm14_1 L0x7fffffffbdc8;
xor ymm4_2@uint64 ymm14_2 L0x7fffffffbdd0;
xor ymm4_3@uint64 ymm14_3 L0x7fffffffbdd8;
(* vmovdqa %ymm9,-0xb0(%rbp)                       #! EA = L0x7fffffffbea0; PC = 0x55555557a260 *)
mov L0x7fffffffbea0 ymm9_0;
mov L0x7fffffffbea8 ymm9_1;
mov L0x7fffffffbeb0 ymm9_2;
mov L0x7fffffffbeb8 ymm9_3;
(* vpxor  %ymm6,%ymm10,%ymm9                       #! PC = 0x55555557a268 *)
xor ymm9_0@uint64 ymm10_0 ymm6_0;
xor ymm9_1@uint64 ymm10_1 ymm6_1;
xor ymm9_2@uint64 ymm10_2 ymm6_2;
xor ymm9_3@uint64 ymm10_3 ymm6_3;
(* vpxor  %ymm1,%ymm0,%ymm6                        #! PC = 0x55555557a26c *)
xor ymm6_0@uint64 ymm0_0 ymm1_0;
xor ymm6_1@uint64 ymm0_1 ymm1_1;
xor ymm6_2@uint64 ymm0_2 ymm1_2;
xor ymm6_3@uint64 ymm0_3 ymm1_3;
(* vpxor  -0x230(%rbp),%ymm2,%ymm1                 #! EA = L0x7fffffffbd20; Value = 0x3e2c73cc1842b6ee; PC = 0x55555557a270 *)
xor ymm1_0@uint64 ymm2_0 L0x7fffffffbd20;
xor ymm1_1@uint64 ymm2_1 L0x7fffffffbd28;
xor ymm1_2@uint64 ymm2_2 L0x7fffffffbd30;
xor ymm1_3@uint64 ymm2_3 L0x7fffffffbd38;
(* vmovdqa %ymm6,-0x2d0(%rbp)                      #! EA = L0x7fffffffbc80; PC = 0x55555557a278 *)
mov L0x7fffffffbc80 ymm6_0;
mov L0x7fffffffbc88 ymm6_1;
mov L0x7fffffffbc90 ymm6_2;
mov L0x7fffffffbc98 ymm6_3;
(* vpsrlq $0x3f,%ymm1,%ymm0                        #! PC = 0x55555557a280 *)
shr ymm0_0 ymm1_0 0x3f@uint64;
shr ymm0_1 ymm1_1 0x3f@uint64;
shr ymm0_2 ymm1_2 0x3f@uint64;
shr ymm0_3 ymm1_3 0x3f@uint64;
(* vpsllq $0x1,%ymm1,%ymm1                         #! PC = 0x55555557a285 *)
shl ymm1_0 ymm1_0 0x1@uint64;
shl ymm1_1 ymm1_1 0x1@uint64;
shl ymm1_2 ymm1_2 0x1@uint64;
shl ymm1_3 ymm1_3 0x1@uint64;
(* vmovdqa %ymm9,-0x270(%rbp)                      #! EA = L0x7fffffffbce0; PC = 0x55555557a28a *)
mov L0x7fffffffbce0 ymm9_0;
mov L0x7fffffffbce8 ymm9_1;
mov L0x7fffffffbcf0 ymm9_2;
mov L0x7fffffffbcf8 ymm9_3;
(* vpor   %ymm1,%ymm0,%ymm0                        #! PC = 0x55555557a292 *)
or ymm0_0@uint64 ymm0_0 ymm1_0;
or ymm0_1@uint64 ymm0_1 ymm1_1;
or ymm0_2@uint64 ymm0_2 ymm1_2;
or ymm0_3@uint64 ymm0_3 ymm1_3;
(* vpsrlq $0x3a,%ymm4,%ymm1                        #! PC = 0x55555557a296 *)
shr ymm1_0 ymm4_0 0x3a@uint64;
shr ymm1_1 ymm4_1 0x3a@uint64;
shr ymm1_2 ymm4_2 0x3a@uint64;
shr ymm1_3 ymm4_3 0x3a@uint64;
(* vpsllq $0x6,%ymm4,%ymm4                         #! PC = 0x55555557a29b *)
shl ymm4_0 ymm4_0 0x6@uint64;
shl ymm4_1 ymm4_1 0x6@uint64;
shl ymm4_2 ymm4_2 0x6@uint64;
shl ymm4_3 ymm4_3 0x6@uint64;
(* vpor   %ymm4,%ymm1,%ymm1                        #! PC = 0x55555557a2a0 *)
or ymm1_0@uint64 ymm1_0 ymm4_0;
or ymm1_1@uint64 ymm1_1 ymm4_1;
or ymm1_2@uint64 ymm1_2 ymm4_2;
or ymm1_3@uint64 ymm1_3 ymm4_3;
(* vpxor  -0x1b0(%rbp),%ymm15,%ymm4                #! EA = L0x7fffffffbda0; Value = 0xdef7504ff47bd92e; PC = 0x55555557a2a4 *)
xor ymm4_0@uint64 ymm15_0 L0x7fffffffbda0;
xor ymm4_1@uint64 ymm15_1 L0x7fffffffbda8;
xor ymm4_2@uint64 ymm15_2 L0x7fffffffbdb0;
xor ymm4_3@uint64 ymm15_3 L0x7fffffffbdb8;
(* vpsrlq $0x27,%ymm4,%ymm12                       #! PC = 0x55555557a2ac *)
shr ymm12_0 ymm4_0 0x27@uint64;
shr ymm12_1 ymm4_1 0x27@uint64;
shr ymm12_2 ymm4_2 0x27@uint64;
shr ymm12_3 ymm4_3 0x27@uint64;
(* vpsllq $0x19,%ymm4,%ymm6                        #! PC = 0x55555557a2b1 *)
shl ymm6_0 ymm4_0 0x19@uint64;
shl ymm6_1 ymm4_1 0x19@uint64;
shl ymm6_2 ymm4_2 0x19@uint64;
shl ymm6_3 ymm4_3 0x19@uint64;
(* vpor   %ymm6,%ymm12,%ymm6                       #! PC = 0x55555557a2b6 *)
or ymm6_0@uint64 ymm12_0 ymm6_0;
or ymm6_1@uint64 ymm12_1 ymm6_1;
or ymm6_2@uint64 ymm12_2 ymm6_2;
or ymm6_3@uint64 ymm12_3 ymm6_3;
(* vpandn %ymm6,%ymm1,%ymm4                        #! PC = 0x55555557a2ba *)
not ymm1_0n@uint64 ymm1_0;
and ymm4_0@uint64 ymm1_0n ymm6_0;
not ymm1_1n@uint64 ymm1_1;
and ymm4_1@uint64 ymm1_1n ymm6_1;
not ymm1_2n@uint64 ymm1_2;
and ymm4_2@uint64 ymm1_2n ymm6_2;
not ymm1_3n@uint64 ymm1_3;
and ymm4_3@uint64 ymm1_3n ymm6_3;
(* vpxor  %ymm0,%ymm4,%ymm4                        #! PC = 0x55555557a2be *)
xor ymm4_0@uint64 ymm4_0 ymm0_0;
xor ymm4_1@uint64 ymm4_1 ymm0_1;
xor ymm4_2@uint64 ymm4_2 ymm0_2;
xor ymm4_3@uint64 ymm4_3 ymm0_3;
(* vmovdqa %ymm4,-0x190(%rbp)                      #! EA = L0x7fffffffbdc0; PC = 0x55555557a2c2 *)
mov L0x7fffffffbdc0 ymm4_0;
mov L0x7fffffffbdc8 ymm4_1;
mov L0x7fffffffbdd0 ymm4_2;
mov L0x7fffffffbdd8 ymm4_3;
(* vpxor  -0x70(%rbp),%ymm11,%ymm4                 #! EA = L0x7fffffffbee0; Value = 0x5c0794ebaa3e4dfd; PC = 0x55555557a2ca *)
xor ymm4_0@uint64 ymm11_0 L0x7fffffffbee0;
xor ymm4_1@uint64 ymm11_1 L0x7fffffffbee8;
xor ymm4_2@uint64 ymm11_2 L0x7fffffffbef0;
xor ymm4_3@uint64 ymm11_3 L0x7fffffffbef8;
(* vpshufb 0x53c28(%rip),%ymm4,%ymm4        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x55555557a2cf *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm4_0, ymm4_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm4_2, ymm4_3, tmp_2, tmp_3);
mov ymm4_0 tmp_0;
mov ymm4_1 tmp_1;
mov ymm4_2 tmp_2;
mov ymm4_3 tmp_3;
(* vpandn %ymm4,%ymm6,%ymm7                        #! PC = 0x55555557a2d8 *)
not ymm6_0n@uint64 ymm6_0;
and ymm7_0@uint64 ymm6_0n ymm4_0;
not ymm6_1n@uint64 ymm6_1;
and ymm7_1@uint64 ymm6_1n ymm4_1;
not ymm6_2n@uint64 ymm6_2;
and ymm7_2@uint64 ymm6_2n ymm4_2;
not ymm6_3n@uint64 ymm6_3;
and ymm7_3@uint64 ymm6_3n ymm4_3;
(* vpxor  %ymm1,%ymm7,%ymm12                       #! PC = 0x55555557a2dc *)
xor ymm12_0@uint64 ymm7_0 ymm1_0;
xor ymm12_1@uint64 ymm7_1 ymm1_1;
xor ymm12_2@uint64 ymm7_2 ymm1_2;
xor ymm12_3@uint64 ymm7_3 ymm1_3;
(* vpsrlq $0x2e,%ymm5,%ymm7                        #! PC = 0x55555557a2e0 *)
shr ymm7_0 ymm5_0 0x2e@uint64;
shr ymm7_1 ymm5_1 0x2e@uint64;
shr ymm7_2 ymm5_2 0x2e@uint64;
shr ymm7_3 ymm5_3 0x2e@uint64;
(* vpsllq $0x12,%ymm5,%ymm5                        #! PC = 0x55555557a2e5 *)
shl ymm5_0 ymm5_0 0x12@uint64;
shl ymm5_1 ymm5_1 0x12@uint64;
shl ymm5_2 ymm5_2 0x12@uint64;
shl ymm5_3 ymm5_3 0x12@uint64;
(* vmovdqa %ymm12,-0x230(%rbp)                     #! EA = L0x7fffffffbd20; PC = 0x55555557a2ea *)
mov L0x7fffffffbd20 ymm12_0;
mov L0x7fffffffbd28 ymm12_1;
mov L0x7fffffffbd30 ymm12_2;
mov L0x7fffffffbd38 ymm12_3;
(* vpor   %ymm5,%ymm7,%ymm5                        #! PC = 0x55555557a2f2 *)
or ymm5_0@uint64 ymm7_0 ymm5_0;
or ymm5_1@uint64 ymm7_1 ymm5_1;
or ymm5_2@uint64 ymm7_2 ymm5_2;
or ymm5_3@uint64 ymm7_3 ymm5_3;
(* vpandn %ymm5,%ymm4,%ymm12                       #! PC = 0x55555557a2f6 *)
not ymm4_0n@uint64 ymm4_0;
and ymm12_0@uint64 ymm4_0n ymm5_0;
not ymm4_1n@uint64 ymm4_1;
and ymm12_1@uint64 ymm4_1n ymm5_1;
not ymm4_2n@uint64 ymm4_2;
and ymm12_2@uint64 ymm4_2n ymm5_2;
not ymm4_3n@uint64 ymm4_3;
and ymm12_3@uint64 ymm4_3n ymm5_3;
(* vpxor  %ymm6,%ymm12,%ymm12                      #! PC = 0x55555557a2fa *)
xor ymm12_0@uint64 ymm12_0 ymm6_0;
xor ymm12_1@uint64 ymm12_1 ymm6_1;
xor ymm12_2@uint64 ymm12_2 ymm6_2;
xor ymm12_3@uint64 ymm12_3 ymm6_3;
(* vpandn %ymm0,%ymm5,%ymm6                        #! PC = 0x55555557a2fe *)
not ymm5_0n@uint64 ymm5_0;
and ymm6_0@uint64 ymm5_0n ymm0_0;
not ymm5_1n@uint64 ymm5_1;
and ymm6_1@uint64 ymm5_1n ymm0_1;
not ymm5_2n@uint64 ymm5_2;
and ymm6_2@uint64 ymm5_2n ymm0_2;
not ymm5_3n@uint64 ymm5_3;
and ymm6_3@uint64 ymm5_3n ymm0_3;
(* vpandn %ymm1,%ymm0,%ymm0                        #! PC = 0x55555557a302 *)
not ymm0_0n@uint64 ymm0_0;
and ymm0_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm0_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm0_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm0_3@uint64 ymm0_3n ymm1_3;
(* vpxor  %ymm4,%ymm6,%ymm4                        #! PC = 0x55555557a306 *)
xor ymm4_0@uint64 ymm6_0 ymm4_0;
xor ymm4_1@uint64 ymm6_1 ymm4_1;
xor ymm4_2@uint64 ymm6_2 ymm4_2;
xor ymm4_3@uint64 ymm6_3 ymm4_3;
(* vpxor  %ymm5,%ymm0,%ymm0                        #! PC = 0x55555557a30a *)
xor ymm0_0@uint64 ymm0_0 ymm5_0;
xor ymm0_1@uint64 ymm0_1 ymm5_1;
xor ymm0_2@uint64 ymm0_2 ymm5_2;
xor ymm0_3@uint64 ymm0_3 ymm5_3;
(* vmovdqa %ymm4,-0x1b0(%rbp)                      #! EA = L0x7fffffffbda0; PC = 0x55555557a30e *)
mov L0x7fffffffbda0 ymm4_0;
mov L0x7fffffffbda8 ymm4_1;
mov L0x7fffffffbdb0 ymm4_2;
mov L0x7fffffffbdb8 ymm4_3;
(* vmovdqa %ymm0,-0x310(%rbp)                      #! EA = L0x7fffffffbc40; PC = 0x55555557a316 *)
mov L0x7fffffffbc40 ymm0_0;
mov L0x7fffffffbc48 ymm0_1;
mov L0x7fffffffbc50 ymm0_2;
mov L0x7fffffffbc58 ymm0_3;
(* vpxor  -0x150(%rbp),%ymm2,%ymm0                 #! EA = L0x7fffffffbe00; Value = 0x2ade13fd70a45485; PC = 0x55555557a31e *)
xor ymm0_0@uint64 ymm2_0 L0x7fffffffbe00;
xor ymm0_1@uint64 ymm2_1 L0x7fffffffbe08;
xor ymm0_2@uint64 ymm2_2 L0x7fffffffbe10;
xor ymm0_3@uint64 ymm2_3 L0x7fffffffbe18;
(* vpxor  -0x290(%rbp),%ymm11,%ymm10               #! EA = L0x7fffffffbcc0; Value = 0x76cd1ada7bddb391; PC = 0x55555557a326 *)
xor ymm10_0@uint64 ymm11_0 L0x7fffffffbcc0;
xor ymm10_1@uint64 ymm11_1 L0x7fffffffbcc8;
xor ymm10_2@uint64 ymm11_2 L0x7fffffffbcd0;
xor ymm10_3@uint64 ymm11_3 L0x7fffffffbcd8;
(* vpxor  -0xf0(%rbp),%ymm3,%ymm6                  #! EA = L0x7fffffffbe60; Value = 0x3329889348e3611e; PC = 0x55555557a32e *)
xor ymm6_0@uint64 ymm3_0 L0x7fffffffbe60;
xor ymm6_1@uint64 ymm3_1 L0x7fffffffbe68;
xor ymm6_2@uint64 ymm3_2 L0x7fffffffbe70;
xor ymm6_3@uint64 ymm3_3 L0x7fffffffbe78;
(* vpxor  -0x210(%rbp),%ymm11,%ymm11               #! EA = L0x7fffffffbd40; Value = 0xbfc5ae08ed82cf6b; PC = 0x55555557a336 *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbd40;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbd48;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbd50;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbd58;
(* vmovdqa %ymm13,-0x210(%rbp)                     #! EA = L0x7fffffffbd40; PC = 0x55555557a33e *)
mov L0x7fffffffbd40 ymm13_0;
mov L0x7fffffffbd48 ymm13_1;
mov L0x7fffffffbd50 ymm13_2;
mov L0x7fffffffbd58 ymm13_3;
(* vpsrlq $0x36,%ymm0,%ymm9                        #! PC = 0x55555557a346 *)
shr ymm9_0 ymm0_0 0x36@uint64;
shr ymm9_1 ymm0_1 0x36@uint64;
shr ymm9_2 ymm0_2 0x36@uint64;
shr ymm9_3 ymm0_3 0x36@uint64;
(* vpsllq $0xa,%ymm0,%ymm0                         #! PC = 0x55555557a34b *)
shl ymm0_0 ymm0_0 0xa@uint64;
shl ymm0_1 ymm0_1 0xa@uint64;
shl ymm0_2 ymm0_2 0xa@uint64;
shl ymm0_3 ymm0_3 0xa@uint64;
(* vpxor  -0x110(%rbp),%ymm3,%ymm3                 #! EA = L0x7fffffffbe40; Value = 0x604b94ba87023ffa; PC = 0x55555557a350 *)
xor ymm3_0@uint64 ymm3_0 L0x7fffffffbe40;
xor ymm3_1@uint64 ymm3_1 L0x7fffffffbe48;
xor ymm3_2@uint64 ymm3_2 L0x7fffffffbe50;
xor ymm3_3@uint64 ymm3_3 L0x7fffffffbe58;
(* vpsrlq $0x25,%ymm10,%ymm1                       #! PC = 0x55555557a358 *)
shr ymm1_0 ymm10_0 0x25@uint64;
shr ymm1_1 ymm10_1 0x25@uint64;
shr ymm1_2 ymm10_2 0x25@uint64;
shr ymm1_3 ymm10_3 0x25@uint64;
(* vpsllq $0x1b,%ymm10,%ymm10                      #! PC = 0x55555557a35e *)
shl ymm10_0 ymm10_0 0x1b@uint64;
shl ymm10_1 ymm10_1 0x1b@uint64;
shl ymm10_2 ymm10_2 0x1b@uint64;
shl ymm10_3 ymm10_3 0x1b@uint64;
(* vpor   %ymm0,%ymm9,%ymm9                        #! PC = 0x55555557a364 *)
or ymm9_0@uint64 ymm9_0 ymm0_0;
or ymm9_1@uint64 ymm9_1 ymm0_1;
or ymm9_2@uint64 ymm9_2 ymm0_2;
or ymm9_3@uint64 ymm9_3 ymm0_3;
(* vpxor  -0x50(%rbp),%ymm14,%ymm0                 #! EA = L0x7fffffffbf00; Value = 0x7e78ea0caeaf0b0f; PC = 0x55555557a368 *)
xor ymm0_0@uint64 ymm14_0 L0x7fffffffbf00;
xor ymm0_1@uint64 ymm14_1 L0x7fffffffbf08;
xor ymm0_2@uint64 ymm14_2 L0x7fffffffbf10;
xor ymm0_3@uint64 ymm14_3 L0x7fffffffbf18;
(* vpsrlq $0x1c,%ymm6,%ymm5                        #! PC = 0x55555557a36d *)
shr ymm5_0 ymm6_0 0x1c@uint64;
shr ymm5_1 ymm6_1 0x1c@uint64;
shr ymm5_2 ymm6_2 0x1c@uint64;
shr ymm5_3 ymm6_3 0x1c@uint64;
(* vpor   %ymm10,%ymm1,%ymm1                       #! PC = 0x55555557a372 *)
or ymm1_0@uint64 ymm1_0 ymm10_0;
or ymm1_1@uint64 ymm1_1 ymm10_1;
or ymm1_2@uint64 ymm1_2 ymm10_2;
or ymm1_3@uint64 ymm1_3 ymm10_3;
(* vpsllq $0x24,%ymm6,%ymm6                        #! PC = 0x55555557a377 *)
shl ymm6_0 ymm6_0 0x24@uint64;
shl ymm6_1 ymm6_1 0x24@uint64;
shl ymm6_2 ymm6_2 0x24@uint64;
shl ymm6_3 ymm6_3 0x24@uint64;
(* vpxor  -0x250(%rbp),%ymm14,%ymm14               #! EA = L0x7fffffffbd00; Value = 0x11acb1eb1af15940; PC = 0x55555557a37c *)
xor ymm14_0@uint64 ymm14_0 L0x7fffffffbd00;
xor ymm14_1@uint64 ymm14_1 L0x7fffffffbd08;
xor ymm14_2@uint64 ymm14_2 L0x7fffffffbd10;
xor ymm14_3@uint64 ymm14_3 L0x7fffffffbd18;
(* vpsrlq $0x31,%ymm0,%ymm7                        #! PC = 0x55555557a384 *)
shr ymm7_0 ymm0_0 0x31@uint64;
shr ymm7_1 ymm0_1 0x31@uint64;
shr ymm7_2 ymm0_2 0x31@uint64;
shr ymm7_3 ymm0_3 0x31@uint64;
(* vpsllq $0xf,%ymm0,%ymm10                        #! PC = 0x55555557a389 *)
shl ymm10_0 ymm0_0 0xf@uint64;
shl ymm10_1 ymm0_1 0xf@uint64;
shl ymm10_2 ymm0_2 0xf@uint64;
shl ymm10_3 ymm0_3 0xf@uint64;
(* vpor   %ymm6,%ymm5,%ymm5                        #! PC = 0x55555557a38e *)
or ymm5_0@uint64 ymm5_0 ymm6_0;
or ymm5_1@uint64 ymm5_1 ymm6_1;
or ymm5_2@uint64 ymm5_2 ymm6_2;
or ymm5_3@uint64 ymm5_3 ymm6_3;
(* vpxor  -0x1f0(%rbp),%ymm15,%ymm0                #! EA = L0x7fffffffbd60; Value = 0xcc0258a30644e223; PC = 0x55555557a392 *)
xor ymm0_0@uint64 ymm15_0 L0x7fffffffbd60;
xor ymm0_1@uint64 ymm15_1 L0x7fffffffbd68;
xor ymm0_2@uint64 ymm15_2 L0x7fffffffbd70;
xor ymm0_3@uint64 ymm15_3 L0x7fffffffbd78;
(* vpor   %ymm10,%ymm7,%ymm10                      #! PC = 0x55555557a39a *)
or ymm10_0@uint64 ymm7_0 ymm10_0;
or ymm10_1@uint64 ymm7_1 ymm10_1;
or ymm10_2@uint64 ymm7_2 ymm10_2;
or ymm10_3@uint64 ymm7_3 ymm10_3;
(* vpandn %ymm9,%ymm5,%ymm4                        #! PC = 0x55555557a39f *)
not ymm5_0n@uint64 ymm5_0;
and ymm4_0@uint64 ymm5_0n ymm9_0;
not ymm5_1n@uint64 ymm5_1;
and ymm4_1@uint64 ymm5_1n ymm9_1;
not ymm5_2n@uint64 ymm5_2;
and ymm4_2@uint64 ymm5_2n ymm9_2;
not ymm5_3n@uint64 ymm5_3;
and ymm4_3@uint64 ymm5_3n ymm9_3;
(* vpxor  %ymm1,%ymm4,%ymm6                        #! PC = 0x55555557a3a4 *)
xor ymm6_0@uint64 ymm4_0 ymm1_0;
xor ymm6_1@uint64 ymm4_1 ymm1_1;
xor ymm6_2@uint64 ymm4_2 ymm1_2;
xor ymm6_3@uint64 ymm4_3 ymm1_3;
(* vpxor  -0x2b0(%rbp),%ymm15,%ymm15               #! EA = L0x7fffffffbca0; Value = 0x9b74448952711318; PC = 0x55555557a3a8 *)
xor ymm15_0@uint64 ymm15_0 L0x7fffffffbca0;
xor ymm15_1@uint64 ymm15_1 L0x7fffffffbca8;
xor ymm15_2@uint64 ymm15_2 L0x7fffffffbcb0;
xor ymm15_3@uint64 ymm15_3 L0x7fffffffbcb8;
(* vpshufb 0x53b27(%rip),%ymm0,%ymm0        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x55555557a3b0 *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm0_0, ymm0_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm0_2, ymm0_3, tmp_2, tmp_3);
mov ymm0_0 tmp_0;
mov ymm0_1 tmp_1;
mov ymm0_2 tmp_2;
mov ymm0_3 tmp_3;
(* vmovdqa %ymm6,-0xf0(%rbp)                       #! EA = L0x7fffffffbe60; PC = 0x55555557a3b9 *)
mov L0x7fffffffbe60 ymm6_0;
mov L0x7fffffffbe68 ymm6_1;
mov L0x7fffffffbe70 ymm6_2;
mov L0x7fffffffbe78 ymm6_3;
(* vpandn %ymm10,%ymm9,%ymm6                       #! PC = 0x55555557a3c1 *)
not ymm9_0n@uint64 ymm9_0;
and ymm6_0@uint64 ymm9_0n ymm10_0;
not ymm9_1n@uint64 ymm9_1;
and ymm6_1@uint64 ymm9_1n ymm10_1;
not ymm9_2n@uint64 ymm9_2;
and ymm6_2@uint64 ymm9_2n ymm10_2;
not ymm9_3n@uint64 ymm9_3;
and ymm6_3@uint64 ymm9_3n ymm10_3;
(* vpandn %ymm0,%ymm10,%ymm4                       #! PC = 0x55555557a3c6 *)
not ymm10_0n@uint64 ymm10_0;
and ymm4_0@uint64 ymm10_0n ymm0_0;
not ymm10_1n@uint64 ymm10_1;
and ymm4_1@uint64 ymm10_1n ymm0_1;
not ymm10_2n@uint64 ymm10_2;
and ymm4_2@uint64 ymm10_2n ymm0_2;
not ymm10_3n@uint64 ymm10_3;
and ymm4_3@uint64 ymm10_3n ymm0_3;
(* vpxor  %ymm5,%ymm6,%ymm6                        #! PC = 0x55555557a3ca *)
xor ymm6_0@uint64 ymm6_0 ymm5_0;
xor ymm6_1@uint64 ymm6_1 ymm5_1;
xor ymm6_2@uint64 ymm6_2 ymm5_2;
xor ymm6_3@uint64 ymm6_3 ymm5_3;
(* vpxor  %ymm9,%ymm4,%ymm7                        #! PC = 0x55555557a3ce *)
xor ymm7_0@uint64 ymm4_0 ymm9_0;
xor ymm7_1@uint64 ymm4_1 ymm9_1;
xor ymm7_2@uint64 ymm4_2 ymm9_2;
xor ymm7_3@uint64 ymm4_3 ymm9_3;
(* vpsrlq $0x2,%ymm14,%ymm4                        #! PC = 0x55555557a3d3 *)
shr ymm4_0 ymm14_0 0x2@uint64;
shr ymm4_1 ymm14_1 0x2@uint64;
shr ymm4_2 ymm14_2 0x2@uint64;
shr ymm4_3 ymm14_3 0x2@uint64;
(* vmovdqa %ymm7,-0x290(%rbp)                      #! EA = L0x7fffffffbcc0; PC = 0x55555557a3d9 *)
mov L0x7fffffffbcc0 ymm7_0;
mov L0x7fffffffbcc8 ymm7_1;
mov L0x7fffffffbcd0 ymm7_2;
mov L0x7fffffffbcd8 ymm7_3;
(* vpandn %ymm1,%ymm0,%ymm7                        #! PC = 0x55555557a3e1 *)
not ymm0_0n@uint64 ymm0_0;
and ymm7_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm7_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm7_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm7_3@uint64 ymm0_3n ymm1_3;
(* vpandn %ymm5,%ymm1,%ymm1                        #! PC = 0x55555557a3e5 *)
not ymm1_0n@uint64 ymm1_0;
and ymm1_0@uint64 ymm1_0n ymm5_0;
not ymm1_1n@uint64 ymm1_1;
and ymm1_1@uint64 ymm1_1n ymm5_1;
not ymm1_2n@uint64 ymm1_2;
and ymm1_2@uint64 ymm1_2n ymm5_2;
not ymm1_3n@uint64 ymm1_3;
and ymm1_3@uint64 ymm1_3n ymm5_3;
(* vpxor  %ymm0,%ymm1,%ymm5                        #! PC = 0x55555557a3e9 *)
xor ymm5_0@uint64 ymm1_0 ymm0_0;
xor ymm5_1@uint64 ymm1_1 ymm0_1;
xor ymm5_2@uint64 ymm1_2 ymm0_2;
xor ymm5_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x9,%ymm15,%ymm0                        #! PC = 0x55555557a3ed *)
shr ymm0_0 ymm15_0 0x9@uint64;
shr ymm0_1 ymm15_1 0x9@uint64;
shr ymm0_2 ymm15_2 0x9@uint64;
shr ymm0_3 ymm15_3 0x9@uint64;
(* vpxor  %ymm10,%ymm7,%ymm7                       #! PC = 0x55555557a3f3 *)
xor ymm7_0@uint64 ymm7_0 ymm10_0;
xor ymm7_1@uint64 ymm7_1 ymm10_1;
xor ymm7_2@uint64 ymm7_2 ymm10_2;
xor ymm7_3@uint64 ymm7_3 ymm10_3;
(* vmovdqa -0x270(%rbp),%ymm10                     #! EA = L0x7fffffffbce0; Value = 0x262012b5083c91ac; PC = 0x55555557a3f8 *)
mov ymm10_0 L0x7fffffffbce0;
mov ymm10_1 L0x7fffffffbce8;
mov ymm10_2 L0x7fffffffbcf0;
mov ymm10_3 L0x7fffffffbcf8;
(* vpsrlq $0x19,%ymm11,%ymm1                       #! PC = 0x55555557a400 *)
shr ymm1_0 ymm11_0 0x19@uint64;
shr ymm1_1 ymm11_1 0x19@uint64;
shr ymm1_2 ymm11_2 0x19@uint64;
shr ymm1_3 ymm11_3 0x19@uint64;
(* vpsllq $0x37,%ymm15,%ymm15                      #! PC = 0x55555557a406 *)
shl ymm15_0 ymm15_0 0x37@uint64;
shl ymm15_1 ymm15_1 0x37@uint64;
shl ymm15_2 ymm15_2 0x37@uint64;
shl ymm15_3 ymm15_3 0x37@uint64;
(* vmovdqa %ymm5,-0x330(%rbp)                      #! EA = L0x7fffffffbc20; PC = 0x55555557a40c *)
mov L0x7fffffffbc20 ymm5_0;
mov L0x7fffffffbc28 ymm5_1;
mov L0x7fffffffbc30 ymm5_2;
mov L0x7fffffffbc38 ymm5_3;
(* vmovdqa -0x190(%rbp),%ymm5                      #! EA = L0x7fffffffbdc0; Value = 0xffb9fc9a0f77452e; PC = 0x55555557a414 *)
mov ymm5_0 L0x7fffffffbdc0;
mov ymm5_1 L0x7fffffffbdc8;
mov ymm5_2 L0x7fffffffbdd0;
mov ymm5_3 L0x7fffffffbdd8;
(* vpsllq $0x27,%ymm11,%ymm11                      #! PC = 0x55555557a41c *)
shl ymm11_0 ymm11_0 0x27@uint64;
shl ymm11_1 ymm11_1 0x27@uint64;
shl ymm11_2 ymm11_2 0x27@uint64;
shl ymm11_3 ymm11_3 0x27@uint64;
(* vpsllq $0x3e,%ymm14,%ymm14                      #! PC = 0x55555557a422 *)
shl ymm14_0 ymm14_0 0x3e@uint64;
shl ymm14_1 ymm14_1 0x3e@uint64;
shl ymm14_2 ymm14_2 0x3e@uint64;
shl ymm14_3 ymm14_3 0x3e@uint64;
(* vpor   %ymm15,%ymm0,%ymm15                      #! PC = 0x55555557a428 *)
or ymm15_0@uint64 ymm0_0 ymm15_0;
or ymm15_1@uint64 ymm0_1 ymm15_1;
or ymm15_2@uint64 ymm0_2 ymm15_2;
or ymm15_3@uint64 ymm0_3 ymm15_3;
(* vpor   %ymm11,%ymm1,%ymm11                      #! PC = 0x55555557a42d *)
or ymm11_0@uint64 ymm1_0 ymm11_0;
or ymm11_1@uint64 ymm1_1 ymm11_1;
or ymm11_2@uint64 ymm1_2 ymm11_2;
or ymm11_3@uint64 ymm1_3 ymm11_3;
(* vpor   %ymm14,%ymm4,%ymm4                       #! PC = 0x55555557a432 *)
or ymm4_0@uint64 ymm4_0 ymm14_0;
or ymm4_1@uint64 ymm4_1 ymm14_1;
or ymm4_2@uint64 ymm4_2 ymm14_2;
or ymm4_3@uint64 ymm4_3 ymm14_3;
(* vpxor  %ymm13,%ymm5,%ymm1                       #! PC = 0x55555557a437 *)
xor ymm1_0@uint64 ymm5_0 ymm13_0;
xor ymm1_1@uint64 ymm5_1 ymm13_1;
xor ymm1_2@uint64 ymm5_2 ymm13_2;
xor ymm1_3@uint64 ymm5_3 ymm13_3;
(* vpandn %ymm11,%ymm15,%ymm0                      #! PC = 0x55555557a43c *)
not ymm15_0n@uint64 ymm15_0;
and ymm0_0@uint64 ymm15_0n ymm11_0;
not ymm15_1n@uint64 ymm15_1;
and ymm0_1@uint64 ymm15_1n ymm11_1;
not ymm15_2n@uint64 ymm15_2;
and ymm0_2@uint64 ymm15_2n ymm11_2;
not ymm15_3n@uint64 ymm15_3;
and ymm0_3@uint64 ymm15_3n ymm11_3;
(* vpxor  -0xd0(%rbp),%ymm8,%ymm5                  #! EA = L0x7fffffffbe80; Value = 0x03c4dfa24420069f; PC = 0x55555557a441 *)
xor ymm5_0@uint64 ymm8_0 L0x7fffffffbe80;
xor ymm5_1@uint64 ymm8_1 L0x7fffffffbe88;
xor ymm5_2@uint64 ymm8_2 L0x7fffffffbe90;
xor ymm5_3@uint64 ymm8_3 L0x7fffffffbe98;
(* vpxor  -0x230(%rbp),%ymm6,%ymm14                #! EA = L0x7fffffffbd20; Value = 0x76004ea89b563e26; PC = 0x55555557a449 *)
xor ymm14_0@uint64 ymm6_0 L0x7fffffffbd20;
xor ymm14_1@uint64 ymm6_1 L0x7fffffffbd28;
xor ymm14_2@uint64 ymm6_2 L0x7fffffffbd30;
xor ymm14_3@uint64 ymm6_3 L0x7fffffffbd38;
(* vpxor  %ymm4,%ymm0,%ymm0                        #! PC = 0x55555557a451 *)
xor ymm0_0@uint64 ymm0_0 ymm4_0;
xor ymm0_1@uint64 ymm0_1 ymm4_1;
xor ymm0_2@uint64 ymm0_2 ymm4_2;
xor ymm0_3@uint64 ymm0_3 ymm4_3;
(* vpxor  -0xf0(%rbp),%ymm0,%ymm9                  #! EA = L0x7fffffffbe60; Value = 0x58c5f58b0b1e40ce; PC = 0x55555557a455 *)
xor ymm9_0@uint64 ymm0_0 L0x7fffffffbe60;
xor ymm9_1@uint64 ymm0_1 L0x7fffffffbe68;
xor ymm9_2@uint64 ymm0_2 L0x7fffffffbe70;
xor ymm9_3@uint64 ymm0_3 L0x7fffffffbe78;
(* vpxor  -0x290(%rbp),%ymm12,%ymm13               #! EA = L0x7fffffffbcc0; Value = 0x0d7175255383fc27; PC = 0x55555557a45d *)
xor ymm13_0@uint64 ymm12_0 L0x7fffffffbcc0;
xor ymm13_1@uint64 ymm12_1 L0x7fffffffbcc8;
xor ymm13_2@uint64 ymm12_2 L0x7fffffffbcd0;
xor ymm13_3@uint64 ymm12_3 L0x7fffffffbcd8;
(* vpxor  %ymm5,%ymm14,%ymm14                      #! PC = 0x55555557a465 *)
xor ymm14_0@uint64 ymm14_0 ymm5_0;
xor ymm14_1@uint64 ymm14_1 ymm5_1;
xor ymm14_2@uint64 ymm14_2 ymm5_2;
xor ymm14_3@uint64 ymm14_3 ymm5_3;
(* vpxor  %ymm1,%ymm9,%ymm9                        #! PC = 0x55555557a469 *)
xor ymm9_0@uint64 ymm9_0 ymm1_0;
xor ymm9_1@uint64 ymm9_1 ymm1_1;
xor ymm9_2@uint64 ymm9_2 ymm1_2;
xor ymm9_3@uint64 ymm9_3 ymm1_3;
(* vpsrlq $0x17,%ymm3,%ymm1                        #! PC = 0x55555557a46d *)
shr ymm1_0 ymm3_0 0x17@uint64;
shr ymm1_1 ymm3_1 0x17@uint64;
shr ymm1_2 ymm3_2 0x17@uint64;
shr ymm1_3 ymm3_3 0x17@uint64;
(* vpxor  -0x90(%rbp),%ymm9,%ymm9                  #! EA = L0x7fffffffbec0; Value = 0x0e2e4980d944da77; PC = 0x55555557a472 *)
xor ymm9_0@uint64 ymm9_0 L0x7fffffffbec0;
xor ymm9_1@uint64 ymm9_1 L0x7fffffffbec8;
xor ymm9_2@uint64 ymm9_2 L0x7fffffffbed0;
xor ymm9_3@uint64 ymm9_3 L0x7fffffffbed8;
(* vpsllq $0x29,%ymm3,%ymm3                        #! PC = 0x55555557a47a *)
shl ymm3_0 ymm3_0 0x29@uint64;
shl ymm3_1 ymm3_1 0x29@uint64;
shl ymm3_2 ymm3_2 0x29@uint64;
shl ymm3_3 ymm3_3 0x29@uint64;
(* vpor   %ymm3,%ymm1,%ymm3                        #! PC = 0x55555557a47f *)
or ymm3_0@uint64 ymm1_0 ymm3_0;
or ymm3_1@uint64 ymm1_1 ymm3_1;
or ymm3_2@uint64 ymm1_2 ymm3_2;
or ymm3_3@uint64 ymm1_3 ymm3_3;
(* vpandn %ymm3,%ymm11,%ymm1                       #! PC = 0x55555557a483 *)
not ymm11_0n@uint64 ymm11_0;
and ymm1_0@uint64 ymm11_0n ymm3_0;
not ymm11_1n@uint64 ymm11_1;
and ymm1_1@uint64 ymm11_1n ymm3_1;
not ymm11_2n@uint64 ymm11_2;
and ymm1_2@uint64 ymm11_2n ymm3_2;
not ymm11_3n@uint64 ymm11_3;
and ymm1_3@uint64 ymm11_3n ymm3_3;
(* vpxor  %ymm15,%ymm1,%ymm1                       #! PC = 0x55555557a487 *)
xor ymm1_0@uint64 ymm1_0 ymm15_0;
xor ymm1_1@uint64 ymm1_1 ymm15_1;
xor ymm1_2@uint64 ymm1_2 ymm15_2;
xor ymm1_3@uint64 ymm1_3 ymm15_3;
(* vpxor  %ymm1,%ymm14,%ymm14                      #! PC = 0x55555557a48c *)
xor ymm14_0@uint64 ymm14_0 ymm1_0;
xor ymm14_1@uint64 ymm14_1 ymm1_1;
xor ymm14_2@uint64 ymm14_2 ymm1_2;
xor ymm14_3@uint64 ymm14_3 ymm1_3;
(* vmovdqa %ymm1,-0x2b0(%rbp)                      #! EA = L0x7fffffffbca0; PC = 0x55555557a490 *)
mov L0x7fffffffbca0 ymm1_0;
mov L0x7fffffffbca8 ymm1_1;
mov L0x7fffffffbcb0 ymm1_2;
mov L0x7fffffffbcb8 ymm1_3;
(* vpxor  -0x130(%rbp),%ymm2,%ymm1                 #! EA = L0x7fffffffbe20; Value = 0x50a2a1d79f1ef2b3; PC = 0x55555557a498 *)
xor ymm1_0@uint64 ymm2_0 L0x7fffffffbe20;
xor ymm1_1@uint64 ymm2_1 L0x7fffffffbe28;
xor ymm1_2@uint64 ymm2_2 L0x7fffffffbe30;
xor ymm1_3@uint64 ymm2_3 L0x7fffffffbe38;
(* vpsllq $0x2,%ymm1,%ymm2                         #! PC = 0x55555557a4a0 *)
shl ymm2_0 ymm1_0 0x2@uint64;
shl ymm2_1 ymm1_1 0x2@uint64;
shl ymm2_2 ymm1_2 0x2@uint64;
shl ymm2_3 ymm1_3 0x2@uint64;
(* vpsrlq $0x3e,%ymm1,%ymm5                        #! PC = 0x55555557a4a5 *)
shr ymm5_0 ymm1_0 0x3e@uint64;
shr ymm5_1 ymm1_1 0x3e@uint64;
shr ymm5_2 ymm1_2 0x3e@uint64;
shr ymm5_3 ymm1_3 0x3e@uint64;
(* vpor   %ymm2,%ymm5,%ymm5                        #! PC = 0x55555557a4aa *)
or ymm5_0@uint64 ymm5_0 ymm2_0;
or ymm5_1@uint64 ymm5_1 ymm2_1;
or ymm5_2@uint64 ymm5_2 ymm2_2;
or ymm5_3@uint64 ymm5_3 ymm2_3;
(* vpandn %ymm5,%ymm3,%ymm1                        #! PC = 0x55555557a4ae *)
not ymm3_0n@uint64 ymm3_0;
and ymm1_0@uint64 ymm3_0n ymm5_0;
not ymm3_1n@uint64 ymm3_1;
and ymm1_1@uint64 ymm3_1n ymm5_1;
not ymm3_2n@uint64 ymm3_2;
and ymm1_2@uint64 ymm3_2n ymm5_2;
not ymm3_3n@uint64 ymm3_3;
and ymm1_3@uint64 ymm3_3n ymm5_3;
(* vpxor  %ymm11,%ymm1,%ymm1                       #! PC = 0x55555557a4b2 *)
xor ymm1_0@uint64 ymm1_0 ymm11_0;
xor ymm1_1@uint64 ymm1_1 ymm11_1;
xor ymm1_2@uint64 ymm1_2 ymm11_2;
xor ymm1_3@uint64 ymm1_3 ymm11_3;
(* vmovdqa -0xb0(%rbp),%ymm11                      #! EA = L0x7fffffffbea0; Value = 0x0ea0ddc0d9366302; PC = 0x55555557a4b7 *)
mov ymm11_0 L0x7fffffffbea0;
mov ymm11_1 L0x7fffffffbea8;
mov ymm11_2 L0x7fffffffbeb0;
mov ymm11_3 L0x7fffffffbeb8;
(* vpxor  -0x2f0(%rbp),%ymm11,%ymm2                #! EA = L0x7fffffffbc60; Value = 0x4247f8944d4ae7fe; PC = 0x55555557a4bf *)
xor ymm2_0@uint64 ymm11_0 L0x7fffffffbc60;
xor ymm2_1@uint64 ymm11_1 L0x7fffffffbc68;
xor ymm2_2@uint64 ymm11_2 L0x7fffffffbc70;
xor ymm2_3@uint64 ymm11_3 L0x7fffffffbc78;
(* vpxor  %ymm2,%ymm13,%ymm13                      #! PC = 0x55555557a4c7 *)
xor ymm13_0@uint64 ymm13_0 ymm2_0;
xor ymm13_1@uint64 ymm13_1 ymm2_1;
xor ymm13_2@uint64 ymm13_2 ymm2_2;
xor ymm13_3@uint64 ymm13_3 ymm2_3;
(* vpandn %ymm4,%ymm5,%ymm2                        #! PC = 0x55555557a4cb *)
not ymm5_0n@uint64 ymm5_0;
and ymm2_0@uint64 ymm5_0n ymm4_0;
not ymm5_1n@uint64 ymm5_1;
and ymm2_1@uint64 ymm5_1n ymm4_1;
not ymm5_2n@uint64 ymm5_2;
and ymm2_2@uint64 ymm5_2n ymm4_2;
not ymm5_3n@uint64 ymm5_3;
and ymm2_3@uint64 ymm5_3n ymm4_3;
(* vpxor  %ymm3,%ymm2,%ymm3                        #! PC = 0x55555557a4cf *)
xor ymm3_0@uint64 ymm2_0 ymm3_0;
xor ymm3_1@uint64 ymm2_1 ymm3_1;
xor ymm3_2@uint64 ymm2_2 ymm3_2;
xor ymm3_3@uint64 ymm2_3 ymm3_3;
(* vpxor  -0x170(%rbp),%ymm10,%ymm2                #! EA = L0x7fffffffbde0; Value = 0x602c35affe556148; PC = 0x55555557a4d3 *)
xor ymm2_0@uint64 ymm10_0 L0x7fffffffbde0;
xor ymm2_1@uint64 ymm10_1 L0x7fffffffbde8;
xor ymm2_2@uint64 ymm10_2 L0x7fffffffbdf0;
xor ymm2_3@uint64 ymm10_3 L0x7fffffffbdf8;
(* vpxor  %ymm1,%ymm13,%ymm13                      #! PC = 0x55555557a4db *)
xor ymm13_0@uint64 ymm13_0 ymm1_0;
xor ymm13_1@uint64 ymm13_1 ymm1_1;
xor ymm13_2@uint64 ymm13_2 ymm1_2;
xor ymm13_3@uint64 ymm13_3 ymm1_3;
(* vpxor  %ymm3,%ymm7,%ymm11                       #! PC = 0x55555557a4df *)
xor ymm11_0@uint64 ymm7_0 ymm3_0;
xor ymm11_1@uint64 ymm7_1 ymm3_1;
xor ymm11_2@uint64 ymm7_2 ymm3_2;
xor ymm11_3@uint64 ymm7_3 ymm3_3;
(* vmovdqa %ymm3,-0x250(%rbp)                      #! EA = L0x7fffffffbd00; PC = 0x55555557a4e3 *)
mov L0x7fffffffbd00 ymm3_0;
mov L0x7fffffffbd08 ymm3_1;
mov L0x7fffffffbd10 ymm3_2;
mov L0x7fffffffbd18 ymm3_3;
(* vpxor  %ymm2,%ymm11,%ymm11                      #! PC = 0x55555557a4eb *)
xor ymm11_0@uint64 ymm11_0 ymm2_0;
xor ymm11_1@uint64 ymm11_1 ymm2_1;
xor ymm11_2@uint64 ymm11_2 ymm2_2;
xor ymm11_3@uint64 ymm11_3 ymm2_3;
(* vpandn %ymm15,%ymm4,%ymm2                       #! PC = 0x55555557a4ef *)
not ymm4_0n@uint64 ymm4_0;
and ymm2_0@uint64 ymm4_0n ymm15_0;
not ymm4_1n@uint64 ymm4_1;
and ymm2_1@uint64 ymm4_1n ymm15_1;
not ymm4_2n@uint64 ymm4_2;
and ymm2_2@uint64 ymm4_2n ymm15_2;
not ymm4_3n@uint64 ymm4_3;
and ymm2_3@uint64 ymm4_3n ymm15_3;
(* vmovdqa -0x2d0(%rbp),%ymm15                     #! EA = L0x7fffffffbc80; Value = 0xe7fb7b40b7e90583; PC = 0x55555557a4f4 *)
mov ymm15_0 L0x7fffffffbc80;
mov ymm15_1 L0x7fffffffbc88;
mov ymm15_2 L0x7fffffffbc90;
mov ymm15_3 L0x7fffffffbc98;
(* vpxor  -0x1d0(%rbp),%ymm15,%ymm10               #! EA = L0x7fffffffbd80; Value = 0xd8f2b86d8510572f; PC = 0x55555557a4fc *)
xor ymm10_0@uint64 ymm15_0 L0x7fffffffbd80;
xor ymm10_1@uint64 ymm15_1 L0x7fffffffbd88;
xor ymm10_2@uint64 ymm15_2 L0x7fffffffbd90;
xor ymm10_3@uint64 ymm15_3 L0x7fffffffbd98;
(* vpxor  %ymm5,%ymm2,%ymm2                        #! PC = 0x55555557a504 *)
xor ymm2_0@uint64 ymm2_0 ymm5_0;
xor ymm2_1@uint64 ymm2_1 ymm5_1;
xor ymm2_2@uint64 ymm2_2 ymm5_2;
xor ymm2_3@uint64 ymm2_3 ymm5_3;
(* vpxor  -0x310(%rbp),%ymm2,%ymm3                 #! EA = L0x7fffffffbc40; Value = 0xe249aeb39c95d7db; PC = 0x55555557a508 *)
xor ymm3_0@uint64 ymm2_0 L0x7fffffffbc40;
xor ymm3_1@uint64 ymm2_1 L0x7fffffffbc48;
xor ymm3_2@uint64 ymm2_2 L0x7fffffffbc50;
xor ymm3_3@uint64 ymm2_3 L0x7fffffffbc58;
(* vpxor  -0x1b0(%rbp),%ymm11,%ymm11               #! EA = L0x7fffffffbda0; Value = 0x16e2aa8036b5013d; PC = 0x55555557a510 *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbda0;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbda8;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbdb0;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbdb8;
(* vpsrlq $0x3f,%ymm14,%ymm4                       #! PC = 0x55555557a518 *)
shr ymm4_0 ymm14_0 0x3f@uint64;
shr ymm4_1 ymm14_1 0x3f@uint64;
shr ymm4_2 ymm14_2 0x3f@uint64;
shr ymm4_3 ymm14_3 0x3f@uint64;
(* vpsllq $0x1,%ymm13,%ymm5                        #! PC = 0x55555557a51e *)
shl ymm5_0 ymm13_0 0x1@uint64;
shl ymm5_1 ymm13_1 0x1@uint64;
shl ymm5_2 ymm13_2 0x1@uint64;
shl ymm5_3 ymm13_3 0x1@uint64;
(* vpxor  %ymm3,%ymm10,%ymm10                      #! PC = 0x55555557a524 *)
xor ymm10_0@uint64 ymm10_0 ymm3_0;
xor ymm10_1@uint64 ymm10_1 ymm3_1;
xor ymm10_2@uint64 ymm10_2 ymm3_2;
xor ymm10_3@uint64 ymm10_3 ymm3_3;
(* vpsllq $0x1,%ymm14,%ymm3                        #! PC = 0x55555557a528 *)
shl ymm3_0 ymm14_0 0x1@uint64;
shl ymm3_1 ymm14_1 0x1@uint64;
shl ymm3_2 ymm14_2 0x1@uint64;
shl ymm3_3 ymm14_3 0x1@uint64;
(* vpxor  -0x330(%rbp),%ymm10,%ymm10               #! EA = L0x7fffffffbc20; Value = 0xc52e05ccf15f628f; PC = 0x55555557a52e *)
xor ymm10_0@uint64 ymm10_0 L0x7fffffffbc20;
xor ymm10_1@uint64 ymm10_1 L0x7fffffffbc28;
xor ymm10_2@uint64 ymm10_2 L0x7fffffffbc30;
xor ymm10_3@uint64 ymm10_3 L0x7fffffffbc38;
(* vpor   %ymm3,%ymm4,%ymm4                        #! PC = 0x55555557a536 *)
or ymm4_0@uint64 ymm4_0 ymm3_0;
or ymm4_1@uint64 ymm4_1 ymm3_1;
or ymm4_2@uint64 ymm4_2 ymm3_2;
or ymm4_3@uint64 ymm4_3 ymm3_3;
(* vpsrlq $0x3f,%ymm13,%ymm3                       #! PC = 0x55555557a53a *)
shr ymm3_0 ymm13_0 0x3f@uint64;
shr ymm3_1 ymm13_1 0x3f@uint64;
shr ymm3_2 ymm13_2 0x3f@uint64;
shr ymm3_3 ymm13_3 0x3f@uint64;
(* vpsllq $0x1,%ymm11,%ymm15                       #! PC = 0x55555557a540 *)
shl ymm15_0 ymm11_0 0x1@uint64;
shl ymm15_1 ymm11_1 0x1@uint64;
shl ymm15_2 ymm11_2 0x1@uint64;
shl ymm15_3 ymm11_3 0x1@uint64;
(* vpor   %ymm5,%ymm3,%ymm3                        #! PC = 0x55555557a546 *)
or ymm3_0@uint64 ymm3_0 ymm5_0;
or ymm3_1@uint64 ymm3_1 ymm5_1;
or ymm3_2@uint64 ymm3_2 ymm5_2;
or ymm3_3@uint64 ymm3_3 ymm5_3;
(* vpxor  %ymm10,%ymm4,%ymm4                       #! PC = 0x55555557a54a *)
xor ymm4_0@uint64 ymm4_0 ymm10_0;
xor ymm4_1@uint64 ymm4_1 ymm10_1;
xor ymm4_2@uint64 ymm4_2 ymm10_2;
xor ymm4_3@uint64 ymm4_3 ymm10_3;
(* vpsrlq $0x3f,%ymm11,%ymm5                       #! PC = 0x55555557a54f *)
shr ymm5_0 ymm11_0 0x3f@uint64;
shr ymm5_1 ymm11_1 0x3f@uint64;
shr ymm5_2 ymm11_2 0x3f@uint64;
shr ymm5_3 ymm11_3 0x3f@uint64;
(* vpxor  %ymm9,%ymm3,%ymm3                        #! PC = 0x55555557a555 *)
xor ymm3_0@uint64 ymm3_0 ymm9_0;
xor ymm3_1@uint64 ymm3_1 ymm9_1;
xor ymm3_2@uint64 ymm3_2 ymm9_2;
xor ymm3_3@uint64 ymm3_3 ymm9_3;
(* vpxor  %ymm4,%ymm0,%ymm0                        #! PC = 0x55555557a55a *)
xor ymm0_0@uint64 ymm0_0 ymm4_0;
xor ymm0_1@uint64 ymm0_1 ymm4_1;
xor ymm0_2@uint64 ymm0_2 ymm4_2;
xor ymm0_3@uint64 ymm0_3 ymm4_3;
(* vpor   %ymm15,%ymm5,%ymm5                       #! PC = 0x55555557a55e *)
or ymm5_0@uint64 ymm5_0 ymm15_0;
or ymm5_1@uint64 ymm5_1 ymm15_1;
or ymm5_2@uint64 ymm5_2 ymm15_2;
or ymm5_3@uint64 ymm5_3 ymm15_3;
(* vpxor  %ymm3,%ymm8,%ymm8                        #! PC = 0x55555557a563 *)
xor ymm8_0@uint64 ymm8_0 ymm3_0;
xor ymm8_1@uint64 ymm8_1 ymm3_1;
xor ymm8_2@uint64 ymm8_2 ymm3_2;
xor ymm8_3@uint64 ymm8_3 ymm3_3;
(* vpxor  %ymm14,%ymm5,%ymm5                       #! PC = 0x55555557a567 *)
xor ymm5_0@uint64 ymm5_0 ymm14_0;
xor ymm5_1@uint64 ymm5_1 ymm14_1;
xor ymm5_2@uint64 ymm5_2 ymm14_2;
xor ymm5_3@uint64 ymm5_3 ymm14_3;
(* vpsrlq $0x3f,%ymm10,%ymm14                      #! PC = 0x55555557a56c *)
shr ymm14_0 ymm10_0 0x3f@uint64;
shr ymm14_1 ymm10_1 0x3f@uint64;
shr ymm14_2 ymm10_2 0x3f@uint64;
shr ymm14_3 ymm10_3 0x3f@uint64;
(* vpsllq $0x1,%ymm10,%ymm10                       #! PC = 0x55555557a572 *)
shl ymm10_0 ymm10_0 0x1@uint64;
shl ymm10_1 ymm10_1 0x1@uint64;
shl ymm10_2 ymm10_2 0x1@uint64;
shl ymm10_3 ymm10_3 0x1@uint64;
(* vpxor  %ymm5,%ymm12,%ymm12                      #! PC = 0x55555557a578 *)
xor ymm12_0@uint64 ymm12_0 ymm5_0;
xor ymm12_1@uint64 ymm12_1 ymm5_1;
xor ymm12_2@uint64 ymm12_2 ymm5_2;
xor ymm12_3@uint64 ymm12_3 ymm5_3;
(* vpxor  %ymm5,%ymm1,%ymm1                        #! PC = 0x55555557a57c *)
xor ymm1_0@uint64 ymm1_0 ymm5_0;
xor ymm1_1@uint64 ymm1_1 ymm5_1;
xor ymm1_2@uint64 ymm1_2 ymm5_2;
xor ymm1_3@uint64 ymm1_3 ymm5_3;
(* vpor   %ymm10,%ymm14,%ymm10                     #! PC = 0x55555557a580 *)
or ymm10_0@uint64 ymm14_0 ymm10_0;
or ymm10_1@uint64 ymm14_1 ymm10_1;
or ymm10_2@uint64 ymm14_2 ymm10_2;
or ymm10_3@uint64 ymm14_3 ymm10_3;
(* vmovq  %r14,%xmm14                              #! PC = 0x55555557a585 *)
mov xmm14_0 r14;
mov xmm14_1 0@uint64;
(* vpxor  %ymm13,%ymm10,%ymm10                     #! PC = 0x55555557a58a *)
xor ymm10_0@uint64 ymm10_0 ymm13_0;
xor ymm10_1@uint64 ymm10_1 ymm13_1;
xor ymm10_2@uint64 ymm10_2 ymm13_2;
xor ymm10_3@uint64 ymm10_3 ymm13_3;
(* vpsrlq $0x3f,%ymm9,%ymm13                       #! PC = 0x55555557a58f *)
shr ymm13_0 ymm9_0 0x3f@uint64;
shr ymm13_1 ymm9_1 0x3f@uint64;
shr ymm13_2 ymm9_2 0x3f@uint64;
shr ymm13_3 ymm9_3 0x3f@uint64;
(* vpbroadcastq %xmm14,%ymm14                      #! PC = 0x55555557a595 *)
mov ymm14_0 xmm14_0;
mov ymm14_1 xmm14_0;
mov ymm14_2 xmm14_0;
mov ymm14_3 xmm14_0;
(* vpsllq $0x1,%ymm9,%ymm9                         #! PC = 0x55555557a59a *)
shl ymm9_0 ymm9_0 0x1@uint64;
shl ymm9_1 ymm9_1 0x1@uint64;
shl ymm9_2 ymm9_2 0x1@uint64;
shl ymm9_3 ymm9_3 0x1@uint64;
(* vpxor  %ymm10,%ymm7,%ymm7                       #! PC = 0x55555557a5a0 *)
xor ymm7_0@uint64 ymm7_0 ymm10_0;
xor ymm7_1@uint64 ymm7_1 ymm10_1;
xor ymm7_2@uint64 ymm7_2 ymm10_2;
xor ymm7_3@uint64 ymm7_3 ymm10_3;
(* vpor   %ymm9,%ymm13,%ymm9                       #! PC = 0x55555557a5a5 *)
or ymm9_0@uint64 ymm13_0 ymm9_0;
or ymm9_1@uint64 ymm13_1 ymm9_1;
or ymm9_2@uint64 ymm13_2 ymm9_2;
or ymm9_3@uint64 ymm13_3 ymm9_3;
(* vpsrlq $0x14,%ymm8,%ymm13                       #! PC = 0x55555557a5aa *)
shr ymm13_0 ymm8_0 0x14@uint64;
shr ymm13_1 ymm8_1 0x14@uint64;
shr ymm13_2 ymm8_2 0x14@uint64;
shr ymm13_3 ymm8_3 0x14@uint64;
(* vpsllq $0x2c,%ymm8,%ymm8                        #! PC = 0x55555557a5b0 *)
shl ymm8_0 ymm8_0 0x2c@uint64;
shl ymm8_1 ymm8_1 0x2c@uint64;
shl ymm8_2 ymm8_2 0x2c@uint64;
shl ymm8_3 ymm8_3 0x2c@uint64;
(* vpxor  %ymm11,%ymm9,%ymm11                      #! PC = 0x55555557a5b6 *)
xor ymm11_0@uint64 ymm9_0 ymm11_0;
xor ymm11_1@uint64 ymm9_1 ymm11_1;
xor ymm11_2@uint64 ymm9_2 ymm11_2;
xor ymm11_3@uint64 ymm9_3 ymm11_3;
(* vpxor  -0x90(%rbp),%ymm4,%ymm9                  #! EA = L0x7fffffffbec0; Value = 0x0e2e4980d944da77; PC = 0x55555557a5bb *)
xor ymm9_0@uint64 ymm4_0 L0x7fffffffbec0;
xor ymm9_1@uint64 ymm4_1 L0x7fffffffbec8;
xor ymm9_2@uint64 ymm4_2 L0x7fffffffbed0;
xor ymm9_3@uint64 ymm4_3 L0x7fffffffbed8;
(* vpor   %ymm8,%ymm13,%ymm8                       #! PC = 0x55555557a5c3 *)
or ymm8_0@uint64 ymm13_0 ymm8_0;
or ymm8_1@uint64 ymm13_1 ymm8_1;
or ymm8_2@uint64 ymm13_2 ymm8_2;
or ymm8_3@uint64 ymm13_3 ymm8_3;
(* vpsrlq $0x15,%ymm12,%ymm13                      #! PC = 0x55555557a5c8 *)
shr ymm13_0 ymm12_0 0x15@uint64;
shr ymm13_1 ymm12_1 0x15@uint64;
shr ymm13_2 ymm12_2 0x15@uint64;
shr ymm13_3 ymm12_3 0x15@uint64;
(* vpxor  %ymm11,%ymm2,%ymm2                       #! PC = 0x55555557a5ce *)
xor ymm2_0@uint64 ymm2_0 ymm11_0;
xor ymm2_1@uint64 ymm2_1 ymm11_1;
xor ymm2_2@uint64 ymm2_2 ymm11_2;
xor ymm2_3@uint64 ymm2_3 ymm11_3;
(* vpsllq $0x2b,%ymm12,%ymm12                      #! PC = 0x55555557a5d3 *)
shl ymm12_0 ymm12_0 0x2b@uint64;
shl ymm12_1 ymm12_1 0x2b@uint64;
shl ymm12_2 ymm12_2 0x2b@uint64;
shl ymm12_3 ymm12_3 0x2b@uint64;
(* vpor   %ymm12,%ymm13,%ymm12                     #! PC = 0x55555557a5d9 *)
or ymm12_0@uint64 ymm13_0 ymm12_0;
or ymm12_1@uint64 ymm13_1 ymm12_1;
or ymm12_2@uint64 ymm13_2 ymm12_2;
or ymm12_3@uint64 ymm13_3 ymm12_3;
(* vpandn %ymm12,%ymm8,%ymm13                      #! PC = 0x55555557a5de *)
not ymm8_0n@uint64 ymm8_0;
and ymm13_0@uint64 ymm8_0n ymm12_0;
not ymm8_1n@uint64 ymm8_1;
and ymm13_1@uint64 ymm8_1n ymm12_1;
not ymm8_2n@uint64 ymm8_2;
and ymm13_2@uint64 ymm8_2n ymm12_2;
not ymm8_3n@uint64 ymm8_3;
and ymm13_3@uint64 ymm8_3n ymm12_3;
(* vpxor  %ymm14,%ymm13,%ymm13                     #! PC = 0x55555557a5e3 *)
xor ymm13_0@uint64 ymm13_0 ymm14_0;
xor ymm13_1@uint64 ymm13_1 ymm14_1;
xor ymm13_2@uint64 ymm13_2 ymm14_2;
xor ymm13_3@uint64 ymm13_3 ymm14_3;
(* vpxor  %ymm9,%ymm13,%ymm13                      #! PC = 0x55555557a5e8 *)
xor ymm13_0@uint64 ymm13_0 ymm9_0;
xor ymm13_1@uint64 ymm13_1 ymm9_1;
xor ymm13_2@uint64 ymm13_2 ymm9_2;
xor ymm13_3@uint64 ymm13_3 ymm9_3;
(* vmovdqa %ymm13,-0x1f0(%rbp)                     #! EA = L0x7fffffffbd60; PC = 0x55555557a5ed *)
mov L0x7fffffffbd60 ymm13_0;
mov L0x7fffffffbd68 ymm13_1;
mov L0x7fffffffbd70 ymm13_2;
mov L0x7fffffffbd78 ymm13_3;
(* vpsrlq $0x2b,%ymm7,%ymm13                       #! PC = 0x55555557a5f5 *)
shr ymm13_0 ymm7_0 0x2b@uint64;
shr ymm13_1 ymm7_1 0x2b@uint64;
shr ymm13_2 ymm7_2 0x2b@uint64;
shr ymm13_3 ymm7_3 0x2b@uint64;
(* vpsllq $0x15,%ymm7,%ymm7                        #! PC = 0x55555557a5fa *)
shl ymm7_0 ymm7_0 0x15@uint64;
shl ymm7_1 ymm7_1 0x15@uint64;
shl ymm7_2 ymm7_2 0x15@uint64;
shl ymm7_3 ymm7_3 0x15@uint64;
(* vpor   %ymm7,%ymm13,%ymm7                       #! PC = 0x55555557a5ff *)
or ymm7_0@uint64 ymm13_0 ymm7_0;
or ymm7_1@uint64 ymm13_1 ymm7_1;
or ymm7_2@uint64 ymm13_2 ymm7_2;
or ymm7_3@uint64 ymm13_3 ymm7_3;
(* vpandn %ymm7,%ymm12,%ymm13                      #! PC = 0x55555557a603 *)
not ymm12_0n@uint64 ymm12_0;
and ymm13_0@uint64 ymm12_0n ymm7_0;
not ymm12_1n@uint64 ymm12_1;
and ymm13_1@uint64 ymm12_1n ymm7_1;
not ymm12_2n@uint64 ymm12_2;
and ymm13_2@uint64 ymm12_2n ymm7_2;
not ymm12_3n@uint64 ymm12_3;
and ymm13_3@uint64 ymm12_3n ymm7_3;
(* vpxor  %ymm8,%ymm13,%ymm13                      #! PC = 0x55555557a607 *)
xor ymm13_0@uint64 ymm13_0 ymm8_0;
xor ymm13_1@uint64 ymm13_1 ymm8_1;
xor ymm13_2@uint64 ymm13_2 ymm8_2;
xor ymm13_3@uint64 ymm13_3 ymm8_3;
(* vmovdqa %ymm13,%ymm14                           #! PC = 0x55555557a60c *)
mov ymm14_0 ymm13_0;
mov ymm14_1 ymm13_1;
mov ymm14_2 ymm13_2;
mov ymm14_3 ymm13_3;
(* vmovdqa %ymm13,-0x3b0(%rbp)                     #! EA = L0x7fffffffbba0; PC = 0x55555557a611 *)
mov L0x7fffffffbba0 ymm13_0;
mov L0x7fffffffbba8 ymm13_1;
mov L0x7fffffffbbb0 ymm13_2;
mov L0x7fffffffbbb8 ymm13_3;
(* vpsrlq $0x32,%ymm2,%ymm13                       #! PC = 0x55555557a619 *)
shr ymm13_0 ymm2_0 0x32@uint64;
shr ymm13_1 ymm2_1 0x32@uint64;
shr ymm13_2 ymm2_2 0x32@uint64;
shr ymm13_3 ymm2_3 0x32@uint64;
(* vpsllq $0xe,%ymm2,%ymm2                         #! PC = 0x55555557a61e *)
shl ymm2_0 ymm2_0 0xe@uint64;
shl ymm2_1 ymm2_1 0xe@uint64;
shl ymm2_2 ymm2_2 0xe@uint64;
shl ymm2_3 ymm2_3 0xe@uint64;
(* vpor   %ymm2,%ymm13,%ymm2                       #! PC = 0x55555557a623 *)
or ymm2_0@uint64 ymm13_0 ymm2_0;
or ymm2_1@uint64 ymm13_1 ymm2_1;
or ymm2_2@uint64 ymm13_2 ymm2_2;
or ymm2_3@uint64 ymm13_3 ymm2_3;
(* vpandn %ymm2,%ymm7,%ymm15                       #! PC = 0x55555557a627 *)
not ymm7_0n@uint64 ymm7_0;
and ymm15_0@uint64 ymm7_0n ymm2_0;
not ymm7_1n@uint64 ymm7_1;
and ymm15_1@uint64 ymm7_1n ymm2_1;
not ymm7_2n@uint64 ymm7_2;
and ymm15_2@uint64 ymm7_2n ymm2_2;
not ymm7_3n@uint64 ymm7_3;
and ymm15_3@uint64 ymm7_3n ymm2_3;
(* vpxor  %ymm12,%ymm15,%ymm12                     #! PC = 0x55555557a62b *)
xor ymm12_0@uint64 ymm15_0 ymm12_0;
xor ymm12_1@uint64 ymm15_1 ymm12_1;
xor ymm12_2@uint64 ymm15_2 ymm12_2;
xor ymm12_3@uint64 ymm15_3 ymm12_3;
(* vmovdqa %ymm12,-0x110(%rbp)                     #! EA = L0x7fffffffbe40; PC = 0x55555557a630 *)
mov L0x7fffffffbe40 ymm12_0;
mov L0x7fffffffbe48 ymm12_1;
mov L0x7fffffffbe50 ymm12_2;
mov L0x7fffffffbe58 ymm12_3;
(* vpandn %ymm9,%ymm2,%ymm12                       #! PC = 0x55555557a638 *)
not ymm2_0n@uint64 ymm2_0;
and ymm12_0@uint64 ymm2_0n ymm9_0;
not ymm2_1n@uint64 ymm2_1;
and ymm12_1@uint64 ymm2_1n ymm9_1;
not ymm2_2n@uint64 ymm2_2;
and ymm12_2@uint64 ymm2_2n ymm9_2;
not ymm2_3n@uint64 ymm2_3;
and ymm12_3@uint64 ymm2_3n ymm9_3;
(* vpandn %ymm8,%ymm9,%ymm9                        #! PC = 0x55555557a63d *)
not ymm9_0n@uint64 ymm9_0;
and ymm9_0@uint64 ymm9_0n ymm8_0;
not ymm9_1n@uint64 ymm9_1;
and ymm9_1@uint64 ymm9_1n ymm8_1;
not ymm9_2n@uint64 ymm9_2;
and ymm9_2@uint64 ymm9_2n ymm8_2;
not ymm9_3n@uint64 ymm9_3;
and ymm9_3@uint64 ymm9_3n ymm8_3;
(* vpxor  %ymm2,%ymm9,%ymm8                        #! PC = 0x55555557a642 *)
xor ymm8_0@uint64 ymm9_0 ymm2_0;
xor ymm8_1@uint64 ymm9_1 ymm2_1;
xor ymm8_2@uint64 ymm9_2 ymm2_2;
xor ymm8_3@uint64 ymm9_3 ymm2_3;
(* vpxor  %ymm7,%ymm12,%ymm12                      #! PC = 0x55555557a646 *)
xor ymm12_0@uint64 ymm12_0 ymm7_0;
xor ymm12_1@uint64 ymm12_1 ymm7_1;
xor ymm12_2@uint64 ymm12_2 ymm7_2;
xor ymm12_3@uint64 ymm12_3 ymm7_3;
(* vpxor  -0x170(%rbp),%ymm10,%ymm7                #! EA = L0x7fffffffbde0; Value = 0x602c35affe556148; PC = 0x55555557a64a *)
xor ymm7_0@uint64 ymm10_0 L0x7fffffffbde0;
xor ymm7_1@uint64 ymm10_1 L0x7fffffffbde8;
xor ymm7_2@uint64 ymm10_2 L0x7fffffffbdf0;
xor ymm7_3@uint64 ymm10_3 L0x7fffffffbdf8;
(* vmovdqa %ymm8,-0x50(%rbp)                       #! EA = L0x7fffffffbf00; PC = 0x55555557a652 *)
mov L0x7fffffffbf00 ymm8_0;
mov L0x7fffffffbf08 ymm8_1;
mov L0x7fffffffbf10 ymm8_2;
mov L0x7fffffffbf18 ymm8_3;
(* vpxor  -0x2d0(%rbp),%ymm11,%ymm8                #! EA = L0x7fffffffbc80; Value = 0xe7fb7b40b7e90583; PC = 0x55555557a657 *)
xor ymm8_0@uint64 ymm11_0 L0x7fffffffbc80;
xor ymm8_1@uint64 ymm11_1 L0x7fffffffbc88;
xor ymm8_2@uint64 ymm11_2 L0x7fffffffbc90;
xor ymm8_3@uint64 ymm11_3 L0x7fffffffbc98;
(* vpxor  -0x190(%rbp),%ymm4,%ymm9                 #! EA = L0x7fffffffbdc0; Value = 0xffb9fc9a0f77452e; PC = 0x55555557a65f *)
xor ymm9_0@uint64 ymm4_0 L0x7fffffffbdc0;
xor ymm9_1@uint64 ymm4_1 L0x7fffffffbdc8;
xor ymm9_2@uint64 ymm4_2 L0x7fffffffbdd0;
xor ymm9_3@uint64 ymm4_3 L0x7fffffffbdd8;
(* vpsrlq $0x24,%ymm7,%ymm2                        #! PC = 0x55555557a667 *)
shr ymm2_0 ymm7_0 0x24@uint64;
shr ymm2_1 ymm7_1 0x24@uint64;
shr ymm2_2 ymm7_2 0x24@uint64;
shr ymm2_3 ymm7_3 0x24@uint64;
(* vpsllq $0x1c,%ymm7,%ymm7                        #! PC = 0x55555557a66c *)
shl ymm7_0 ymm7_0 0x1c@uint64;
shl ymm7_1 ymm7_1 0x1c@uint64;
shl ymm7_2 ymm7_2 0x1c@uint64;
shl ymm7_3 ymm7_3 0x1c@uint64;
(* vmovdqa %ymm12,-0x130(%rbp)                     #! EA = L0x7fffffffbe20; PC = 0x55555557a671 *)
mov L0x7fffffffbe20 ymm12_0;
mov L0x7fffffffbe28 ymm12_1;
mov L0x7fffffffbe30 ymm12_2;
mov L0x7fffffffbe38 ymm12_3;
(* vpor   %ymm7,%ymm2,%ymm7                        #! PC = 0x55555557a679 *)
or ymm7_0@uint64 ymm2_0 ymm7_0;
or ymm7_1@uint64 ymm2_1 ymm7_1;
or ymm7_2@uint64 ymm2_2 ymm7_2;
or ymm7_3@uint64 ymm2_3 ymm7_3;
(* vpsrlq $0x2c,%ymm8,%ymm2                        #! PC = 0x55555557a67d *)
shr ymm2_0 ymm8_0 0x2c@uint64;
shr ymm2_1 ymm8_1 0x2c@uint64;
shr ymm2_2 ymm8_2 0x2c@uint64;
shr ymm2_3 ymm8_3 0x2c@uint64;
(* vpsllq $0x14,%ymm8,%ymm8                        #! PC = 0x55555557a683 *)
shl ymm8_0 ymm8_0 0x14@uint64;
shl ymm8_1 ymm8_1 0x14@uint64;
shl ymm8_2 ymm8_2 0x14@uint64;
shl ymm8_3 ymm8_3 0x14@uint64;
(* vpor   %ymm8,%ymm2,%ymm2                        #! PC = 0x55555557a689 *)
or ymm2_0@uint64 ymm2_0 ymm8_0;
or ymm2_1@uint64 ymm2_1 ymm8_1;
or ymm2_2@uint64 ymm2_2 ymm8_2;
or ymm2_3@uint64 ymm2_3 ymm8_3;
(* vpsrlq $0x3d,%ymm9,%ymm8                        #! PC = 0x55555557a68e *)
shr ymm8_0 ymm9_0 0x3d@uint64;
shr ymm8_1 ymm9_1 0x3d@uint64;
shr ymm8_2 ymm9_2 0x3d@uint64;
shr ymm8_3 ymm9_3 0x3d@uint64;
(* vpsllq $0x3,%ymm9,%ymm9                         #! PC = 0x55555557a694 *)
shl ymm9_0 ymm9_0 0x3@uint64;
shl ymm9_1 ymm9_1 0x3@uint64;
shl ymm9_2 ymm9_2 0x3@uint64;
shl ymm9_3 ymm9_3 0x3@uint64;
(* vpor   %ymm9,%ymm8,%ymm8                        #! PC = 0x55555557a69a *)
or ymm8_0@uint64 ymm8_0 ymm9_0;
or ymm8_1@uint64 ymm8_1 ymm9_1;
or ymm8_2@uint64 ymm8_2 ymm9_2;
or ymm8_3@uint64 ymm8_3 ymm9_3;
(* vpandn %ymm8,%ymm2,%ymm9                        #! PC = 0x55555557a69f *)
not ymm2_0n@uint64 ymm2_0;
and ymm9_0@uint64 ymm2_0n ymm8_0;
not ymm2_1n@uint64 ymm2_1;
and ymm9_1@uint64 ymm2_1n ymm8_1;
not ymm2_2n@uint64 ymm2_2;
and ymm9_2@uint64 ymm2_2n ymm8_2;
not ymm2_3n@uint64 ymm2_3;
and ymm9_3@uint64 ymm2_3n ymm8_3;
(* vpxor  %ymm7,%ymm9,%ymm9                        #! PC = 0x55555557a6a4 *)
xor ymm9_0@uint64 ymm9_0 ymm7_0;
xor ymm9_1@uint64 ymm9_1 ymm7_1;
xor ymm9_2@uint64 ymm9_2 ymm7_2;
xor ymm9_3@uint64 ymm9_3 ymm7_3;
(* vmovdqa %ymm9,-0x150(%rbp)                      #! EA = L0x7fffffffbe00; PC = 0x55555557a6a8 *)
mov L0x7fffffffbe00 ymm9_0;
mov L0x7fffffffbe08 ymm9_1;
mov L0x7fffffffbe10 ymm9_2;
mov L0x7fffffffbe18 ymm9_3;
(* vpxor  %ymm3,%ymm6,%ymm9                        #! PC = 0x55555557a6b0 *)
xor ymm9_0@uint64 ymm6_0 ymm3_0;
xor ymm9_1@uint64 ymm6_1 ymm3_1;
xor ymm9_2@uint64 ymm6_2 ymm3_2;
xor ymm9_3@uint64 ymm6_3 ymm3_3;
(* vpsrlq $0x13,%ymm9,%ymm12                       #! PC = 0x55555557a6b4 *)
shr ymm12_0 ymm9_0 0x13@uint64;
shr ymm12_1 ymm9_1 0x13@uint64;
shr ymm12_2 ymm9_2 0x13@uint64;
shr ymm12_3 ymm9_3 0x13@uint64;
(* vpsllq $0x2d,%ymm9,%ymm6                        #! PC = 0x55555557a6ba *)
shl ymm6_0 ymm9_0 0x2d@uint64;
shl ymm6_1 ymm9_1 0x2d@uint64;
shl ymm6_2 ymm9_2 0x2d@uint64;
shl ymm6_3 ymm9_3 0x2d@uint64;
(* vpor   %ymm6,%ymm12,%ymm6                       #! PC = 0x55555557a6c0 *)
or ymm6_0@uint64 ymm12_0 ymm6_0;
or ymm6_1@uint64 ymm12_1 ymm6_1;
or ymm6_2@uint64 ymm12_2 ymm6_2;
or ymm6_3@uint64 ymm12_3 ymm6_3;
(* vpandn %ymm6,%ymm8,%ymm9                        #! PC = 0x55555557a6c4 *)
not ymm8_0n@uint64 ymm8_0;
and ymm9_0@uint64 ymm8_0n ymm6_0;
not ymm8_1n@uint64 ymm8_1;
and ymm9_1@uint64 ymm8_1n ymm6_1;
not ymm8_2n@uint64 ymm8_2;
and ymm9_2@uint64 ymm8_2n ymm6_2;
not ymm8_3n@uint64 ymm8_3;
and ymm9_3@uint64 ymm8_3n ymm6_3;
(* vpxor  %ymm2,%ymm9,%ymm9                        #! PC = 0x55555557a6c8 *)
xor ymm9_0@uint64 ymm9_0 ymm2_0;
xor ymm9_1@uint64 ymm9_1 ymm2_1;
xor ymm9_2@uint64 ymm9_2 ymm2_2;
xor ymm9_3@uint64 ymm9_3 ymm2_3;
(* vmovdqa %ymm9,-0x170(%rbp)                      #! EA = L0x7fffffffbde0; PC = 0x55555557a6cc *)
mov L0x7fffffffbde0 ymm9_0;
mov L0x7fffffffbde8 ymm9_1;
mov L0x7fffffffbdf0 ymm9_2;
mov L0x7fffffffbdf8 ymm9_3;
(* vpsrlq $0x3,%ymm1,%ymm9                         #! PC = 0x55555557a6d4 *)
shr ymm9_0 ymm1_0 0x3@uint64;
shr ymm9_1 ymm1_1 0x3@uint64;
shr ymm9_2 ymm1_2 0x3@uint64;
shr ymm9_3 ymm1_3 0x3@uint64;
(* vpsllq $0x3d,%ymm1,%ymm1                        #! PC = 0x55555557a6d9 *)
shl ymm1_0 ymm1_0 0x3d@uint64;
shl ymm1_1 ymm1_1 0x3d@uint64;
shl ymm1_2 ymm1_2 0x3d@uint64;
shl ymm1_3 ymm1_3 0x3d@uint64;
(* vpor   %ymm1,%ymm9,%ymm1                        #! PC = 0x55555557a6de *)
or ymm1_0@uint64 ymm9_0 ymm1_0;
or ymm1_1@uint64 ymm9_1 ymm1_1;
or ymm1_2@uint64 ymm9_2 ymm1_2;
or ymm1_3@uint64 ymm9_3 ymm1_3;
(* vpandn %ymm1,%ymm6,%ymm9                        #! PC = 0x55555557a6e2 *)
not ymm6_0n@uint64 ymm6_0;
and ymm9_0@uint64 ymm6_0n ymm1_0;
not ymm6_1n@uint64 ymm6_1;
and ymm9_1@uint64 ymm6_1n ymm1_1;
not ymm6_2n@uint64 ymm6_2;
and ymm9_2@uint64 ymm6_2n ymm1_2;
not ymm6_3n@uint64 ymm6_3;
and ymm9_3@uint64 ymm6_3n ymm1_3;
(* vpxor  %ymm8,%ymm9,%ymm8                        #! PC = 0x55555557a6e6 *)
xor ymm8_0@uint64 ymm9_0 ymm8_0;
xor ymm8_1@uint64 ymm9_1 ymm8_1;
xor ymm8_2@uint64 ymm9_2 ymm8_2;
xor ymm8_3@uint64 ymm9_3 ymm8_3;
(* vmovdqa %ymm8,-0x190(%rbp)                      #! EA = L0x7fffffffbdc0; PC = 0x55555557a6eb *)
mov L0x7fffffffbdc0 ymm8_0;
mov L0x7fffffffbdc8 ymm8_1;
mov L0x7fffffffbdd0 ymm8_2;
mov L0x7fffffffbdd8 ymm8_3;
(* vpandn %ymm7,%ymm1,%ymm8                        #! PC = 0x55555557a6f3 *)
not ymm1_0n@uint64 ymm1_0;
and ymm8_0@uint64 ymm1_0n ymm7_0;
not ymm1_1n@uint64 ymm1_1;
and ymm8_1@uint64 ymm1_1n ymm7_1;
not ymm1_2n@uint64 ymm1_2;
and ymm8_2@uint64 ymm1_2n ymm7_2;
not ymm1_3n@uint64 ymm1_3;
and ymm8_3@uint64 ymm1_3n ymm7_3;
(* vpandn %ymm2,%ymm7,%ymm7                        #! PC = 0x55555557a6f7 *)
not ymm7_0n@uint64 ymm7_0;
and ymm7_0@uint64 ymm7_0n ymm2_0;
not ymm7_1n@uint64 ymm7_1;
and ymm7_1@uint64 ymm7_1n ymm2_1;
not ymm7_2n@uint64 ymm7_2;
and ymm7_2@uint64 ymm7_2n ymm2_2;
not ymm7_3n@uint64 ymm7_3;
and ymm7_3@uint64 ymm7_3n ymm2_3;
(* vpxor  %ymm1,%ymm7,%ymm2                        #! PC = 0x55555557a6fb *)
xor ymm2_0@uint64 ymm7_0 ymm1_0;
xor ymm2_1@uint64 ymm7_1 ymm1_1;
xor ymm2_2@uint64 ymm7_2 ymm1_2;
xor ymm2_3@uint64 ymm7_3 ymm1_3;
(* vpxor  -0xb0(%rbp),%ymm5,%ymm7                  #! EA = L0x7fffffffbea0; Value = 0x0ea0ddc0d9366302; PC = 0x55555557a6ff *)
xor ymm7_0@uint64 ymm5_0 L0x7fffffffbea0;
xor ymm7_1@uint64 ymm5_1 L0x7fffffffbea8;
xor ymm7_2@uint64 ymm5_2 L0x7fffffffbeb0;
xor ymm7_3@uint64 ymm5_3 L0x7fffffffbeb8;
(* vpxor  %ymm6,%ymm8,%ymm12                       #! PC = 0x55555557a707 *)
xor ymm12_0@uint64 ymm8_0 ymm6_0;
xor ymm12_1@uint64 ymm8_1 ymm6_1;
xor ymm12_2@uint64 ymm8_2 ymm6_2;
xor ymm12_3@uint64 ymm8_3 ymm6_3;
(* vmovdqa %ymm2,-0x90(%rbp)                       #! EA = L0x7fffffffbec0; PC = 0x55555557a70b *)
mov L0x7fffffffbec0 ymm2_0;
mov L0x7fffffffbec8 ymm2_1;
mov L0x7fffffffbed0 ymm2_2;
mov L0x7fffffffbed8 ymm2_3;
(* vpxor  -0xd0(%rbp),%ymm3,%ymm2                  #! EA = L0x7fffffffbe80; Value = 0x03c4dfa24420069f; PC = 0x55555557a713 *)
xor ymm2_0@uint64 ymm3_0 L0x7fffffffbe80;
xor ymm2_1@uint64 ymm3_1 L0x7fffffffbe88;
xor ymm2_2@uint64 ymm3_2 L0x7fffffffbe90;
xor ymm2_3@uint64 ymm3_3 L0x7fffffffbe98;
(* vpxor  -0x1b0(%rbp),%ymm10,%ymm6                #! EA = L0x7fffffffbda0; Value = 0x16e2aa8036b5013d; PC = 0x55555557a71b *)
xor ymm6_0@uint64 ymm10_0 L0x7fffffffbda0;
xor ymm6_1@uint64 ymm10_1 L0x7fffffffbda8;
xor ymm6_2@uint64 ymm10_2 L0x7fffffffbdb0;
xor ymm6_3@uint64 ymm10_3 L0x7fffffffbdb8;
(* vpsllq $0x6,%ymm7,%ymm9                         #! PC = 0x55555557a723 *)
shl ymm9_0 ymm7_0 0x6@uint64;
shl ymm9_1 ymm7_1 0x6@uint64;
shl ymm9_2 ymm7_2 0x6@uint64;
shl ymm9_3 ymm7_3 0x6@uint64;
(* vmovdqa %ymm12,-0x70(%rbp)                      #! EA = L0x7fffffffbee0; PC = 0x55555557a728 *)
mov L0x7fffffffbee0 ymm12_0;
mov L0x7fffffffbee8 ymm12_1;
mov L0x7fffffffbef0 ymm12_2;
mov L0x7fffffffbef8 ymm12_3;
(* vpxor  -0x330(%rbp),%ymm11,%ymm12               #! EA = L0x7fffffffbc20; Value = 0xc52e05ccf15f628f; PC = 0x55555557a72d *)
xor ymm12_0@uint64 ymm11_0 L0x7fffffffbc20;
xor ymm12_1@uint64 ymm11_1 L0x7fffffffbc28;
xor ymm12_2@uint64 ymm11_2 L0x7fffffffbc30;
xor ymm12_3@uint64 ymm11_3 L0x7fffffffbc38;
(* vpsrlq $0x3f,%ymm2,%ymm1                        #! PC = 0x55555557a735 *)
shr ymm1_0 ymm2_0 0x3f@uint64;
shr ymm1_1 ymm2_1 0x3f@uint64;
shr ymm1_2 ymm2_2 0x3f@uint64;
shr ymm1_3 ymm2_3 0x3f@uint64;
(* vpsllq $0x1,%ymm2,%ymm2                         #! PC = 0x55555557a73a *)
shl ymm2_0 ymm2_0 0x1@uint64;
shl ymm2_1 ymm2_1 0x1@uint64;
shl ymm2_2 ymm2_2 0x1@uint64;
shl ymm2_3 ymm2_3 0x1@uint64;
(* vpor   %ymm2,%ymm1,%ymm1                        #! PC = 0x55555557a73f *)
or ymm1_0@uint64 ymm1_0 ymm2_0;
or ymm1_1@uint64 ymm1_1 ymm2_1;
or ymm1_2@uint64 ymm1_2 ymm2_2;
or ymm1_3@uint64 ymm1_3 ymm2_3;
(* vpsrlq $0x3a,%ymm7,%ymm2                        #! PC = 0x55555557a743 *)
shr ymm2_0 ymm7_0 0x3a@uint64;
shr ymm2_1 ymm7_1 0x3a@uint64;
shr ymm2_2 ymm7_2 0x3a@uint64;
shr ymm2_3 ymm7_3 0x3a@uint64;
(* vpshufb 0x537af(%rip),%ymm12,%ymm12        # 0x5555555cdf00 <rho8>#! EA = L0x5555555cdf00; Value = 0x0605040302010007; PC = 0x55555557a748 *)
inline vpshufb128(L0x5555555cdf00, L0x5555555cdf08, ymm12_0, ymm12_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdf10, L0x5555555cdf18, ymm12_2, ymm12_3, tmp_2, tmp_3);
mov ymm12_0 tmp_0;
mov ymm12_1 tmp_1;
mov ymm12_2 tmp_2;
mov ymm12_3 tmp_3;
(* vpor   %ymm9,%ymm2,%ymm9                        #! PC = 0x55555557a751 *)
or ymm9_0@uint64 ymm2_0 ymm9_0;
or ymm9_1@uint64 ymm2_1 ymm9_1;
or ymm9_2@uint64 ymm2_2 ymm9_2;
or ymm9_3@uint64 ymm2_3 ymm9_3;
(* vpsrlq $0x27,%ymm6,%ymm13                       #! PC = 0x55555557a756 *)
shr ymm13_0 ymm6_0 0x27@uint64;
shr ymm13_1 ymm6_1 0x27@uint64;
shr ymm13_2 ymm6_2 0x27@uint64;
shr ymm13_3 ymm6_3 0x27@uint64;
(* vpxor  -0x230(%rbp),%ymm3,%ymm7                 #! EA = L0x7fffffffbd20; Value = 0x76004ea89b563e26; PC = 0x55555557a75b *)
xor ymm7_0@uint64 ymm3_0 L0x7fffffffbd20;
xor ymm7_1@uint64 ymm3_1 L0x7fffffffbd28;
xor ymm7_2@uint64 ymm3_2 L0x7fffffffbd30;
xor ymm7_3@uint64 ymm3_3 L0x7fffffffbd38;
(* vpsllq $0x19,%ymm6,%ymm2                        #! PC = 0x55555557a763 *)
shl ymm2_0 ymm6_0 0x19@uint64;
shl ymm2_1 ymm6_1 0x19@uint64;
shl ymm2_2 ymm6_2 0x19@uint64;
shl ymm2_3 ymm6_3 0x19@uint64;
(* vpor   %ymm2,%ymm13,%ymm2                       #! PC = 0x55555557a768 *)
or ymm2_0@uint64 ymm13_0 ymm2_0;
or ymm2_1@uint64 ymm13_1 ymm2_1;
or ymm2_2@uint64 ymm13_2 ymm2_2;
or ymm2_3@uint64 ymm13_3 ymm2_3;
(* vpandn %ymm2,%ymm9,%ymm6                        #! PC = 0x55555557a76c *)
not ymm9_0n@uint64 ymm9_0;
and ymm6_0@uint64 ymm9_0n ymm2_0;
not ymm9_1n@uint64 ymm9_1;
and ymm6_1@uint64 ymm9_1n ymm2_1;
not ymm9_2n@uint64 ymm9_2;
and ymm6_2@uint64 ymm9_2n ymm2_2;
not ymm9_3n@uint64 ymm9_3;
and ymm6_3@uint64 ymm9_3n ymm2_3;
(* vpandn %ymm12,%ymm2,%ymm8                       #! PC = 0x55555557a770 *)
not ymm2_0n@uint64 ymm2_0;
and ymm8_0@uint64 ymm2_0n ymm12_0;
not ymm2_1n@uint64 ymm2_1;
and ymm8_1@uint64 ymm2_1n ymm12_1;
not ymm2_2n@uint64 ymm2_2;
and ymm8_2@uint64 ymm2_2n ymm12_2;
not ymm2_3n@uint64 ymm2_3;
and ymm8_3@uint64 ymm2_3n ymm12_3;
(* vpxor  %ymm1,%ymm6,%ymm13                       #! PC = 0x55555557a775 *)
xor ymm13_0@uint64 ymm6_0 ymm1_0;
xor ymm13_1@uint64 ymm6_1 ymm1_1;
xor ymm13_2@uint64 ymm6_2 ymm1_2;
xor ymm13_3@uint64 ymm6_3 ymm1_3;
(* vpsrlq $0x2e,%ymm0,%ymm6                        #! PC = 0x55555557a779 *)
shr ymm6_0 ymm0_0 0x2e@uint64;
shr ymm6_1 ymm0_1 0x2e@uint64;
shr ymm6_2 ymm0_2 0x2e@uint64;
shr ymm6_3 ymm0_3 0x2e@uint64;
(* vpxor  %ymm9,%ymm8,%ymm15                       #! PC = 0x55555557a77e *)
xor ymm15_0@uint64 ymm8_0 ymm9_0;
xor ymm15_1@uint64 ymm8_1 ymm9_1;
xor ymm15_2@uint64 ymm8_2 ymm9_2;
xor ymm15_3@uint64 ymm8_3 ymm9_3;
(* vpsllq $0x12,%ymm0,%ymm0                        #! PC = 0x55555557a783 *)
shl ymm0_0 ymm0_0 0x12@uint64;
shl ymm0_1 ymm0_1 0x12@uint64;
shl ymm0_2 ymm0_2 0x12@uint64;
shl ymm0_3 ymm0_3 0x12@uint64;
(* vmovdqa %ymm13,-0x1b0(%rbp)                     #! EA = L0x7fffffffbda0; PC = 0x55555557a788 *)
mov L0x7fffffffbda0 ymm13_0;
mov L0x7fffffffbda8 ymm13_1;
mov L0x7fffffffbdb0 ymm13_2;
mov L0x7fffffffbdb8 ymm13_3;
(* vpxor  -0x210(%rbp),%ymm4,%ymm8                 #! EA = L0x7fffffffbd40; Value = 0x04a77bfa126fc093; PC = 0x55555557a790 *)
xor ymm8_0@uint64 ymm4_0 L0x7fffffffbd40;
xor ymm8_1@uint64 ymm4_1 L0x7fffffffbd48;
xor ymm8_2@uint64 ymm4_2 L0x7fffffffbd50;
xor ymm8_3@uint64 ymm4_3 L0x7fffffffbd58;
(* vpor   %ymm0,%ymm6,%ymm0                        #! PC = 0x55555557a798 *)
or ymm0_0@uint64 ymm6_0 ymm0_0;
or ymm0_1@uint64 ymm6_1 ymm0_1;
or ymm0_2@uint64 ymm6_2 ymm0_2;
or ymm0_3@uint64 ymm6_3 ymm0_3;
(* vmovdqa %ymm15,-0x3d0(%rbp)                     #! EA = L0x7fffffffbb80; PC = 0x55555557a79c *)
mov L0x7fffffffbb80 ymm15_0;
mov L0x7fffffffbb88 ymm15_1;
mov L0x7fffffffbb90 ymm15_2;
mov L0x7fffffffbb98 ymm15_3;
(* vpandn %ymm0,%ymm12,%ymm13                      #! PC = 0x55555557a7a4 *)
not ymm12_0n@uint64 ymm12_0;
and ymm13_0@uint64 ymm12_0n ymm0_0;
not ymm12_1n@uint64 ymm12_1;
and ymm13_1@uint64 ymm12_1n ymm0_1;
not ymm12_2n@uint64 ymm12_2;
and ymm13_2@uint64 ymm12_2n ymm0_2;
not ymm12_3n@uint64 ymm12_3;
and ymm13_3@uint64 ymm12_3n ymm0_3;
(* vpxor  %ymm2,%ymm13,%ymm13                      #! PC = 0x55555557a7a8 *)
xor ymm13_0@uint64 ymm13_0 ymm2_0;
xor ymm13_1@uint64 ymm13_1 ymm2_1;
xor ymm13_2@uint64 ymm13_2 ymm2_2;
xor ymm13_3@uint64 ymm13_3 ymm2_3;
(* vpandn %ymm1,%ymm0,%ymm2                        #! PC = 0x55555557a7ac *)
not ymm0_0n@uint64 ymm0_0;
and ymm2_0@uint64 ymm0_0n ymm1_0;
not ymm0_1n@uint64 ymm0_1;
and ymm2_1@uint64 ymm0_1n ymm1_1;
not ymm0_2n@uint64 ymm0_2;
and ymm2_2@uint64 ymm0_2n ymm1_2;
not ymm0_3n@uint64 ymm0_3;
and ymm2_3@uint64 ymm0_3n ymm1_3;
(* vpandn %ymm9,%ymm1,%ymm1                        #! PC = 0x55555557a7b0 *)
not ymm1_0n@uint64 ymm1_0;
and ymm1_0@uint64 ymm1_0n ymm9_0;
not ymm1_1n@uint64 ymm1_1;
and ymm1_1@uint64 ymm1_1n ymm9_1;
not ymm1_2n@uint64 ymm1_2;
and ymm1_2@uint64 ymm1_2n ymm9_2;
not ymm1_3n@uint64 ymm1_3;
and ymm1_3@uint64 ymm1_3n ymm9_3;
(* vpxor  %ymm0,%ymm1,%ymm9                        #! PC = 0x55555557a7b5 *)
xor ymm9_0@uint64 ymm1_0 ymm0_0;
xor ymm9_1@uint64 ymm1_1 ymm0_1;
xor ymm9_2@uint64 ymm1_2 ymm0_2;
xor ymm9_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x36,%ymm7,%ymm0                        #! PC = 0x55555557a7b9 *)
shr ymm0_0 ymm7_0 0x36@uint64;
shr ymm0_1 ymm7_1 0x36@uint64;
shr ymm0_2 ymm7_2 0x36@uint64;
shr ymm0_3 ymm7_3 0x36@uint64;
(* vpxor  %ymm12,%ymm2,%ymm6                       #! PC = 0x55555557a7be *)
xor ymm6_0@uint64 ymm2_0 ymm12_0;
xor ymm6_1@uint64 ymm2_1 ymm12_1;
xor ymm6_2@uint64 ymm2_2 ymm12_2;
xor ymm6_3@uint64 ymm2_3 ymm12_3;
(* vmovdqa %ymm9,-0xb0(%rbp)                       #! EA = L0x7fffffffbea0; PC = 0x55555557a7c3 *)
mov L0x7fffffffbea0 ymm9_0;
mov L0x7fffffffbea8 ymm9_1;
mov L0x7fffffffbeb0 ymm9_2;
mov L0x7fffffffbeb8 ymm9_3;
(* vpxor  -0x1d0(%rbp),%ymm11,%ymm9                #! EA = L0x7fffffffbd80; Value = 0xd8f2b86d8510572f; PC = 0x55555557a7cb *)
xor ymm9_0@uint64 ymm11_0 L0x7fffffffbd80;
xor ymm9_1@uint64 ymm11_1 L0x7fffffffbd88;
xor ymm9_2@uint64 ymm11_2 L0x7fffffffbd90;
xor ymm9_3@uint64 ymm11_3 L0x7fffffffbd98;
(* vpsrlq $0x1c,%ymm8,%ymm12                       #! PC = 0x55555557a7d3 *)
shr ymm12_0 ymm8_0 0x1c@uint64;
shr ymm12_1 ymm8_1 0x1c@uint64;
shr ymm12_2 ymm8_2 0x1c@uint64;
shr ymm12_3 ymm8_3 0x1c@uint64;
(* vpsllq $0xa,%ymm7,%ymm7                         #! PC = 0x55555557a7d9 *)
shl ymm7_0 ymm7_0 0xa@uint64;
shl ymm7_1 ymm7_1 0xa@uint64;
shl ymm7_2 ymm7_2 0xa@uint64;
shl ymm7_3 ymm7_3 0xa@uint64;
(* vpsllq $0x24,%ymm8,%ymm8                        #! PC = 0x55555557a7de *)
shl ymm8_0 ymm8_0 0x24@uint64;
shl ymm8_1 ymm8_1 0x24@uint64;
shl ymm8_2 ymm8_2 0x24@uint64;
shl ymm8_3 ymm8_3 0x24@uint64;
(* vmovdqa %ymm6,-0xd0(%rbp)                       #! EA = L0x7fffffffbe80; PC = 0x55555557a7e4 *)
mov L0x7fffffffbe80 ymm6_0;
mov L0x7fffffffbe88 ymm6_1;
mov L0x7fffffffbe90 ymm6_2;
mov L0x7fffffffbe98 ymm6_3;
(* vmovdqa -0x170(%rbp),%ymm6                      #! EA = L0x7fffffffbde0; Value = 0x90fee5a0a44647c4; PC = 0x55555557a7ec *)
mov ymm6_0 L0x7fffffffbde0;
mov ymm6_1 L0x7fffffffbde8;
mov ymm6_2 L0x7fffffffbdf0;
mov ymm6_3 L0x7fffffffbdf8;
(* vpor   %ymm8,%ymm12,%ymm12                      #! PC = 0x55555557a7f4 *)
or ymm12_0@uint64 ymm12_0 ymm8_0;
or ymm12_1@uint64 ymm12_1 ymm8_1;
or ymm12_2@uint64 ymm12_2 ymm8_2;
or ymm12_3@uint64 ymm12_3 ymm8_3;
(* vpsrlq $0x25,%ymm9,%ymm2                        #! PC = 0x55555557a7f9 *)
shr ymm2_0 ymm9_0 0x25@uint64;
shr ymm2_1 ymm9_1 0x25@uint64;
shr ymm2_2 ymm9_2 0x25@uint64;
shr ymm2_3 ymm9_3 0x25@uint64;
(* vpor   %ymm7,%ymm0,%ymm7                        #! PC = 0x55555557a7ff *)
or ymm7_0@uint64 ymm0_0 ymm7_0;
or ymm7_1@uint64 ymm0_1 ymm7_1;
or ymm7_2@uint64 ymm0_2 ymm7_2;
or ymm7_3@uint64 ymm0_3 ymm7_3;
(* vpsllq $0x1b,%ymm9,%ymm9                        #! PC = 0x55555557a803 *)
shl ymm9_0 ymm9_0 0x1b@uint64;
shl ymm9_1 ymm9_1 0x1b@uint64;
shl ymm9_2 ymm9_2 0x1b@uint64;
shl ymm9_3 ymm9_3 0x1b@uint64;
(* vpandn %ymm7,%ymm12,%ymm0                       #! PC = 0x55555557a809 *)
not ymm12_0n@uint64 ymm12_0;
and ymm0_0@uint64 ymm12_0n ymm7_0;
not ymm12_1n@uint64 ymm12_1;
and ymm0_1@uint64 ymm12_1n ymm7_1;
not ymm12_2n@uint64 ymm12_2;
and ymm0_2@uint64 ymm12_2n ymm7_2;
not ymm12_3n@uint64 ymm12_3;
and ymm0_3@uint64 ymm12_3n ymm7_3;
(* vpor   %ymm9,%ymm2,%ymm2                        #! PC = 0x55555557a80d *)
or ymm2_0@uint64 ymm2_0 ymm9_0;
or ymm2_1@uint64 ymm2_1 ymm9_1;
or ymm2_2@uint64 ymm2_2 ymm9_2;
or ymm2_3@uint64 ymm2_3 ymm9_3;
(* vpxor  %ymm2,%ymm0,%ymm1                        #! PC = 0x55555557a812 *)
xor ymm1_0@uint64 ymm0_0 ymm2_0;
xor ymm1_1@uint64 ymm0_1 ymm2_1;
xor ymm1_2@uint64 ymm0_2 ymm2_2;
xor ymm1_3@uint64 ymm0_3 ymm2_3;
(* vpxor  -0x290(%rbp),%ymm5,%ymm0                 #! EA = L0x7fffffffbcc0; Value = 0x0d7175255383fc27; PC = 0x55555557a816 *)
xor ymm0_0@uint64 ymm5_0 L0x7fffffffbcc0;
xor ymm0_1@uint64 ymm5_1 L0x7fffffffbcc8;
xor ymm0_2@uint64 ymm5_2 L0x7fffffffbcd0;
xor ymm0_3@uint64 ymm5_3 L0x7fffffffbcd8;
(* vmovdqa %ymm1,-0x1d0(%rbp)                      #! EA = L0x7fffffffbd80; PC = 0x55555557a81e *)
mov L0x7fffffffbd80 ymm1_0;
mov L0x7fffffffbd88 ymm1_1;
mov L0x7fffffffbd90 ymm1_2;
mov L0x7fffffffbd98 ymm1_3;
(* vpxor  %ymm14,%ymm6,%ymm1                       #! PC = 0x55555557a826 *)
xor ymm1_0@uint64 ymm6_0 ymm14_0;
xor ymm1_1@uint64 ymm6_1 ymm14_1;
xor ymm1_2@uint64 ymm6_2 ymm14_2;
xor ymm1_3@uint64 ymm6_3 ymm14_3;
(* vpxor  -0x250(%rbp),%ymm10,%ymm14               #! EA = L0x7fffffffbd00; Value = 0x9d2fe354084dd32a; PC = 0x55555557a82b *)
xor ymm14_0@uint64 ymm10_0 L0x7fffffffbd00;
xor ymm14_1@uint64 ymm10_1 L0x7fffffffbd08;
xor ymm14_2@uint64 ymm10_2 L0x7fffffffbd10;
xor ymm14_3@uint64 ymm10_3 L0x7fffffffbd18;
(* vpsrlq $0x31,%ymm0,%ymm9                        #! PC = 0x55555557a833 *)
shr ymm9_0 ymm0_0 0x31@uint64;
shr ymm9_1 ymm0_1 0x31@uint64;
shr ymm9_2 ymm0_2 0x31@uint64;
shr ymm9_3 ymm0_3 0x31@uint64;
(* vpsllq $0xf,%ymm0,%ymm0                         #! PC = 0x55555557a838 *)
shl ymm0_0 ymm0_0 0xf@uint64;
shl ymm0_1 ymm0_1 0xf@uint64;
shl ymm0_2 ymm0_2 0xf@uint64;
shl ymm0_3 ymm0_3 0xf@uint64;
(* vpor   %ymm0,%ymm9,%ymm9                        #! PC = 0x55555557a83d *)
or ymm9_0@uint64 ymm9_0 ymm0_0;
or ymm9_1@uint64 ymm9_1 ymm0_1;
or ymm9_2@uint64 ymm9_2 ymm0_2;
or ymm9_3@uint64 ymm9_3 ymm0_3;
(* vpandn %ymm9,%ymm7,%ymm0                        #! PC = 0x55555557a841 *)
not ymm7_0n@uint64 ymm7_0;
and ymm0_0@uint64 ymm7_0n ymm9_0;
not ymm7_1n@uint64 ymm7_1;
and ymm0_1@uint64 ymm7_1n ymm9_1;
not ymm7_2n@uint64 ymm7_2;
and ymm0_2@uint64 ymm7_2n ymm9_2;
not ymm7_3n@uint64 ymm7_3;
and ymm0_3@uint64 ymm7_3n ymm9_3;
(* vpxor  %ymm12,%ymm0,%ymm0                       #! PC = 0x55555557a846 *)
xor ymm0_0@uint64 ymm0_0 ymm12_0;
xor ymm0_1@uint64 ymm0_1 ymm12_1;
xor ymm0_2@uint64 ymm0_2 ymm12_2;
xor ymm0_3@uint64 ymm0_3 ymm12_3;
(* vpxor  %ymm15,%ymm0,%ymm8                       #! PC = 0x55555557a84b *)
xor ymm8_0@uint64 ymm0_0 ymm15_0;
xor ymm8_1@uint64 ymm0_1 ymm15_1;
xor ymm8_2@uint64 ymm0_2 ymm15_2;
xor ymm8_3@uint64 ymm0_3 ymm15_3;
(* vmovdqa %ymm0,-0x210(%rbp)                      #! EA = L0x7fffffffbd40; PC = 0x55555557a850 *)
mov L0x7fffffffbd40 ymm0_0;
mov L0x7fffffffbd48 ymm0_1;
mov L0x7fffffffbd50 ymm0_2;
mov L0x7fffffffbd58 ymm0_3;
(* vpshufb 0x5367f(%rip),%ymm14,%ymm0        # 0x5555555cdee0 <rho56>#! EA = L0x5555555cdee0; Value = 0x0007060504030201; PC = 0x55555557a858 *)
inline vpshufb128(L0x5555555cdee0, L0x5555555cdee8, ymm14_0, ymm14_1, tmp_0, tmp_1);
inline vpshufb128(L0x5555555cdef0, L0x5555555cdef8, ymm14_2, ymm14_3, tmp_2, tmp_3);
mov ymm0_0 tmp_0;
mov ymm0_1 tmp_1;
mov ymm0_2 tmp_2;
mov ymm0_3 tmp_3;
(* vmovdqa -0x190(%rbp),%ymm14                     #! EA = L0x7fffffffbdc0; Value = 0x8c5bda0cd6192e76; PC = 0x55555557a861 *)
mov ymm14_0 L0x7fffffffbdc0;
mov ymm14_1 L0x7fffffffbdc8;
mov ymm14_2 L0x7fffffffbdd0;
mov ymm14_3 L0x7fffffffbdd8;
(* vpxor  %ymm1,%ymm8,%ymm8                        #! PC = 0x55555557a869 *)
xor ymm8_0@uint64 ymm8_0 ymm1_0;
xor ymm8_1@uint64 ymm8_1 ymm1_1;
xor ymm8_2@uint64 ymm8_2 ymm1_2;
xor ymm8_3@uint64 ymm8_3 ymm1_3;
(* vpandn %ymm0,%ymm9,%ymm1                        #! PC = 0x55555557a86d *)
not ymm9_0n@uint64 ymm9_0;
and ymm1_0@uint64 ymm9_0n ymm0_0;
not ymm9_1n@uint64 ymm9_1;
and ymm1_1@uint64 ymm9_1n ymm0_1;
not ymm9_2n@uint64 ymm9_2;
and ymm1_2@uint64 ymm9_2n ymm0_2;
not ymm9_3n@uint64 ymm9_3;
and ymm1_3@uint64 ymm9_3n ymm0_3;
(* vpxor  -0x270(%rbp),%ymm10,%ymm10               #! EA = L0x7fffffffbce0; Value = 0x262012b5083c91ac; PC = 0x55555557a871 *)
xor ymm10_0@uint64 ymm10_0 L0x7fffffffbce0;
xor ymm10_1@uint64 ymm10_1 L0x7fffffffbce8;
xor ymm10_2@uint64 ymm10_2 L0x7fffffffbcf0;
xor ymm10_3@uint64 ymm10_3 L0x7fffffffbcf8;
(* vpxor  %ymm7,%ymm1,%ymm7                        #! PC = 0x55555557a879 *)
xor ymm7_0@uint64 ymm1_0 ymm7_0;
xor ymm7_1@uint64 ymm1_1 ymm7_1;
xor ymm7_2@uint64 ymm1_2 ymm7_2;
xor ymm7_3@uint64 ymm1_3 ymm7_3;
(* vpxor  -0x310(%rbp),%ymm11,%ymm11               #! EA = L0x7fffffffbc40; Value = 0xe249aeb39c95d7db; PC = 0x55555557a87d *)
xor ymm11_0@uint64 ymm11_0 L0x7fffffffbc40;
xor ymm11_1@uint64 ymm11_1 L0x7fffffffbc48;
xor ymm11_2@uint64 ymm11_2 L0x7fffffffbc50;
xor ymm11_3@uint64 ymm11_3 L0x7fffffffbc58;
(* vpxor  -0x110(%rbp),%ymm14,%ymm1                #! EA = L0x7fffffffbe40; Value = 0xd598261ea65aa9ee; PC = 0x55555557a885 *)
xor ymm1_0@uint64 ymm14_0 L0x7fffffffbe40;
xor ymm1_1@uint64 ymm14_1 L0x7fffffffbe48;
xor ymm1_2@uint64 ymm14_2 L0x7fffffffbe50;
xor ymm1_3@uint64 ymm14_3 L0x7fffffffbe58;
(* vpxor  %ymm7,%ymm13,%ymm6                       #! PC = 0x55555557a88d *)
xor ymm6_0@uint64 ymm13_0 ymm7_0;
xor ymm6_1@uint64 ymm13_1 ymm7_1;
xor ymm6_2@uint64 ymm13_2 ymm7_2;
xor ymm6_3@uint64 ymm13_3 ymm7_3;
(* vpandn %ymm12,%ymm2,%ymm14                      #! PC = 0x55555557a891 *)
not ymm2_0n@uint64 ymm2_0;
and ymm14_0@uint64 ymm2_0n ymm12_0;
not ymm2_1n@uint64 ymm2_1;
and ymm14_1@uint64 ymm2_1n ymm12_1;
not ymm2_2n@uint64 ymm2_2;
and ymm14_2@uint64 ymm2_2n ymm12_2;
not ymm2_3n@uint64 ymm2_3;
and ymm14_3@uint64 ymm2_3n ymm12_3;
(* vpxor  -0x2f0(%rbp),%ymm5,%ymm5                 #! EA = L0x7fffffffbc60; Value = 0x4247f8944d4ae7fe; PC = 0x55555557a896 *)
xor ymm5_0@uint64 ymm5_0 L0x7fffffffbc60;
xor ymm5_1@uint64 ymm5_1 L0x7fffffffbc68;
xor ymm5_2@uint64 ymm5_2 L0x7fffffffbc70;
xor ymm5_3@uint64 ymm5_3 L0x7fffffffbc78;
(* vmovdqa %ymm7,-0x230(%rbp)                      #! EA = L0x7fffffffbd20; PC = 0x55555557a89e *)
mov L0x7fffffffbd20 ymm7_0;
mov L0x7fffffffbd28 ymm7_1;
mov L0x7fffffffbd30 ymm7_2;
mov L0x7fffffffbd38 ymm7_3;
(* vpsrlq $0x19,%ymm11,%ymm15                      #! PC = 0x55555557a8a6 *)
shr ymm15_0 ymm11_0 0x19@uint64;
shr ymm15_1 ymm11_1 0x19@uint64;
shr ymm15_2 ymm11_2 0x19@uint64;
shr ymm15_3 ymm11_3 0x19@uint64;
(* vpxor  %ymm1,%ymm6,%ymm6                        #! PC = 0x55555557a8ac *)
xor ymm6_0@uint64 ymm6_0 ymm1_0;
xor ymm6_1@uint64 ymm6_1 ymm1_1;
xor ymm6_2@uint64 ymm6_2 ymm1_2;
xor ymm6_3@uint64 ymm6_3 ymm1_3;
(* vpxor  %ymm0,%ymm14,%ymm14                      #! PC = 0x55555557a8b0 *)
xor ymm14_0@uint64 ymm14_0 ymm0_0;
xor ymm14_1@uint64 ymm14_1 ymm0_1;
xor ymm14_2@uint64 ymm14_2 ymm0_2;
xor ymm14_3@uint64 ymm14_3 ymm0_3;
(* vpsllq $0x27,%ymm11,%ymm11                      #! PC = 0x55555557a8b4 *)
shl ymm11_0 ymm11_0 0x27@uint64;
shl ymm11_1 ymm11_1 0x27@uint64;
shl ymm11_2 ymm11_2 0x27@uint64;
shl ymm11_3 ymm11_3 0x27@uint64;
(* vpandn %ymm2,%ymm0,%ymm1                        #! PC = 0x55555557a8ba *)
not ymm0_0n@uint64 ymm0_0;
and ymm1_0@uint64 ymm0_0n ymm2_0;
not ymm0_1n@uint64 ymm0_1;
and ymm1_1@uint64 ymm0_1n ymm2_1;
not ymm0_2n@uint64 ymm0_2;
and ymm1_2@uint64 ymm0_2n ymm2_2;
not ymm0_3n@uint64 ymm0_3;
and ymm1_3@uint64 ymm0_3n ymm2_3;
(* vpxor  -0xf0(%rbp),%ymm4,%ymm4                  #! EA = L0x7fffffffbe60; Value = 0x58c5f58b0b1e40ce; PC = 0x55555557a8be *)
xor ymm4_0@uint64 ymm4_0 L0x7fffffffbe60;
xor ymm4_1@uint64 ymm4_1 L0x7fffffffbe68;
xor ymm4_2@uint64 ymm4_2 L0x7fffffffbe70;
xor ymm4_3@uint64 ymm4_3 L0x7fffffffbe78;
(* vpsrlq $0x9,%ymm10,%ymm0                        #! PC = 0x55555557a8c6 *)
shr ymm0_0 ymm10_0 0x9@uint64;
shr ymm0_1 ymm10_1 0x9@uint64;
shr ymm0_2 ymm10_2 0x9@uint64;
shr ymm0_3 ymm10_3 0x9@uint64;
(* vpsllq $0x37,%ymm10,%ymm10                      #! PC = 0x55555557a8cc *)
shl ymm10_0 ymm10_0 0x37@uint64;
shl ymm10_1 ymm10_1 0x37@uint64;
shl ymm10_2 ymm10_2 0x37@uint64;
shl ymm10_3 ymm10_3 0x37@uint64;
(* vpor   %ymm11,%ymm15,%ymm11                     #! PC = 0x55555557a8d2 *)
or ymm11_0@uint64 ymm15_0 ymm11_0;
or ymm11_1@uint64 ymm15_1 ymm11_1;
or ymm11_2@uint64 ymm15_2 ymm11_2;
or ymm11_3@uint64 ymm15_3 ymm11_3;
(* vpor   %ymm10,%ymm0,%ymm10                      #! PC = 0x55555557a8d7 *)
or ymm10_0@uint64 ymm0_0 ymm10_0;
or ymm10_1@uint64 ymm0_1 ymm10_1;
or ymm10_2@uint64 ymm0_2 ymm10_2;
or ymm10_3@uint64 ymm0_3 ymm10_3;
(* vpsrlq $0x2,%ymm5,%ymm12                        #! PC = 0x55555557a8dc *)
shr ymm12_0 ymm5_0 0x2@uint64;
shr ymm12_1 ymm5_1 0x2@uint64;
shr ymm12_2 ymm5_2 0x2@uint64;
shr ymm12_3 ymm5_3 0x2@uint64;
(* vpxor  %ymm9,%ymm1,%ymm9                        #! PC = 0x55555557a8e1 *)
xor ymm9_0@uint64 ymm1_0 ymm9_0;
xor ymm9_1@uint64 ymm1_1 ymm9_1;
xor ymm9_2@uint64 ymm1_2 ymm9_2;
xor ymm9_3@uint64 ymm1_3 ymm9_3;
(* vmovdqa -0x1b0(%rbp),%ymm0                      #! EA = L0x7fffffffbda0; Value = 0xeb5aa93f2317d635; PC = 0x55555557a8e6 *)
mov ymm0_0 L0x7fffffffbda0;
mov ymm0_1 L0x7fffffffbda8;
mov ymm0_2 L0x7fffffffbdb0;
mov ymm0_3 L0x7fffffffbdb8;
(* vpsllq $0x3e,%ymm5,%ymm5                        #! PC = 0x55555557a8ee *)
shl ymm5_0 ymm5_0 0x3e@uint64;
shl ymm5_1 ymm5_1 0x3e@uint64;
shl ymm5_2 ymm5_2 0x3e@uint64;
shl ymm5_3 ymm5_3 0x3e@uint64;
(* vpandn %ymm11,%ymm10,%ymm7                      #! PC = 0x55555557a8f3 *)
not ymm10_0n@uint64 ymm10_0;
and ymm7_0@uint64 ymm10_0n ymm11_0;
not ymm10_1n@uint64 ymm10_1;
and ymm7_1@uint64 ymm10_1n ymm11_1;
not ymm10_2n@uint64 ymm10_2;
and ymm7_2@uint64 ymm10_2n ymm11_2;
not ymm10_3n@uint64 ymm10_3;
and ymm7_3@uint64 ymm10_3n ymm11_3;
(* vpxor  -0x150(%rbp),%ymm0,%ymm0                 #! EA = L0x7fffffffbe00; Value = 0xff97a42d7f8e6fd4; PC = 0x55555557a8f8 *)
xor ymm0_0@uint64 ymm0_0 L0x7fffffffbe00;
xor ymm0_1@uint64 ymm0_1 L0x7fffffffbe08;
xor ymm0_2@uint64 ymm0_2 L0x7fffffffbe10;
xor ymm0_3@uint64 ymm0_3 L0x7fffffffbe18;
(* vpor   %ymm5,%ymm12,%ymm12                      #! PC = 0x55555557a900 *)
or ymm12_0@uint64 ymm12_0 ymm5_0;
or ymm12_1@uint64 ymm12_1 ymm5_1;
or ymm12_2@uint64 ymm12_2 ymm5_2;
or ymm12_3@uint64 ymm12_3 ymm5_3;
(* vpxor  -0x2b0(%rbp),%ymm3,%ymm3                 #! EA = L0x7fffffffbca0; Value = 0xd6a59cd027a68607; PC = 0x55555557a904 *)
xor ymm3_0@uint64 ymm3_0 L0x7fffffffbca0;
xor ymm3_1@uint64 ymm3_1 L0x7fffffffbca8;
xor ymm3_2@uint64 ymm3_2 L0x7fffffffbcb0;
xor ymm3_3@uint64 ymm3_3 L0x7fffffffbcb8;
(* vmovdqa -0x70(%rbp),%ymm5                       #! EA = L0x7fffffffbee0; Value = 0xad30a6f71b19059c; PC = 0x55555557a90c *)
mov ymm5_0 L0x7fffffffbee0;
mov ymm5_1 L0x7fffffffbee8;
mov ymm5_2 L0x7fffffffbef0;
mov ymm5_3 L0x7fffffffbef8;
(* vpxor  %ymm12,%ymm7,%ymm7                       #! PC = 0x55555557a911 *)
xor ymm7_0@uint64 ymm7_0 ymm12_0;
xor ymm7_1@uint64 ymm7_1 ymm12_1;
xor ymm7_2@uint64 ymm7_2 ymm12_2;
xor ymm7_3@uint64 ymm7_3 ymm12_3;
(* vpxor  -0x1d0(%rbp),%ymm7,%ymm1                 #! EA = L0x7fffffffbd80; Value = 0x05e5635a21d9ae61; PC = 0x55555557a916 *)
xor ymm1_0@uint64 ymm7_0 L0x7fffffffbd80;
xor ymm1_1@uint64 ymm7_1 L0x7fffffffbd88;
xor ymm1_2@uint64 ymm7_2 L0x7fffffffbd90;
xor ymm1_3@uint64 ymm7_3 L0x7fffffffbd98;
(* vmovdqa %ymm7,-0x250(%rbp)                      #! EA = L0x7fffffffbd00; PC = 0x55555557a91e *)
mov L0x7fffffffbd00 ymm7_0;
mov L0x7fffffffbd08 ymm7_1;
mov L0x7fffffffbd10 ymm7_2;
mov L0x7fffffffbd18 ymm7_3;
(* vpxor  %ymm0,%ymm1,%ymm0                        #! PC = 0x55555557a926 *)
xor ymm0_0@uint64 ymm1_0 ymm0_0;
xor ymm0_1@uint64 ymm1_1 ymm0_1;
xor ymm0_2@uint64 ymm1_2 ymm0_2;
xor ymm0_3@uint64 ymm1_3 ymm0_3;
(* vpsrlq $0x17,%ymm4,%ymm1                        #! PC = 0x55555557a92a *)
shr ymm1_0 ymm4_0 0x17@uint64;
shr ymm1_1 ymm4_1 0x17@uint64;
shr ymm1_2 ymm4_2 0x17@uint64;
shr ymm1_3 ymm4_3 0x17@uint64;
(* vpsllq $0x29,%ymm4,%ymm4                        #! PC = 0x55555557a92f *)
shl ymm4_0 ymm4_0 0x29@uint64;
shl ymm4_1 ymm4_1 0x29@uint64;
shl ymm4_2 ymm4_2 0x29@uint64;
shl ymm4_3 ymm4_3 0x29@uint64;
(* vpor   %ymm4,%ymm1,%ymm4                        #! PC = 0x55555557a934 *)
or ymm4_0@uint64 ymm1_0 ymm4_0;
or ymm4_1@uint64 ymm1_1 ymm4_1;
or ymm4_2@uint64 ymm1_2 ymm4_2;
or ymm4_3@uint64 ymm1_3 ymm4_3;
(* vpandn %ymm4,%ymm11,%ymm2                       #! PC = 0x55555557a938 *)
not ymm11_0n@uint64 ymm11_0;
and ymm2_0@uint64 ymm11_0n ymm4_0;
not ymm11_1n@uint64 ymm11_1;
and ymm2_1@uint64 ymm11_1n ymm4_1;
not ymm11_2n@uint64 ymm11_2;
and ymm2_2@uint64 ymm11_2n ymm4_2;
not ymm11_3n@uint64 ymm11_3;
and ymm2_3@uint64 ymm11_3n ymm4_3;
(* vpxor  %ymm10,%ymm2,%ymm15                      #! PC = 0x55555557a93c *)
xor ymm15_0@uint64 ymm2_0 ymm10_0;
xor ymm15_1@uint64 ymm2_1 ymm10_1;
xor ymm15_2@uint64 ymm2_2 ymm10_2;
xor ymm15_3@uint64 ymm2_3 ymm10_3;
(* vpsrlq $0x3e,%ymm3,%ymm2                        #! PC = 0x55555557a941 *)
shr ymm2_0 ymm3_0 0x3e@uint64;
shr ymm2_1 ymm3_1 0x3e@uint64;
shr ymm2_2 ymm3_2 0x3e@uint64;
shr ymm2_3 ymm3_3 0x3e@uint64;
(* vpandn %ymm10,%ymm12,%ymm10                     #! PC = 0x55555557a946 *)
not ymm12_0n@uint64 ymm12_0;
and ymm10_0@uint64 ymm12_0n ymm10_0;
not ymm12_1n@uint64 ymm12_1;
and ymm10_1@uint64 ymm12_1n ymm10_1;
not ymm12_2n@uint64 ymm12_2;
and ymm10_2@uint64 ymm12_2n ymm10_2;
not ymm12_3n@uint64 ymm12_3;
and ymm10_3@uint64 ymm12_3n ymm10_3;
(* vpsllq $0x2,%ymm3,%ymm3                         #! PC = 0x55555557a94b *)
shl ymm3_0 ymm3_0 0x2@uint64;
shl ymm3_1 ymm3_1 0x2@uint64;
shl ymm3_2 ymm3_2 0x2@uint64;
shl ymm3_3 ymm3_3 0x2@uint64;
(* vmovdqa %ymm15,-0xf0(%rbp)                      #! EA = L0x7fffffffbe60; PC = 0x55555557a950 *)
mov L0x7fffffffbe60 ymm15_0;
mov L0x7fffffffbe68 ymm15_1;
mov L0x7fffffffbe70 ymm15_2;
mov L0x7fffffffbe78 ymm15_3;
(* vpor   %ymm3,%ymm2,%ymm1                        #! PC = 0x55555557a958 *)
or ymm1_0@uint64 ymm2_0 ymm3_0;
or ymm1_1@uint64 ymm2_1 ymm3_1;
or ymm1_2@uint64 ymm2_2 ymm3_2;
or ymm1_3@uint64 ymm2_3 ymm3_3;
(* vpxor  -0x130(%rbp),%ymm5,%ymm3                 #! EA = L0x7fffffffbe20; Value = 0xbd1547306f80494d; PC = 0x55555557a95c *)
xor ymm3_0@uint64 ymm5_0 L0x7fffffffbe20;
xor ymm3_1@uint64 ymm5_1 L0x7fffffffbe28;
xor ymm3_2@uint64 ymm5_2 L0x7fffffffbe30;
xor ymm3_3@uint64 ymm5_3 L0x7fffffffbe38;
(* vmovdqa -0x90(%rbp),%ymm5                       #! EA = L0x7fffffffbec0; Value = 0x30935ab7d08ffc64; PC = 0x55555557a964 *)
mov ymm5_0 L0x7fffffffbec0;
mov ymm5_1 L0x7fffffffbec8;
mov ymm5_2 L0x7fffffffbed0;
mov ymm5_3 L0x7fffffffbed8;
(* vpandn %ymm1,%ymm4,%ymm15                       #! PC = 0x55555557a96c *)
not ymm4_0n@uint64 ymm4_0;
and ymm15_0@uint64 ymm4_0n ymm1_0;
not ymm4_1n@uint64 ymm4_1;
and ymm15_1@uint64 ymm4_1n ymm1_1;
not ymm4_2n@uint64 ymm4_2;
and ymm15_2@uint64 ymm4_2n ymm1_2;
not ymm4_3n@uint64 ymm4_3;
and ymm15_3@uint64 ymm4_3n ymm1_3;
(* vpandn %ymm12,%ymm1,%ymm2                       #! PC = 0x55555557a970 *)
not ymm1_0n@uint64 ymm1_0;
and ymm2_0@uint64 ymm1_0n ymm12_0;
not ymm1_1n@uint64 ymm1_1;
and ymm2_1@uint64 ymm1_1n ymm12_1;
not ymm1_2n@uint64 ymm1_2;
and ymm2_2@uint64 ymm1_2n ymm12_2;
not ymm1_3n@uint64 ymm1_3;
and ymm2_3@uint64 ymm1_3n ymm12_3;
(* vpxor  %ymm1,%ymm10,%ymm10                      #! PC = 0x55555557a975 *)
xor ymm10_0@uint64 ymm10_0 ymm1_0;
xor ymm10_1@uint64 ymm10_1 ymm1_1;
xor ymm10_2@uint64 ymm10_2 ymm1_2;
xor ymm10_3@uint64 ymm10_3 ymm1_3;
(* vpxor  %ymm4,%ymm2,%ymm2                        #! PC = 0x55555557a979 *)
xor ymm2_0@uint64 ymm2_0 ymm4_0;
xor ymm2_1@uint64 ymm2_1 ymm4_1;
xor ymm2_2@uint64 ymm2_2 ymm4_2;
xor ymm2_3@uint64 ymm2_3 ymm4_3;
(* vpxor  %ymm11,%ymm15,%ymm15                     #! PC = 0x55555557a97d *)
xor ymm15_0@uint64 ymm15_0 ymm11_0;
xor ymm15_1@uint64 ymm15_1 ymm11_1;
xor ymm15_2@uint64 ymm15_2 ymm11_2;
xor ymm15_3@uint64 ymm15_3 ymm11_3;
(* vpxor  -0x50(%rbp),%ymm5,%ymm1                  #! EA = L0x7fffffffbf00; Value = 0x8b284e056253d057; PC = 0x55555557a982 *)
xor ymm1_0@uint64 ymm5_0 L0x7fffffffbf00;
xor ymm1_1@uint64 ymm5_1 L0x7fffffffbf08;
xor ymm1_2@uint64 ymm5_2 L0x7fffffffbf10;
xor ymm1_3@uint64 ymm5_3 L0x7fffffffbf18;
(* vpxor  -0xb0(%rbp),%ymm10,%ymm11                #! EA = L0x7fffffffbea0; Value = 0x01f22f1a11a5569f; PC = 0x55555557a987 *)
xor ymm11_0@uint64 ymm10_0 L0x7fffffffbea0;
xor ymm11_1@uint64 ymm10_1 L0x7fffffffbea8;
xor ymm11_2@uint64 ymm10_2 L0x7fffffffbeb0;
xor ymm11_3@uint64 ymm10_3 L0x7fffffffbeb8;
(* vpxor  %ymm2,%ymm9,%ymm4                        #! PC = 0x55555557a98f *)
xor ymm4_0@uint64 ymm9_0 ymm2_0;
xor ymm4_1@uint64 ymm9_1 ymm2_1;
xor ymm4_2@uint64 ymm9_2 ymm2_2;
xor ymm4_3@uint64 ymm9_3 ymm2_3;
(* vmovdqa %ymm2,-0x270(%rbp)                      #! EA = L0x7fffffffbce0; PC = 0x55555557a993 *)
mov L0x7fffffffbce0 ymm2_0;
mov L0x7fffffffbce8 ymm2_1;
mov L0x7fffffffbcf0 ymm2_2;
mov L0x7fffffffbcf8 ymm2_3;
(* vpxor  %ymm3,%ymm4,%ymm4                        #! PC = 0x55555557a99b *)
xor ymm4_0@uint64 ymm4_0 ymm3_0;
xor ymm4_1@uint64 ymm4_1 ymm3_1;
xor ymm4_2@uint64 ymm4_2 ymm3_2;
xor ymm4_3@uint64 ymm4_3 ymm3_3;
(* vpxor  %ymm1,%ymm11,%ymm11                      #! PC = 0x55555557a99f *)
xor ymm11_0@uint64 ymm11_0 ymm1_0;
xor ymm11_1@uint64 ymm11_1 ymm1_1;
xor ymm11_2@uint64 ymm11_2 ymm1_2;
xor ymm11_3@uint64 ymm11_3 ymm1_3;
(* #je     0x55555557aa08 <KeccakP1600times4_PermuteAll_24rounds+14488>#! PC = 0x55555557a9a6 *)
#je     0x55555557aa08 <KeccakP1600times4_PermuteAll_24rounds+14488>#! 0x55555557a9a6 = 0x55555557a9a6;
(* vmovdqa -0x1f0(%rbp),%ymm6                      #! EA = L0x7fffffffbd60; Value = 0xf1258f7940e1dde7; PC = 0x55555557aa08 *)
mov ymm6_0 L0x7fffffffbd60;
mov ymm6_1 L0x7fffffffbd68;
mov ymm6_2 L0x7fffffffbd70;
mov ymm6_3 L0x7fffffffbd78;
(* vmovdqa %ymm13,0x180(%rdi)                      #! EA = L0x7fffffffc4a0; PC = 0x55555557aa10 *)
mov L0x7fffffffc4a0 ymm13_0;
mov L0x7fffffffc4a8 ymm13_1;
mov L0x7fffffffc4b0 ymm13_2;
mov L0x7fffffffc4b8 ymm13_3;
(* vmovdqa %ymm6,(%rdi)                            #! EA = L0x7fffffffc320; PC = 0x55555557aa18 *)
mov L0x7fffffffc320 ymm6_0;
mov L0x7fffffffc328 ymm6_1;
mov L0x7fffffffc330 ymm6_2;
mov L0x7fffffffc338 ymm6_3;
(* vmovdqa -0x3b0(%rbp),%ymm6                      #! EA = L0x7fffffffbba0; Value = 0x84d5ccf933c0478a; PC = 0x55555557aa1c *)
mov ymm6_0 L0x7fffffffbba0;
mov ymm6_1 L0x7fffffffbba8;
mov ymm6_2 L0x7fffffffbbb0;
mov ymm6_3 L0x7fffffffbbb8;
(* vmovdqa %ymm6,0x20(%rdi)                        #! EA = L0x7fffffffc340; PC = 0x55555557aa24 *)
mov L0x7fffffffc340 ymm6_0;
mov L0x7fffffffc348 ymm6_1;
mov L0x7fffffffc350 ymm6_2;
mov L0x7fffffffc358 ymm6_3;
(* vmovdqa -0x110(%rbp),%ymm6                      #! EA = L0x7fffffffbe40; Value = 0xd598261ea65aa9ee; PC = 0x55555557aa29 *)
mov ymm6_0 L0x7fffffffbe40;
mov ymm6_1 L0x7fffffffbe48;
mov ymm6_2 L0x7fffffffbe50;
mov ymm6_3 L0x7fffffffbe58;
(* vmovdqa %ymm6,0x40(%rdi)                        #! EA = L0x7fffffffc360; PC = 0x55555557aa31 *)
mov L0x7fffffffc360 ymm6_0;
mov L0x7fffffffc368 ymm6_1;
mov L0x7fffffffc370 ymm6_2;
mov L0x7fffffffc378 ymm6_3;
(* vmovdqa -0x130(%rbp),%ymm6                      #! EA = L0x7fffffffbe20; Value = 0xbd1547306f80494d; PC = 0x55555557aa36 *)
mov ymm6_0 L0x7fffffffbe20;
mov ymm6_1 L0x7fffffffbe28;
mov ymm6_2 L0x7fffffffbe30;
mov ymm6_3 L0x7fffffffbe38;
(* vmovdqa %ymm6,0x60(%rdi)                        #! EA = L0x7fffffffc380; PC = 0x55555557aa3e *)
mov L0x7fffffffc380 ymm6_0;
mov L0x7fffffffc388 ymm6_1;
mov L0x7fffffffc390 ymm6_2;
mov L0x7fffffffc398 ymm6_3;
(* vmovdqa -0x50(%rbp),%ymm6                       #! EA = L0x7fffffffbf00; Value = 0x8b284e056253d057; PC = 0x55555557aa43 *)
mov ymm6_0 L0x7fffffffbf00;
mov ymm6_1 L0x7fffffffbf08;
mov ymm6_2 L0x7fffffffbf10;
mov ymm6_3 L0x7fffffffbf18;
(* vmovdqa %ymm6,0x80(%rdi)                        #! EA = L0x7fffffffc3a0; PC = 0x55555557aa48 *)
mov L0x7fffffffc3a0 ymm6_0;
mov L0x7fffffffc3a8 ymm6_1;
mov L0x7fffffffc3b0 ymm6_2;
mov L0x7fffffffc3b8 ymm6_3;
(* vmovdqa -0x150(%rbp),%ymm6                      #! EA = L0x7fffffffbe00; Value = 0xff97a42d7f8e6fd4; PC = 0x55555557aa50 *)
mov ymm6_0 L0x7fffffffbe00;
mov ymm6_1 L0x7fffffffbe08;
mov ymm6_2 L0x7fffffffbe10;
mov ymm6_3 L0x7fffffffbe18;
(* vmovdqa %ymm6,0xa0(%rdi)                        #! EA = L0x7fffffffc3c0; PC = 0x55555557aa58 *)
mov L0x7fffffffc3c0 ymm6_0;
mov L0x7fffffffc3c8 ymm6_1;
mov L0x7fffffffc3d0 ymm6_2;
mov L0x7fffffffc3d8 ymm6_3;
(* vmovdqa -0x170(%rbp),%ymm6                      #! EA = L0x7fffffffbde0; Value = 0x90fee5a0a44647c4; PC = 0x55555557aa60 *)
mov ymm6_0 L0x7fffffffbde0;
mov ymm6_1 L0x7fffffffbde8;
mov ymm6_2 L0x7fffffffbdf0;
mov ymm6_3 L0x7fffffffbdf8;
(* vmovdqa %ymm6,0xc0(%rdi)                        #! EA = L0x7fffffffc3e0; PC = 0x55555557aa68 *)
mov L0x7fffffffc3e0 ymm6_0;
mov L0x7fffffffc3e8 ymm6_1;
mov L0x7fffffffc3f0 ymm6_2;
mov L0x7fffffffc3f8 ymm6_3;
(* vmovdqa -0x190(%rbp),%ymm6                      #! EA = L0x7fffffffbdc0; Value = 0x8c5bda0cd6192e76; PC = 0x55555557aa70 *)
mov ymm6_0 L0x7fffffffbdc0;
mov ymm6_1 L0x7fffffffbdc8;
mov ymm6_2 L0x7fffffffbdd0;
mov ymm6_3 L0x7fffffffbdd8;
(* vmovdqa %ymm6,0xe0(%rdi)                        #! EA = L0x7fffffffc400; PC = 0x55555557aa78 *)
mov L0x7fffffffc400 ymm6_0;
mov L0x7fffffffc408 ymm6_1;
mov L0x7fffffffc410 ymm6_2;
mov L0x7fffffffc418 ymm6_3;
(* vmovdqa -0x70(%rbp),%ymm6                       #! EA = L0x7fffffffbee0; Value = 0xad30a6f71b19059c; PC = 0x55555557aa80 *)
mov ymm6_0 L0x7fffffffbee0;
mov ymm6_1 L0x7fffffffbee8;
mov ymm6_2 L0x7fffffffbef0;
mov ymm6_3 L0x7fffffffbef8;
(* vmovdqa %ymm6,0x100(%rdi)                       #! EA = L0x7fffffffc420; PC = 0x55555557aa85 *)
mov L0x7fffffffc420 ymm6_0;
mov L0x7fffffffc428 ymm6_1;
mov L0x7fffffffc430 ymm6_2;
mov L0x7fffffffc438 ymm6_3;
(* vmovdqa -0x90(%rbp),%ymm6                       #! EA = L0x7fffffffbec0; Value = 0x30935ab7d08ffc64; PC = 0x55555557aa8d *)
mov ymm6_0 L0x7fffffffbec0;
mov ymm6_1 L0x7fffffffbec8;
mov ymm6_2 L0x7fffffffbed0;
mov ymm6_3 L0x7fffffffbed8;
(* vmovdqa %ymm6,0x120(%rdi)                       #! EA = L0x7fffffffc440; PC = 0x55555557aa95 *)
mov L0x7fffffffc440 ymm6_0;
mov L0x7fffffffc448 ymm6_1;
mov L0x7fffffffc450 ymm6_2;
mov L0x7fffffffc458 ymm6_3;
(* vmovdqa -0x1b0(%rbp),%ymm6                      #! EA = L0x7fffffffbda0; Value = 0xeb5aa93f2317d635; PC = 0x55555557aa9d *)
mov ymm6_0 L0x7fffffffbda0;
mov ymm6_1 L0x7fffffffbda8;
mov ymm6_2 L0x7fffffffbdb0;
mov ymm6_3 L0x7fffffffbdb8;
(* vmovdqa %ymm6,0x140(%rdi)                       #! EA = L0x7fffffffc460; PC = 0x55555557aaa5 *)
mov L0x7fffffffc460 ymm6_0;
mov L0x7fffffffc468 ymm6_1;
mov L0x7fffffffc470 ymm6_2;
mov L0x7fffffffc478 ymm6_3;
(* vmovdqa -0x3d0(%rbp),%ymm6                      #! EA = L0x7fffffffbb80; Value = 0xa9a6e6260d712103; PC = 0x55555557aaad *)
mov ymm6_0 L0x7fffffffbb80;
mov ymm6_1 L0x7fffffffbb88;
mov ymm6_2 L0x7fffffffbb90;
mov ymm6_3 L0x7fffffffbb98;
(* vmovdqa %ymm6,0x160(%rdi)                       #! EA = L0x7fffffffc480; PC = 0x55555557aab5 *)
mov L0x7fffffffc480 ymm6_0;
mov L0x7fffffffc488 ymm6_1;
mov L0x7fffffffc490 ymm6_2;
mov L0x7fffffffc498 ymm6_3;
(* vmovdqa -0xd0(%rbp),%ymm6                       #! EA = L0x7fffffffbe80; Value = 0x43b831cd0347c826; PC = 0x55555557aabd *)
mov ymm6_0 L0x7fffffffbe80;
mov ymm6_1 L0x7fffffffbe88;
mov ymm6_2 L0x7fffffffbe90;
mov ymm6_3 L0x7fffffffbe98;
(* vmovdqa %ymm6,0x1a0(%rdi)                       #! EA = L0x7fffffffc4c0; PC = 0x55555557aac5 *)
mov L0x7fffffffc4c0 ymm6_0;
mov L0x7fffffffc4c8 ymm6_1;
mov L0x7fffffffc4d0 ymm6_2;
mov L0x7fffffffc4d8 ymm6_3;
(* vmovdqa -0xb0(%rbp),%ymm6                       #! EA = L0x7fffffffbea0; Value = 0x01f22f1a11a5569f; PC = 0x55555557aacd *)
mov ymm6_0 L0x7fffffffbea0;
mov ymm6_1 L0x7fffffffbea8;
mov ymm6_2 L0x7fffffffbeb0;
mov ymm6_3 L0x7fffffffbeb8;
(* vmovdqa %ymm6,0x1c0(%rdi)                       #! EA = L0x7fffffffc4e0; PC = 0x55555557aad5 *)
mov L0x7fffffffc4e0 ymm6_0;
mov L0x7fffffffc4e8 ymm6_1;
mov L0x7fffffffc4f0 ymm6_2;
mov L0x7fffffffc4f8 ymm6_3;
(* vmovdqa -0x1d0(%rbp),%ymm6                      #! EA = L0x7fffffffbd80; Value = 0x05e5635a21d9ae61; PC = 0x55555557aadd *)
mov ymm6_0 L0x7fffffffbd80;
mov ymm6_1 L0x7fffffffbd88;
mov ymm6_2 L0x7fffffffbd90;
mov ymm6_3 L0x7fffffffbd98;
(* vmovdqa %ymm6,0x1e0(%rdi)                       #! EA = L0x7fffffffc500; PC = 0x55555557aae5 *)
mov L0x7fffffffc500 ymm6_0;
mov L0x7fffffffc508 ymm6_1;
mov L0x7fffffffc510 ymm6_2;
mov L0x7fffffffc518 ymm6_3;
(* vmovdqa -0x210(%rbp),%ymm6                      #! EA = L0x7fffffffbd40; Value = 0x64befef28cc970f2; PC = 0x55555557aaed *)
mov ymm6_0 L0x7fffffffbd40;
mov ymm6_1 L0x7fffffffbd48;
mov ymm6_2 L0x7fffffffbd50;
mov ymm6_3 L0x7fffffffbd58;
(* vmovdqa %ymm6,0x200(%rdi)                       #! EA = L0x7fffffffc520; PC = 0x55555557aaf5 *)
mov L0x7fffffffc520 ymm6_0;
mov L0x7fffffffc528 ymm6_1;
mov L0x7fffffffc530 ymm6_2;
mov L0x7fffffffc538 ymm6_3;
(* vmovdqa -0x230(%rbp),%ymm6                      #! EA = L0x7fffffffbd20; Value = 0x613670957bc46611; PC = 0x55555557aafd *)
mov ymm6_0 L0x7fffffffbd20;
mov ymm6_1 L0x7fffffffbd28;
mov ymm6_2 L0x7fffffffbd30;
mov ymm6_3 L0x7fffffffbd38;
(* vmovdqa %ymm9,0x240(%rdi)                       #! EA = L0x7fffffffc560; PC = 0x55555557ab05 *)
mov L0x7fffffffc560 ymm9_0;
mov L0x7fffffffc568 ymm9_1;
mov L0x7fffffffc570 ymm9_2;
mov L0x7fffffffc578 ymm9_3;
(* vmovdqa %ymm6,0x220(%rdi)                       #! EA = L0x7fffffffc540; PC = 0x55555557ab0d *)
mov L0x7fffffffc540 ymm6_0;
mov L0x7fffffffc548 ymm6_1;
mov L0x7fffffffc550 ymm6_2;
mov L0x7fffffffc558 ymm6_3;
(* vmovdqa -0x250(%rbp),%ymm6                      #! EA = L0x7fffffffbd00; Value = 0x940c7922ae3a2614; PC = 0x55555557ab15 *)
mov ymm6_0 L0x7fffffffbd00;
mov ymm6_1 L0x7fffffffbd08;
mov ymm6_2 L0x7fffffffbd10;
mov ymm6_3 L0x7fffffffbd18;
(* vmovdqa %ymm14,0x260(%rdi)                      #! EA = L0x7fffffffc580; PC = 0x55555557ab1d *)
mov L0x7fffffffc580 ymm14_0;
mov L0x7fffffffc588 ymm14_1;
mov L0x7fffffffc590 ymm14_2;
mov L0x7fffffffc598 ymm14_3;
(* vmovdqa %ymm6,0x280(%rdi)                       #! EA = L0x7fffffffc5a0; PC = 0x55555557ab25 *)
mov L0x7fffffffc5a0 ymm6_0;
mov L0x7fffffffc5a8 ymm6_1;
mov L0x7fffffffc5b0 ymm6_2;
mov L0x7fffffffc5b8 ymm6_3;
(* vmovdqa -0xf0(%rbp),%ymm6                       #! EA = L0x7fffffffbe60; Value = 0x1841f924a2c509e4; PC = 0x55555557ab2d *)
mov ymm6_0 L0x7fffffffbe60;
mov ymm6_1 L0x7fffffffbe68;
mov ymm6_2 L0x7fffffffbe70;
mov ymm6_3 L0x7fffffffbe78;
(* vmovdqa %ymm15,0x2c0(%rdi)                      #! EA = L0x7fffffffc5e0; PC = 0x55555557ab35 *)
mov L0x7fffffffc5e0 ymm15_0;
mov L0x7fffffffc5e8 ymm15_1;
mov L0x7fffffffc5f0 ymm15_2;
mov L0x7fffffffc5f8 ymm15_3;
(* vmovdqa %ymm6,0x2a0(%rdi)                       #! EA = L0x7fffffffc5c0; PC = 0x55555557ab3d *)
mov L0x7fffffffc5c0 ymm6_0;
mov L0x7fffffffc5c8 ymm6_1;
mov L0x7fffffffc5d0 ymm6_2;
mov L0x7fffffffc5d8 ymm6_3;
(* vmovdqa -0x270(%rbp),%ymm6                      #! EA = L0x7fffffffbce0; Value = 0x75f644e97f30a13b; PC = 0x55555557ab45 *)
mov ymm6_0 L0x7fffffffbce0;
mov ymm6_1 L0x7fffffffbce8;
mov ymm6_2 L0x7fffffffbcf0;
mov ymm6_3 L0x7fffffffbcf8;
(* vmovdqa %ymm10,0x300(%rdi)                      #! EA = L0x7fffffffc620; PC = 0x55555557ab4d *)
mov L0x7fffffffc620 ymm10_0;
mov L0x7fffffffc628 ymm10_1;
mov L0x7fffffffc630 ymm10_2;
mov L0x7fffffffc638 ymm10_3;
(* vmovdqa %ymm6,0x2e0(%rdi)                       #! EA = L0x7fffffffc600; PC = 0x55555557ab55 *)
mov L0x7fffffffc600 ymm6_0;
mov L0x7fffffffc608 ymm6_1;
mov L0x7fffffffc610 ymm6_2;
mov L0x7fffffffc618 ymm6_3;
(* vzeroupper                                      #! PC = 0x55555557ab5d *)
mov ymm0_2 0@uint64;
mov ymm0_3 0@uint64;
mov ymm1_2 0@uint64;
mov ymm1_3 0@uint64;
mov ymm2_2 0@uint64;
mov ymm2_3 0@uint64;
mov ymm3_2 0@uint64;
mov ymm3_3 0@uint64;
mov ymm4_2 0@uint64;
mov ymm4_3 0@uint64;
mov ymm5_2 0@uint64;
mov ymm5_3 0@uint64;
mov ymm6_2 0@uint64;
mov ymm6_3 0@uint64;
mov ymm7_2 0@uint64;
mov ymm7_3 0@uint64;
mov ymm8_2 0@uint64;
mov ymm8_3 0@uint64;
mov ymm9_2 0@uint64;
mov ymm9_3 0@uint64;
mov ymm10_2 0@uint64;
mov ymm10_3 0@uint64;
mov ymm11_2 0@uint64;
mov ymm11_3 0@uint64;
mov ymm12_2 0@uint64;
mov ymm12_3 0@uint64;
mov ymm13_2 0@uint64;
mov ymm13_3 0@uint64;
mov ymm14_2 0@uint64;
mov ymm14_3 0@uint64;
mov ymm15_2 0@uint64;
mov ymm15_3 0@uint64;
(* add    $0x328,%rsp                              #! PC = 0x55555557ab60 *)
adds carry rsp rsp 0x328@uint64;
(* #! <- SP = 0x7fffffffbf68 *)
#! 0x7fffffffbf68 = 0x7fffffffbf68;
(* #retq                                           #! PC = 0x55555557ab77 *)
#retq                                           #! 0x55555557ab77 = 0x55555557ab77;

(* ===== Outputs ===== *)

mov a00 L0x7fffffffc320;
mov b00 L0x7fffffffc328;
mov c00 L0x7fffffffc330;
mov d00 L0x7fffffffc338;
mov a01 L0x7fffffffc340;
mov b01 L0x7fffffffc348;
mov c01 L0x7fffffffc350;
mov d01 L0x7fffffffc358;
mov a02 L0x7fffffffc360;
mov b02 L0x7fffffffc368;
mov c02 L0x7fffffffc370;
mov d02 L0x7fffffffc378;
mov a03 L0x7fffffffc380;
mov b03 L0x7fffffffc388;
mov c03 L0x7fffffffc390;
mov d03 L0x7fffffffc398;
mov a04 L0x7fffffffc3a0;
mov b04 L0x7fffffffc3a8;
mov c04 L0x7fffffffc3b0;
mov d04 L0x7fffffffc3b8;
mov a05 L0x7fffffffc3c0;
mov b05 L0x7fffffffc3c8;
mov c05 L0x7fffffffc3d0;
mov d05 L0x7fffffffc3d8;
mov a06 L0x7fffffffc3e0;
mov b06 L0x7fffffffc3e8;
mov c06 L0x7fffffffc3f0;
mov d06 L0x7fffffffc3f8;
mov a07 L0x7fffffffc400;
mov b07 L0x7fffffffc408;
mov c07 L0x7fffffffc410;
mov d07 L0x7fffffffc418;
mov a08 L0x7fffffffc420;
mov b08 L0x7fffffffc428;
mov c08 L0x7fffffffc430;
mov d08 L0x7fffffffc438;
mov a09 L0x7fffffffc440;
mov b09 L0x7fffffffc448;
mov c09 L0x7fffffffc450;
mov d09 L0x7fffffffc458;
mov a10 L0x7fffffffc460;
mov b10 L0x7fffffffc468;
mov c10 L0x7fffffffc470;
mov d10 L0x7fffffffc478;
mov a11 L0x7fffffffc480;
mov b11 L0x7fffffffc488;
mov c11 L0x7fffffffc490;
mov d11 L0x7fffffffc498;
mov a12 L0x7fffffffc4a0;
mov b12 L0x7fffffffc4a8;
mov c12 L0x7fffffffc4b0;
mov d12 L0x7fffffffc4b8;
mov a13 L0x7fffffffc4c0;
mov b13 L0x7fffffffc4c8;
mov c13 L0x7fffffffc4d0;
mov d13 L0x7fffffffc4d8;
mov a14 L0x7fffffffc4e0;
mov b14 L0x7fffffffc4e8;
mov c14 L0x7fffffffc4f0;
mov d14 L0x7fffffffc4f8;
mov a15 L0x7fffffffc500;
mov b15 L0x7fffffffc508;
mov c15 L0x7fffffffc510;
mov d15 L0x7fffffffc518;
mov a16 L0x7fffffffc520;
mov b16 L0x7fffffffc528;
mov c16 L0x7fffffffc530;
mov d16 L0x7fffffffc538;
mov a17 L0x7fffffffc540;
mov b17 L0x7fffffffc548;
mov c17 L0x7fffffffc550;
mov d17 L0x7fffffffc558;
mov a18 L0x7fffffffc560;
mov b18 L0x7fffffffc568;
mov c18 L0x7fffffffc570;
mov d18 L0x7fffffffc578;
mov a19 L0x7fffffffc580;
mov b19 L0x7fffffffc588;
mov c19 L0x7fffffffc590;
mov d19 L0x7fffffffc598;
mov a20 L0x7fffffffc5a0;
mov b20 L0x7fffffffc5a8;
mov c20 L0x7fffffffc5b0;
mov d20 L0x7fffffffc5b8;
mov a21 L0x7fffffffc5c0;
mov b21 L0x7fffffffc5c8;
mov c21 L0x7fffffffc5d0;
mov d21 L0x7fffffffc5d8;
mov a22 L0x7fffffffc5e0;
mov b22 L0x7fffffffc5e8;
mov c22 L0x7fffffffc5f0;
mov d22 L0x7fffffffc5f8;
mov a23 L0x7fffffffc600;
mov b23 L0x7fffffffc608;
mov c23 L0x7fffffffc610;
mov d23 L0x7fffffffc618;
mov a24 L0x7fffffffc620;
mov b24 L0x7fffffffc628;
mov c24 L0x7fffffffc630;
mov d24 L0x7fffffffc638;

{
  true
  &&
  true
}

