(* frege: -v -isafety -isafety_timeout 14400 -jobs 24 -slicing -no_carry_constraint __asm_ntt_4_5_6_7_32.cl
Parsing Cryptoline file:                [OK]            0.180413 seconds
Checking well-formedness:               [OK]            0.166010 seconds
Transforming to SSA form:               [OK]            0.028450 seconds
Rewriting assignments:                  [OK]            0.026935 seconds
Verifying program safety:               [OK]            4511.091832 seconds
Verifying range assertions:             [OK]            0.180685 seconds
Verifying range specification:          [OK]            3192.050171 seconds
Rewriting value-preserved casting:      [OK]            0.001427 seconds
Verifying algebraic assertions:         [OK]            312.941497 seconds
Verifying algebraic specification:      [OK]            180.887000 seconds
Verification result:                    [OK]            8197.567940 seconds
*)

(* quine: -v -isafety -jobs 24 -slicing -no_carry_constraint __asm_ntt_4_5_6_7_32.cl
Parsing Cryptoline file:                [OK]            0.203168 seconds
Checking well-formedness:               [OK]            0.224888 seconds
Transforming to SSA form:               [OK]            0.036044 seconds
Rewriting assignments:                  [OK]            0.038676 seconds
Verifying program safety:               [OK]            5478.522354 seconds
Verifying range assertions:             [OK]            1.817347 seconds
Verifying range specification:          [OK]            3514.588423 seconds
Rewriting value-preserved casting:      [OK]            0.003257 seconds
Verifying algebraic assertions:         [OK]            617.406176 seconds
Verifying algebraic specification:      [OK]            227.453260 seconds
Verification result:                    [OK]            9840.315863 seconds
                      Round 1 (8132 safety conditions, timeout = 300 seconds)
*)


proc main (bit x, bit cinp_poly,
sint32 L0x20018fd0, sint32 L0x20018fd4, sint32 L0x20018fd8,
sint32 L0x20018fdc, sint32 L0x20018fe0, sint32 L0x20018fe4,
sint32 L0x20018fe8, sint32 L0x20018fec, sint32 L0x20018ff0,
sint32 L0x20018ff4, sint32 L0x20018ff8, sint32 L0x20018ffc,
sint32 L0x20019000, sint32 L0x20019004, sint32 L0x20019008,
sint32 L0x2001900c, sint32 L0x20019010, sint32 L0x20019014,
sint32 L0x20019018, sint32 L0x2001901c, sint32 L0x20019020,
sint32 L0x20019024, sint32 L0x20019028, sint32 L0x2001902c,
sint32 L0x20019030, sint32 L0x20019034, sint32 L0x20019038,
sint32 L0x2001903c, sint32 L0x20019040, sint32 L0x20019044,
sint32 L0x20019048, sint32 L0x2001904c, sint32 L0x20019050,
sint32 L0x20019054, sint32 L0x20019058, sint32 L0x2001905c,
sint32 L0x20019060, sint32 L0x20019064, sint32 L0x20019068,
sint32 L0x2001906c, sint32 L0x20019070, sint32 L0x20019074,
sint32 L0x20019078, sint32 L0x2001907c, sint32 L0x20019080,
sint32 L0x20019084, sint32 L0x20019088, sint32 L0x2001908c,
sint32 L0x20019090, sint32 L0x20019094, sint32 L0x20019098,
sint32 L0x2001909c, sint32 L0x200190a0, sint32 L0x200190a4,
sint32 L0x200190a8, sint32 L0x200190ac, sint32 L0x200190b0,
sint32 L0x200190b4, sint32 L0x200190b8, sint32 L0x200190bc,
sint32 L0x200190c0, sint32 L0x200190c4, sint32 L0x200190c8,
sint32 L0x200190cc, sint32 L0x200190d0, sint32 L0x200190d4,
sint32 L0x200190d8, sint32 L0x200190dc, sint32 L0x200190e0,
sint32 L0x200190e4, sint32 L0x200190e8, sint32 L0x200190ec,
sint32 L0x200190f0, sint32 L0x200190f4, sint32 L0x200190f8,
sint32 L0x200190fc, sint32 L0x20019100, sint32 L0x20019104,
sint32 L0x20019108, sint32 L0x2001910c, sint32 L0x20019110,
sint32 L0x20019114, sint32 L0x20019118, sint32 L0x2001911c,
sint32 L0x20019120, sint32 L0x20019124, sint32 L0x20019128,
sint32 L0x2001912c, sint32 L0x20019130, sint32 L0x20019134,
sint32 L0x20019138, sint32 L0x2001913c, sint32 L0x20019140,
sint32 L0x20019144, sint32 L0x20019148, sint32 L0x2001914c,
sint32 L0x20019150, sint32 L0x20019154, sint32 L0x20019158,
sint32 L0x2001915c, sint32 L0x20019160, sint32 L0x20019164,
sint32 L0x20019168, sint32 L0x2001916c, sint32 L0x20019170,
sint32 L0x20019174, sint32 L0x20019178, sint32 L0x2001917c,
sint32 L0x20019180, sint32 L0x20019184, sint32 L0x20019188,
sint32 L0x2001918c, sint32 L0x20019190, sint32 L0x20019194,
sint32 L0x20019198, sint32 L0x2001919c, sint32 L0x200191a0,
sint32 L0x200191a4, sint32 L0x200191a8, sint32 L0x200191ac,
sint32 L0x200191b0, sint32 L0x200191b4, sint32 L0x200191b8,
sint32 L0x200191bc, sint32 L0x200191c0, sint32 L0x200191c4,
sint32 L0x200191c8, sint32 L0x200191cc, sint32 L0x200191d0,
sint32 L0x200191d4, sint32 L0x200191d8, sint32 L0x200191dc,
sint32 L0x200191e0, sint32 L0x200191e4, sint32 L0x200191e8,
sint32 L0x200191ec, sint32 L0x200191f0, sint32 L0x200191f4,
sint32 L0x200191f8, sint32 L0x200191fc, sint32 L0x20019200,
sint32 L0x20019204, sint32 L0x20019208, sint32 L0x2001920c,
sint32 L0x20019210, sint32 L0x20019214, sint32 L0x20019218,
sint32 L0x2001921c, sint32 L0x20019220, sint32 L0x20019224,
sint32 L0x20019228, sint32 L0x2001922c, sint32 L0x20019230,
sint32 L0x20019234, sint32 L0x20019238, sint32 L0x2001923c,
sint32 L0x20019240, sint32 L0x20019244, sint32 L0x20019248,
sint32 L0x2001924c, sint32 L0x20019250, sint32 L0x20019254,
sint32 L0x20019258, sint32 L0x2001925c, sint32 L0x20019260,
sint32 L0x20019264, sint32 L0x20019268, sint32 L0x2001926c,
sint32 L0x20019270, sint32 L0x20019274, sint32 L0x20019278,
sint32 L0x2001927c, sint32 L0x20019280, sint32 L0x20019284,
sint32 L0x20019288, sint32 L0x2001928c, sint32 L0x20019290,
sint32 L0x20019294, sint32 L0x20019298, sint32 L0x2001929c,
sint32 L0x200192a0, sint32 L0x200192a4, sint32 L0x200192a8,
sint32 L0x200192ac, sint32 L0x200192b0, sint32 L0x200192b4,
sint32 L0x200192b8, sint32 L0x200192bc, sint32 L0x200192c0,
sint32 L0x200192c4, sint32 L0x200192c8, sint32 L0x200192cc,
sint32 L0x200192d0, sint32 L0x200192d4, sint32 L0x200192d8,
sint32 L0x200192dc, sint32 L0x200192e0, sint32 L0x200192e4,
sint32 L0x200192e8, sint32 L0x200192ec, sint32 L0x200192f0,
sint32 L0x200192f4, sint32 L0x200192f8, sint32 L0x200192fc,
sint32 L0x20019300, sint32 L0x20019304, sint32 L0x20019308,
sint32 L0x2001930c, sint32 L0x20019310, sint32 L0x20019314,
sint32 L0x20019318, sint32 L0x2001931c, sint32 L0x20019320,
sint32 L0x20019324, sint32 L0x20019328, sint32 L0x2001932c,
sint32 L0x20019330, sint32 L0x20019334, sint32 L0x20019338,
sint32 L0x2001933c, sint32 L0x20019340, sint32 L0x20019344,
sint32 L0x20019348, sint32 L0x2001934c, sint32 L0x20019350,
sint32 L0x20019354, sint32 L0x20019358, sint32 L0x2001935c,
sint32 L0x20019360, sint32 L0x20019364, sint32 L0x20019368,
sint32 L0x2001936c, sint32 L0x20019370, sint32 L0x20019374,
sint32 L0x20019378, sint32 L0x2001937c, sint32 L0x20019380,
sint32 L0x20019384, sint32 L0x20019388, sint32 L0x2001938c,
sint32 L0x20019390, sint32 L0x20019394, sint32 L0x20019398,
sint32 L0x2001939c, sint32 L0x200193a0, sint32 L0x200193a4,
sint32 L0x200193a8, sint32 L0x200193ac, sint32 L0x200193b0,
sint32 L0x200193b4, sint32 L0x200193b8, sint32 L0x200193bc,
sint32 L0x200193c0, sint32 L0x200193c4, sint32 L0x200193c8,
sint32 L0x200193cc, sint32 L0x200193d0, sint32 L0x200193d4,
sint32 L0x200193d8, sint32 L0x200193dc, sint32 L0x200193e0,
sint32 L0x200193e4, sint32 L0x200193e8, sint32 L0x200193ec,
sint32 L0x200193f0, sint32 L0x200193f4, sint32 L0x200193f8,
sint32 L0x200193fc, sint32 L0x20019400, sint32 L0x20019404,
sint32 L0x20019408, sint32 L0x2001940c, sint32 L0x20019410,
sint32 L0x20019414, sint32 L0x20019418, sint32 L0x2001941c,
sint32 L0x20019420, sint32 L0x20019424, sint32 L0x20019428,
sint32 L0x2001942c, sint32 L0x20019430, sint32 L0x20019434,
sint32 L0x20019438, sint32 L0x2001943c, sint32 L0x20019440,
sint32 L0x20019444, sint32 L0x20019448, sint32 L0x2001944c,
sint32 L0x20019450, sint32 L0x20019454, sint32 L0x20019458,
sint32 L0x2001945c, sint32 L0x20019460, sint32 L0x20019464,
sint32 L0x20019468, sint32 L0x2001946c, sint32 L0x20019470,
sint32 L0x20019474, sint32 L0x20019478, sint32 L0x2001947c,
sint32 L0x20019480, sint32 L0x20019484, sint32 L0x20019488,
sint32 L0x2001948c, sint32 L0x20019490, sint32 L0x20019494,
sint32 L0x20019498, sint32 L0x2001949c, sint32 L0x200194a0,
sint32 L0x200194a4, sint32 L0x200194a8, sint32 L0x200194ac,
sint32 L0x200194b0, sint32 L0x200194b4, sint32 L0x200194b8,
sint32 L0x200194bc, sint32 L0x200194c0, sint32 L0x200194c4,
sint32 L0x200194c8, sint32 L0x200194cc, sint32 L0x200194d0,
sint32 L0x200194d4, sint32 L0x200194d8, sint32 L0x200194dc,
sint32 L0x200194e0, sint32 L0x200194e4, sint32 L0x200194e8,
sint32 L0x200194ec, sint32 L0x200194f0, sint32 L0x200194f4,
sint32 L0x200194f8, sint32 L0x200194fc, sint32 L0x20019500,
sint32 L0x20019504, sint32 L0x20019508, sint32 L0x2001950c,
sint32 L0x20019510, sint32 L0x20019514, sint32 L0x20019518,
sint32 L0x2001951c, sint32 L0x20019520, sint32 L0x20019524,
sint32 L0x20019528, sint32 L0x2001952c, sint32 L0x20019530,
sint32 L0x20019534, sint32 L0x20019538, sint32 L0x2001953c,
sint32 L0x20019540, sint32 L0x20019544, sint32 L0x20019548,
sint32 L0x2001954c, sint32 L0x20019550, sint32 L0x20019554,
sint32 L0x20019558, sint32 L0x2001955c, sint32 L0x20019560,
sint32 L0x20019564, sint32 L0x20019568, sint32 L0x2001956c,
sint32 L0x20019570, sint32 L0x20019574, sint32 L0x20019578,
sint32 L0x2001957c, sint32 L0x20019580, sint32 L0x20019584,
sint32 L0x20019588, sint32 L0x2001958c, sint32 L0x20019590,
sint32 L0x20019594, sint32 L0x20019598, sint32 L0x2001959c,
sint32 L0x200195a0, sint32 L0x200195a4, sint32 L0x200195a8,
sint32 L0x200195ac, sint32 L0x200195b0, sint32 L0x200195b4,
sint32 L0x200195b8, sint32 L0x200195bc, sint32 L0x200195c0,
sint32 L0x200195c4, sint32 L0x200195c8, sint32 L0x200195cc,
sint32 L0x200195d0, sint32 L0x200195d4, sint32 L0x200195d8,
sint32 L0x200195dc, sint32 L0x200195e0, sint32 L0x200195e4,
sint32 L0x200195e8, sint32 L0x200195ec, sint32 L0x200195f0,
sint32 L0x200195f4, sint32 L0x200195f8, sint32 L0x200195fc,
sint32 L0x20019600, sint32 L0x20019604, sint32 L0x20019608,
sint32 L0x2001960c, sint32 L0x20019610, sint32 L0x20019614,
sint32 L0x20019618, sint32 L0x2001961c, sint32 L0x20019620,
sint32 L0x20019624, sint32 L0x20019628, sint32 L0x2001962c,
sint32 L0x20019630, sint32 L0x20019634, sint32 L0x20019638,
sint32 L0x2001963c, sint32 L0x20019640, sint32 L0x20019644,
sint32 L0x20019648, sint32 L0x2001964c, sint32 L0x20019650,
sint32 L0x20019654, sint32 L0x20019658, sint32 L0x2001965c,
sint32 L0x20019660, sint32 L0x20019664, sint32 L0x20019668,
sint32 L0x2001966c, sint32 L0x20019670, sint32 L0x20019674,
sint32 L0x20019678, sint32 L0x2001967c, sint32 L0x20019680,
sint32 L0x20019684, sint32 L0x20019688, sint32 L0x2001968c,
sint32 L0x20019690, sint32 L0x20019694, sint32 L0x20019698,
sint32 L0x2001969c, sint32 L0x200196a0, sint32 L0x200196a4,
sint32 L0x200196a8, sint32 L0x200196ac, sint32 L0x200196b0,
sint32 L0x200196b4, sint32 L0x200196b8, sint32 L0x200196bc,
sint32 L0x200196c0, sint32 L0x200196c4, sint32 L0x200196c8,
sint32 L0x200196cc, sint32 L0x200196d0, sint32 L0x200196d4,
sint32 L0x200196d8, sint32 L0x200196dc, sint32 L0x200196e0,
sint32 L0x200196e4, sint32 L0x200196e8, sint32 L0x200196ec,
sint32 L0x200196f0, sint32 L0x200196f4, sint32 L0x200196f8,
sint32 L0x200196fc, sint32 L0x20019700, sint32 L0x20019704,
sint32 L0x20019708, sint32 L0x2001970c, sint32 L0x20019710,
sint32 L0x20019714, sint32 L0x20019718, sint32 L0x2001971c,
sint32 L0x20019720, sint32 L0x20019724, sint32 L0x20019728,
sint32 L0x2001972c, sint32 L0x20019730, sint32 L0x20019734,
sint32 L0x20019738, sint32 L0x2001973c, sint32 L0x20019740,
sint32 L0x20019744, sint32 L0x20019748, sint32 L0x2001974c,
sint32 L0x20019750, sint32 L0x20019754, sint32 L0x20019758,
sint32 L0x2001975c, sint32 L0x20019760, sint32 L0x20019764,
sint32 L0x20019768, sint32 L0x2001976c, sint32 L0x20019770,
sint32 L0x20019774, sint32 L0x20019778, sint32 L0x2001977c,
sint32 L0x20019780, sint32 L0x20019784, sint32 L0x20019788,
sint32 L0x2001978c, sint32 L0x20019790, sint32 L0x20019794,
sint32 L0x20019798, sint32 L0x2001979c, sint32 L0x200197a0,
sint32 L0x200197a4, sint32 L0x200197a8, sint32 L0x200197ac,
sint32 L0x200197b0, sint32 L0x200197b4, sint32 L0x200197b8,
sint32 L0x200197bc, sint32 L0x200197c0, sint32 L0x200197c4,
sint32 L0x200197c8, sint32 L0x200197cc, sint32 L0x200197d0,
sint32 L0x200197d4, sint32 L0x200197d8, sint32 L0x200197dc,
sint32 L0x200197e0, sint32 L0x200197e4, sint32 L0x200197e8,
sint32 L0x200197ec, sint32 L0x200197f0, sint32 L0x200197f4,
sint32 L0x200197f8, sint32 L0x200197fc, sint32 L0x20019800,
sint32 L0x20019804, sint32 L0x20019808, sint32 L0x2001980c,
sint32 L0x20019810, sint32 L0x20019814, sint32 L0x20019818,
sint32 L0x2001981c, sint32 L0x20019820, sint32 L0x20019824,
sint32 L0x20019828, sint32 L0x2001982c, sint32 L0x20019830,
sint32 L0x20019834, sint32 L0x20019838, sint32 L0x2001983c,
sint32 L0x20019840, sint32 L0x20019844, sint32 L0x20019848,
sint32 L0x2001984c, sint32 L0x20019850, sint32 L0x20019854,
sint32 L0x20019858, sint32 L0x2001985c, sint32 L0x20019860,
sint32 L0x20019864, sint32 L0x20019868, sint32 L0x2001986c,
sint32 L0x20019870, sint32 L0x20019874, sint32 L0x20019878,
sint32 L0x2001987c, sint32 L0x20019880, sint32 L0x20019884,
sint32 L0x20019888, sint32 L0x2001988c, sint32 L0x20019890,
sint32 L0x20019894, sint32 L0x20019898, sint32 L0x2001989c,
sint32 L0x200198a0, sint32 L0x200198a4, sint32 L0x200198a8,
sint32 L0x200198ac, sint32 L0x200198b0, sint32 L0x200198b4,
sint32 L0x200198b8, sint32 L0x200198bc, sint32 L0x200198c0,
sint32 L0x200198c4, sint32 L0x200198c8, sint32 L0x200198cc,
sint32 L0x200198d0, sint32 L0x200198d4, sint32 L0x200198d8,
sint32 L0x200198dc, sint32 L0x200198e0, sint32 L0x200198e4,
sint32 L0x200198e8, sint32 L0x200198ec, sint32 L0x200198f0,
sint32 L0x200198f4, sint32 L0x200198f8, sint32 L0x200198fc,
sint32 L0x20019900, sint32 L0x20019904, sint32 L0x20019908,
sint32 L0x2001990c, sint32 L0x20019910, sint32 L0x20019914,
sint32 L0x20019918, sint32 L0x2001991c, sint32 L0x20019920,
sint32 L0x20019924, sint32 L0x20019928, sint32 L0x2001992c,
sint32 L0x20019930, sint32 L0x20019934, sint32 L0x20019938,
sint32 L0x2001993c, sint32 L0x20019940, sint32 L0x20019944,
sint32 L0x20019948, sint32 L0x2001994c, sint32 L0x20019950,
sint32 L0x20019954, sint32 L0x20019958, sint32 L0x2001995c,
sint32 L0x20019960, sint32 L0x20019964, sint32 L0x20019968,
sint32 L0x2001996c, sint32 L0x20019970, sint32 L0x20019974,
sint32 L0x20019978, sint32 L0x2001997c, sint32 L0x20019980,
sint32 L0x20019984, sint32 L0x20019988, sint32 L0x2001998c,
sint32 L0x20019990, sint32 L0x20019994, sint32 L0x20019998,
sint32 L0x2001999c, sint32 L0x200199a0, sint32 L0x200199a4,
sint32 L0x200199a8, sint32 L0x200199ac, sint32 L0x200199b0,
sint32 L0x200199b4, sint32 L0x200199b8, sint32 L0x200199bc,
sint32 L0x200199c0, sint32 L0x200199c4, sint32 L0x200199c8,
sint32 L0x200199cc, sint32 L0x200199d0, sint32 L0x200199d4,
sint32 L0x200199d8, sint32 L0x200199dc, sint32 L0x200199e0,
sint32 L0x200199e4, sint32 L0x200199e8, sint32 L0x200199ec,
sint32 L0x200199f0, sint32 L0x200199f4, sint32 L0x200199f8,
sint32 L0x200199fc, sint32 L0x20019a00, sint32 L0x20019a04,
sint32 L0x20019a08, sint32 L0x20019a0c, sint32 L0x20019a10,
sint32 L0x20019a14, sint32 L0x20019a18, sint32 L0x20019a1c,
sint32 L0x20019a20, sint32 L0x20019a24, sint32 L0x20019a28,
sint32 L0x20019a2c, sint32 L0x20019a30, sint32 L0x20019a34,
sint32 L0x20019a38, sint32 L0x20019a3c, sint32 L0x20019a40,
sint32 L0x20019a44, sint32 L0x20019a48, sint32 L0x20019a4c,
sint32 L0x20019a50, sint32 L0x20019a54, sint32 L0x20019a58,
sint32 L0x20019a5c, sint32 L0x20019a60, sint32 L0x20019a64,
sint32 L0x20019a68, sint32 L0x20019a6c, sint32 L0x20019a70,
sint32 L0x20019a74, sint32 L0x20019a78, sint32 L0x20019a7c,
sint32 L0x20019a80, sint32 L0x20019a84, sint32 L0x20019a88,
sint32 L0x20019a8c, sint32 L0x20019a90, sint32 L0x20019a94,
sint32 L0x20019a98, sint32 L0x20019a9c, sint32 L0x20019aa0,
sint32 L0x20019aa4, sint32 L0x20019aa8, sint32 L0x20019aac,
sint32 L0x20019ab0, sint32 L0x20019ab4, sint32 L0x20019ab8,
sint32 L0x20019abc, sint32 L0x20019ac0, sint32 L0x20019ac4,
sint32 L0x20019ac8, sint32 L0x20019acc, sint32 L0x20019ad0,
sint32 L0x20019ad4, sint32 L0x20019ad8, sint32 L0x20019adc,
sint32 L0x20019ae0, sint32 L0x20019ae4, sint32 L0x20019ae8,
sint32 L0x20019aec, sint32 L0x20019af0, sint32 L0x20019af4,
sint32 L0x20019af8, sint32 L0x20019afc, sint32 L0x20019b00,
sint32 L0x20019b04, sint32 L0x20019b08, sint32 L0x20019b0c,
sint32 L0x20019b10, sint32 L0x20019b14, sint32 L0x20019b18,
sint32 L0x20019b1c, sint32 L0x20019b20, sint32 L0x20019b24,
sint32 L0x20019b28, sint32 L0x20019b2c, sint32 L0x20019b30,
sint32 L0x20019b34, sint32 L0x20019b38, sint32 L0x20019b3c,
sint32 L0x20019b40, sint32 L0x20019b44, sint32 L0x20019b48,
sint32 L0x20019b4c, sint32 L0x20019b50, sint32 L0x20019b54,
sint32 L0x20019b58, sint32 L0x20019b5c, sint32 L0x20019b60,
sint32 L0x20019b64, sint32 L0x20019b68, sint32 L0x20019b6c,
sint32 L0x20019b70, sint32 L0x20019b74, sint32 L0x20019b78,
sint32 L0x20019b7c, sint32 L0x20019b80, sint32 L0x20019b84,
sint32 L0x20019b88, sint32 L0x20019b8c, sint32 L0x20019b90,
sint32 L0x20019b94, sint32 L0x20019b98, sint32 L0x20019b9c,
sint32 L0x20019ba0, sint32 L0x20019ba4, sint32 L0x20019ba8,
sint32 L0x20019bac, sint32 L0x20019bb0, sint32 L0x20019bb4,
sint32 L0x20019bb8, sint32 L0x20019bbc, sint32 L0x20019bc0,
sint32 L0x20019bc4, sint32 L0x20019bc8, sint32 L0x20019bcc,
sint32 L0x20019bd0, sint32 L0x20019bd4, sint32 L0x20019bd8,
sint32 L0x20019bdc, sint32 L0x20019be0, sint32 L0x20019be4,
sint32 L0x20019be8, sint32 L0x20019bec, sint32 L0x20019bf0,
sint32 L0x20019bf4, sint32 L0x20019bf8, sint32 L0x20019bfc,
sint32 L0x20019c00, sint32 L0x20019c04, sint32 L0x20019c08,
sint32 L0x20019c0c, sint32 L0x20019c10, sint32 L0x20019c14,
sint32 L0x20019c18, sint32 L0x20019c1c, sint32 L0x20019c20,
sint32 L0x20019c24, sint32 L0x20019c28, sint32 L0x20019c2c,
sint32 L0x20019c30, sint32 L0x20019c34, sint32 L0x20019c38,
sint32 L0x20019c3c, sint32 L0x20019c40, sint32 L0x20019c44,
sint32 L0x20019c48, sint32 L0x20019c4c, sint32 L0x20019c50,
sint32 L0x20019c54, sint32 L0x20019c58, sint32 L0x20019c5c,
sint32 L0x20019c60, sint32 L0x20019c64, sint32 L0x20019c68,
sint32 L0x20019c6c, sint32 L0x20019c70, sint32 L0x20019c74,
sint32 L0x20019c78, sint32 L0x20019c7c, sint32 L0x20019c80,
sint32 L0x20019c84, sint32 L0x20019c88, sint32 L0x20019c8c,
sint32 L0x20019c90, sint32 L0x20019c94, sint32 L0x20019c98,
sint32 L0x20019c9c, sint32 L0x20019ca0, sint32 L0x20019ca4,
sint32 L0x20019ca8, sint32 L0x20019cac, sint32 L0x20019cb0,
sint32 L0x20019cb4, sint32 L0x20019cb8, sint32 L0x20019cbc,
sint32 L0x20019cc0, sint32 L0x20019cc4, sint32 L0x20019cc8,
sint32 L0x20019ccc, sint32 L0x20019cd0, sint32 L0x20019cd4,
sint32 L0x20019cd8, sint32 L0x20019cdc, sint32 L0x20019ce0,
sint32 L0x20019ce4, sint32 L0x20019ce8, sint32 L0x20019cec,
sint32 L0x20019cf0, sint32 L0x20019cf4, sint32 L0x20019cf8,
sint32 L0x20019cfc, sint32 L0x20019d00, sint32 L0x20019d04,
sint32 L0x20019d08, sint32 L0x20019d0c, sint32 L0x20019d10,
sint32 L0x20019d14, sint32 L0x20019d18, sint32 L0x20019d1c,
sint32 L0x20019d20, sint32 L0x20019d24, sint32 L0x20019d28,
sint32 L0x20019d2c, sint32 L0x20019d30, sint32 L0x20019d34,
sint32 L0x20019d38, sint32 L0x20019d3c, sint32 L0x20019d40,
sint32 L0x20019d44, sint32 L0x20019d48, sint32 L0x20019d4c,
sint32 L0x20019d50, sint32 L0x20019d54, sint32 L0x20019d58,
sint32 L0x20019d5c, sint32 L0x20019d60, sint32 L0x20019d64,
sint32 L0x20019d68, sint32 L0x20019d6c, sint32 L0x20019d70,
sint32 L0x20019d74, sint32 L0x20019d78, sint32 L0x20019d7c,
sint32 L0x20019d80, sint32 L0x20019d84, sint32 L0x20019d88,
sint32 L0x20019d8c, sint32 L0x20019d90, sint32 L0x20019d94,
sint32 L0x20019d98, sint32 L0x20019d9c, sint32 L0x20019da0,
sint32 L0x20019da4, sint32 L0x20019da8, sint32 L0x20019dac,
sint32 L0x20019db0, sint32 L0x20019db4, sint32 L0x20019db8,
sint32 L0x20019dbc, sint32 L0x20019dc0, sint32 L0x20019dc4,
sint32 L0x20019dc8, sint32 L0x20019dcc, sint32 L0x20019dd0,
sint32 L0x20019dd4, sint32 L0x20019dd8, sint32 L0x20019ddc,
sint32 L0x20019de0, sint32 L0x20019de4, sint32 L0x20019de8,
sint32 L0x20019dec, sint32 L0x20019df0, sint32 L0x20019df4,
sint32 L0x20019df8, sint32 L0x20019dfc, sint32 L0x20019e00,
sint32 L0x20019e04, sint32 L0x20019e08, sint32 L0x20019e0c,
sint32 L0x20019e10, sint32 L0x20019e14, sint32 L0x20019e18,
sint32 L0x20019e1c, sint32 L0x20019e20, sint32 L0x20019e24,
sint32 L0x20019e28, sint32 L0x20019e2c, sint32 L0x20019e30,
sint32 L0x20019e34, sint32 L0x20019e38, sint32 L0x20019e3c,
sint32 L0x20019e40, sint32 L0x20019e44, sint32 L0x20019e48,
sint32 L0x20019e4c, sint32 L0x20019e50, sint32 L0x20019e54,
sint32 L0x20019e58, sint32 L0x20019e5c, sint32 L0x20019e60,
sint32 L0x20019e64, sint32 L0x20019e68, sint32 L0x20019e6c,
sint32 L0x20019e70, sint32 L0x20019e74, sint32 L0x20019e78,
sint32 L0x20019e7c, sint32 L0x20019e80, sint32 L0x20019e84,
sint32 L0x20019e88, sint32 L0x20019e8c, sint32 L0x20019e90,
sint32 L0x20019e94, sint32 L0x20019e98, sint32 L0x20019e9c,
sint32 L0x20019ea0, sint32 L0x20019ea4, sint32 L0x20019ea8,
sint32 L0x20019eac, sint32 L0x20019eb0, sint32 L0x20019eb4,
sint32 L0x20019eb8, sint32 L0x20019ebc, sint32 L0x20019ec0,
sint32 L0x20019ec4, sint32 L0x20019ec8, sint32 L0x20019ecc,
sint32 L0x20019ed0, sint32 L0x20019ed4, sint32 L0x20019ed8,
sint32 L0x20019edc, sint32 L0x20019ee0, sint32 L0x20019ee4,
sint32 L0x20019ee8, sint32 L0x20019eec, sint32 L0x20019ef0,
sint32 L0x20019ef4, sint32 L0x20019ef8, sint32 L0x20019efc,
sint32 L0x20019f00, sint32 L0x20019f04, sint32 L0x20019f08,
sint32 L0x20019f0c, sint32 L0x20019f10, sint32 L0x20019f14,
sint32 L0x20019f18, sint32 L0x20019f1c, sint32 L0x20019f20,
sint32 L0x20019f24, sint32 L0x20019f28, sint32 L0x20019f2c,
sint32 L0x20019f30, sint32 L0x20019f34, sint32 L0x20019f38,
sint32 L0x20019f3c, sint32 L0x20019f40, sint32 L0x20019f44,
sint32 L0x20019f48, sint32 L0x20019f4c, sint32 L0x20019f50,
sint32 L0x20019f54, sint32 L0x20019f58, sint32 L0x20019f5c,
sint32 L0x20019f60, sint32 L0x20019f64, sint32 L0x20019f68,
sint32 L0x20019f6c, sint32 L0x20019f70, sint32 L0x20019f74,
sint32 L0x20019f78, sint32 L0x20019f7c, sint32 L0x20019f80,
sint32 L0x20019f84, sint32 L0x20019f88, sint32 L0x20019f8c,
sint32 L0x20019f90, sint32 L0x20019f94, sint32 L0x20019f98,
sint32 L0x20019f9c, sint32 L0x20019fa0, sint32 L0x20019fa4,
sint32 L0x20019fa8, sint32 L0x20019fac, sint32 L0x20019fb0,
sint32 L0x20019fb4, sint32 L0x20019fb8, sint32 L0x20019fbc,
sint32 L0x20019fc0, sint32 L0x20019fc4, sint32 L0x20019fc8,
sint32 L0x20019fcc
) =
{
and [
eqmod (cinp_poly**2)
    (L0x20018fd0*(x** 0)+L0x20018fd4*(x** 1)+L0x20018fd8*(x** 2)+
     L0x20018fdc*(x** 3)+L0x20018fe0*(x** 4)+L0x20018fe4*(x** 5)+
     L0x20018fe8*(x** 6)+L0x20018fec*(x** 7)+L0x20018ff0*(x** 8)+
     L0x20018ff4*(x** 9)+L0x20018ff8*(x**10)+L0x20018ffc*(x**11)+
     L0x20019000*(x**12)+L0x20019004*(x**13)+L0x20019008*(x**14)+
     L0x2001900c*(x**15)+L0x20019010*(x**16)+L0x20019014*(x**17)+
     L0x20019018*(x**18)+L0x2001901c*(x**19)+L0x20019020*(x**20)+
     L0x20019024*(x**21)+L0x20019028*(x**22)+L0x2001902c*(x**23)+
     L0x20019030*(x**24)+L0x20019034*(x**25)+L0x20019038*(x**26)+
     L0x2001903c*(x**27)+L0x20019040*(x**28)+L0x20019044*(x**29)+
     L0x20019048*(x**30)+L0x2001904c*(x**31)+L0x20019050*(x**32)+
     L0x20019054*(x**33)+L0x20019058*(x**34)+L0x2001905c*(x**35)+
     L0x20019060*(x**36)+L0x20019064*(x**37)+L0x20019068*(x**38)+
     L0x2001906c*(x**39)+L0x20019070*(x**40)+L0x20019074*(x**41)+
     L0x20019078*(x**42)+L0x2001907c*(x**43)+L0x20019080*(x**44)+
     L0x20019084*(x**45)+L0x20019088*(x**46)+L0x2001908c*(x**47)+
     L0x20019090*(x**48)+L0x20019094*(x**49)+L0x20019098*(x**50)+
     L0x2001909c*(x**51)+L0x200190a0*(x**52)+L0x200190a4*(x**53)+
     L0x200190a8*(x**54)+L0x200190ac*(x**55)+L0x200190b0*(x**56)+
     L0x200190b4*(x**57)+L0x200190b8*(x**58)+L0x200190bc*(x**59)+
     L0x200190c0*(x**60)+L0x200190c4*(x**61)+L0x200190c8*(x**62)+
     L0x200190cc*(x**63))
    [1043969, x**64 - 1],
eqmod (cinp_poly**2)
    (L0x200190d0*(x** 0)+L0x200190d4*(x** 1)+L0x200190d8*(x** 2)+
     L0x200190dc*(x** 3)+L0x200190e0*(x** 4)+L0x200190e4*(x** 5)+
     L0x200190e8*(x** 6)+L0x200190ec*(x** 7)+L0x200190f0*(x** 8)+
     L0x200190f4*(x** 9)+L0x200190f8*(x**10)+L0x200190fc*(x**11)+
     L0x20019100*(x**12)+L0x20019104*(x**13)+L0x20019108*(x**14)+
     L0x2001910c*(x**15)+L0x20019110*(x**16)+L0x20019114*(x**17)+
     L0x20019118*(x**18)+L0x2001911c*(x**19)+L0x20019120*(x**20)+
     L0x20019124*(x**21)+L0x20019128*(x**22)+L0x2001912c*(x**23)+
     L0x20019130*(x**24)+L0x20019134*(x**25)+L0x20019138*(x**26)+
     L0x2001913c*(x**27)+L0x20019140*(x**28)+L0x20019144*(x**29)+
     L0x20019148*(x**30)+L0x2001914c*(x**31)+L0x20019150*(x**32)+
     L0x20019154*(x**33)+L0x20019158*(x**34)+L0x2001915c*(x**35)+
     L0x20019160*(x**36)+L0x20019164*(x**37)+L0x20019168*(x**38)+
     L0x2001916c*(x**39)+L0x20019170*(x**40)+L0x20019174*(x**41)+
     L0x20019178*(x**42)+L0x2001917c*(x**43)+L0x20019180*(x**44)+
     L0x20019184*(x**45)+L0x20019188*(x**46)+L0x2001918c*(x**47)+
     L0x20019190*(x**48)+L0x20019194*(x**49)+L0x20019198*(x**50)+
     L0x2001919c*(x**51)+L0x200191a0*(x**52)+L0x200191a4*(x**53)+
     L0x200191a8*(x**54)+L0x200191ac*(x**55)+L0x200191b0*(x**56)+
     L0x200191b4*(x**57)+L0x200191b8*(x**58)+L0x200191bc*(x**59)+
     L0x200191c0*(x**60)+L0x200191c4*(x**61)+L0x200191c8*(x**62)+
     L0x200191cc*(x**63))
    [1043969, x**64 - 1043968],
eqmod (cinp_poly**2)
    (L0x200191d0*(x** 0)+L0x200191d4*(x** 1)+L0x200191d8*(x** 2)+
     L0x200191dc*(x** 3)+L0x200191e0*(x** 4)+L0x200191e4*(x** 5)+
     L0x200191e8*(x** 6)+L0x200191ec*(x** 7)+L0x200191f0*(x** 8)+
     L0x200191f4*(x** 9)+L0x200191f8*(x**10)+L0x200191fc*(x**11)+
     L0x20019200*(x**12)+L0x20019204*(x**13)+L0x20019208*(x**14)+
     L0x2001920c*(x**15)+L0x20019210*(x**16)+L0x20019214*(x**17)+
     L0x20019218*(x**18)+L0x2001921c*(x**19)+L0x20019220*(x**20)+
     L0x20019224*(x**21)+L0x20019228*(x**22)+L0x2001922c*(x**23)+
     L0x20019230*(x**24)+L0x20019234*(x**25)+L0x20019238*(x**26)+
     L0x2001923c*(x**27)+L0x20019240*(x**28)+L0x20019244*(x**29)+
     L0x20019248*(x**30)+L0x2001924c*(x**31)+L0x20019250*(x**32)+
     L0x20019254*(x**33)+L0x20019258*(x**34)+L0x2001925c*(x**35)+
     L0x20019260*(x**36)+L0x20019264*(x**37)+L0x20019268*(x**38)+
     L0x2001926c*(x**39)+L0x20019270*(x**40)+L0x20019274*(x**41)+
     L0x20019278*(x**42)+L0x2001927c*(x**43)+L0x20019280*(x**44)+
     L0x20019284*(x**45)+L0x20019288*(x**46)+L0x2001928c*(x**47)+
     L0x20019290*(x**48)+L0x20019294*(x**49)+L0x20019298*(x**50)+
     L0x2001929c*(x**51)+L0x200192a0*(x**52)+L0x200192a4*(x**53)+
     L0x200192a8*(x**54)+L0x200192ac*(x**55)+L0x200192b0*(x**56)+
     L0x200192b4*(x**57)+L0x200192b8*(x**58)+L0x200192bc*(x**59)+
     L0x200192c0*(x**60)+L0x200192c4*(x**61)+L0x200192c8*(x**62)+
     L0x200192cc*(x**63))
    [1043969, x**64 - 554923],
eqmod (cinp_poly**2)
    (L0x200192d0*(x** 0)+L0x200192d4*(x** 1)+L0x200192d8*(x** 2)+
     L0x200192dc*(x** 3)+L0x200192e0*(x** 4)+L0x200192e4*(x** 5)+
     L0x200192e8*(x** 6)+L0x200192ec*(x** 7)+L0x200192f0*(x** 8)+
     L0x200192f4*(x** 9)+L0x200192f8*(x**10)+L0x200192fc*(x**11)+
     L0x20019300*(x**12)+L0x20019304*(x**13)+L0x20019308*(x**14)+
     L0x2001930c*(x**15)+L0x20019310*(x**16)+L0x20019314*(x**17)+
     L0x20019318*(x**18)+L0x2001931c*(x**19)+L0x20019320*(x**20)+
     L0x20019324*(x**21)+L0x20019328*(x**22)+L0x2001932c*(x**23)+
     L0x20019330*(x**24)+L0x20019334*(x**25)+L0x20019338*(x**26)+
     L0x2001933c*(x**27)+L0x20019340*(x**28)+L0x20019344*(x**29)+
     L0x20019348*(x**30)+L0x2001934c*(x**31)+L0x20019350*(x**32)+
     L0x20019354*(x**33)+L0x20019358*(x**34)+L0x2001935c*(x**35)+
     L0x20019360*(x**36)+L0x20019364*(x**37)+L0x20019368*(x**38)+
     L0x2001936c*(x**39)+L0x20019370*(x**40)+L0x20019374*(x**41)+
     L0x20019378*(x**42)+L0x2001937c*(x**43)+L0x20019380*(x**44)+
     L0x20019384*(x**45)+L0x20019388*(x**46)+L0x2001938c*(x**47)+
     L0x20019390*(x**48)+L0x20019394*(x**49)+L0x20019398*(x**50)+
     L0x2001939c*(x**51)+L0x200193a0*(x**52)+L0x200193a4*(x**53)+
     L0x200193a8*(x**54)+L0x200193ac*(x**55)+L0x200193b0*(x**56)+
     L0x200193b4*(x**57)+L0x200193b8*(x**58)+L0x200193bc*(x**59)+
     L0x200193c0*(x**60)+L0x200193c4*(x**61)+L0x200193c8*(x**62)+
     L0x200193cc*(x**63))
    [1043969, x**64 - 489046],
eqmod (cinp_poly**2)
    (L0x200193d0*(x** 0)+L0x200193d4*(x** 1)+L0x200193d8*(x** 2)+
     L0x200193dc*(x** 3)+L0x200193e0*(x** 4)+L0x200193e4*(x** 5)+
     L0x200193e8*(x** 6)+L0x200193ec*(x** 7)+L0x200193f0*(x** 8)+
     L0x200193f4*(x** 9)+L0x200193f8*(x**10)+L0x200193fc*(x**11)+
     L0x20019400*(x**12)+L0x20019404*(x**13)+L0x20019408*(x**14)+
     L0x2001940c*(x**15)+L0x20019410*(x**16)+L0x20019414*(x**17)+
     L0x20019418*(x**18)+L0x2001941c*(x**19)+L0x20019420*(x**20)+
     L0x20019424*(x**21)+L0x20019428*(x**22)+L0x2001942c*(x**23)+
     L0x20019430*(x**24)+L0x20019434*(x**25)+L0x20019438*(x**26)+
     L0x2001943c*(x**27)+L0x20019440*(x**28)+L0x20019444*(x**29)+
     L0x20019448*(x**30)+L0x2001944c*(x**31)+L0x20019450*(x**32)+
     L0x20019454*(x**33)+L0x20019458*(x**34)+L0x2001945c*(x**35)+
     L0x20019460*(x**36)+L0x20019464*(x**37)+L0x20019468*(x**38)+
     L0x2001946c*(x**39)+L0x20019470*(x**40)+L0x20019474*(x**41)+
     L0x20019478*(x**42)+L0x2001947c*(x**43)+L0x20019480*(x**44)+
     L0x20019484*(x**45)+L0x20019488*(x**46)+L0x2001948c*(x**47)+
     L0x20019490*(x**48)+L0x20019494*(x**49)+L0x20019498*(x**50)+
     L0x2001949c*(x**51)+L0x200194a0*(x**52)+L0x200194a4*(x**53)+
     L0x200194a8*(x**54)+L0x200194ac*(x**55)+L0x200194b0*(x**56)+
     L0x200194b4*(x**57)+L0x200194b8*(x**58)+L0x200194bc*(x**59)+
     L0x200194c0*(x**60)+L0x200194c4*(x**61)+L0x200194c8*(x**62)+
     L0x200194cc*(x**63))
    [1043969, x**64 - 287998],
eqmod (cinp_poly**2)
    (L0x200194d0*(x** 0)+L0x200194d4*(x** 1)+L0x200194d8*(x** 2)+
     L0x200194dc*(x** 3)+L0x200194e0*(x** 4)+L0x200194e4*(x** 5)+
     L0x200194e8*(x** 6)+L0x200194ec*(x** 7)+L0x200194f0*(x** 8)+
     L0x200194f4*(x** 9)+L0x200194f8*(x**10)+L0x200194fc*(x**11)+
     L0x20019500*(x**12)+L0x20019504*(x**13)+L0x20019508*(x**14)+
     L0x2001950c*(x**15)+L0x20019510*(x**16)+L0x20019514*(x**17)+
     L0x20019518*(x**18)+L0x2001951c*(x**19)+L0x20019520*(x**20)+
     L0x20019524*(x**21)+L0x20019528*(x**22)+L0x2001952c*(x**23)+
     L0x20019530*(x**24)+L0x20019534*(x**25)+L0x20019538*(x**26)+
     L0x2001953c*(x**27)+L0x20019540*(x**28)+L0x20019544*(x**29)+
     L0x20019548*(x**30)+L0x2001954c*(x**31)+L0x20019550*(x**32)+
     L0x20019554*(x**33)+L0x20019558*(x**34)+L0x2001955c*(x**35)+
     L0x20019560*(x**36)+L0x20019564*(x**37)+L0x20019568*(x**38)+
     L0x2001956c*(x**39)+L0x20019570*(x**40)+L0x20019574*(x**41)+
     L0x20019578*(x**42)+L0x2001957c*(x**43)+L0x20019580*(x**44)+
     L0x20019584*(x**45)+L0x20019588*(x**46)+L0x2001958c*(x**47)+
     L0x20019590*(x**48)+L0x20019594*(x**49)+L0x20019598*(x**50)+
     L0x2001959c*(x**51)+L0x200195a0*(x**52)+L0x200195a4*(x**53)+
     L0x200195a8*(x**54)+L0x200195ac*(x**55)+L0x200195b0*(x**56)+
     L0x200195b4*(x**57)+L0x200195b8*(x**58)+L0x200195bc*(x**59)+
     L0x200195c0*(x**60)+L0x200195c4*(x**61)+L0x200195c8*(x**62)+
     L0x200195cc*(x**63))
    [1043969, x**64 - 755971],
eqmod (cinp_poly**2)
    (L0x200195d0*(x** 0)+L0x200195d4*(x** 1)+L0x200195d8*(x** 2)+
     L0x200195dc*(x** 3)+L0x200195e0*(x** 4)+L0x200195e4*(x** 5)+
     L0x200195e8*(x** 6)+L0x200195ec*(x** 7)+L0x200195f0*(x** 8)+
     L0x200195f4*(x** 9)+L0x200195f8*(x**10)+L0x200195fc*(x**11)+
     L0x20019600*(x**12)+L0x20019604*(x**13)+L0x20019608*(x**14)+
     L0x2001960c*(x**15)+L0x20019610*(x**16)+L0x20019614*(x**17)+
     L0x20019618*(x**18)+L0x2001961c*(x**19)+L0x20019620*(x**20)+
     L0x20019624*(x**21)+L0x20019628*(x**22)+L0x2001962c*(x**23)+
     L0x20019630*(x**24)+L0x20019634*(x**25)+L0x20019638*(x**26)+
     L0x2001963c*(x**27)+L0x20019640*(x**28)+L0x20019644*(x**29)+
     L0x20019648*(x**30)+L0x2001964c*(x**31)+L0x20019650*(x**32)+
     L0x20019654*(x**33)+L0x20019658*(x**34)+L0x2001965c*(x**35)+
     L0x20019660*(x**36)+L0x20019664*(x**37)+L0x20019668*(x**38)+
     L0x2001966c*(x**39)+L0x20019670*(x**40)+L0x20019674*(x**41)+
     L0x20019678*(x**42)+L0x2001967c*(x**43)+L0x20019680*(x**44)+
     L0x20019684*(x**45)+L0x20019688*(x**46)+L0x2001968c*(x**47)+
     L0x20019690*(x**48)+L0x20019694*(x**49)+L0x20019698*(x**50)+
     L0x2001969c*(x**51)+L0x200196a0*(x**52)+L0x200196a4*(x**53)+
     L0x200196a8*(x**54)+L0x200196ac*(x**55)+L0x200196b0*(x**56)+
     L0x200196b4*(x**57)+L0x200196b8*(x**58)+L0x200196bc*(x**59)+
     L0x200196c0*(x**60)+L0x200196c4*(x**61)+L0x200196c8*(x**62)+
     L0x200196cc*(x**63))
    [1043969, x**64 - 719789],
eqmod (cinp_poly**2)
    (L0x200196d0*(x** 0)+L0x200196d4*(x** 1)+L0x200196d8*(x** 2)+
     L0x200196dc*(x** 3)+L0x200196e0*(x** 4)+L0x200196e4*(x** 5)+
     L0x200196e8*(x** 6)+L0x200196ec*(x** 7)+L0x200196f0*(x** 8)+
     L0x200196f4*(x** 9)+L0x200196f8*(x**10)+L0x200196fc*(x**11)+
     L0x20019700*(x**12)+L0x20019704*(x**13)+L0x20019708*(x**14)+
     L0x2001970c*(x**15)+L0x20019710*(x**16)+L0x20019714*(x**17)+
     L0x20019718*(x**18)+L0x2001971c*(x**19)+L0x20019720*(x**20)+
     L0x20019724*(x**21)+L0x20019728*(x**22)+L0x2001972c*(x**23)+
     L0x20019730*(x**24)+L0x20019734*(x**25)+L0x20019738*(x**26)+
     L0x2001973c*(x**27)+L0x20019740*(x**28)+L0x20019744*(x**29)+
     L0x20019748*(x**30)+L0x2001974c*(x**31)+L0x20019750*(x**32)+
     L0x20019754*(x**33)+L0x20019758*(x**34)+L0x2001975c*(x**35)+
     L0x20019760*(x**36)+L0x20019764*(x**37)+L0x20019768*(x**38)+
     L0x2001976c*(x**39)+L0x20019770*(x**40)+L0x20019774*(x**41)+
     L0x20019778*(x**42)+L0x2001977c*(x**43)+L0x20019780*(x**44)+
     L0x20019784*(x**45)+L0x20019788*(x**46)+L0x2001978c*(x**47)+
     L0x20019790*(x**48)+L0x20019794*(x**49)+L0x20019798*(x**50)+
     L0x2001979c*(x**51)+L0x200197a0*(x**52)+L0x200197a4*(x**53)+
     L0x200197a8*(x**54)+L0x200197ac*(x**55)+L0x200197b0*(x**56)+
     L0x200197b4*(x**57)+L0x200197b8*(x**58)+L0x200197bc*(x**59)+
     L0x200197c0*(x**60)+L0x200197c4*(x**61)+L0x200197c8*(x**62)+
     L0x200197cc*(x**63))
    [1043969, x**64 - 324180],
eqmod (cinp_poly**2)
    (L0x200197d0*(x** 0)+L0x200197d4*(x** 1)+L0x200197d8*(x** 2)+
     L0x200197dc*(x** 3)+L0x200197e0*(x** 4)+L0x200197e4*(x** 5)+
     L0x200197e8*(x** 6)+L0x200197ec*(x** 7)+L0x200197f0*(x** 8)+
     L0x200197f4*(x** 9)+L0x200197f8*(x**10)+L0x200197fc*(x**11)+
     L0x20019800*(x**12)+L0x20019804*(x**13)+L0x20019808*(x**14)+
     L0x2001980c*(x**15)+L0x20019810*(x**16)+L0x20019814*(x**17)+
     L0x20019818*(x**18)+L0x2001981c*(x**19)+L0x20019820*(x**20)+
     L0x20019824*(x**21)+L0x20019828*(x**22)+L0x2001982c*(x**23)+
     L0x20019830*(x**24)+L0x20019834*(x**25)+L0x20019838*(x**26)+
     L0x2001983c*(x**27)+L0x20019840*(x**28)+L0x20019844*(x**29)+
     L0x20019848*(x**30)+L0x2001984c*(x**31)+L0x20019850*(x**32)+
     L0x20019854*(x**33)+L0x20019858*(x**34)+L0x2001985c*(x**35)+
     L0x20019860*(x**36)+L0x20019864*(x**37)+L0x20019868*(x**38)+
     L0x2001986c*(x**39)+L0x20019870*(x**40)+L0x20019874*(x**41)+
     L0x20019878*(x**42)+L0x2001987c*(x**43)+L0x20019880*(x**44)+
     L0x20019884*(x**45)+L0x20019888*(x**46)+L0x2001988c*(x**47)+
     L0x20019890*(x**48)+L0x20019894*(x**49)+L0x20019898*(x**50)+
     L0x2001989c*(x**51)+L0x200198a0*(x**52)+L0x200198a4*(x**53)+
     L0x200198a8*(x**54)+L0x200198ac*(x**55)+L0x200198b0*(x**56)+
     L0x200198b4*(x**57)+L0x200198b8*(x**58)+L0x200198bc*(x**59)+
     L0x200198c0*(x**60)+L0x200198c4*(x**61)+L0x200198c8*(x**62)+
     L0x200198cc*(x**63))
    [1043969, x**64 - 29512],
eqmod (cinp_poly**2)
    (L0x200198d0*(x** 0)+L0x200198d4*(x** 1)+L0x200198d8*(x** 2)+
     L0x200198dc*(x** 3)+L0x200198e0*(x** 4)+L0x200198e4*(x** 5)+
     L0x200198e8*(x** 6)+L0x200198ec*(x** 7)+L0x200198f0*(x** 8)+
     L0x200198f4*(x** 9)+L0x200198f8*(x**10)+L0x200198fc*(x**11)+
     L0x20019900*(x**12)+L0x20019904*(x**13)+L0x20019908*(x**14)+
     L0x2001990c*(x**15)+L0x20019910*(x**16)+L0x20019914*(x**17)+
     L0x20019918*(x**18)+L0x2001991c*(x**19)+L0x20019920*(x**20)+
     L0x20019924*(x**21)+L0x20019928*(x**22)+L0x2001992c*(x**23)+
     L0x20019930*(x**24)+L0x20019934*(x**25)+L0x20019938*(x**26)+
     L0x2001993c*(x**27)+L0x20019940*(x**28)+L0x20019944*(x**29)+
     L0x20019948*(x**30)+L0x2001994c*(x**31)+L0x20019950*(x**32)+
     L0x20019954*(x**33)+L0x20019958*(x**34)+L0x2001995c*(x**35)+
     L0x20019960*(x**36)+L0x20019964*(x**37)+L0x20019968*(x**38)+
     L0x2001996c*(x**39)+L0x20019970*(x**40)+L0x20019974*(x**41)+
     L0x20019978*(x**42)+L0x2001997c*(x**43)+L0x20019980*(x**44)+
     L0x20019984*(x**45)+L0x20019988*(x**46)+L0x2001998c*(x**47)+
     L0x20019990*(x**48)+L0x20019994*(x**49)+L0x20019998*(x**50)+
     L0x2001999c*(x**51)+L0x200199a0*(x**52)+L0x200199a4*(x**53)+
     L0x200199a8*(x**54)+L0x200199ac*(x**55)+L0x200199b0*(x**56)+
     L0x200199b4*(x**57)+L0x200199b8*(x**58)+L0x200199bc*(x**59)+
     L0x200199c0*(x**60)+L0x200199c4*(x**61)+L0x200199c8*(x**62)+
     L0x200199cc*(x**63))
    [1043969, x**64 - 1014457],
eqmod (cinp_poly**2)
    (L0x200199d0*(x** 0)+L0x200199d4*(x** 1)+L0x200199d8*(x** 2)+
     L0x200199dc*(x** 3)+L0x200199e0*(x** 4)+L0x200199e4*(x** 5)+
     L0x200199e8*(x** 6)+L0x200199ec*(x** 7)+L0x200199f0*(x** 8)+
     L0x200199f4*(x** 9)+L0x200199f8*(x**10)+L0x200199fc*(x**11)+
     L0x20019a00*(x**12)+L0x20019a04*(x**13)+L0x20019a08*(x**14)+
     L0x20019a0c*(x**15)+L0x20019a10*(x**16)+L0x20019a14*(x**17)+
     L0x20019a18*(x**18)+L0x20019a1c*(x**19)+L0x20019a20*(x**20)+
     L0x20019a24*(x**21)+L0x20019a28*(x**22)+L0x20019a2c*(x**23)+
     L0x20019a30*(x**24)+L0x20019a34*(x**25)+L0x20019a38*(x**26)+
     L0x20019a3c*(x**27)+L0x20019a40*(x**28)+L0x20019a44*(x**29)+
     L0x20019a48*(x**30)+L0x20019a4c*(x**31)+L0x20019a50*(x**32)+
     L0x20019a54*(x**33)+L0x20019a58*(x**34)+L0x20019a5c*(x**35)+
     L0x20019a60*(x**36)+L0x20019a64*(x**37)+L0x20019a68*(x**38)+
     L0x20019a6c*(x**39)+L0x20019a70*(x**40)+L0x20019a74*(x**41)+
     L0x20019a78*(x**42)+L0x20019a7c*(x**43)+L0x20019a80*(x**44)+
     L0x20019a84*(x**45)+L0x20019a88*(x**46)+L0x20019a8c*(x**47)+
     L0x20019a90*(x**48)+L0x20019a94*(x**49)+L0x20019a98*(x**50)+
     L0x20019a9c*(x**51)+L0x20019aa0*(x**52)+L0x20019aa4*(x**53)+
     L0x20019aa8*(x**54)+L0x20019aac*(x**55)+L0x20019ab0*(x**56)+
     L0x20019ab4*(x**57)+L0x20019ab8*(x**58)+L0x20019abc*(x**59)+
     L0x20019ac0*(x**60)+L0x20019ac4*(x**61)+L0x20019ac8*(x**62)+
     L0x20019acc*(x**63))
    [1043969, x**64 - 145873],
eqmod (cinp_poly**2)
    (L0x20019ad0*(x** 0)+L0x20019ad4*(x** 1)+L0x20019ad8*(x** 2)+
     L0x20019adc*(x** 3)+L0x20019ae0*(x** 4)+L0x20019ae4*(x** 5)+
     L0x20019ae8*(x** 6)+L0x20019aec*(x** 7)+L0x20019af0*(x** 8)+
     L0x20019af4*(x** 9)+L0x20019af8*(x**10)+L0x20019afc*(x**11)+
     L0x20019b00*(x**12)+L0x20019b04*(x**13)+L0x20019b08*(x**14)+
     L0x20019b0c*(x**15)+L0x20019b10*(x**16)+L0x20019b14*(x**17)+
     L0x20019b18*(x**18)+L0x20019b1c*(x**19)+L0x20019b20*(x**20)+
     L0x20019b24*(x**21)+L0x20019b28*(x**22)+L0x20019b2c*(x**23)+
     L0x20019b30*(x**24)+L0x20019b34*(x**25)+L0x20019b38*(x**26)+
     L0x20019b3c*(x**27)+L0x20019b40*(x**28)+L0x20019b44*(x**29)+
     L0x20019b48*(x**30)+L0x20019b4c*(x**31)+L0x20019b50*(x**32)+
     L0x20019b54*(x**33)+L0x20019b58*(x**34)+L0x20019b5c*(x**35)+
     L0x20019b60*(x**36)+L0x20019b64*(x**37)+L0x20019b68*(x**38)+
     L0x20019b6c*(x**39)+L0x20019b70*(x**40)+L0x20019b74*(x**41)+
     L0x20019b78*(x**42)+L0x20019b7c*(x**43)+L0x20019b80*(x**44)+
     L0x20019b84*(x**45)+L0x20019b88*(x**46)+L0x20019b8c*(x**47)+
     L0x20019b90*(x**48)+L0x20019b94*(x**49)+L0x20019b98*(x**50)+
     L0x20019b9c*(x**51)+L0x20019ba0*(x**52)+L0x20019ba4*(x**53)+
     L0x20019ba8*(x**54)+L0x20019bac*(x**55)+L0x20019bb0*(x**56)+
     L0x20019bb4*(x**57)+L0x20019bb8*(x**58)+L0x20019bbc*(x**59)+
     L0x20019bc0*(x**60)+L0x20019bc4*(x**61)+L0x20019bc8*(x**62)+
     L0x20019bcc*(x**63))
    [1043969, x**64 - 898096],
eqmod (cinp_poly**2)
    (L0x20019bd0*(x** 0)+L0x20019bd4*(x** 1)+L0x20019bd8*(x** 2)+
     L0x20019bdc*(x** 3)+L0x20019be0*(x** 4)+L0x20019be4*(x** 5)+
     L0x20019be8*(x** 6)+L0x20019bec*(x** 7)+L0x20019bf0*(x** 8)+
     L0x20019bf4*(x** 9)+L0x20019bf8*(x**10)+L0x20019bfc*(x**11)+
     L0x20019c00*(x**12)+L0x20019c04*(x**13)+L0x20019c08*(x**14)+
     L0x20019c0c*(x**15)+L0x20019c10*(x**16)+L0x20019c14*(x**17)+
     L0x20019c18*(x**18)+L0x20019c1c*(x**19)+L0x20019c20*(x**20)+
     L0x20019c24*(x**21)+L0x20019c28*(x**22)+L0x20019c2c*(x**23)+
     L0x20019c30*(x**24)+L0x20019c34*(x**25)+L0x20019c38*(x**26)+
     L0x20019c3c*(x**27)+L0x20019c40*(x**28)+L0x20019c44*(x**29)+
     L0x20019c48*(x**30)+L0x20019c4c*(x**31)+L0x20019c50*(x**32)+
     L0x20019c54*(x**33)+L0x20019c58*(x**34)+L0x20019c5c*(x**35)+
     L0x20019c60*(x**36)+L0x20019c64*(x**37)+L0x20019c68*(x**38)+
     L0x20019c6c*(x**39)+L0x20019c70*(x**40)+L0x20019c74*(x**41)+
     L0x20019c78*(x**42)+L0x20019c7c*(x**43)+L0x20019c80*(x**44)+
     L0x20019c84*(x**45)+L0x20019c88*(x**46)+L0x20019c8c*(x**47)+
     L0x20019c90*(x**48)+L0x20019c94*(x**49)+L0x20019c98*(x**50)+
     L0x20019c9c*(x**51)+L0x20019ca0*(x**52)+L0x20019ca4*(x**53)+
     L0x20019ca8*(x**54)+L0x20019cac*(x**55)+L0x20019cb0*(x**56)+
     L0x20019cb4*(x**57)+L0x20019cb8*(x**58)+L0x20019cbc*(x**59)+
     L0x20019cc0*(x**60)+L0x20019cc4*(x**61)+L0x20019cc8*(x**62)+
     L0x20019ccc*(x**63))
    [1043969, x**64 - 445347],
eqmod (cinp_poly**2)
    (L0x20019cd0*(x** 0)+L0x20019cd4*(x** 1)+L0x20019cd8*(x** 2)+
     L0x20019cdc*(x** 3)+L0x20019ce0*(x** 4)+L0x20019ce4*(x** 5)+
     L0x20019ce8*(x** 6)+L0x20019cec*(x** 7)+L0x20019cf0*(x** 8)+
     L0x20019cf4*(x** 9)+L0x20019cf8*(x**10)+L0x20019cfc*(x**11)+
     L0x20019d00*(x**12)+L0x20019d04*(x**13)+L0x20019d08*(x**14)+
     L0x20019d0c*(x**15)+L0x20019d10*(x**16)+L0x20019d14*(x**17)+
     L0x20019d18*(x**18)+L0x20019d1c*(x**19)+L0x20019d20*(x**20)+
     L0x20019d24*(x**21)+L0x20019d28*(x**22)+L0x20019d2c*(x**23)+
     L0x20019d30*(x**24)+L0x20019d34*(x**25)+L0x20019d38*(x**26)+
     L0x20019d3c*(x**27)+L0x20019d40*(x**28)+L0x20019d44*(x**29)+
     L0x20019d48*(x**30)+L0x20019d4c*(x**31)+L0x20019d50*(x**32)+
     L0x20019d54*(x**33)+L0x20019d58*(x**34)+L0x20019d5c*(x**35)+
     L0x20019d60*(x**36)+L0x20019d64*(x**37)+L0x20019d68*(x**38)+
     L0x20019d6c*(x**39)+L0x20019d70*(x**40)+L0x20019d74*(x**41)+
     L0x20019d78*(x**42)+L0x20019d7c*(x**43)+L0x20019d80*(x**44)+
     L0x20019d84*(x**45)+L0x20019d88*(x**46)+L0x20019d8c*(x**47)+
     L0x20019d90*(x**48)+L0x20019d94*(x**49)+L0x20019d98*(x**50)+
     L0x20019d9c*(x**51)+L0x20019da0*(x**52)+L0x20019da4*(x**53)+
     L0x20019da8*(x**54)+L0x20019dac*(x**55)+L0x20019db0*(x**56)+
     L0x20019db4*(x**57)+L0x20019db8*(x**58)+L0x20019dbc*(x**59)+
     L0x20019dc0*(x**60)+L0x20019dc4*(x**61)+L0x20019dc8*(x**62)+
     L0x20019dcc*(x**63))
    [1043969, x**64 - 598622],
eqmod (cinp_poly**2)
    (L0x20019dd0*(x** 0)+L0x20019dd4*(x** 1)+L0x20019dd8*(x** 2)+
     L0x20019ddc*(x** 3)+L0x20019de0*(x** 4)+L0x20019de4*(x** 5)+
     L0x20019de8*(x** 6)+L0x20019dec*(x** 7)+L0x20019df0*(x** 8)+
     L0x20019df4*(x** 9)+L0x20019df8*(x**10)+L0x20019dfc*(x**11)+
     L0x20019e00*(x**12)+L0x20019e04*(x**13)+L0x20019e08*(x**14)+
     L0x20019e0c*(x**15)+L0x20019e10*(x**16)+L0x20019e14*(x**17)+
     L0x20019e18*(x**18)+L0x20019e1c*(x**19)+L0x20019e20*(x**20)+
     L0x20019e24*(x**21)+L0x20019e28*(x**22)+L0x20019e2c*(x**23)+
     L0x20019e30*(x**24)+L0x20019e34*(x**25)+L0x20019e38*(x**26)+
     L0x20019e3c*(x**27)+L0x20019e40*(x**28)+L0x20019e44*(x**29)+
     L0x20019e48*(x**30)+L0x20019e4c*(x**31)+L0x20019e50*(x**32)+
     L0x20019e54*(x**33)+L0x20019e58*(x**34)+L0x20019e5c*(x**35)+
     L0x20019e60*(x**36)+L0x20019e64*(x**37)+L0x20019e68*(x**38)+
     L0x20019e6c*(x**39)+L0x20019e70*(x**40)+L0x20019e74*(x**41)+
     L0x20019e78*(x**42)+L0x20019e7c*(x**43)+L0x20019e80*(x**44)+
     L0x20019e84*(x**45)+L0x20019e88*(x**46)+L0x20019e8c*(x**47)+
     L0x20019e90*(x**48)+L0x20019e94*(x**49)+L0x20019e98*(x**50)+
     L0x20019e9c*(x**51)+L0x20019ea0*(x**52)+L0x20019ea4*(x**53)+
     L0x20019ea8*(x**54)+L0x20019eac*(x**55)+L0x20019eb0*(x**56)+
     L0x20019eb4*(x**57)+L0x20019eb8*(x**58)+L0x20019ebc*(x**59)+
     L0x20019ec0*(x**60)+L0x20019ec4*(x**61)+L0x20019ec8*(x**62)+
     L0x20019ecc*(x**63))
    [1043969, x**64 - 775725],
eqmod (cinp_poly**2)
    (L0x20019ed0*(x** 0)+L0x20019ed4*(x** 1)+L0x20019ed8*(x** 2)+
     L0x20019edc*(x** 3)+L0x20019ee0*(x** 4)+L0x20019ee4*(x** 5)+
     L0x20019ee8*(x** 6)+L0x20019eec*(x** 7)+L0x20019ef0*(x** 8)+
     L0x20019ef4*(x** 9)+L0x20019ef8*(x**10)+L0x20019efc*(x**11)+
     L0x20019f00*(x**12)+L0x20019f04*(x**13)+L0x20019f08*(x**14)+
     L0x20019f0c*(x**15)+L0x20019f10*(x**16)+L0x20019f14*(x**17)+
     L0x20019f18*(x**18)+L0x20019f1c*(x**19)+L0x20019f20*(x**20)+
     L0x20019f24*(x**21)+L0x20019f28*(x**22)+L0x20019f2c*(x**23)+
     L0x20019f30*(x**24)+L0x20019f34*(x**25)+L0x20019f38*(x**26)+
     L0x20019f3c*(x**27)+L0x20019f40*(x**28)+L0x20019f44*(x**29)+
     L0x20019f48*(x**30)+L0x20019f4c*(x**31)+L0x20019f50*(x**32)+
     L0x20019f54*(x**33)+L0x20019f58*(x**34)+L0x20019f5c*(x**35)+
     L0x20019f60*(x**36)+L0x20019f64*(x**37)+L0x20019f68*(x**38)+
     L0x20019f6c*(x**39)+L0x20019f70*(x**40)+L0x20019f74*(x**41)+
     L0x20019f78*(x**42)+L0x20019f7c*(x**43)+L0x20019f80*(x**44)+
     L0x20019f84*(x**45)+L0x20019f88*(x**46)+L0x20019f8c*(x**47)+
     L0x20019f90*(x**48)+L0x20019f94*(x**49)+L0x20019f98*(x**50)+
     L0x20019f9c*(x**51)+L0x20019fa0*(x**52)+L0x20019fa4*(x**53)+
     L0x20019fa8*(x**54)+L0x20019fac*(x**55)+L0x20019fb0*(x**56)+
     L0x20019fb4*(x**57)+L0x20019fb8*(x**58)+L0x20019fbc*(x**59)+
     L0x20019fc0*(x**60)+L0x20019fc4*(x**61)+L0x20019fc8*(x**62)+
     L0x20019fcc*(x**63))
    [1043969, x**64 - 268244]
] && and [
(-5)@32*1043969@32 <s L0x20018fd0, L0x20018fd0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018fd4, L0x20018fd4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018fd8, L0x20018fd8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018fdc, L0x20018fdc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018fe0, L0x20018fe0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018fe4, L0x20018fe4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018fe8, L0x20018fe8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018fec, L0x20018fec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018ff0, L0x20018ff0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018ff4, L0x20018ff4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018ff8, L0x20018ff8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018ffc, L0x20018ffc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019000, L0x20019000 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019004, L0x20019004 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019008, L0x20019008 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001900c, L0x2001900c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019010, L0x20019010 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019014, L0x20019014 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019018, L0x20019018 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001901c, L0x2001901c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019020, L0x20019020 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019024, L0x20019024 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019028, L0x20019028 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001902c, L0x2001902c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019030, L0x20019030 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019034, L0x20019034 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019038, L0x20019038 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001903c, L0x2001903c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019040, L0x20019040 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019044, L0x20019044 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019048, L0x20019048 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001904c, L0x2001904c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019050, L0x20019050 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019054, L0x20019054 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019058, L0x20019058 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001905c, L0x2001905c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019060, L0x20019060 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019064, L0x20019064 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019068, L0x20019068 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001906c, L0x2001906c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019070, L0x20019070 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019074, L0x20019074 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019078, L0x20019078 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001907c, L0x2001907c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019080, L0x20019080 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019084, L0x20019084 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019088, L0x20019088 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001908c, L0x2001908c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019090, L0x20019090 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019094, L0x20019094 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019098, L0x20019098 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001909c, L0x2001909c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190a0, L0x200190a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190a4, L0x200190a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190a8, L0x200190a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190ac, L0x200190ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190b0, L0x200190b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190b4, L0x200190b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190b8, L0x200190b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190bc, L0x200190bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190c0, L0x200190c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190c4, L0x200190c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190c8, L0x200190c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190cc, L0x200190cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190d0, L0x200190d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190d4, L0x200190d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190d8, L0x200190d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190dc, L0x200190dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190e0, L0x200190e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190e4, L0x200190e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190e8, L0x200190e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190ec, L0x200190ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190f0, L0x200190f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190f4, L0x200190f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190f8, L0x200190f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190fc, L0x200190fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019100, L0x20019100 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019104, L0x20019104 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019108, L0x20019108 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001910c, L0x2001910c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019110, L0x20019110 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019114, L0x20019114 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019118, L0x20019118 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001911c, L0x2001911c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019120, L0x20019120 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019124, L0x20019124 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019128, L0x20019128 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001912c, L0x2001912c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019130, L0x20019130 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019134, L0x20019134 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019138, L0x20019138 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001913c, L0x2001913c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019140, L0x20019140 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019144, L0x20019144 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019148, L0x20019148 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001914c, L0x2001914c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019150, L0x20019150 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019154, L0x20019154 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019158, L0x20019158 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001915c, L0x2001915c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019160, L0x20019160 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019164, L0x20019164 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019168, L0x20019168 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001916c, L0x2001916c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019170, L0x20019170 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019174, L0x20019174 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019178, L0x20019178 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001917c, L0x2001917c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019180, L0x20019180 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019184, L0x20019184 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019188, L0x20019188 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001918c, L0x2001918c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019190, L0x20019190 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019194, L0x20019194 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019198, L0x20019198 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001919c, L0x2001919c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191a0, L0x200191a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191a4, L0x200191a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191a8, L0x200191a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191ac, L0x200191ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191b0, L0x200191b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191b4, L0x200191b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191b8, L0x200191b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191bc, L0x200191bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191c0, L0x200191c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191c4, L0x200191c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191c8, L0x200191c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191cc, L0x200191cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191d0, L0x200191d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191d4, L0x200191d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191d8, L0x200191d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191dc, L0x200191dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191e0, L0x200191e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191e4, L0x200191e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191e8, L0x200191e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191ec, L0x200191ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191f0, L0x200191f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191f4, L0x200191f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191f8, L0x200191f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191fc, L0x200191fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019200, L0x20019200 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019204, L0x20019204 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019208, L0x20019208 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001920c, L0x2001920c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019210, L0x20019210 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019214, L0x20019214 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019218, L0x20019218 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001921c, L0x2001921c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019220, L0x20019220 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019224, L0x20019224 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019228, L0x20019228 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001922c, L0x2001922c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019230, L0x20019230 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019234, L0x20019234 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019238, L0x20019238 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001923c, L0x2001923c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019240, L0x20019240 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019244, L0x20019244 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019248, L0x20019248 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001924c, L0x2001924c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019250, L0x20019250 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019254, L0x20019254 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019258, L0x20019258 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001925c, L0x2001925c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019260, L0x20019260 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019264, L0x20019264 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019268, L0x20019268 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001926c, L0x2001926c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019270, L0x20019270 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019274, L0x20019274 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019278, L0x20019278 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001927c, L0x2001927c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019280, L0x20019280 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019284, L0x20019284 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019288, L0x20019288 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001928c, L0x2001928c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019290, L0x20019290 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019294, L0x20019294 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019298, L0x20019298 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001929c, L0x2001929c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192a0, L0x200192a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192a4, L0x200192a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192a8, L0x200192a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192ac, L0x200192ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192b0, L0x200192b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192b4, L0x200192b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192b8, L0x200192b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192bc, L0x200192bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192c0, L0x200192c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192c4, L0x200192c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192c8, L0x200192c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192cc, L0x200192cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192d0, L0x200192d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192d4, L0x200192d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192d8, L0x200192d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192dc, L0x200192dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192e0, L0x200192e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192e4, L0x200192e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192e8, L0x200192e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192ec, L0x200192ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192f0, L0x200192f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192f4, L0x200192f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192f8, L0x200192f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192fc, L0x200192fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019300, L0x20019300 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019304, L0x20019304 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019308, L0x20019308 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001930c, L0x2001930c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019310, L0x20019310 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019314, L0x20019314 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019318, L0x20019318 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001931c, L0x2001931c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019320, L0x20019320 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019324, L0x20019324 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019328, L0x20019328 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001932c, L0x2001932c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019330, L0x20019330 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019334, L0x20019334 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019338, L0x20019338 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001933c, L0x2001933c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019340, L0x20019340 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019344, L0x20019344 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019348, L0x20019348 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001934c, L0x2001934c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019350, L0x20019350 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019354, L0x20019354 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019358, L0x20019358 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001935c, L0x2001935c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019360, L0x20019360 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019364, L0x20019364 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019368, L0x20019368 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001936c, L0x2001936c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019370, L0x20019370 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019374, L0x20019374 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019378, L0x20019378 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001937c, L0x2001937c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019380, L0x20019380 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019384, L0x20019384 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019388, L0x20019388 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001938c, L0x2001938c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019390, L0x20019390 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019394, L0x20019394 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019398, L0x20019398 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001939c, L0x2001939c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193a0, L0x200193a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193a4, L0x200193a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193a8, L0x200193a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193ac, L0x200193ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193b0, L0x200193b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193b4, L0x200193b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193b8, L0x200193b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193bc, L0x200193bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193c0, L0x200193c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193c4, L0x200193c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193c8, L0x200193c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193cc, L0x200193cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193d0, L0x200193d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193d4, L0x200193d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193d8, L0x200193d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193dc, L0x200193dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193e0, L0x200193e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193e4, L0x200193e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193e8, L0x200193e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193ec, L0x200193ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193f0, L0x200193f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193f4, L0x200193f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193f8, L0x200193f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193fc, L0x200193fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019400, L0x20019400 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019404, L0x20019404 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019408, L0x20019408 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001940c, L0x2001940c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019410, L0x20019410 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019414, L0x20019414 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019418, L0x20019418 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001941c, L0x2001941c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019420, L0x20019420 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019424, L0x20019424 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019428, L0x20019428 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001942c, L0x2001942c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019430, L0x20019430 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019434, L0x20019434 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019438, L0x20019438 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001943c, L0x2001943c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019440, L0x20019440 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019444, L0x20019444 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019448, L0x20019448 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001944c, L0x2001944c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019450, L0x20019450 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019454, L0x20019454 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019458, L0x20019458 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001945c, L0x2001945c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019460, L0x20019460 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019464, L0x20019464 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019468, L0x20019468 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001946c, L0x2001946c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019470, L0x20019470 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019474, L0x20019474 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019478, L0x20019478 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001947c, L0x2001947c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019480, L0x20019480 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019484, L0x20019484 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019488, L0x20019488 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001948c, L0x2001948c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019490, L0x20019490 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019494, L0x20019494 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019498, L0x20019498 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001949c, L0x2001949c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194a0, L0x200194a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194a4, L0x200194a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194a8, L0x200194a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194ac, L0x200194ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194b0, L0x200194b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194b4, L0x200194b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194b8, L0x200194b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194bc, L0x200194bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194c0, L0x200194c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194c4, L0x200194c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194c8, L0x200194c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194cc, L0x200194cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194d0, L0x200194d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194d4, L0x200194d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194d8, L0x200194d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194dc, L0x200194dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194e0, L0x200194e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194e4, L0x200194e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194e8, L0x200194e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194ec, L0x200194ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194f0, L0x200194f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194f4, L0x200194f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194f8, L0x200194f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194fc, L0x200194fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019500, L0x20019500 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019504, L0x20019504 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019508, L0x20019508 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001950c, L0x2001950c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019510, L0x20019510 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019514, L0x20019514 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019518, L0x20019518 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001951c, L0x2001951c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019520, L0x20019520 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019524, L0x20019524 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019528, L0x20019528 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001952c, L0x2001952c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019530, L0x20019530 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019534, L0x20019534 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019538, L0x20019538 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001953c, L0x2001953c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019540, L0x20019540 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019544, L0x20019544 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019548, L0x20019548 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001954c, L0x2001954c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019550, L0x20019550 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019554, L0x20019554 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019558, L0x20019558 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001955c, L0x2001955c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019560, L0x20019560 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019564, L0x20019564 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019568, L0x20019568 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001956c, L0x2001956c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019570, L0x20019570 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019574, L0x20019574 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019578, L0x20019578 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001957c, L0x2001957c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019580, L0x20019580 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019584, L0x20019584 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019588, L0x20019588 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001958c, L0x2001958c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019590, L0x20019590 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019594, L0x20019594 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019598, L0x20019598 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001959c, L0x2001959c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195a0, L0x200195a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195a4, L0x200195a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195a8, L0x200195a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195ac, L0x200195ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195b0, L0x200195b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195b4, L0x200195b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195b8, L0x200195b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195bc, L0x200195bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195c0, L0x200195c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195c4, L0x200195c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195c8, L0x200195c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195cc, L0x200195cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195d0, L0x200195d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195d4, L0x200195d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195d8, L0x200195d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195dc, L0x200195dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195e0, L0x200195e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195e4, L0x200195e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195e8, L0x200195e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195ec, L0x200195ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195f0, L0x200195f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195f4, L0x200195f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195f8, L0x200195f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195fc, L0x200195fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019600, L0x20019600 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019604, L0x20019604 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019608, L0x20019608 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001960c, L0x2001960c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019610, L0x20019610 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019614, L0x20019614 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019618, L0x20019618 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001961c, L0x2001961c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019620, L0x20019620 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019624, L0x20019624 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019628, L0x20019628 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001962c, L0x2001962c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019630, L0x20019630 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019634, L0x20019634 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019638, L0x20019638 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001963c, L0x2001963c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019640, L0x20019640 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019644, L0x20019644 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019648, L0x20019648 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001964c, L0x2001964c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019650, L0x20019650 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019654, L0x20019654 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019658, L0x20019658 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001965c, L0x2001965c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019660, L0x20019660 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019664, L0x20019664 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019668, L0x20019668 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001966c, L0x2001966c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019670, L0x20019670 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019674, L0x20019674 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019678, L0x20019678 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001967c, L0x2001967c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019680, L0x20019680 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019684, L0x20019684 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019688, L0x20019688 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001968c, L0x2001968c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019690, L0x20019690 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019694, L0x20019694 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019698, L0x20019698 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001969c, L0x2001969c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196a0, L0x200196a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196a4, L0x200196a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196a8, L0x200196a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196ac, L0x200196ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196b0, L0x200196b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196b4, L0x200196b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196b8, L0x200196b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196bc, L0x200196bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196c0, L0x200196c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196c4, L0x200196c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196c8, L0x200196c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196cc, L0x200196cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196d0, L0x200196d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196d4, L0x200196d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196d8, L0x200196d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196dc, L0x200196dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196e0, L0x200196e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196e4, L0x200196e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196e8, L0x200196e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196ec, L0x200196ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196f0, L0x200196f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196f4, L0x200196f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196f8, L0x200196f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196fc, L0x200196fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019700, L0x20019700 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019704, L0x20019704 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019708, L0x20019708 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001970c, L0x2001970c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019710, L0x20019710 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019714, L0x20019714 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019718, L0x20019718 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001971c, L0x2001971c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019720, L0x20019720 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019724, L0x20019724 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019728, L0x20019728 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001972c, L0x2001972c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019730, L0x20019730 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019734, L0x20019734 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019738, L0x20019738 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001973c, L0x2001973c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019740, L0x20019740 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019744, L0x20019744 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019748, L0x20019748 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001974c, L0x2001974c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019750, L0x20019750 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019754, L0x20019754 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019758, L0x20019758 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001975c, L0x2001975c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019760, L0x20019760 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019764, L0x20019764 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019768, L0x20019768 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001976c, L0x2001976c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019770, L0x20019770 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019774, L0x20019774 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019778, L0x20019778 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001977c, L0x2001977c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019780, L0x20019780 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019784, L0x20019784 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019788, L0x20019788 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001978c, L0x2001978c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019790, L0x20019790 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019794, L0x20019794 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019798, L0x20019798 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001979c, L0x2001979c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197a0, L0x200197a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197a4, L0x200197a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197a8, L0x200197a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197ac, L0x200197ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197b0, L0x200197b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197b4, L0x200197b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197b8, L0x200197b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197bc, L0x200197bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197c0, L0x200197c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197c4, L0x200197c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197c8, L0x200197c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197cc, L0x200197cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197d0, L0x200197d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197d4, L0x200197d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197d8, L0x200197d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197dc, L0x200197dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197e0, L0x200197e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197e4, L0x200197e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197e8, L0x200197e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197ec, L0x200197ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197f0, L0x200197f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197f4, L0x200197f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197f8, L0x200197f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197fc, L0x200197fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019800, L0x20019800 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019804, L0x20019804 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019808, L0x20019808 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001980c, L0x2001980c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019810, L0x20019810 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019814, L0x20019814 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019818, L0x20019818 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001981c, L0x2001981c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019820, L0x20019820 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019824, L0x20019824 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019828, L0x20019828 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001982c, L0x2001982c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019830, L0x20019830 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019834, L0x20019834 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019838, L0x20019838 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001983c, L0x2001983c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019840, L0x20019840 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019844, L0x20019844 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019848, L0x20019848 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001984c, L0x2001984c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019850, L0x20019850 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019854, L0x20019854 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019858, L0x20019858 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001985c, L0x2001985c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019860, L0x20019860 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019864, L0x20019864 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019868, L0x20019868 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001986c, L0x2001986c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019870, L0x20019870 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019874, L0x20019874 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019878, L0x20019878 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001987c, L0x2001987c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019880, L0x20019880 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019884, L0x20019884 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019888, L0x20019888 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001988c, L0x2001988c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019890, L0x20019890 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019894, L0x20019894 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019898, L0x20019898 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001989c, L0x2001989c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198a0, L0x200198a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198a4, L0x200198a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198a8, L0x200198a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198ac, L0x200198ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198b0, L0x200198b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198b4, L0x200198b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198b8, L0x200198b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198bc, L0x200198bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198c0, L0x200198c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198c4, L0x200198c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198c8, L0x200198c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198cc, L0x200198cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198d0, L0x200198d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198d4, L0x200198d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198d8, L0x200198d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198dc, L0x200198dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198e0, L0x200198e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198e4, L0x200198e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198e8, L0x200198e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198ec, L0x200198ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198f0, L0x200198f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198f4, L0x200198f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198f8, L0x200198f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198fc, L0x200198fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019900, L0x20019900 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019904, L0x20019904 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019908, L0x20019908 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001990c, L0x2001990c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019910, L0x20019910 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019914, L0x20019914 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019918, L0x20019918 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001991c, L0x2001991c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019920, L0x20019920 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019924, L0x20019924 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019928, L0x20019928 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001992c, L0x2001992c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019930, L0x20019930 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019934, L0x20019934 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019938, L0x20019938 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001993c, L0x2001993c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019940, L0x20019940 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019944, L0x20019944 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019948, L0x20019948 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001994c, L0x2001994c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019950, L0x20019950 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019954, L0x20019954 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019958, L0x20019958 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001995c, L0x2001995c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019960, L0x20019960 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019964, L0x20019964 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019968, L0x20019968 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001996c, L0x2001996c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019970, L0x20019970 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019974, L0x20019974 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019978, L0x20019978 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001997c, L0x2001997c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019980, L0x20019980 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019984, L0x20019984 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019988, L0x20019988 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001998c, L0x2001998c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019990, L0x20019990 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019994, L0x20019994 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019998, L0x20019998 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001999c, L0x2001999c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199a0, L0x200199a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199a4, L0x200199a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199a8, L0x200199a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199ac, L0x200199ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199b0, L0x200199b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199b4, L0x200199b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199b8, L0x200199b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199bc, L0x200199bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199c0, L0x200199c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199c4, L0x200199c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199c8, L0x200199c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199cc, L0x200199cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199d0, L0x200199d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199d4, L0x200199d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199d8, L0x200199d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199dc, L0x200199dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199e0, L0x200199e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199e4, L0x200199e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199e8, L0x200199e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199ec, L0x200199ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199f0, L0x200199f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199f4, L0x200199f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199f8, L0x200199f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199fc, L0x200199fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a00, L0x20019a00 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a04, L0x20019a04 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a08, L0x20019a08 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a0c, L0x20019a0c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a10, L0x20019a10 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a14, L0x20019a14 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a18, L0x20019a18 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a1c, L0x20019a1c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a20, L0x20019a20 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a24, L0x20019a24 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a28, L0x20019a28 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a2c, L0x20019a2c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a30, L0x20019a30 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a34, L0x20019a34 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a38, L0x20019a38 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a3c, L0x20019a3c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a40, L0x20019a40 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a44, L0x20019a44 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a48, L0x20019a48 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a4c, L0x20019a4c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a50, L0x20019a50 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a54, L0x20019a54 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a58, L0x20019a58 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a5c, L0x20019a5c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a60, L0x20019a60 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a64, L0x20019a64 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a68, L0x20019a68 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a6c, L0x20019a6c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a70, L0x20019a70 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a74, L0x20019a74 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a78, L0x20019a78 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a7c, L0x20019a7c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a80, L0x20019a80 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a84, L0x20019a84 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a88, L0x20019a88 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a8c, L0x20019a8c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a90, L0x20019a90 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a94, L0x20019a94 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a98, L0x20019a98 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a9c, L0x20019a9c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019aa0, L0x20019aa0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019aa4, L0x20019aa4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019aa8, L0x20019aa8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019aac, L0x20019aac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ab0, L0x20019ab0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ab4, L0x20019ab4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ab8, L0x20019ab8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019abc, L0x20019abc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ac0, L0x20019ac0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ac4, L0x20019ac4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ac8, L0x20019ac8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019acc, L0x20019acc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ad0, L0x20019ad0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ad4, L0x20019ad4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ad8, L0x20019ad8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019adc, L0x20019adc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ae0, L0x20019ae0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ae4, L0x20019ae4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ae8, L0x20019ae8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019aec, L0x20019aec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019af0, L0x20019af0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019af4, L0x20019af4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019af8, L0x20019af8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019afc, L0x20019afc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b00, L0x20019b00 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b04, L0x20019b04 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b08, L0x20019b08 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b0c, L0x20019b0c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b10, L0x20019b10 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b14, L0x20019b14 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b18, L0x20019b18 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b1c, L0x20019b1c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b20, L0x20019b20 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b24, L0x20019b24 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b28, L0x20019b28 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b2c, L0x20019b2c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b30, L0x20019b30 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b34, L0x20019b34 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b38, L0x20019b38 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b3c, L0x20019b3c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b40, L0x20019b40 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b44, L0x20019b44 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b48, L0x20019b48 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b4c, L0x20019b4c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b50, L0x20019b50 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b54, L0x20019b54 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b58, L0x20019b58 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b5c, L0x20019b5c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b60, L0x20019b60 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b64, L0x20019b64 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b68, L0x20019b68 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b6c, L0x20019b6c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b70, L0x20019b70 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b74, L0x20019b74 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b78, L0x20019b78 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b7c, L0x20019b7c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b80, L0x20019b80 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b84, L0x20019b84 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b88, L0x20019b88 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b8c, L0x20019b8c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b90, L0x20019b90 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b94, L0x20019b94 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b98, L0x20019b98 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b9c, L0x20019b9c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ba0, L0x20019ba0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ba4, L0x20019ba4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ba8, L0x20019ba8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bac, L0x20019bac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bb0, L0x20019bb0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bb4, L0x20019bb4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bb8, L0x20019bb8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bbc, L0x20019bbc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bc0, L0x20019bc0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bc4, L0x20019bc4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bc8, L0x20019bc8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bcc, L0x20019bcc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bd0, L0x20019bd0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bd4, L0x20019bd4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bd8, L0x20019bd8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bdc, L0x20019bdc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019be0, L0x20019be0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019be4, L0x20019be4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019be8, L0x20019be8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bec, L0x20019bec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bf0, L0x20019bf0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bf4, L0x20019bf4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bf8, L0x20019bf8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bfc, L0x20019bfc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c00, L0x20019c00 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c04, L0x20019c04 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c08, L0x20019c08 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c0c, L0x20019c0c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c10, L0x20019c10 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c14, L0x20019c14 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c18, L0x20019c18 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c1c, L0x20019c1c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c20, L0x20019c20 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c24, L0x20019c24 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c28, L0x20019c28 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c2c, L0x20019c2c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c30, L0x20019c30 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c34, L0x20019c34 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c38, L0x20019c38 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c3c, L0x20019c3c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c40, L0x20019c40 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c44, L0x20019c44 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c48, L0x20019c48 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c4c, L0x20019c4c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c50, L0x20019c50 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c54, L0x20019c54 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c58, L0x20019c58 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c5c, L0x20019c5c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c60, L0x20019c60 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c64, L0x20019c64 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c68, L0x20019c68 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c6c, L0x20019c6c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c70, L0x20019c70 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c74, L0x20019c74 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c78, L0x20019c78 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c7c, L0x20019c7c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c80, L0x20019c80 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c84, L0x20019c84 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c88, L0x20019c88 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c8c, L0x20019c8c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c90, L0x20019c90 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c94, L0x20019c94 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c98, L0x20019c98 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c9c, L0x20019c9c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ca0, L0x20019ca0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ca4, L0x20019ca4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ca8, L0x20019ca8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cac, L0x20019cac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cb0, L0x20019cb0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cb4, L0x20019cb4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cb8, L0x20019cb8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cbc, L0x20019cbc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cc0, L0x20019cc0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cc4, L0x20019cc4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cc8, L0x20019cc8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ccc, L0x20019ccc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cd0, L0x20019cd0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cd4, L0x20019cd4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cd8, L0x20019cd8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cdc, L0x20019cdc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ce0, L0x20019ce0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ce4, L0x20019ce4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ce8, L0x20019ce8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cec, L0x20019cec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cf0, L0x20019cf0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cf4, L0x20019cf4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cf8, L0x20019cf8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cfc, L0x20019cfc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d00, L0x20019d00 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d04, L0x20019d04 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d08, L0x20019d08 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d0c, L0x20019d0c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d10, L0x20019d10 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d14, L0x20019d14 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d18, L0x20019d18 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d1c, L0x20019d1c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d20, L0x20019d20 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d24, L0x20019d24 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d28, L0x20019d28 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d2c, L0x20019d2c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d30, L0x20019d30 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d34, L0x20019d34 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d38, L0x20019d38 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d3c, L0x20019d3c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d40, L0x20019d40 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d44, L0x20019d44 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d48, L0x20019d48 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d4c, L0x20019d4c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d50, L0x20019d50 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d54, L0x20019d54 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d58, L0x20019d58 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d5c, L0x20019d5c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d60, L0x20019d60 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d64, L0x20019d64 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d68, L0x20019d68 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d6c, L0x20019d6c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d70, L0x20019d70 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d74, L0x20019d74 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d78, L0x20019d78 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d7c, L0x20019d7c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d80, L0x20019d80 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d84, L0x20019d84 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d88, L0x20019d88 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d8c, L0x20019d8c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d90, L0x20019d90 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d94, L0x20019d94 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d98, L0x20019d98 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d9c, L0x20019d9c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019da0, L0x20019da0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019da4, L0x20019da4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019da8, L0x20019da8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dac, L0x20019dac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019db0, L0x20019db0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019db4, L0x20019db4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019db8, L0x20019db8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dbc, L0x20019dbc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dc0, L0x20019dc0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dc4, L0x20019dc4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dc8, L0x20019dc8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dcc, L0x20019dcc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dd0, L0x20019dd0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dd4, L0x20019dd4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dd8, L0x20019dd8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ddc, L0x20019ddc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019de0, L0x20019de0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019de4, L0x20019de4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019de8, L0x20019de8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dec, L0x20019dec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019df0, L0x20019df0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019df4, L0x20019df4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019df8, L0x20019df8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dfc, L0x20019dfc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e00, L0x20019e00 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e04, L0x20019e04 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e08, L0x20019e08 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e0c, L0x20019e0c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e10, L0x20019e10 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e14, L0x20019e14 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e18, L0x20019e18 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e1c, L0x20019e1c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e20, L0x20019e20 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e24, L0x20019e24 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e28, L0x20019e28 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e2c, L0x20019e2c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e30, L0x20019e30 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e34, L0x20019e34 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e38, L0x20019e38 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e3c, L0x20019e3c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e40, L0x20019e40 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e44, L0x20019e44 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e48, L0x20019e48 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e4c, L0x20019e4c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e50, L0x20019e50 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e54, L0x20019e54 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e58, L0x20019e58 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e5c, L0x20019e5c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e60, L0x20019e60 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e64, L0x20019e64 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e68, L0x20019e68 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e6c, L0x20019e6c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e70, L0x20019e70 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e74, L0x20019e74 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e78, L0x20019e78 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e7c, L0x20019e7c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e80, L0x20019e80 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e84, L0x20019e84 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e88, L0x20019e88 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e8c, L0x20019e8c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e90, L0x20019e90 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e94, L0x20019e94 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e98, L0x20019e98 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e9c, L0x20019e9c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ea0, L0x20019ea0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ea4, L0x20019ea4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ea8, L0x20019ea8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019eac, L0x20019eac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019eb0, L0x20019eb0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019eb4, L0x20019eb4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019eb8, L0x20019eb8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ebc, L0x20019ebc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ec0, L0x20019ec0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ec4, L0x20019ec4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ec8, L0x20019ec8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ecc, L0x20019ecc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ed0, L0x20019ed0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ed4, L0x20019ed4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ed8, L0x20019ed8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019edc, L0x20019edc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ee0, L0x20019ee0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ee4, L0x20019ee4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ee8, L0x20019ee8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019eec, L0x20019eec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ef0, L0x20019ef0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ef4, L0x20019ef4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ef8, L0x20019ef8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019efc, L0x20019efc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f00, L0x20019f00 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f04, L0x20019f04 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f08, L0x20019f08 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f0c, L0x20019f0c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f10, L0x20019f10 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f14, L0x20019f14 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f18, L0x20019f18 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f1c, L0x20019f1c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f20, L0x20019f20 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f24, L0x20019f24 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f28, L0x20019f28 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f2c, L0x20019f2c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f30, L0x20019f30 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f34, L0x20019f34 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f38, L0x20019f38 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f3c, L0x20019f3c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f40, L0x20019f40 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f44, L0x20019f44 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f48, L0x20019f48 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f4c, L0x20019f4c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f50, L0x20019f50 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f54, L0x20019f54 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f58, L0x20019f58 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f5c, L0x20019f5c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f60, L0x20019f60 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f64, L0x20019f64 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f68, L0x20019f68 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f6c, L0x20019f6c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f70, L0x20019f70 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f74, L0x20019f74 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f78, L0x20019f78 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f7c, L0x20019f7c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f80, L0x20019f80 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f84, L0x20019f84 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f88, L0x20019f88 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f8c, L0x20019f8c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f90, L0x20019f90 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f94, L0x20019f94 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f98, L0x20019f98 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f9c, L0x20019f9c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fa0, L0x20019fa0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fa4, L0x20019fa4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fa8, L0x20019fa8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fac, L0x20019fac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fb0, L0x20019fb0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fb4, L0x20019fb4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fb8, L0x20019fb8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fbc, L0x20019fbc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fc0, L0x20019fc0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fc4, L0x20019fc4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fc8, L0x20019fc8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fcc, L0x20019fcc <s 5@32*1043969@32
]
}

(**************** initialization ****************)

mov r2 1993076223@uint32; mov r3 1043969@sint32;



(**************** pointers ****************)

nondet lr@uint32; nondet r0@uint32; nondet r1@uint32;




(**************** constants ****************)

mov L0x8006ddc (  78830)@sint32; mov L0x8006de0 (  78830)@sint32;
mov L0x8006de4 ( 191052)@sint32; mov L0x8006de8 (  78830)@sint32;
mov L0x8006dec ( 191052)@sint32; mov L0x8006df0 (-311503)@sint32;
mov L0x8006df4 ( 207751)@sint32; mov L0x8006df8 (  78830)@sint32;
mov L0x8006dfc ( 191052)@sint32; mov L0x8006e00 (-311503)@sint32;
mov L0x8006e04 ( 207751)@sint32; mov L0x8006e08 ( 468028)@sint32;
mov L0x8006e0c (-149945)@sint32; mov L0x8006e10 ( 114478)@sint32;
mov L0x8006e14 ( -82425)@sint32; mov L0x8006e18 (  78830)@sint32;
mov L0x8006e1c (  78830)@sint32; mov L0x8006e20 ( 191052)@sint32;
mov L0x8006e24 (  78830)@sint32; mov L0x8006e28 ( 191052)@sint32;
mov L0x8006e2c (-311503)@sint32; mov L0x8006e30 ( 207751)@sint32;
mov L0x8006e34 (  78830)@sint32; mov L0x8006e38 ( 191052)@sint32;
mov L0x8006e3c (-311503)@sint32; mov L0x8006e40 ( 207751)@sint32;
mov L0x8006e44 ( 468028)@sint32; mov L0x8006e48 (-149945)@sint32;
mov L0x8006e4c ( 114478)@sint32; mov L0x8006e50 ( -82425)@sint32;
mov L0x8006e54 ( 191052)@sint32; mov L0x8006e58 (-311503)@sint32;
mov L0x8006e5c ( 207751)@sint32; mov L0x8006e60 ( 468028)@sint32;
mov L0x8006e64 (-149945)@sint32; mov L0x8006e68 ( 114478)@sint32;
mov L0x8006e6c ( -82425)@sint32; mov L0x8006e70 ( 254425)@sint32;
mov L0x8006e74 ( -83285)@sint32; mov L0x8006e78 (-205022)@sint32;
mov L0x8006e7c ( 318314)@sint32; mov L0x8006e80 ( 365552)@sint32;
mov L0x8006e84 (-403894)@sint32; mov L0x8006e88 ( 235060)@sint32;
mov L0x8006e8c ( 449706)@sint32; mov L0x8006e90 (-311503)@sint32;
mov L0x8006e94 ( 468028)@sint32; mov L0x8006e98 (-149945)@sint32;
mov L0x8006e9c ( 254425)@sint32; mov L0x8006ea0 ( -83285)@sint32;
mov L0x8006ea4 (-205022)@sint32; mov L0x8006ea8 ( 318314)@sint32;
mov L0x8006eac ( 378933)@sint32; mov L0x8006eb0 ( 313241)@sint32;
mov L0x8006eb4 (-397250)@sint32; mov L0x8006eb8 ( 288321)@sint32;
mov L0x8006ebc (  74768)@sint32; mov L0x8006ec0 (  22897)@sint32;
mov L0x8006ec4 ( 129870)@sint32; mov L0x8006ec8 (-461967)@sint32;
mov L0x8006ecc ( 207751)@sint32; mov L0x8006ed0 ( 114478)@sint32;
mov L0x8006ed4 ( -82425)@sint32; mov L0x8006ed8 ( 365552)@sint32;
mov L0x8006edc (-403894)@sint32; mov L0x8006ee0 ( 235060)@sint32;
mov L0x8006ee4 ( 449706)@sint32; mov L0x8006ee8 ( -35962)@sint32;
mov L0x8006eec ( 370478)@sint32; mov L0x8006ef0 ( 232373)@sint32;
mov L0x8006ef4 ( 159337)@sint32; mov L0x8006ef8 ( 405929)@sint32;
mov L0x8006efc (  59399)@sint32; mov L0x8006f00 ( -40385)@sint32;
mov L0x8006f04 ( 317168)@sint32; mov L0x8006f08 ( 468028)@sint32;
mov L0x8006f0c ( 254425)@sint32; mov L0x8006f10 ( -83285)@sint32;
mov L0x8006f14 ( 378933)@sint32; mov L0x8006f18 ( 313241)@sint32;
mov L0x8006f1c (-397250)@sint32; mov L0x8006f20 ( 288321)@sint32;
mov L0x8006f24 (  13867)@sint32; mov L0x8006f28 (  21742)@sint32;
mov L0x8006f2c ( 486841)@sint32; mov L0x8006f30 ( -73546)@sint32;
mov L0x8006f34 (   7056)@sint32; mov L0x8006f38 (-391031)@sint32;
mov L0x8006f3c (-493755)@sint32; mov L0x8006f40 ( -78001)@sint32;
mov L0x8006f44 (-149945)@sint32; mov L0x8006f48 (-205022)@sint32;
mov L0x8006f4c ( 318314)@sint32; mov L0x8006f50 (  74768)@sint32;
mov L0x8006f54 (  22897)@sint32; mov L0x8006f58 ( 129870)@sint32;
mov L0x8006f5c (-461967)@sint32; mov L0x8006f60 (-495107)@sint32;
mov L0x8006f64 ( 279814)@sint32; mov L0x8006f68 (-363890)@sint32;
mov L0x8006f6c (-182676)@sint32; mov L0x8006f70 (-207660)@sint32;
mov L0x8006f74 (  75978)@sint32; mov L0x8006f78 ( 187423)@sint32;
mov L0x8006f7c ( -78196)@sint32; mov L0x8006f80 ( 114478)@sint32;
mov L0x8006f84 ( 365552)@sint32; mov L0x8006f88 (-403894)@sint32;
mov L0x8006f8c ( -35962)@sint32; mov L0x8006f90 ( 370478)@sint32;
mov L0x8006f94 ( 232373)@sint32; mov L0x8006f98 ( 159337)@sint32;
mov L0x8006f9c ( 507011)@sint32; mov L0x8006fa0 ( 331715)@sint32;
mov L0x8006fa4 ( 297886)@sint32; mov L0x8006fa8 (-346620)@sint32;
mov L0x8006fac (-299045)@sint32; mov L0x8006fb0 ( 275767)@sint32;
mov L0x8006fb4 ( -51317)@sint32; mov L0x8006fb8 ( 402791)@sint32;
mov L0x8006fbc ( -82425)@sint32; mov L0x8006fc0 ( 235060)@sint32;
mov L0x8006fc4 ( 449706)@sint32; mov L0x8006fc8 ( 405929)@sint32;
mov L0x8006fcc (  59399)@sint32; mov L0x8006fd0 ( -40385)@sint32;
mov L0x8006fd4 ( 317168)@sint32; mov L0x8006fd8 (-272172)@sint32;
mov L0x8006fdc (-375619)@sint32; mov L0x8006fe0 ( 376740)@sint32;
mov L0x8006fe4 (-409013)@sint32; mov L0x8006fe8 ( -42578)@sint32;
mov L0x8006fec (-405086)@sint32; mov L0x8006ff0 (  81030)@sint32;
mov L0x8006ff4 (-422078)@sint32; mov L0x8006ff8 ( 254425)@sint32;
mov L0x8006ffc ( 378933)@sint32; mov L0x8007000 ( 313241)@sint32;
mov L0x8007004 (  13867)@sint32; mov L0x8007008 (  21742)@sint32;
mov L0x800700c ( 486841)@sint32; mov L0x8007010 ( -73546)@sint32;
mov L0x8007014 ( 487892)@sint32; mov L0x8007018 (-428144)@sint32;
mov L0x800701c ( -43370)@sint32; mov L0x8007020 (-393153)@sint32;
mov L0x8007024 ( 248256)@sint32; mov L0x8007028 (-228921)@sint32;
mov L0x800702c ( -29446)@sint32; mov L0x8007030 ( -59870)@sint32;
mov L0x8007034 ( -83285)@sint32; mov L0x8007038 (-397250)@sint32;
mov L0x800703c ( 288321)@sint32; mov L0x8007040 (   7056)@sint32;
mov L0x8007044 (-391031)@sint32; mov L0x8007048 (-493755)@sint32;
mov L0x800704c ( -78001)@sint32; mov L0x8007050 ( 285179)@sint32;
mov L0x8007054 ( 257414)@sint32; mov L0x8007058 (-147526)@sint32;
mov L0x800705c ( 390544)@sint32; mov L0x8007060 (-275430)@sint32;
mov L0x8007064 (-160445)@sint32; mov L0x8007068 (-436582)@sint32;
mov L0x800706c ( 316768)@sint32; mov L0x8007070 (-205022)@sint32;
mov L0x8007074 (  74768)@sint32; mov L0x8007078 (  22897)@sint32;
mov L0x800707c (-495107)@sint32; mov L0x8007080 ( 279814)@sint32;
mov L0x8007084 (-363890)@sint32; mov L0x8007088 (-182676)@sint32;
mov L0x800708c (  27120)@sint32; mov L0x8007090 (-345344)@sint32;
mov L0x8007094 (-470298)@sint32; mov L0x8007098 (-498651)@sint32;
mov L0x800709c (-358783)@sint32; mov L0x80070a0 ( 477219)@sint32;
mov L0x80070a4 ( 133279)@sint32; mov L0x80070a8 (-401288)@sint32;
mov L0x80070ac ( 318314)@sint32; mov L0x80070b0 ( 129870)@sint32;
mov L0x80070b4 (-461967)@sint32; mov L0x80070b8 (-207660)@sint32;
mov L0x80070bc (  75978)@sint32; mov L0x80070c0 ( 187423)@sint32;
mov L0x80070c4 ( -78196)@sint32; mov L0x80070c8 ( 288431)@sint32;
mov L0x80070cc (-155391)@sint32; mov L0x80070d0 ( -18223)@sint32;
mov L0x80070d4 (-478095)@sint32; mov L0x80070d8 (-347554)@sint32;
mov L0x80070dc ( 256625)@sint32; mov L0x80070e0 (-153141)@sint32;
mov L0x80070e4 (-298605)@sint32; mov L0x80070e8 ( 365552)@sint32;
mov L0x80070ec ( -35962)@sint32; mov L0x80070f0 ( 370478)@sint32;
mov L0x80070f4 ( 507011)@sint32; mov L0x80070f8 ( 331715)@sint32;
mov L0x80070fc ( 297886)@sint32; mov L0x8007100 (-346620)@sint32;
mov L0x8007104 (-367175)@sint32; mov L0x8007108 (-334857)@sint32;
mov L0x800710c (  42298)@sint32; mov L0x8007110 (-465942)@sint32;
mov L0x8007114 ( 329620)@sint32; mov L0x8007118 ( -89230)@sint32;
mov L0x800711c (-288348)@sint32; mov L0x8007120 ( 279364)@sint32;
mov L0x8007124 (-403894)@sint32; mov L0x8007128 ( 232373)@sint32;
mov L0x800712c ( 159337)@sint32; mov L0x8007130 (-299045)@sint32;
mov L0x8007134 ( 275767)@sint32; mov L0x8007138 ( -51317)@sint32;
mov L0x800713c ( 402791)@sint32; mov L0x8007140 ( 268720)@sint32;
mov L0x8007144 ( 464538)@sint32; mov L0x8007148 ( 356621)@sint32;
mov L0x800714c ( 343605)@sint32; mov L0x8007150 ( 476116)@sint32;
mov L0x8007154 (  44548)@sint32; mov L0x8007158 ( 347463)@sint32;
mov L0x800715c ( 399863)@sint32; mov L0x8007160 ( 235060)@sint32;
mov L0x8007164 ( 405929)@sint32; mov L0x8007168 (  59399)@sint32;
mov L0x800716c (-272172)@sint32; mov L0x8007170 (-375619)@sint32;
mov L0x8007174 ( 376740)@sint32; mov L0x8007178 (-409013)@sint32;
mov L0x800717c (-188449)@sint32; mov L0x8007180 (-309697)@sint32;
mov L0x8007184 (-118699)@sint32; mov L0x8007188 ( 418878)@sint32;
mov L0x800718c (-284025)@sint32; mov L0x8007190 ( 170731)@sint32;
mov L0x8007194 ( 515076)@sint32; mov L0x8007198 ( 290607)@sint32;
mov L0x800719c ( 449706)@sint32; mov L0x80071a0 ( -40385)@sint32;
mov L0x80071a4 ( 317168)@sint32; mov L0x80071a8 ( -42578)@sint32;
mov L0x80071ac (-405086)@sint32; mov L0x80071b0 (  81030)@sint32;
mov L0x80071b4 (-422078)@sint32; mov L0x80071b8 ( 445216)@sint32;
mov L0x80071bc ( 114673)@sint32; mov L0x80071c0 (   1019)@sint32;
mov L0x80071c4 (-364661)@sint32; mov L0x80071c8 (-179242)@sint32;
mov L0x80071cc (-317922)@sint32; mov L0x80071d0 (-202373)@sint32;
mov L0x80071d4 ( 400989)@sint32;

(* #! -> SP = 0x20016fb0 *)
#! 0x20016fb0 = 0x20016fb0;
(* #stmdb	sp!, {r4, r5, r6, r7, r8, r9, r10, r11, r12, lr}#! EA = L0x20016fb0; PC = 0x8001430 *)
#stmdb	sp!, {%%r4, %%r5, %%r6, %%r7, %%r8, %%r9, %%r10, %%r11, %%r12, %%lr}#! L0x20016fb0 = L0x20016fb0; 0x8001430 = 0x8001430;
(* #vpush	{s16-s31}                                 #! PC = 0x8001434 *)
#vpush	{%%s16-%%s31}                                 #! 0x8001434 = 0x8001434;
(* #ldr.w	lr, [sp, #104]	; 0x68                     #! EA = L0x20016fb0; Value = 0x20016fd0; PC = 0x8001438 *)
#ldr.w	%%lr, [sp, #104]	; 0x68                     #! L0x20016fb0 = L0x20016fb0; 0x20016fd0 = 0x20016fd0; 0x8001438 = 0x8001438;
(* #add.w	r1, r1, #60	; 0x3c                        #! PC = 0x800143c *)
#add.w	%%r1, %%r1, #60	; 0x3c                        #! 0x800143c = 0x800143c;
(* vldmia	r1!, {s4-s18}                            #! EA = L0x8006e18; PC = 0x8001440 *)
mov s4 L0x8006e18;
mov s5 L0x8006e1c;
mov s6 L0x8006e20;
mov s7 L0x8006e24;
mov s8 L0x8006e28;
mov s9 L0x8006e2c;
mov s10 L0x8006e30;
mov s11 L0x8006e34;
mov s12 L0x8006e38;
mov s13 L0x8006e3c;
mov s14 L0x8006e40;
mov s15 L0x8006e44;
mov s16 L0x8006e48;
mov s17 L0x8006e4c;
mov s18 L0x8006e50;
(* vmov	s1, r1                                     #! PC = 0x8001444 *)
mov s1 r1;
(* add.w	r12, r0, #16                              #! PC = 0x8001448 *)
adds dontcare r12 r0 16@uint32;
(* vmov	s3, r12                                    #! PC = 0x800144c *)
mov s3 r12;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20018fe0; Value = 0xffffffae; PC = 0x8001450 *)
mov r4 L0x20018fe0;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019000; Value = 0x000007ff; PC = 0x8001454 *)
mov r5 L0x20019000;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019020; Value = 0x000008e7; PC = 0x8001458 *)
mov r6 L0x20019020;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019040; Value = 0xffffffc6; PC = 0x800145c *)
mov r7 L0x20019040;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019060; Value = 0x00000545; PC = 0x8001460 *)
mov r8 L0x20019060;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019080; Value = 0xfffffacf; PC = 0x8001464 *)
mov r9 L0x20019080;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200190a0; Value = 0xfffffeeb; PC = 0x8001468 *)
mov r10 L0x200190a0;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200190c0; Value = 0xfffff5a9; PC = 0x800146c *)
mov r11 L0x200190c0;
(* add	r4, r8                                      #! PC = 0x8001470 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001472 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001474 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001476 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001478 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800147c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001480 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001484 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s6                                     #! PC = 0x8001488 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800148c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001490 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001494 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001498 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800149c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80014a0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80014a4 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80014a6 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x80014a8 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x80014aa *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80014ac *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80014b0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80014b4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80014b8 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x80014bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80014c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80014c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80014c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80014cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80014d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80014d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80014d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80014dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80014e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80014e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80014e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80014ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80014ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80014ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80014f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80014f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80014f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80014fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s12                                    #! PC = 0x8001500 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001504 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001508 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800150c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001510 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001514 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001518 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800151c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001520 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001524 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001528 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800152c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001530 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001534 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001538 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800153c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001540 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001544 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001548 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800154c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001550 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001554 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001558 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800155c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001560 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001564 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001568 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800156c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001570 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001574 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001578 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800157c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20018fd0; Value = 0x00000160; PC = 0x8001580 *)
mov r4 L0x20018fd0;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20018ff0; Value = 0xfffff847; PC = 0x8001584 *)
mov r5 L0x20018ff0;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019010; Value = 0xfffffd21; PC = 0x8001588 *)
mov r6 L0x20019010;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019030; Value = 0xfffffe70; PC = 0x800158c *)
mov r7 L0x20019030;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019050; Value = 0xfffffe12; PC = 0x8001590 *)
mov r8 L0x20019050;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019070; Value = 0x000004c1; PC = 0x8001594 *)
mov r9 L0x20019070;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019090; Value = 0xfffffe3d; PC = 0x8001598 *)
mov r10 L0x20019090;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200190b0; Value = 0x0000001e; PC = 0x800159c *)
mov r11 L0x200190b0;
(* add	r4, r8                                      #! PC = 0x80015a0 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80015a2 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80015a4 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80015a6 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80015a8 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80015ac *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80015b0 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80015b4 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s6                                     #! PC = 0x80015b8 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x80015bc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80015c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80015c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80015c8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80015cc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80015d0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80015d4 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80015d6 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x80015d8 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x80015da *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80015dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80015e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80015e4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80015e8 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x80015ec *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80015f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80015f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80015f8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80015fc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001600 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001604 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x8001608 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x800160c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001610 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001614 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8001618 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800161a *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x800161c *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x800161e *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001620 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001624 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8001628 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x800162c *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8001630 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8001634 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8001638 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x800163c *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8001640 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8001642 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001644 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001646 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001648 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x800164c *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001650 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001654 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019050; PC = 0x8001658 *)
mov L0x20019050 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019060; PC = 0x800165c *)
mov L0x20019060 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019070; PC = 0x8001660 *)
mov L0x20019070 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019080; PC = 0x8001664 *)
mov L0x20019080 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019090; PC = 0x8001668 *)
mov L0x20019090 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200190a0; PC = 0x800166c *)
mov L0x200190a0 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200190b0; PC = 0x8001670 *)
mov L0x200190b0 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200190c0; PC = 0x8001674 *)
mov L0x200190c0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8001678 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x800167c *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8001680 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8001684 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8001688 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x800168a *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x800168c *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x800168e *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001690 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001694 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001698 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x800169c *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019010; PC = 0x80016a0 *)
mov L0x20019010 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019020; PC = 0x80016a4 *)
mov L0x20019020 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019030; PC = 0x80016a8 *)
mov L0x20019030 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019040; PC = 0x80016ac *)
mov L0x20019040 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20018fe0; PC = 0x80016b0 *)
mov L0x20018fe0 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20018ff0; PC = 0x80016b4 *)
mov L0x20018ff0 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019000; PC = 0x80016b8 *)
mov L0x20019000 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018fd0; PC = 0x80016bc *)
mov L0x20018fd0 r8;
(* vmov	r12, s3                                    #! PC = 0x80016c0 *)
mov r12 s3;
(* cmp.w	r0, r12                                   #! PC = 0x80016c4 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8001450 <_4_5_6_7_light>               #! PC = 0x80016c8 *)
#bne.w	0x8001450 <_4_5_6_7_light>               #! 0x80016c8 = 0x80016c8;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20018fe4; Value = 0xfffffa8e; PC = 0x8001450 *)
mov r4 L0x20018fe4;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019004; Value = 0xfffff7c9; PC = 0x8001454 *)
mov r5 L0x20019004;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019024; Value = 0xfffffd7d; PC = 0x8001458 *)
mov r6 L0x20019024;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019044; Value = 0x000003d7; PC = 0x800145c *)
mov r7 L0x20019044;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019064; Value = 0x000002ba; PC = 0x8001460 *)
mov r8 L0x20019064;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019084; Value = 0x00000b35; PC = 0x8001464 *)
mov r9 L0x20019084;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200190a4; Value = 0x00000360; PC = 0x8001468 *)
mov r10 L0x200190a4;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200190c4; Value = 0xfffffe15; PC = 0x800146c *)
mov r11 L0x200190c4;
(* add	r4, r8                                      #! PC = 0x8001470 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001472 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001474 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001476 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001478 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800147c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001480 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001484 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s6                                     #! PC = 0x8001488 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800148c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001490 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001494 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001498 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800149c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80014a0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80014a4 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80014a6 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x80014a8 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x80014aa *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80014ac *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80014b0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80014b4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80014b8 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x80014bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80014c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80014c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80014c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80014cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80014d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80014d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80014d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80014dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80014e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80014e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80014e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80014ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80014ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80014ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80014f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80014f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80014f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80014fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s12                                    #! PC = 0x8001500 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001504 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001508 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800150c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001510 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001514 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001518 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800151c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001520 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001524 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001528 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800152c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001530 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001534 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001538 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800153c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001540 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001544 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001548 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800154c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001550 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001554 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001558 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800155c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001560 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001564 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001568 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800156c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001570 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001574 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001578 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800157c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20018fd4; Value = 0xfffffc0c; PC = 0x8001580 *)
mov r4 L0x20018fd4;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20018ff4; Value = 0x00000d28; PC = 0x8001584 *)
mov r5 L0x20018ff4;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019014; Value = 0x00000751; PC = 0x8001588 *)
mov r6 L0x20019014;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019034; Value = 0x00000a83; PC = 0x800158c *)
mov r7 L0x20019034;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019054; Value = 0xfffffefb; PC = 0x8001590 *)
mov r8 L0x20019054;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019074; Value = 0x0000064e; PC = 0x8001594 *)
mov r9 L0x20019074;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019094; Value = 0x0000093c; PC = 0x8001598 *)
mov r10 L0x20019094;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200190b4; Value = 0x000004cb; PC = 0x800159c *)
mov r11 L0x200190b4;
(* add	r4, r8                                      #! PC = 0x80015a0 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80015a2 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80015a4 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80015a6 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80015a8 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80015ac *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80015b0 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80015b4 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s6                                     #! PC = 0x80015b8 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x80015bc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80015c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80015c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80015c8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80015cc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80015d0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80015d4 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80015d6 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x80015d8 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x80015da *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80015dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80015e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80015e4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80015e8 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x80015ec *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80015f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80015f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80015f8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80015fc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001600 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001604 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x8001608 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x800160c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001610 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001614 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8001618 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800161a *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x800161c *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x800161e *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001620 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001624 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8001628 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x800162c *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8001630 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8001634 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8001638 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x800163c *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8001640 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8001642 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001644 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001646 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001648 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x800164c *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001650 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001654 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019054; PC = 0x8001658 *)
mov L0x20019054 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019064; PC = 0x800165c *)
mov L0x20019064 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019074; PC = 0x8001660 *)
mov L0x20019074 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019084; PC = 0x8001664 *)
mov L0x20019084 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019094; PC = 0x8001668 *)
mov L0x20019094 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200190a4; PC = 0x800166c *)
mov L0x200190a4 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200190b4; PC = 0x8001670 *)
mov L0x200190b4 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200190c4; PC = 0x8001674 *)
mov L0x200190c4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8001678 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x800167c *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8001680 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8001684 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8001688 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x800168a *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x800168c *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x800168e *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001690 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001694 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001698 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x800169c *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019014; PC = 0x80016a0 *)
mov L0x20019014 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019024; PC = 0x80016a4 *)
mov L0x20019024 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019034; PC = 0x80016a8 *)
mov L0x20019034 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019044; PC = 0x80016ac *)
mov L0x20019044 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20018fe4; PC = 0x80016b0 *)
mov L0x20018fe4 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20018ff4; PC = 0x80016b4 *)
mov L0x20018ff4 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019004; PC = 0x80016b8 *)
mov L0x20019004 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018fd4; PC = 0x80016bc *)
mov L0x20018fd4 r8;
(* vmov	r12, s3                                    #! PC = 0x80016c0 *)
mov r12 s3;
(* cmp.w	r0, r12                                   #! PC = 0x80016c4 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8001450 <_4_5_6_7_light>               #! PC = 0x80016c8 *)
#bne.w	0x8001450 <_4_5_6_7_light>               #! 0x80016c8 = 0x80016c8;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20018fe8; Value = 0xfffff9bc; PC = 0x8001450 *)
mov r4 L0x20018fe8;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019008; Value = 0xfffff6fb; PC = 0x8001454 *)
mov r5 L0x20019008;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019028; Value = 0xfffffeea; PC = 0x8001458 *)
mov r6 L0x20019028;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019048; Value = 0xfffffada; PC = 0x800145c *)
mov r7 L0x20019048;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019068; Value = 0x000007db; PC = 0x8001460 *)
mov r8 L0x20019068;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019088; Value = 0xfffffd1c; PC = 0x8001464 *)
mov r9 L0x20019088;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200190a8; Value = 0x00000007; PC = 0x8001468 *)
mov r10 L0x200190a8;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200190c8; Value = 0x0000022d; PC = 0x800146c *)
mov r11 L0x200190c8;
(* add	r4, r8                                      #! PC = 0x8001470 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001472 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001474 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001476 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001478 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800147c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001480 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001484 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s6                                     #! PC = 0x8001488 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800148c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001490 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001494 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001498 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800149c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80014a0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80014a4 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80014a6 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x80014a8 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x80014aa *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80014ac *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80014b0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80014b4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80014b8 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x80014bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80014c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80014c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80014c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80014cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80014d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80014d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80014d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80014dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80014e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80014e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80014e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80014ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80014ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80014ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80014f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80014f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80014f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80014fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s12                                    #! PC = 0x8001500 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001504 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001508 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800150c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001510 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001514 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001518 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800151c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001520 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001524 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001528 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800152c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001530 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001534 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001538 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800153c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001540 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001544 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001548 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800154c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001550 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001554 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001558 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800155c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001560 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001564 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001568 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800156c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001570 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001574 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001578 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800157c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20018fd8; Value = 0xffffffd1; PC = 0x8001580 *)
mov r4 L0x20018fd8;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20018ff8; Value = 0x0000022f; PC = 0x8001584 *)
mov r5 L0x20018ff8;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019018; Value = 0xfffffab6; PC = 0x8001588 *)
mov r6 L0x20019018;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019038; Value = 0xfffff412; PC = 0x800158c *)
mov r7 L0x20019038;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019058; Value = 0xfffffd44; PC = 0x8001590 *)
mov r8 L0x20019058;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019078; Value = 0x00000113; PC = 0x8001594 *)
mov r9 L0x20019078;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019098; Value = 0x00000d1e; PC = 0x8001598 *)
mov r10 L0x20019098;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200190b8; Value = 0xfffff618; PC = 0x800159c *)
mov r11 L0x200190b8;
(* add	r4, r8                                      #! PC = 0x80015a0 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80015a2 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80015a4 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80015a6 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80015a8 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80015ac *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80015b0 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80015b4 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s6                                     #! PC = 0x80015b8 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x80015bc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80015c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80015c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80015c8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80015cc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80015d0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80015d4 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80015d6 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x80015d8 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x80015da *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80015dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80015e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80015e4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80015e8 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x80015ec *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80015f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80015f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80015f8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80015fc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001600 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001604 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x8001608 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x800160c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001610 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001614 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8001618 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800161a *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x800161c *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x800161e *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001620 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001624 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8001628 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x800162c *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8001630 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8001634 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8001638 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x800163c *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8001640 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8001642 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001644 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001646 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001648 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x800164c *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001650 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001654 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019058; PC = 0x8001658 *)
mov L0x20019058 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019068; PC = 0x800165c *)
mov L0x20019068 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019078; PC = 0x8001660 *)
mov L0x20019078 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019088; PC = 0x8001664 *)
mov L0x20019088 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019098; PC = 0x8001668 *)
mov L0x20019098 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200190a8; PC = 0x800166c *)
mov L0x200190a8 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200190b8; PC = 0x8001670 *)
mov L0x200190b8 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200190c8; PC = 0x8001674 *)
mov L0x200190c8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8001678 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x800167c *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8001680 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8001684 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8001688 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x800168a *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x800168c *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x800168e *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001690 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001694 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001698 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x800169c *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019018; PC = 0x80016a0 *)
mov L0x20019018 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019028; PC = 0x80016a4 *)
mov L0x20019028 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019038; PC = 0x80016a8 *)
mov L0x20019038 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019048; PC = 0x80016ac *)
mov L0x20019048 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20018fe8; PC = 0x80016b0 *)
mov L0x20018fe8 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20018ff8; PC = 0x80016b4 *)
mov L0x20018ff8 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019008; PC = 0x80016b8 *)
mov L0x20019008 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018fd8; PC = 0x80016bc *)
mov L0x20018fd8 r8;
(* vmov	r12, s3                                    #! PC = 0x80016c0 *)
mov r12 s3;
(* cmp.w	r0, r12                                   #! PC = 0x80016c4 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8001450 <_4_5_6_7_light>               #! PC = 0x80016c8 *)
#bne.w	0x8001450 <_4_5_6_7_light>               #! 0x80016c8 = 0x80016c8;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20018fec; Value = 0xfffffcf0; PC = 0x8001450 *)
mov r4 L0x20018fec;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x2001900c; Value = 0xfffff43d; PC = 0x8001454 *)
mov r5 L0x2001900c;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x2001902c; Value = 0xfffffc3c; PC = 0x8001458 *)
mov r6 L0x2001902c;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x2001904c; Value = 0xfffff79d; PC = 0x800145c *)
mov r7 L0x2001904c;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x2001906c; Value = 0x00000268; PC = 0x8001460 *)
mov r8 L0x2001906c;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x2001908c; Value = 0xfffffcd0; PC = 0x8001464 *)
mov r9 L0x2001908c;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200190ac; Value = 0x00000332; PC = 0x8001468 *)
mov r10 L0x200190ac;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200190cc; Value = 0x000002a0; PC = 0x800146c *)
mov r11 L0x200190cc;
(* add	r4, r8                                      #! PC = 0x8001470 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001472 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001474 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001476 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001478 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800147c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001480 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001484 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s6                                     #! PC = 0x8001488 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800148c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001490 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001494 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001498 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800149c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80014a0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80014a4 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80014a6 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x80014a8 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x80014aa *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80014ac *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80014b0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80014b4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80014b8 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x80014bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80014c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80014c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80014c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80014cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80014d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80014d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80014d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80014dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80014e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80014e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80014e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80014ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80014ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80014ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80014f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80014f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80014f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80014fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s12                                    #! PC = 0x8001500 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001504 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001508 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800150c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001510 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001514 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001518 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800151c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001520 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001524 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001528 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800152c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001530 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001534 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001538 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800153c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001540 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001544 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001548 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800154c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001550 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001554 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001558 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800155c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001560 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001564 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001568 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800156c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001570 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001574 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001578 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800157c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20018fdc; Value = 0x0000036b; PC = 0x8001580 *)
mov r4 L0x20018fdc;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20018ffc; Value = 0xfffffef0; PC = 0x8001584 *)
mov r5 L0x20018ffc;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x2001901c; Value = 0xfffff4e4; PC = 0x8001588 *)
mov r6 L0x2001901c;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x2001903c; Value = 0x000006eb; PC = 0x800158c *)
mov r7 L0x2001903c;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x2001905c; Value = 0xfffff90c; PC = 0x8001590 *)
mov r8 L0x2001905c;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x2001907c; Value = 0xfffffe86; PC = 0x8001594 *)
mov r9 L0x2001907c;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x2001909c; Value = 0x000001c1; PC = 0x8001598 *)
mov r10 L0x2001909c;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200190bc; Value = 0xffffff33; PC = 0x800159c *)
mov r11 L0x200190bc;
(* add	r4, r8                                      #! PC = 0x80015a0 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80015a2 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80015a4 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80015a6 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80015a8 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80015ac *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80015b0 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80015b4 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s6                                     #! PC = 0x80015b8 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x80015bc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80015c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80015c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80015c8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80015cc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80015d0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x80015d4 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x80015d6 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x80015d8 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x80015da *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x80015dc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x80015e0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80015e4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80015e8 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x80015ec *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80015f0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80015f4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80015f8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80015fc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001600 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001604 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x8001608 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x800160c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001610 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001614 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8001618 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800161a *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x800161c *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x800161e *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001620 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001624 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8001628 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x800162c *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8001630 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8001634 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8001638 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x800163c *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8001640 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8001642 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001644 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001646 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001648 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x800164c *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001650 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001654 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x2001905c; PC = 0x8001658 *)
mov L0x2001905c r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x2001906c; PC = 0x800165c *)
mov L0x2001906c r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x2001907c; PC = 0x8001660 *)
mov L0x2001907c r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x2001908c; PC = 0x8001664 *)
mov L0x2001908c r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x2001909c; PC = 0x8001668 *)
mov L0x2001909c r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200190ac; PC = 0x800166c *)
mov L0x200190ac r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200190bc; PC = 0x8001670 *)
mov L0x200190bc r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200190cc; PC = 0x8001674 *)
mov L0x200190cc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8001678 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x800167c *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8001680 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8001684 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8001688 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x800168a *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x800168c *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x800168e *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001690 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001694 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001698 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x800169c *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x2001901c; PC = 0x80016a0 *)
mov L0x2001901c r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x2001902c; PC = 0x80016a4 *)
mov L0x2001902c r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x2001903c; PC = 0x80016a8 *)
mov L0x2001903c r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x2001904c; PC = 0x80016ac *)
mov L0x2001904c r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20018fec; PC = 0x80016b0 *)
mov L0x20018fec r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20018ffc; PC = 0x80016b4 *)
mov L0x20018ffc r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x2001900c; PC = 0x80016b8 *)
mov L0x2001900c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018fdc; PC = 0x80016bc *)
mov L0x20018fdc r8;
(* vmov	r12, s3                                    #! PC = 0x80016c0 *)
mov r12 s3;
(* cmp.w	r0, r12                                   #! PC = 0x80016c4 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8001450 <_4_5_6_7_light>               #! PC = 0x80016c8 *)
#bne.w	0x8001450 <_4_5_6_7_light>               #! 0x80016c8 = 0x80016c8;
(* add.w	r0, r0, #240	; 0xf0                       #! PC = 0x80016cc *)
adds dontcare r0 r0 240@uint32;
(* add.w	r12, r0, #3840	; 0xf00                    #! PC = 0x80016d0 *)
adds dontcare r12 r0 3840@uint32;
(* vmov	s2, r12                                    #! PC = 0x80016d4 *)
mov s2 r12;
(* vmov	r1, s1                                     #! PC = 0x80016d8 *)
mov r1 s1;
(* vldmia	r1!, {s4-s18}                            #! EA = L0x8006e54; PC = 0x80016dc *)
mov s4 L0x8006e54;
mov s5 L0x8006e58;
mov s6 L0x8006e5c;
mov s7 L0x8006e60;
mov s8 L0x8006e64;
mov s9 L0x8006e68;
mov s10 L0x8006e6c;
mov s11 L0x8006e70;
mov s12 L0x8006e74;
mov s13 L0x8006e78;
mov s14 L0x8006e7c;
mov s15 L0x8006e80;
mov s16 L0x8006e84;
mov s17 L0x8006e88;
mov s18 L0x8006e8c;
(* vmov	s1, r1                                     #! PC = 0x80016e0 *)
mov s1 r1;
(* add.w	lr, r0, #16                               #! PC = 0x80016e4 *)
adds dontcare lr r0 16@uint32;
(* vmov	s3, lr                                     #! PC = 0x80016e8 *)
mov s3 lr;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200190e0; Value = 0xfffffbd2; PC = 0x80016ec *)
mov r4 L0x200190e0;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019100; Value = 0x00000039; PC = 0x80016f0 *)
mov r5 L0x20019100;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019120; Value = 0x000000cb; PC = 0x80016f4 *)
mov r6 L0x20019120;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019140; Value = 0x000004d6; PC = 0x80016f8 *)
mov r7 L0x20019140;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019160; Value = 0x00000199; PC = 0x80016fc *)
mov r8 L0x20019160;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019180; Value = 0x000000df; PC = 0x8001700 *)
mov r9 L0x20019180;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200191a0; Value = 0x00000ef7; PC = 0x8001704 *)
mov r10 L0x200191a0;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200191c0; Value = 0xffffffcd; PC = 0x8001708 *)
mov r11 L0x200191c0;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200190d0; Value = 0xffffffda; PC = 0x8001890 *)
mov r4 L0x200190d0;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200190f0; Value = 0x000004d3; PC = 0x8001894 *)
mov r5 L0x200190f0;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019110; Value = 0x00000c31; PC = 0x8001898 *)
mov r6 L0x20019110;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019130; Value = 0xffffff80; PC = 0x800189c *)
mov r7 L0x20019130;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019150; Value = 0xfffffd10; PC = 0x80018a0 *)
mov r8 L0x20019150;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019170; Value = 0x00000213; PC = 0x80018a4 *)
mov r9 L0x20019170;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019190; Value = 0xfffff7d1; PC = 0x80018a8 *)
mov r10 L0x20019190;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200191b0; Value = 0xfffffc42; PC = 0x80018ac *)
mov r11 L0x200191b0;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019150; PC = 0x80019cc *)
mov L0x20019150 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019160; PC = 0x80019d0 *)
mov L0x20019160 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019170; PC = 0x80019d4 *)
mov L0x20019170 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019180; PC = 0x80019d8 *)
mov L0x20019180 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019190; PC = 0x80019dc *)
mov L0x20019190 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200191a0; PC = 0x80019e0 *)
mov L0x200191a0 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200191b0; PC = 0x80019e4 *)
mov L0x200191b0 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200191c0; PC = 0x80019e8 *)
mov L0x200191c0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019110; PC = 0x8001a14 *)
mov L0x20019110 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019120; PC = 0x8001a18 *)
mov L0x20019120 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019130; PC = 0x8001a1c *)
mov L0x20019130 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019140; PC = 0x8001a20 *)
mov L0x20019140 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200190e0; PC = 0x8001a24 *)
mov L0x200190e0 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200190f0; PC = 0x8001a28 *)
mov L0x200190f0 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019100; PC = 0x8001a2c *)
mov L0x20019100 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190d0; PC = 0x8001a30 *)
mov L0x200190d0 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200190e4; Value = 0x000005f2; PC = 0x80016ec *)
mov r4 L0x200190e4;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019104; Value = 0xffffffd7; PC = 0x80016f0 *)
mov r5 L0x20019104;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019124; Value = 0xfffff593; PC = 0x80016f4 *)
mov r6 L0x20019124;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019144; Value = 0xfffff3e7; PC = 0x80016f8 *)
mov r7 L0x20019144;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019164; Value = 0xfffff8ac; PC = 0x80016fc *)
mov r8 L0x20019164;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019184; Value = 0xfffffcbb; PC = 0x8001700 *)
mov r9 L0x20019184;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200191a4; Value = 0xfffffc12; PC = 0x8001704 *)
mov r10 L0x200191a4;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200191c4; Value = 0x000003d7; PC = 0x8001708 *)
mov r11 L0x200191c4;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200190d4; Value = 0xffffffea; PC = 0x8001890 *)
mov r4 L0x200190d4;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200190f4; Value = 0xfffff9ca; PC = 0x8001894 *)
mov r5 L0x200190f4;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019114; Value = 0x00000dd3; PC = 0x8001898 *)
mov r6 L0x20019114;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019134; Value = 0x00000929; PC = 0x800189c *)
mov r7 L0x20019134;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019154; Value = 0xfffffced; PC = 0x80018a0 *)
mov r8 L0x20019154;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019174; Value = 0x0000000c; PC = 0x80018a4 *)
mov r9 L0x20019174;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019194; Value = 0x00000336; PC = 0x80018a8 *)
mov r10 L0x20019194;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200191b4; Value = 0xffffff27; PC = 0x80018ac *)
mov r11 L0x200191b4;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019154; PC = 0x80019cc *)
mov L0x20019154 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019164; PC = 0x80019d0 *)
mov L0x20019164 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019174; PC = 0x80019d4 *)
mov L0x20019174 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019184; PC = 0x80019d8 *)
mov L0x20019184 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019194; PC = 0x80019dc *)
mov L0x20019194 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200191a4; PC = 0x80019e0 *)
mov L0x200191a4 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200191b4; PC = 0x80019e4 *)
mov L0x200191b4 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200191c4; PC = 0x80019e8 *)
mov L0x200191c4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019114; PC = 0x8001a14 *)
mov L0x20019114 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019124; PC = 0x8001a18 *)
mov L0x20019124 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019134; PC = 0x8001a1c *)
mov L0x20019134 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019144; PC = 0x8001a20 *)
mov L0x20019144 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200190e4; PC = 0x8001a24 *)
mov L0x200190e4 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200190f4; PC = 0x8001a28 *)
mov L0x200190f4 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019104; PC = 0x8001a2c *)
mov L0x20019104 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190d4; PC = 0x8001a30 *)
mov L0x200190d4 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200190e8; Value = 0xfffffc2c; PC = 0x80016ec *)
mov r4 L0x200190e8;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019108; Value = 0xfffff9b3; PC = 0x80016f0 *)
mov r5 L0x20019108;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019128; Value = 0xfffff93e; PC = 0x80016f4 *)
mov r6 L0x20019128;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019148; Value = 0xfffffc94; PC = 0x80016f8 *)
mov r7 L0x20019148;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019168; Value = 0x000002f5; PC = 0x80016fc *)
mov r8 L0x20019168;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019188; Value = 0xfffffbd2; PC = 0x8001700 *)
mov r9 L0x20019188;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200191a8; Value = 0xfffff2b9; PC = 0x8001704 *)
mov r10 L0x200191a8;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200191c8; Value = 0x00000925; PC = 0x8001708 *)
mov r11 L0x200191c8;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200190d8; Value = 0x000001a1; PC = 0x8001890 *)
mov r4 L0x200190d8;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200190f8; Value = 0xfffffeab; PC = 0x8001894 *)
mov r5 L0x200190f8;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019118; Value = 0xffffff76; PC = 0x8001898 *)
mov r6 L0x20019118;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019138; Value = 0x000000a0; PC = 0x800189c *)
mov r7 L0x20019138;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019158; Value = 0xffffffdc; PC = 0x80018a0 *)
mov r8 L0x20019158;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019178; Value = 0x00000903; PC = 0x80018a4 *)
mov r9 L0x20019178;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019198; Value = 0x00000074; PC = 0x80018a8 *)
mov r10 L0x20019198;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200191b8; Value = 0x000002ae; PC = 0x80018ac *)
mov r11 L0x200191b8;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019158; PC = 0x80019cc *)
mov L0x20019158 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019168; PC = 0x80019d0 *)
mov L0x20019168 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019178; PC = 0x80019d4 *)
mov L0x20019178 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019188; PC = 0x80019d8 *)
mov L0x20019188 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019198; PC = 0x80019dc *)
mov L0x20019198 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200191a8; PC = 0x80019e0 *)
mov L0x200191a8 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200191b8; PC = 0x80019e4 *)
mov L0x200191b8 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200191c8; PC = 0x80019e8 *)
mov L0x200191c8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019118; PC = 0x8001a14 *)
mov L0x20019118 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019128; PC = 0x8001a18 *)
mov L0x20019128 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019138; PC = 0x8001a1c *)
mov L0x20019138 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019148; PC = 0x8001a20 *)
mov L0x20019148 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200190e8; PC = 0x8001a24 *)
mov L0x200190e8 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200190f8; PC = 0x8001a28 *)
mov L0x200190f8 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019108; PC = 0x8001a2c *)
mov L0x20019108 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190d8; PC = 0x8001a30 *)
mov L0x200190d8 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200190ec; Value = 0xfffff732; PC = 0x80016ec *)
mov r4 L0x200190ec;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x2001910c; Value = 0x00000565; PC = 0x80016f0 *)
mov r5 L0x2001910c;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x2001912c; Value = 0x000004e4; PC = 0x80016f4 *)
mov r6 L0x2001912c;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x2001914c; Value = 0x0000088b; PC = 0x80016f8 *)
mov r7 L0x2001914c;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x2001916c; Value = 0x00000382; PC = 0x80016fc *)
mov r8 L0x2001916c;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x2001918c; Value = 0x000009f6; PC = 0x8001700 *)
mov r9 L0x2001918c;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200191ac; Value = 0x00000202; PC = 0x8001704 *)
mov r10 L0x200191ac;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200191cc; Value = 0xfffff4b4; PC = 0x8001708 *)
mov r11 L0x200191cc;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200190dc; Value = 0xfffffb79; PC = 0x8001890 *)
mov r4 L0x200190dc;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200190fc; Value = 0x0000005e; PC = 0x8001894 *)
mov r5 L0x200190fc;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x2001911c; Value = 0xfffff724; PC = 0x8001898 *)
mov r6 L0x2001911c;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x2001913c; Value = 0xffffff57; PC = 0x800189c *)
mov r7 L0x2001913c;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x2001915c; Value = 0xfffff81a; PC = 0x80018a0 *)
mov r8 L0x2001915c;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x2001917c; Value = 0x000003b0; PC = 0x80018a4 *)
mov r9 L0x2001917c;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x2001919c; Value = 0xffffff49; PC = 0x80018a8 *)
mov r10 L0x2001919c;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200191bc; Value = 0x0000071b; PC = 0x80018ac *)
mov r11 L0x200191bc;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x2001915c; PC = 0x80019cc *)
mov L0x2001915c r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x2001916c; PC = 0x80019d0 *)
mov L0x2001916c r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x2001917c; PC = 0x80019d4 *)
mov L0x2001917c r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x2001918c; PC = 0x80019d8 *)
mov L0x2001918c r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x2001919c; PC = 0x80019dc *)
mov L0x2001919c r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200191ac; PC = 0x80019e0 *)
mov L0x200191ac r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200191bc; PC = 0x80019e4 *)
mov L0x200191bc r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200191cc; PC = 0x80019e8 *)
mov L0x200191cc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x2001911c; PC = 0x8001a14 *)
mov L0x2001911c r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x2001912c; PC = 0x8001a18 *)
mov L0x2001912c r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x2001913c; PC = 0x8001a1c *)
mov L0x2001913c r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x2001914c; PC = 0x8001a20 *)
mov L0x2001914c r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200190ec; PC = 0x8001a24 *)
mov L0x200190ec r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200190fc; PC = 0x8001a28 *)
mov L0x200190fc r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x2001910c; PC = 0x8001a2c *)
mov L0x2001910c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190dc; PC = 0x8001a30 *)
mov L0x200190dc r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* add.w	r0, r0, #240	; 0xf0                       #! PC = 0x8001a40 *)
adds dontcare r0 r0 240@uint32;
(* vmov	r12, s2                                    #! PC = 0x8001a44 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8001a48 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x80016d8 <_4_5_6_7>                     #! PC = 0x8001a4c *)
#bne.w	0x80016d8 <_4_5_6_7>                     #! 0x8001a4c = 0x8001a4c;
(* vmov	r1, s1                                     #! PC = 0x80016d8 *)
mov r1 s1;
(* vldmia	r1!, {s4-s18}                            #! EA = L0x8006e90; PC = 0x80016dc *)
mov s4 L0x8006e90;
mov s5 L0x8006e94;
mov s6 L0x8006e98;
mov s7 L0x8006e9c;
mov s8 L0x8006ea0;
mov s9 L0x8006ea4;
mov s10 L0x8006ea8;
mov s11 L0x8006eac;
mov s12 L0x8006eb0;
mov s13 L0x8006eb4;
mov s14 L0x8006eb8;
mov s15 L0x8006ebc;
mov s16 L0x8006ec0;
mov s17 L0x8006ec4;
mov s18 L0x8006ec8;
(* vmov	s1, r1                                     #! PC = 0x80016e0 *)
mov s1 r1;
(* add.w	lr, r0, #16                               #! PC = 0x80016e4 *)
adds dontcare lr r0 16@uint32;
(* vmov	s3, lr                                     #! PC = 0x80016e8 *)
mov s3 lr;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200191e0; Value = 0xfffb6902; PC = 0x80016ec *)
mov r4 L0x200191e0;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019200; Value = 0x0001f061; PC = 0x80016f0 *)
mov r5 L0x20019200;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019220; Value = 0xfffe5fd5; PC = 0x80016f4 *)
mov r6 L0x20019220;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019240; Value = 0xfffaf5de; PC = 0x80016f8 *)
mov r7 L0x20019240;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019260; Value = 0xfffd4cdc; PC = 0x80016fc *)
mov r8 L0x20019260;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019280; Value = 0x00078d6f; PC = 0x8001700 *)
mov r9 L0x20019280;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200192a0; Value = 0x00069999; PC = 0x8001704 *)
mov r10 L0x200192a0;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200192c0; Value = 0xfffe87a2; PC = 0x8001708 *)
mov r11 L0x200192c0;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200191d0; Value = 0xfff9db61; PC = 0x8001890 *)
mov r4 L0x200191d0;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200191f0; Value = 0xfff89260; PC = 0x8001894 *)
mov r5 L0x200191f0;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019210; Value = 0x000136f1; PC = 0x8001898 *)
mov r6 L0x20019210;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019230; Value = 0xfffedf5a; PC = 0x800189c *)
mov r7 L0x20019230;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019250; Value = 0x00039d3a; PC = 0x80018a0 *)
mov r8 L0x20019250;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019270; Value = 0x000269cd; PC = 0x80018a4 *)
mov r9 L0x20019270;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019290; Value = 0xfffc91cf; PC = 0x80018a8 *)
mov r10 L0x20019290;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200192b0; Value = 0xffff085e; PC = 0x80018ac *)
mov r11 L0x200192b0;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019250; PC = 0x80019cc *)
mov L0x20019250 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019260; PC = 0x80019d0 *)
mov L0x20019260 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019270; PC = 0x80019d4 *)
mov L0x20019270 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019280; PC = 0x80019d8 *)
mov L0x20019280 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019290; PC = 0x80019dc *)
mov L0x20019290 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200192a0; PC = 0x80019e0 *)
mov L0x200192a0 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200192b0; PC = 0x80019e4 *)
mov L0x200192b0 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200192c0; PC = 0x80019e8 *)
mov L0x200192c0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019210; PC = 0x8001a14 *)
mov L0x20019210 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019220; PC = 0x8001a18 *)
mov L0x20019220 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019230; PC = 0x8001a1c *)
mov L0x20019230 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019240; PC = 0x8001a20 *)
mov L0x20019240 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200191e0; PC = 0x8001a24 *)
mov L0x200191e0 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200191f0; PC = 0x8001a28 *)
mov L0x200191f0 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019200; PC = 0x8001a2c *)
mov L0x20019200 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200191d0; PC = 0x8001a30 *)
mov L0x200191d0 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200191e4; Value = 0x0004d5c1; PC = 0x80016ec *)
mov r4 L0x200191e4;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019204; Value = 0xfffffc88; PC = 0x80016f0 *)
mov r5 L0x20019204;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019224; Value = 0xfffc01c5; PC = 0x80016f4 *)
mov r6 L0x20019224;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019244; Value = 0x0000d5ae; PC = 0x80016f8 *)
mov r7 L0x20019244;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019264; Value = 0x0003de13; PC = 0x80016fc *)
mov r8 L0x20019264;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019284; Value = 0xfffbe912; PC = 0x8001700 *)
mov r9 L0x20019284;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200192a4; Value = 0x0002190e; PC = 0x8001704 *)
mov r10 L0x200192a4;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200192c4; Value = 0xfff8583a; PC = 0x8001708 *)
mov r11 L0x200192c4;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200191d4; Value = 0x00032593; PC = 0x8001890 *)
mov r4 L0x200191d4;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200191f4; Value = 0x00020d72; PC = 0x8001894 *)
mov r5 L0x200191f4;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019214; Value = 0x0005fafe; PC = 0x8001898 *)
mov r6 L0x20019214;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019234; Value = 0x00059a8c; PC = 0x800189c *)
mov r7 L0x20019234;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019254; Value = 0x00023037; PC = 0x80018a0 *)
mov r8 L0x20019254;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019274; Value = 0xffff762a; PC = 0x80018a4 *)
mov r9 L0x20019274;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019294; Value = 0xfff9873e; PC = 0x80018a8 *)
mov r10 L0x20019294;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200192b4; Value = 0x00014f98; PC = 0x80018ac *)
mov r11 L0x200192b4;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019254; PC = 0x80019cc *)
mov L0x20019254 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019264; PC = 0x80019d0 *)
mov L0x20019264 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019274; PC = 0x80019d4 *)
mov L0x20019274 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019284; PC = 0x80019d8 *)
mov L0x20019284 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019294; PC = 0x80019dc *)
mov L0x20019294 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200192a4; PC = 0x80019e0 *)
mov L0x200192a4 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200192b4; PC = 0x80019e4 *)
mov L0x200192b4 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200192c4; PC = 0x80019e8 *)
mov L0x200192c4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019214; PC = 0x8001a14 *)
mov L0x20019214 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019224; PC = 0x8001a18 *)
mov L0x20019224 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019234; PC = 0x8001a1c *)
mov L0x20019234 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019244; PC = 0x8001a20 *)
mov L0x20019244 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200191e4; PC = 0x8001a24 *)
mov L0x200191e4 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200191f4; PC = 0x8001a28 *)
mov L0x200191f4 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019204; PC = 0x8001a2c *)
mov L0x20019204 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200191d4; PC = 0x8001a30 *)
mov L0x200191d4 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200191e8; Value = 0xfffed916; PC = 0x80016ec *)
mov r4 L0x200191e8;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019208; Value = 0x00048f44; PC = 0x80016f0 *)
mov r5 L0x20019208;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019228; Value = 0x0002415e; PC = 0x80016f4 *)
mov r6 L0x20019228;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019248; Value = 0xfff954dd; PC = 0x80016f8 *)
mov r7 L0x20019248;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019268; Value = 0x0005f5d6; PC = 0x80016fc *)
mov r8 L0x20019268;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019288; Value = 0x0000973c; PC = 0x8001700 *)
mov r9 L0x20019288;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200192a8; Value = 0xffffd2f1; PC = 0x8001704 *)
mov r10 L0x200192a8;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200192c8; Value = 0xfffab635; PC = 0x8001708 *)
mov r11 L0x200192c8;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200191d8; Value = 0x000292ea; PC = 0x8001890 *)
mov r4 L0x200191d8;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200191f8; Value = 0xfff8e0e5; PC = 0x8001894 *)
mov r5 L0x200191f8;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019218; Value = 0x0007e624; PC = 0x8001898 *)
mov r6 L0x20019218;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019238; Value = 0x0001d3a4; PC = 0x800189c *)
mov r7 L0x20019238;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019258; Value = 0xfffb158b; PC = 0x80018a0 *)
mov r8 L0x20019258;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019278; Value = 0x00020d9d; PC = 0x80018a4 *)
mov r9 L0x20019278;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019298; Value = 0x0000b84f; PC = 0x80018a8 *)
mov r10 L0x20019298;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200192b8; Value = 0x0007def9; PC = 0x80018ac *)
mov r11 L0x200192b8;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019258; PC = 0x80019cc *)
mov L0x20019258 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019268; PC = 0x80019d0 *)
mov L0x20019268 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019278; PC = 0x80019d4 *)
mov L0x20019278 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019288; PC = 0x80019d8 *)
mov L0x20019288 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019298; PC = 0x80019dc *)
mov L0x20019298 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200192a8; PC = 0x80019e0 *)
mov L0x200192a8 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200192b8; PC = 0x80019e4 *)
mov L0x200192b8 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200192c8; PC = 0x80019e8 *)
mov L0x200192c8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019218; PC = 0x8001a14 *)
mov L0x20019218 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019228; PC = 0x8001a18 *)
mov L0x20019228 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019238; PC = 0x8001a1c *)
mov L0x20019238 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019248; PC = 0x8001a20 *)
mov L0x20019248 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200191e8; PC = 0x8001a24 *)
mov L0x200191e8 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200191f8; PC = 0x8001a28 *)
mov L0x200191f8 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019208; PC = 0x8001a2c *)
mov L0x20019208 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200191d8; PC = 0x8001a30 *)
mov L0x200191d8 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200191ec; Value = 0x000303cb; PC = 0x80016ec *)
mov r4 L0x200191ec;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x2001920c; Value = 0xfff9a12f; PC = 0x80016f0 *)
mov r5 L0x2001920c;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x2001922c; Value = 0x0000bff2; PC = 0x80016f4 *)
mov r6 L0x2001922c;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x2001924c; Value = 0x0005fb78; PC = 0x80016f8 *)
mov r7 L0x2001924c;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x2001926c; Value = 0x0005a453; PC = 0x80016fc *)
mov r8 L0x2001926c;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x2001928c; Value = 0x00032c3d; PC = 0x8001700 *)
mov r9 L0x2001928c;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200192ac; Value = 0x00018011; PC = 0x8001704 *)
mov r10 L0x200192ac;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200192cc; Value = 0x00039bb9; PC = 0x8001708 *)
mov r11 L0x200192cc;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200191dc; Value = 0x0007847e; PC = 0x8001890 *)
mov r4 L0x200191dc;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200191fc; Value = 0x00049b5d; PC = 0x8001894 *)
mov r5 L0x200191fc;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x2001921c; Value = 0xfffa5d4d; PC = 0x8001898 *)
mov r6 L0x2001921c;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x2001923c; Value = 0xfffad21f; PC = 0x800189c *)
mov r7 L0x2001923c;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x2001925c; Value = 0xffff4fb5; PC = 0x80018a0 *)
mov r8 L0x2001925c;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x2001927c; Value = 0xfffa8b8e; PC = 0x80018a4 *)
mov r9 L0x2001927c;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x2001929c; Value = 0xfff857e9; PC = 0x80018a8 *)
mov r10 L0x2001929c;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200192bc; Value = 0x00004fd1; PC = 0x80018ac *)
mov r11 L0x200192bc;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x2001925c; PC = 0x80019cc *)
mov L0x2001925c r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x2001926c; PC = 0x80019d0 *)
mov L0x2001926c r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x2001927c; PC = 0x80019d4 *)
mov L0x2001927c r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x2001928c; PC = 0x80019d8 *)
mov L0x2001928c r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x2001929c; PC = 0x80019dc *)
mov L0x2001929c r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200192ac; PC = 0x80019e0 *)
mov L0x200192ac r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200192bc; PC = 0x80019e4 *)
mov L0x200192bc r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200192cc; PC = 0x80019e8 *)
mov L0x200192cc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x2001921c; PC = 0x8001a14 *)
mov L0x2001921c r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x2001922c; PC = 0x8001a18 *)
mov L0x2001922c r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x2001923c; PC = 0x8001a1c *)
mov L0x2001923c r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x2001924c; PC = 0x8001a20 *)
mov L0x2001924c r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200191ec; PC = 0x8001a24 *)
mov L0x200191ec r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200191fc; PC = 0x8001a28 *)
mov L0x200191fc r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x2001920c; PC = 0x8001a2c *)
mov L0x2001920c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200191dc; PC = 0x8001a30 *)
mov L0x200191dc r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* add.w	r0, r0, #240	; 0xf0                       #! PC = 0x8001a40 *)
adds dontcare r0 r0 240@uint32;
(* vmov	r12, s2                                    #! PC = 0x8001a44 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8001a48 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x80016d8 <_4_5_6_7>                     #! PC = 0x8001a4c *)
#bne.w	0x80016d8 <_4_5_6_7>                     #! 0x8001a4c = 0x8001a4c;
(* vmov	r1, s1                                     #! PC = 0x80016d8 *)
mov r1 s1;
(* vldmia	r1!, {s4-s18}                            #! EA = L0x8006ecc; PC = 0x80016dc *)
mov s4 L0x8006ecc;
mov s5 L0x8006ed0;
mov s6 L0x8006ed4;
mov s7 L0x8006ed8;
mov s8 L0x8006edc;
mov s9 L0x8006ee0;
mov s10 L0x8006ee4;
mov s11 L0x8006ee8;
mov s12 L0x8006eec;
mov s13 L0x8006ef0;
mov s14 L0x8006ef4;
mov s15 L0x8006ef8;
mov s16 L0x8006efc;
mov s17 L0x8006f00;
mov s18 L0x8006f04;
(* vmov	s1, r1                                     #! PC = 0x80016e0 *)
mov s1 r1;
(* add.w	lr, r0, #16                               #! PC = 0x80016e4 *)
adds dontcare lr r0 16@uint32;
(* vmov	s3, lr                                     #! PC = 0x80016e8 *)
mov s3 lr;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200192e0; Value = 0x00049916; PC = 0x80016ec *)
mov r4 L0x200192e0;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019300; Value = 0xfffe0a9f; PC = 0x80016f0 *)
mov r5 L0x20019300;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019320; Value = 0x000195a9; PC = 0x80016f4 *)
mov r6 L0x20019320;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019340; Value = 0x00050a7e; PC = 0x80016f8 *)
mov r7 L0x20019340;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019360; Value = 0x0002a902; PC = 0x80016fc *)
mov r8 L0x20019360;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019380; Value = 0xfff871b7; PC = 0x8001700 *)
mov r9 L0x20019380;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200193a0; Value = 0xfff96c5d; PC = 0x8001704 *)
mov r10 L0x200193a0;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200193c0; Value = 0x00017d74; PC = 0x8001708 *)
mov r11 L0x200193c0;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200192d0; Value = 0x00062749; PC = 0x8001890 *)
mov r4 L0x200192d0;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200192f0; Value = 0x00075f7a; PC = 0x8001894 *)
mov r5 L0x200192f0;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019310; Value = 0xfffed09d; PC = 0x8001898 *)
mov r6 L0x20019310;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019330; Value = 0x00012206; PC = 0x800189c *)
mov r7 L0x20019330;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019350; Value = 0xfffc5dfc; PC = 0x80018a0 *)
mov r8 L0x20019350;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019370; Value = 0xfffd9f7f; PC = 0x80018a4 *)
mov r9 L0x20019370;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019390; Value = 0x0003770b; PC = 0x80018a8 *)
mov r10 L0x20019390;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200193b0; Value = 0x0000eeae; PC = 0x80018ac *)
mov r11 L0x200193b0;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019350; PC = 0x80019cc *)
mov L0x20019350 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019360; PC = 0x80019d0 *)
mov L0x20019360 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019370; PC = 0x80019d4 *)
mov L0x20019370 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019380; PC = 0x80019d8 *)
mov L0x20019380 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019390; PC = 0x80019dc *)
mov L0x20019390 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200193a0; PC = 0x80019e0 *)
mov L0x200193a0 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200193b0; PC = 0x80019e4 *)
mov L0x200193b0 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200193c0; PC = 0x80019e8 *)
mov L0x200193c0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019310; PC = 0x8001a14 *)
mov L0x20019310 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019320; PC = 0x8001a18 *)
mov L0x20019320 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019330; PC = 0x8001a1c *)
mov L0x20019330 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019340; PC = 0x8001a20 *)
mov L0x20019340 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200192e0; PC = 0x8001a24 *)
mov L0x200192e0 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200192f0; PC = 0x8001a28 *)
mov L0x200192f0 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019300; PC = 0x8001a2c *)
mov L0x20019300 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200192d0; PC = 0x8001a30 *)
mov L0x200192d0 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200192e4; Value = 0xfffb24cb; PC = 0x80016ec *)
mov r4 L0x200192e4;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019304; Value = 0xffffffc0; PC = 0x80016f0 *)
mov r5 L0x20019304;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019324; Value = 0x000401e7; PC = 0x80016f4 *)
mov r6 L0x20019324;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019344; Value = 0xffff28f0; PC = 0x80016f8 *)
mov r7 L0x20019344;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019364; Value = 0xfffc1113; PC = 0x80016fc *)
mov r8 L0x20019364;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019384; Value = 0x0004142a; PC = 0x8001700 *)
mov r9 L0x20019384;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200193a4; Value = 0xfffdebfc; PC = 0x8001704 *)
mov r10 L0x200193a4;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200193c4; Value = 0x0007a96e; PC = 0x8001708 *)
mov r11 L0x200193c4;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200192d4; Value = 0xfffcda5b; PC = 0x8001890 *)
mov r4 L0x200192d4;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200192f4; Value = 0xfffe0408; PC = 0x8001894 *)
mov r5 L0x200192f4;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019314; Value = 0xfffa0502; PC = 0x8001898 *)
mov r6 L0x20019314;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019334; Value = 0xfffa6be4; PC = 0x800189c *)
mov r7 L0x20019334;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019354; Value = 0xfffdca71; PC = 0x80018a0 *)
mov r8 L0x20019354;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019374; Value = 0x000088e4; PC = 0x80018a4 *)
mov r9 L0x20019374;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019394; Value = 0x00067140; PC = 0x80018a8 *)
mov r10 L0x20019394;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200193b4; Value = 0xfffea942; PC = 0x80018ac *)
mov r11 L0x200193b4;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019354; PC = 0x80019cc *)
mov L0x20019354 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019364; PC = 0x80019d0 *)
mov L0x20019364 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019374; PC = 0x80019d4 *)
mov L0x20019374 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019384; PC = 0x80019d8 *)
mov L0x20019384 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019394; PC = 0x80019dc *)
mov L0x20019394 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200193a4; PC = 0x80019e0 *)
mov L0x200193a4 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200193b4; PC = 0x80019e4 *)
mov L0x200193b4 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200193c4; PC = 0x80019e8 *)
mov L0x200193c4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019314; PC = 0x8001a14 *)
mov L0x20019314 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019324; PC = 0x8001a18 *)
mov L0x20019324 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019334; PC = 0x8001a1c *)
mov L0x20019334 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019344; PC = 0x8001a20 *)
mov L0x20019344 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200192e4; PC = 0x8001a24 *)
mov L0x200192e4 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200192f4; PC = 0x8001a28 *)
mov L0x200192f4 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019304; PC = 0x8001a2c *)
mov L0x20019304 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200192d4; PC = 0x8001a30 *)
mov L0x200192d4 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200192e8; Value = 0x000138fa; PC = 0x80016ec *)
mov r4 L0x200192e8;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019308; Value = 0xfffb7b8a; PC = 0x80016f0 *)
mov r5 L0x20019308;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019328; Value = 0xfffdc8e6; PC = 0x80016f4 *)
mov r6 L0x20019328;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019348; Value = 0x0006b125; PC = 0x80016f8 *)
mov r7 L0x20019348;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019368; Value = 0xfff9ffda; PC = 0x80016fc *)
mov r8 L0x20019368;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019388; Value = 0xffff674a; PC = 0x8001700 *)
mov r9 L0x20019388;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200193a8; Value = 0x00001e57; PC = 0x8001704 *)
mov r10 L0x200193a8;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200193c8; Value = 0x00054469; PC = 0x8001708 *)
mov r11 L0x200193c8;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200192d8; Value = 0xfffd6f78; PC = 0x8001890 *)
mov r4 L0x200192d8;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200192f8; Value = 0x000726ad; PC = 0x8001894 *)
mov r5 L0x200192f8;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019318; Value = 0xfff81824; PC = 0x8001898 *)
mov r6 L0x20019318;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019338; Value = 0xfffe2ec2; PC = 0x800189c *)
mov r7 L0x20019338;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019358; Value = 0x0004f099; PC = 0x80018a0 *)
mov r8 L0x20019358;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019378; Value = 0xfffde4a1; PC = 0x80018a4 *)
mov r9 L0x20019378;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019398; Value = 0xffff485b; PC = 0x80018a8 *)
mov r10 L0x20019398;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200193b8; Value = 0xfff822cd; PC = 0x80018ac *)
mov r11 L0x200193b8;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019358; PC = 0x80019cc *)
mov L0x20019358 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019368; PC = 0x80019d0 *)
mov L0x20019368 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019378; PC = 0x80019d4 *)
mov L0x20019378 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019388; PC = 0x80019d8 *)
mov L0x20019388 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019398; PC = 0x80019dc *)
mov L0x20019398 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200193a8; PC = 0x80019e0 *)
mov L0x200193a8 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200193b8; PC = 0x80019e4 *)
mov L0x200193b8 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200193c8; PC = 0x80019e8 *)
mov L0x200193c8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019318; PC = 0x8001a14 *)
mov L0x20019318 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019328; PC = 0x8001a18 *)
mov L0x20019328 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019338; PC = 0x8001a1c *)
mov L0x20019338 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019348; PC = 0x8001a20 *)
mov L0x20019348 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200192e8; PC = 0x8001a24 *)
mov L0x200192e8 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200192f8; PC = 0x8001a28 *)
mov L0x200192f8 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019308; PC = 0x8001a2c *)
mov L0x20019308 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200192d8; PC = 0x8001a30 *)
mov L0x200192d8 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200192ec; Value = 0xfffcf147; PC = 0x80016ec *)
mov r4 L0x200192ec;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x2001930c; Value = 0x0006720f; PC = 0x80016f0 *)
mov r5 L0x2001930c;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x2001932c; Value = 0xffff3e2a; PC = 0x80016f4 *)
mov r6 L0x2001932c;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x2001934c; Value = 0xfffa0bec; PC = 0x80016f8 *)
mov r7 L0x2001934c;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x2001936c; Value = 0xfffa609f; PC = 0x80016fc *)
mov r8 L0x2001936c;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x2001938c; Value = 0xfffcc3cd; PC = 0x8001700 *)
mov r9 L0x2001938c;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200193ac; Value = 0xfffe7533; PC = 0x8001704 *)
mov r10 L0x200193ac;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200193cc; Value = 0xfffc5943; PC = 0x8001708 *)
mov r11 L0x200193cc;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200192dc; Value = 0xfff86bfe; PC = 0x8001890 *)
mov r4 L0x200192dc;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200192fc; Value = 0xfffb6811; PC = 0x8001894 *)
mov r5 L0x200192fc;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x2001931c; Value = 0x0005ad43; PC = 0x8001898 *)
mov r6 L0x2001931c;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x2001933c; Value = 0x0005347f; PC = 0x800189c *)
mov r7 L0x2001933c;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x2001935c; Value = 0x0000a42d; PC = 0x80018a0 *)
mov r8 L0x2001935c;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x2001937c; Value = 0x000573c4; PC = 0x80018a4 *)
mov r9 L0x2001937c;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x2001939c; Value = 0x0007ac55; PC = 0x80018a8 *)
mov r10 L0x2001939c;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200193bc; Value = 0xffffb295; PC = 0x80018ac *)
mov r11 L0x200193bc;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x2001935c; PC = 0x80019cc *)
mov L0x2001935c r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x2001936c; PC = 0x80019d0 *)
mov L0x2001936c r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x2001937c; PC = 0x80019d4 *)
mov L0x2001937c r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x2001938c; PC = 0x80019d8 *)
mov L0x2001938c r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x2001939c; PC = 0x80019dc *)
mov L0x2001939c r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200193ac; PC = 0x80019e0 *)
mov L0x200193ac r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200193bc; PC = 0x80019e4 *)
mov L0x200193bc r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200193cc; PC = 0x80019e8 *)
mov L0x200193cc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x2001931c; PC = 0x8001a14 *)
mov L0x2001931c r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x2001932c; PC = 0x8001a18 *)
mov L0x2001932c r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x2001933c; PC = 0x8001a1c *)
mov L0x2001933c r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x2001934c; PC = 0x8001a20 *)
mov L0x2001934c r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200192ec; PC = 0x8001a24 *)
mov L0x200192ec r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200192fc; PC = 0x8001a28 *)
mov L0x200192fc r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x2001930c; PC = 0x8001a2c *)
mov L0x2001930c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200192dc; PC = 0x8001a30 *)
mov L0x200192dc r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* add.w	r0, r0, #240	; 0xf0                       #! PC = 0x8001a40 *)
adds dontcare r0 r0 240@uint32;
(* vmov	r12, s2                                    #! PC = 0x8001a44 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8001a48 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x80016d8 <_4_5_6_7>                     #! PC = 0x8001a4c *)
#bne.w	0x80016d8 <_4_5_6_7>                     #! 0x8001a4c = 0x8001a4c;
(* vmov	r1, s1                                     #! PC = 0x80016d8 *)
mov r1 s1;
(* vldmia	r1!, {s4-s18}                            #! EA = L0x8006f08; PC = 0x80016dc *)
mov s4 L0x8006f08;
mov s5 L0x8006f0c;
mov s6 L0x8006f10;
mov s7 L0x8006f14;
mov s8 L0x8006f18;
mov s9 L0x8006f1c;
mov s10 L0x8006f20;
mov s11 L0x8006f24;
mov s12 L0x8006f28;
mov s13 L0x8006f2c;
mov s14 L0x8006f30;
mov s15 L0x8006f34;
mov s16 L0x8006f38;
mov s17 L0x8006f3c;
mov s18 L0x8006f40;
(* vmov	s1, r1                                     #! PC = 0x80016e0 *)
mov s1 r1;
(* add.w	lr, r0, #16                               #! PC = 0x80016e4 *)
adds dontcare lr r0 16@uint32;
(* vmov	s3, lr                                     #! PC = 0x80016e8 *)
mov s3 lr;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200193e0; Value = 0x000bc38c; PC = 0x80016ec *)
mov r4 L0x200193e0;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019400; Value = 0x000386e6; PC = 0x80016f0 *)
mov r5 L0x20019400;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019420; Value = 0x0001c902; PC = 0x80016f4 *)
mov r6 L0x20019420;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019440; Value = 0x00074057; PC = 0x80016f8 *)
mov r7 L0x20019440;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019460; Value = 0x000a35c8; PC = 0x80016fc *)
mov r8 L0x20019460;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019480; Value = 0x0000feb1; PC = 0x8001700 *)
mov r9 L0x20019480;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200194a0; Value = 0x000107ae; PC = 0x8001704 *)
mov r10 L0x200194a0;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200194c0; Value = 0xfff5f475; PC = 0x8001708 *)
mov r11 L0x200194c0;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200193d0; Value = 0xfff2820e; PC = 0x8001890 *)
mov r4 L0x200193d0;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200193f0; Value = 0x0004ab6e; PC = 0x8001894 *)
mov r5 L0x200193f0;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019410; Value = 0x00059e59; PC = 0x8001898 *)
mov r6 L0x20019410;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019430; Value = 0x000091ae; PC = 0x800189c *)
mov r7 L0x20019430;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019450; Value = 0x00008526; PC = 0x80018a0 *)
mov r8 L0x20019450;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019470; Value = 0x00043f61; PC = 0x80018a4 *)
mov r9 L0x20019470;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019490; Value = 0xfff63c7b; PC = 0x80018a8 *)
mov r10 L0x20019490;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200194b0; Value = 0xfff6a060; PC = 0x80018ac *)
mov r11 L0x200194b0;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019450; PC = 0x80019cc *)
mov L0x20019450 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019460; PC = 0x80019d0 *)
mov L0x20019460 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019470; PC = 0x80019d4 *)
mov L0x20019470 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019480; PC = 0x80019d8 *)
mov L0x20019480 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019490; PC = 0x80019dc *)
mov L0x20019490 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200194a0; PC = 0x80019e0 *)
mov L0x200194a0 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200194b0; PC = 0x80019e4 *)
mov L0x200194b0 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200194c0; PC = 0x80019e8 *)
mov L0x200194c0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019410; PC = 0x8001a14 *)
mov L0x20019410 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019420; PC = 0x8001a18 *)
mov L0x20019420 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019430; PC = 0x8001a1c *)
mov L0x20019430 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019440; PC = 0x8001a20 *)
mov L0x20019440 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200193e0; PC = 0x8001a24 *)
mov L0x200193e0 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200193f0; PC = 0x8001a28 *)
mov L0x200193f0 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019400; PC = 0x8001a2c *)
mov L0x20019400 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200193d0; PC = 0x8001a30 *)
mov L0x200193d0 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200193e4; Value = 0x00091005; PC = 0x80016ec *)
mov r4 L0x200193e4;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019404; Value = 0x0004c5cb; PC = 0x80016f0 *)
mov r5 L0x20019404;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019424; Value = 0xfffcb7d6; PC = 0x80016f4 *)
mov r6 L0x20019424;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019444; Value = 0xfffff1d9; PC = 0x80016f8 *)
mov r7 L0x20019444;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019464; Value = 0x0009cc11; PC = 0x80016fc *)
mov r8 L0x20019464;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019484; Value = 0xfff59283; PC = 0x8001700 *)
mov r9 L0x20019484;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200194a4; Value = 0x0002c67d; PC = 0x8001704 *)
mov r10 L0x200194a4;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200194c4; Value = 0x0003694c; PC = 0x8001708 *)
mov r11 L0x200194c4;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200193d4; Value = 0x000d836d; PC = 0x8001890 *)
mov r4 L0x200193d4;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200193f4; Value = 0x000024c8; PC = 0x8001894 *)
mov r5 L0x200193f4;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019414; Value = 0xfffc3e55; PC = 0x8001898 *)
mov r6 L0x20019414;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019434; Value = 0x0007080a; PC = 0x800189c *)
mov r7 L0x20019434;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019454; Value = 0xffffd5e9; PC = 0x80018a0 *)
mov r8 L0x20019454;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019474; Value = 0x0001a25f; PC = 0x80018a4 *)
mov r9 L0x20019474;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019494; Value = 0xfff9a19a; PC = 0x80018a8 *)
mov r10 L0x20019494;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200194b4; Value = 0xfffebb16; PC = 0x80018ac *)
mov r11 L0x200194b4;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019454; PC = 0x80019cc *)
mov L0x20019454 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019464; PC = 0x80019d0 *)
mov L0x20019464 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019474; PC = 0x80019d4 *)
mov L0x20019474 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019484; PC = 0x80019d8 *)
mov L0x20019484 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019494; PC = 0x80019dc *)
mov L0x20019494 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200194a4; PC = 0x80019e0 *)
mov L0x200194a4 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200194b4; PC = 0x80019e4 *)
mov L0x200194b4 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200194c4; PC = 0x80019e8 *)
mov L0x200194c4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019414; PC = 0x8001a14 *)
mov L0x20019414 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019424; PC = 0x8001a18 *)
mov L0x20019424 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019434; PC = 0x8001a1c *)
mov L0x20019434 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019444; PC = 0x8001a20 *)
mov L0x20019444 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200193e4; PC = 0x8001a24 *)
mov L0x200193e4 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200193f4; PC = 0x8001a28 *)
mov L0x200193f4 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019404; PC = 0x8001a2c *)
mov L0x20019404 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200193d4; PC = 0x8001a30 *)
mov L0x200193d4 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200193e8; Value = 0xfff5cd6d; PC = 0x80016ec *)
mov r4 L0x200193e8;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019408; Value = 0x000bb4a3; PC = 0x80016f0 *)
mov r5 L0x20019408;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019428; Value = 0xfff88f4d; PC = 0x80016f4 *)
mov r6 L0x20019428;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019448; Value = 0x00028afe; PC = 0x80016f8 *)
mov r7 L0x20019448;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019468; Value = 0x0005607c; PC = 0x80016fc *)
mov r8 L0x20019468;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019488; Value = 0x000bac72; PC = 0x8001700 *)
mov r9 L0x20019488;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200194a8; Value = 0x0007649c; PC = 0x8001704 *)
mov r10 L0x200194a8;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200194c8; Value = 0x00015820; PC = 0x8001708 *)
mov r11 L0x200194c8;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200193d8; Value = 0x00023a47; PC = 0x8001890 *)
mov r4 L0x200193d8;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200193f8; Value = 0x00022258; PC = 0x8001894 *)
mov r5 L0x200193f8;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019418; Value = 0x0001afe0; PC = 0x8001898 *)
mov r6 L0x20019418;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019438; Value = 0xfffdac0e; PC = 0x800189c *)
mov r7 L0x20019438;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019458; Value = 0x0009aba3; PC = 0x80018a0 *)
mov r8 L0x20019458;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019478; Value = 0x0003c103; PC = 0x80018a4 *)
mov r9 L0x20019478;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019498; Value = 0x0004fdc8; PC = 0x80018a8 *)
mov r10 L0x20019498;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200194b8; Value = 0x0002fe3f; PC = 0x80018ac *)
mov r11 L0x200194b8;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019458; PC = 0x80019cc *)
mov L0x20019458 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019468; PC = 0x80019d0 *)
mov L0x20019468 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019478; PC = 0x80019d4 *)
mov L0x20019478 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019488; PC = 0x80019d8 *)
mov L0x20019488 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019498; PC = 0x80019dc *)
mov L0x20019498 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200194a8; PC = 0x80019e0 *)
mov L0x200194a8 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200194b8; PC = 0x80019e4 *)
mov L0x200194b8 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200194c8; PC = 0x80019e8 *)
mov L0x200194c8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019418; PC = 0x8001a14 *)
mov L0x20019418 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019428; PC = 0x8001a18 *)
mov L0x20019428 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019438; PC = 0x8001a1c *)
mov L0x20019438 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019448; PC = 0x8001a20 *)
mov L0x20019448 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200193e8; PC = 0x8001a24 *)
mov L0x200193e8 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200193f8; PC = 0x8001a28 *)
mov L0x200193f8 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019408; PC = 0x8001a2c *)
mov L0x20019408 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200193d8; PC = 0x8001a30 *)
mov L0x200193d8 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200193ec; Value = 0xfffb47e9; PC = 0x80016ec *)
mov r4 L0x200193ec;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x2001940c; Value = 0xfffb3025; PC = 0x80016f0 *)
mov r5 L0x2001940c;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x2001942c; Value = 0x00086f66; PC = 0x80016f4 *)
mov r6 L0x2001942c;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x2001944c; Value = 0x0005b9c8; PC = 0x80016f8 *)
mov r7 L0x2001944c;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x2001946c; Value = 0x0000ec3a; PC = 0x80016fc *)
mov r8 L0x2001946c;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x2001948c; Value = 0x0006d00f; PC = 0x8001700 *)
mov r9 L0x2001948c;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200194ac; Value = 0x0001457f; PC = 0x8001704 *)
mov r10 L0x200194ac;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200194cc; Value = 0x00050600; PC = 0x8001708 *)
mov r11 L0x200194cc;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200193dc; Value = 0xfffba44b; PC = 0x8001890 *)
mov r4 L0x200193dc;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200193fc; Value = 0x0008b309; PC = 0x8001894 *)
mov r5 L0x200193fc;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x2001941c; Value = 0x0003b017; PC = 0x8001898 *)
mov r6 L0x2001941c;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x2001943c; Value = 0xfffc6d98; PC = 0x800189c *)
mov r7 L0x2001943c;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x2001945c; Value = 0x000452f2; PC = 0x80018a0 *)
mov r8 L0x2001945c;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x2001947c; Value = 0xfffc97a1; PC = 0x80018a4 *)
mov r9 L0x2001947c;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x2001949c; Value = 0xfffe35e7; PC = 0x80018a8 *)
mov r10 L0x2001949c;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200194bc; Value = 0xffff8a8f; PC = 0x80018ac *)
mov r11 L0x200194bc;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x2001945c; PC = 0x80019cc *)
mov L0x2001945c r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x2001946c; PC = 0x80019d0 *)
mov L0x2001946c r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x2001947c; PC = 0x80019d4 *)
mov L0x2001947c r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x2001948c; PC = 0x80019d8 *)
mov L0x2001948c r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x2001949c; PC = 0x80019dc *)
mov L0x2001949c r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200194ac; PC = 0x80019e0 *)
mov L0x200194ac r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200194bc; PC = 0x80019e4 *)
mov L0x200194bc r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200194cc; PC = 0x80019e8 *)
mov L0x200194cc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x2001941c; PC = 0x8001a14 *)
mov L0x2001941c r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x2001942c; PC = 0x8001a18 *)
mov L0x2001942c r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x2001943c; PC = 0x8001a1c *)
mov L0x2001943c r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x2001944c; PC = 0x8001a20 *)
mov L0x2001944c r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200193ec; PC = 0x8001a24 *)
mov L0x200193ec r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200193fc; PC = 0x8001a28 *)
mov L0x200193fc r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x2001940c; PC = 0x8001a2c *)
mov L0x2001940c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200193dc; PC = 0x8001a30 *)
mov L0x200193dc r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* add.w	r0, r0, #240	; 0xf0                       #! PC = 0x8001a40 *)
adds dontcare r0 r0 240@uint32;
(* vmov	r12, s2                                    #! PC = 0x8001a44 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8001a48 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x80016d8 <_4_5_6_7>                     #! PC = 0x8001a4c *)
#bne.w	0x80016d8 <_4_5_6_7>                     #! 0x8001a4c = 0x8001a4c;
(* vmov	r1, s1                                     #! PC = 0x80016d8 *)
mov r1 s1;
(* vldmia	r1!, {s4-s18}                            #! EA = L0x8006f44; PC = 0x80016dc *)
mov s4 L0x8006f44;
mov s5 L0x8006f48;
mov s6 L0x8006f4c;
mov s7 L0x8006f50;
mov s8 L0x8006f54;
mov s9 L0x8006f58;
mov s10 L0x8006f5c;
mov s11 L0x8006f60;
mov s12 L0x8006f64;
mov s13 L0x8006f68;
mov s14 L0x8006f6c;
mov s15 L0x8006f70;
mov s16 L0x8006f74;
mov s17 L0x8006f78;
mov s18 L0x8006f7c;
(* vmov	s1, r1                                     #! PC = 0x80016e0 *)
mov s1 r1;
(* add.w	lr, r0, #16                               #! PC = 0x80016e4 *)
adds dontcare lr r0 16@uint32;
(* vmov	s3, lr                                     #! PC = 0x80016e8 *)
mov s3 lr;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200194e0; Value = 0x0001e8b4; PC = 0x80016ec *)
mov r4 L0x200194e0;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019500; Value = 0xfff80bde; PC = 0x80016f0 *)
mov r5 L0x20019500;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019520; Value = 0xfff757f6; PC = 0x80016f4 *)
mov r6 L0x20019520;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019540; Value = 0xfff9c555; PC = 0x80016f8 *)
mov r7 L0x20019540;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019560; Value = 0x0001453a; PC = 0x80016fc *)
mov r8 L0x20019560;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019580; Value = 0x00033d3f; PC = 0x8001700 *)
mov r9 L0x20019580;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200195a0; Value = 0xfff62bfa; PC = 0x8001704 *)
mov r10 L0x200195a0;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200195c0; Value = 0xfffeb98d; PC = 0x8001708 *)
mov r11 L0x200195c0;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200194d0; Value = 0x000116cc; PC = 0x8001890 *)
mov r4 L0x200194d0;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200194f0; Value = 0x00031210; PC = 0x8001894 *)
mov r5 L0x200194f0;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019510; Value = 0xffff308f; PC = 0x8001898 *)
mov r6 L0x20019510;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019530; Value = 0xfff7cabc; PC = 0x800189c *)
mov r7 L0x20019530;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019550; Value = 0xfff1d4b0; PC = 0x80018a0 *)
mov r8 L0x20019550;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019570; Value = 0x0004e7a1; PC = 0x80018a4 *)
mov r9 L0x20019570;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019590; Value = 0xffff9b5b; PC = 0x80018a8 *)
mov r10 L0x20019590;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200195b0; Value = 0x0005f876; PC = 0x80018ac *)
mov r11 L0x200195b0;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019550; PC = 0x80019cc *)
mov L0x20019550 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019560; PC = 0x80019d0 *)
mov L0x20019560 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019570; PC = 0x80019d4 *)
mov L0x20019570 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019580; PC = 0x80019d8 *)
mov L0x20019580 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019590; PC = 0x80019dc *)
mov L0x20019590 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200195a0; PC = 0x80019e0 *)
mov L0x200195a0 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200195b0; PC = 0x80019e4 *)
mov L0x200195b0 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200195c0; PC = 0x80019e8 *)
mov L0x200195c0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019510; PC = 0x8001a14 *)
mov L0x20019510 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019520; PC = 0x8001a18 *)
mov L0x20019520 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019530; PC = 0x8001a1c *)
mov L0x20019530 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019540; PC = 0x8001a20 *)
mov L0x20019540 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200194e0; PC = 0x8001a24 *)
mov L0x200194e0 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200194f0; PC = 0x8001a28 *)
mov L0x200194f0 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019500; PC = 0x8001a2c *)
mov L0x20019500 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200194d0; PC = 0x8001a30 *)
mov L0x200194d0 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200194e4; Value = 0xffffc199; PC = 0x80016ec *)
mov r4 L0x200194e4;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019504; Value = 0x0006518b; PC = 0x80016f0 *)
mov r5 L0x20019504;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019524; Value = 0x00067c12; PC = 0x80016f4 *)
mov r6 L0x20019524;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019544; Value = 0x00086e7f; PC = 0x80016f8 *)
mov r7 L0x20019544;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019564; Value = 0x00024e79; PC = 0x80016fc *)
mov r8 L0x20019564;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019584; Value = 0xfffe8be3; PC = 0x8001700 *)
mov r9 L0x20019584;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200195a4; Value = 0x00050f5d; PC = 0x8001704 *)
mov r10 L0x200195a4;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200195c4; Value = 0x00047436; PC = 0x8001708 *)
mov r11 L0x200195c4;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200194d4; Value = 0x00000d31; PC = 0x8001890 *)
mov r4 L0x200194d4;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200194f4; Value = 0xfff832a0; PC = 0x8001894 *)
mov r5 L0x200194f4;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019514; Value = 0x000364ff; PC = 0x8001898 *)
mov r6 L0x20019514;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019534; Value = 0xfff95a16; PC = 0x800189c *)
mov r7 L0x20019534;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019554; Value = 0xfffae6d1; PC = 0x80018a0 *)
mov r8 L0x20019554;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019574; Value = 0x0007137b; PC = 0x80018a4 *)
mov r9 L0x20019574;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019594; Value = 0x000861fe; PC = 0x80018a8 *)
mov r10 L0x20019594;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200195b4; Value = 0xfff2f7a6; PC = 0x80018ac *)
mov r11 L0x200195b4;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019554; PC = 0x80019cc *)
mov L0x20019554 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019564; PC = 0x80019d0 *)
mov L0x20019564 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019574; PC = 0x80019d4 *)
mov L0x20019574 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019584; PC = 0x80019d8 *)
mov L0x20019584 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019594; PC = 0x80019dc *)
mov L0x20019594 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200195a4; PC = 0x80019e0 *)
mov L0x200195a4 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200195b4; PC = 0x80019e4 *)
mov L0x200195b4 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200195c4; PC = 0x80019e8 *)
mov L0x200195c4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019514; PC = 0x8001a14 *)
mov L0x20019514 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019524; PC = 0x8001a18 *)
mov L0x20019524 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019534; PC = 0x8001a1c *)
mov L0x20019534 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019544; PC = 0x8001a20 *)
mov L0x20019544 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200194e4; PC = 0x8001a24 *)
mov L0x200194e4 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200194f4; PC = 0x8001a28 *)
mov L0x200194f4 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019504; PC = 0x8001a2c *)
mov L0x20019504 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200194d4; PC = 0x8001a30 *)
mov L0x200194d4 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200194e8; Value = 0xfffe5023; PC = 0x80016ec *)
mov r4 L0x200194e8;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019508; Value = 0x000148b3; PC = 0x80016f0 *)
mov r5 L0x20019508;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019528; Value = 0x00067851; PC = 0x80016f4 *)
mov r6 L0x20019528;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019548; Value = 0x000cd74a; PC = 0x80016f8 *)
mov r7 L0x20019548;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019568; Value = 0xfff94120; PC = 0x80016fc *)
mov r8 L0x20019568;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019588; Value = 0xfffce8c2; PC = 0x8001700 *)
mov r9 L0x20019588;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200195a8; Value = 0xffff410e; PC = 0x8001704 *)
mov r10 L0x200195a8;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200195c8; Value = 0x000d4aca; PC = 0x8001708 *)
mov r11 L0x200195c8;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200194d8; Value = 0xfffd04b5; PC = 0x8001890 *)
mov r4 L0x200194d8;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200194f8; Value = 0x00058f8e; PC = 0x8001894 *)
mov r5 L0x200194f8;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019518; Value = 0xfff75dac; PC = 0x8001898 *)
mov r6 L0x20019518;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019538; Value = 0xfffa778e; PC = 0x800189c *)
mov r7 L0x20019538;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019558; Value = 0x00043b1f; PC = 0x80018a0 *)
mov r8 L0x20019558;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019578; Value = 0x00010fb1; PC = 0x80018a4 *)
mov r9 L0x20019578;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019598; Value = 0x0004e3bc; PC = 0x80018a8 *)
mov r10 L0x20019598;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200195b8; Value = 0x0000b7ed; PC = 0x80018ac *)
mov r11 L0x200195b8;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019558; PC = 0x80019cc *)
mov L0x20019558 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019568; PC = 0x80019d0 *)
mov L0x20019568 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019578; PC = 0x80019d4 *)
mov L0x20019578 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019588; PC = 0x80019d8 *)
mov L0x20019588 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019598; PC = 0x80019dc *)
mov L0x20019598 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200195a8; PC = 0x80019e0 *)
mov L0x200195a8 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200195b8; PC = 0x80019e4 *)
mov L0x200195b8 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200195c8; PC = 0x80019e8 *)
mov L0x200195c8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019518; PC = 0x8001a14 *)
mov L0x20019518 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019528; PC = 0x8001a18 *)
mov L0x20019528 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019538; PC = 0x8001a1c *)
mov L0x20019538 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019548; PC = 0x8001a20 *)
mov L0x20019548 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200194e8; PC = 0x8001a24 *)
mov L0x200194e8 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200194f8; PC = 0x8001a28 *)
mov L0x200194f8 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019508; PC = 0x8001a2c *)
mov L0x20019508 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200194d8; PC = 0x8001a30 *)
mov L0x200194d8 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200194ec; Value = 0x00089c89; PC = 0x80016ec *)
mov r4 L0x200194ec;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x2001950c; Value = 0x000852bb; PC = 0x80016f0 *)
mov r5 L0x2001950c;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x2001952c; Value = 0xfff8a224; PC = 0x80016f4 *)
mov r6 L0x2001952c;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x2001954c; Value = 0xfff72dde; PC = 0x80016f8 *)
mov r7 L0x2001954c;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x2001956c; Value = 0x00011604; PC = 0x80016fc *)
mov r8 L0x2001956c;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x2001958c; Value = 0x0001a5d7; PC = 0x8001700 *)
mov r9 L0x2001958c;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200195ac; Value = 0x00070883; PC = 0x8001704 *)
mov r10 L0x200195ac;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200195cc; Value = 0xfffb667e; PC = 0x8001708 *)
mov r11 L0x200195cc;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200194dc; Value = 0xfff808ab; PC = 0x8001890 *)
mov r4 L0x200194dc;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200194fc; Value = 0xfffaa6ef; PC = 0x8001894 *)
mov r5 L0x200194fc;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x2001951c; Value = 0xfffe9695; PC = 0x8001898 *)
mov r6 L0x2001951c;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x2001953c; Value = 0xfff3f4c6; PC = 0x800189c *)
mov r7 L0x2001953c;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x2001955c; Value = 0x0000bc2a; PC = 0x80018a0 *)
mov r8 L0x2001955c;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x2001957c; Value = 0x00000c29; PC = 0x80018a4 *)
mov r9 L0x2001957c;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x2001959c; Value = 0x000cbacb; PC = 0x80018a8 *)
mov r10 L0x2001959c;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200195bc; Value = 0xfffdc091; PC = 0x80018ac *)
mov r11 L0x200195bc;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x2001955c; PC = 0x80019cc *)
mov L0x2001955c r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x2001956c; PC = 0x80019d0 *)
mov L0x2001956c r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x2001957c; PC = 0x80019d4 *)
mov L0x2001957c r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x2001958c; PC = 0x80019d8 *)
mov L0x2001958c r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x2001959c; PC = 0x80019dc *)
mov L0x2001959c r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200195ac; PC = 0x80019e0 *)
mov L0x200195ac r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200195bc; PC = 0x80019e4 *)
mov L0x200195bc r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200195cc; PC = 0x80019e8 *)
mov L0x200195cc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x2001951c; PC = 0x8001a14 *)
mov L0x2001951c r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x2001952c; PC = 0x8001a18 *)
mov L0x2001952c r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x2001953c; PC = 0x8001a1c *)
mov L0x2001953c r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x2001954c; PC = 0x8001a20 *)
mov L0x2001954c r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200194ec; PC = 0x8001a24 *)
mov L0x200194ec r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200194fc; PC = 0x8001a28 *)
mov L0x200194fc r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x2001950c; PC = 0x8001a2c *)
mov L0x2001950c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200194dc; PC = 0x8001a30 *)
mov L0x200194dc r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* add.w	r0, r0, #240	; 0xf0                       #! PC = 0x8001a40 *)
adds dontcare r0 r0 240@uint32;
(* vmov	r12, s2                                    #! PC = 0x8001a44 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8001a48 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x80016d8 <_4_5_6_7>                     #! PC = 0x8001a4c *)
#bne.w	0x80016d8 <_4_5_6_7>                     #! 0x8001a4c = 0x8001a4c;
(* vmov	r1, s1                                     #! PC = 0x80016d8 *)
mov r1 s1;
(* vldmia	r1!, {s4-s18}                            #! EA = L0x8006f80; PC = 0x80016dc *)
mov s4 L0x8006f80;
mov s5 L0x8006f84;
mov s6 L0x8006f88;
mov s7 L0x8006f8c;
mov s8 L0x8006f90;
mov s9 L0x8006f94;
mov s10 L0x8006f98;
mov s11 L0x8006f9c;
mov s12 L0x8006fa0;
mov s13 L0x8006fa4;
mov s14 L0x8006fa8;
mov s15 L0x8006fac;
mov s16 L0x8006fb0;
mov s17 L0x8006fb4;
mov s18 L0x8006fb8;
(* vmov	s1, r1                                     #! PC = 0x80016e0 *)
mov s1 r1;
(* add.w	lr, r0, #16                               #! PC = 0x80016e4 *)
adds dontcare lr r0 16@uint32;
(* vmov	s3, lr                                     #! PC = 0x80016e8 *)
mov s3 lr;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200195e0; Value = 0xfffdc3a7; PC = 0x80016ec *)
mov r4 L0x200195e0;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019600; Value = 0x00013090; PC = 0x80016f0 *)
mov r5 L0x20019600;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019620; Value = 0x000625d2; PC = 0x80016f4 *)
mov r6 L0x20019620;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019640; Value = 0xfff9265b; PC = 0x80016f8 *)
mov r7 L0x20019640;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019660; Value = 0xfff3ecee; PC = 0x80016fc *)
mov r8 L0x20019660;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019680; Value = 0xfff887c0; PC = 0x8001700 *)
mov r9 L0x20019680;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200196a0; Value = 0xfffeb671; PC = 0x8001704 *)
mov r10 L0x200196a0;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200196c0; Value = 0x000ce3e5; PC = 0x8001708 *)
mov r11 L0x200196c0;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200195d0; Value = 0x000d991b; PC = 0x8001890 *)
mov r4 L0x200195d0;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200195f0; Value = 0xfff67e98; PC = 0x8001894 *)
mov r5 L0x200195f0;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019610; Value = 0xfffd2c0e; PC = 0x8001898 *)
mov r6 L0x20019610;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019630; Value = 0x0002a6eb; PC = 0x800189c *)
mov r7 L0x20019630;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019650; Value = 0x000dbbfd; PC = 0x80018a0 *)
mov r8 L0x20019650;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019670; Value = 0xfffffdd0; PC = 0x80018a4 *)
mov r9 L0x20019670;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019690; Value = 0x0004d358; PC = 0x80018a8 *)
mov r10 L0x20019690;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200196b0; Value = 0x000319da; PC = 0x80018ac *)
mov r11 L0x200196b0;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019650; PC = 0x80019cc *)
mov L0x20019650 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019660; PC = 0x80019d0 *)
mov L0x20019660 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019670; PC = 0x80019d4 *)
mov L0x20019670 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019680; PC = 0x80019d8 *)
mov L0x20019680 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019690; PC = 0x80019dc *)
mov L0x20019690 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200196a0; PC = 0x80019e0 *)
mov L0x200196a0 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200196b0; PC = 0x80019e4 *)
mov L0x200196b0 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200196c0; PC = 0x80019e8 *)
mov L0x200196c0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019610; PC = 0x8001a14 *)
mov L0x20019610 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019620; PC = 0x8001a18 *)
mov L0x20019620 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019630; PC = 0x8001a1c *)
mov L0x20019630 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019640; PC = 0x8001a20 *)
mov L0x20019640 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200195e0; PC = 0x8001a24 *)
mov L0x200195e0 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200195f0; PC = 0x8001a28 *)
mov L0x200195f0 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019600; PC = 0x8001a2c *)
mov L0x20019600 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200195d0; PC = 0x8001a30 *)
mov L0x200195d0 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200195e4; Value = 0xfff9fa2a; PC = 0x80016ec *)
mov r4 L0x200195e4;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019604; Value = 0xfff2d928; PC = 0x80016f0 *)
mov r5 L0x20019604;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019624; Value = 0xfff988a9; PC = 0x80016f4 *)
mov r6 L0x20019624;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019644; Value = 0x000128f3; PC = 0x80016f8 *)
mov r7 L0x20019644;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019664; Value = 0xfff7d40e; PC = 0x80016fc *)
mov r8 L0x20019664;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019684; Value = 0xfffe4a3d; PC = 0x8001700 *)
mov r9 L0x20019684;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200196a4; Value = 0xfff4b1bf; PC = 0x8001704 *)
mov r10 L0x200196a4;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200196c4; Value = 0xfff92ba2; PC = 0x8001708 *)
mov r11 L0x200196c4;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200195d4; Value = 0xfffc3787; PC = 0x8001890 *)
mov r4 L0x200195d4;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200195f4; Value = 0xffffc10c; PC = 0x8001894 *)
mov r5 L0x200195f4;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019614; Value = 0x0003476f; PC = 0x8001898 *)
mov r6 L0x20019614;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019634; Value = 0xfff885c7; PC = 0x800189c *)
mov r7 L0x20019634;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019654; Value = 0xfffd052b; PC = 0x80018a0 *)
mov r8 L0x20019654;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019674; Value = 0xfff78d17; PC = 0x80018a4 *)
mov r9 L0x20019674;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019694; Value = 0xfff75151; PC = 0x80018a8 *)
mov r10 L0x20019694;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200196b4; Value = 0x0009134d; PC = 0x80018ac *)
mov r11 L0x200196b4;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019654; PC = 0x80019cc *)
mov L0x20019654 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019664; PC = 0x80019d0 *)
mov L0x20019664 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019674; PC = 0x80019d4 *)
mov L0x20019674 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019684; PC = 0x80019d8 *)
mov L0x20019684 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019694; PC = 0x80019dc *)
mov L0x20019694 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200196a4; PC = 0x80019e0 *)
mov L0x200196a4 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200196b4; PC = 0x80019e4 *)
mov L0x200196b4 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200196c4; PC = 0x80019e8 *)
mov L0x200196c4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019614; PC = 0x8001a14 *)
mov L0x20019614 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019624; PC = 0x8001a18 *)
mov L0x20019624 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019634; PC = 0x8001a1c *)
mov L0x20019634 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019644; PC = 0x8001a20 *)
mov L0x20019644 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200195e4; PC = 0x8001a24 *)
mov L0x200195e4 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200195f4; PC = 0x8001a28 *)
mov L0x200195f4 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019604; PC = 0x8001a2c *)
mov L0x20019604 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200195d4; PC = 0x8001a30 *)
mov L0x200195d4 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200195e8; Value = 0x0008fffb; PC = 0x80016ec *)
mov r4 L0x200195e8;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019608; Value = 0xfff585e0; PC = 0x80016f0 *)
mov r5 L0x20019608;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019628; Value = 0xfffa4112; PC = 0x80016f4 *)
mov r6 L0x20019628;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019648; Value = 0xfff25c5c; PC = 0x80016f8 *)
mov r7 L0x20019648;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019668; Value = 0x00009ae9; PC = 0x80016fc *)
mov r8 L0x20019668;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019688; Value = 0xfff53d82; PC = 0x8001700 *)
mov r9 L0x20019688;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200196a8; Value = 0x00002f4f; PC = 0x8001704 *)
mov r10 L0x200196a8;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200196c8; Value = 0xfffe4a74; PC = 0x8001708 *)
mov r11 L0x200196c8;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200195d8; Value = 0x0003f12e; PC = 0x8001890 *)
mov r4 L0x200195d8;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200195f8; Value = 0x0002324e; PC = 0x8001894 *)
mov r5 L0x200195f8;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019618; Value = 0xfffef34b; PC = 0x8001898 *)
mov r6 L0x20019618;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019638; Value = 0x00045657; PC = 0x800189c *)
mov r7 L0x20019638;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019658; Value = 0xfff9a2d6; PC = 0x80018a0 *)
mov r8 L0x20019658;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019678; Value = 0xfffc3e56; PC = 0x80018a4 *)
mov r9 L0x20019678;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019698; Value = 0xfff676d0; PC = 0x80018a8 *)
mov r10 L0x20019698;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200196b8; Value = 0x00022364; PC = 0x80018ac *)
mov r11 L0x200196b8;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019658; PC = 0x80019cc *)
mov L0x20019658 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019668; PC = 0x80019d0 *)
mov L0x20019668 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019678; PC = 0x80019d4 *)
mov L0x20019678 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019688; PC = 0x80019d8 *)
mov L0x20019688 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019698; PC = 0x80019dc *)
mov L0x20019698 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200196a8; PC = 0x80019e0 *)
mov L0x200196a8 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200196b8; PC = 0x80019e4 *)
mov L0x200196b8 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200196c8; PC = 0x80019e8 *)
mov L0x200196c8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019618; PC = 0x8001a14 *)
mov L0x20019618 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019628; PC = 0x8001a18 *)
mov L0x20019628 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019638; PC = 0x8001a1c *)
mov L0x20019638 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019648; PC = 0x8001a20 *)
mov L0x20019648 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200195e8; PC = 0x8001a24 *)
mov L0x200195e8 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200195f8; PC = 0x8001a28 *)
mov L0x200195f8 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019608; PC = 0x8001a2c *)
mov L0x20019608 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200195d8; PC = 0x8001a30 *)
mov L0x200195d8 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200195ec; Value = 0xfffbf98d; PC = 0x80016ec *)
mov r4 L0x200195ec;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x2001960c; Value = 0x0005a45c; PC = 0x80016f0 *)
mov r5 L0x2001960c;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x2001962c; Value = 0x0000d4db; PC = 0x80016f4 *)
mov r6 L0x2001962c;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x2001964c; Value = 0xfffa31a2; PC = 0x80016f8 *)
mov r7 L0x2001964c;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x2001966c; Value = 0xfffa1026; PC = 0x80016fc *)
mov r8 L0x2001966c;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x2001968c; Value = 0xfff53b93; PC = 0x8001700 *)
mov r9 L0x2001968c;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200196ac; Value = 0xfff9c075; PC = 0x8001704 *)
mov r10 L0x200196ac;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200196cc; Value = 0x000221f0; PC = 0x8001708 *)
mov r11 L0x200196cc;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200195dc; Value = 0x0000d9c2; PC = 0x8001890 *)
mov r4 L0x200195dc;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200195fc; Value = 0xfff943b6; PC = 0x8001894 *)
mov r5 L0x200195fc;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x2001961c; Value = 0xfffeeb94; PC = 0x8001898 *)
mov r6 L0x2001961c;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x2001963c; Value = 0x000b711c; PC = 0x800189c *)
mov r7 L0x2001963c;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x2001965c; Value = 0xfffb1342; PC = 0x80018a0 *)
mov r8 L0x2001965c;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x2001967c; Value = 0xfffacd83; PC = 0x80018a4 *)
mov r9 L0x2001967c;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x2001969c; Value = 0xfffe3164; PC = 0x80018a8 *)
mov r10 L0x2001969c;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200196bc; Value = 0xffffdaef; PC = 0x80018ac *)
mov r11 L0x200196bc;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x2001965c; PC = 0x80019cc *)
mov L0x2001965c r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x2001966c; PC = 0x80019d0 *)
mov L0x2001966c r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x2001967c; PC = 0x80019d4 *)
mov L0x2001967c r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x2001968c; PC = 0x80019d8 *)
mov L0x2001968c r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x2001969c; PC = 0x80019dc *)
mov L0x2001969c r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200196ac; PC = 0x80019e0 *)
mov L0x200196ac r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200196bc; PC = 0x80019e4 *)
mov L0x200196bc r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200196cc; PC = 0x80019e8 *)
mov L0x200196cc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x2001961c; PC = 0x8001a14 *)
mov L0x2001961c r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x2001962c; PC = 0x8001a18 *)
mov L0x2001962c r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x2001963c; PC = 0x8001a1c *)
mov L0x2001963c r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x2001964c; PC = 0x8001a20 *)
mov L0x2001964c r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200195ec; PC = 0x8001a24 *)
mov L0x200195ec r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200195fc; PC = 0x8001a28 *)
mov L0x200195fc r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x2001960c; PC = 0x8001a2c *)
mov L0x2001960c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200195dc; PC = 0x8001a30 *)
mov L0x200195dc r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* add.w	r0, r0, #240	; 0xf0                       #! PC = 0x8001a40 *)
adds dontcare r0 r0 240@uint32;
(* vmov	r12, s2                                    #! PC = 0x8001a44 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8001a48 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x80016d8 <_4_5_6_7>                     #! PC = 0x8001a4c *)
#bne.w	0x80016d8 <_4_5_6_7>                     #! 0x8001a4c = 0x8001a4c;
(* vmov	r1, s1                                     #! PC = 0x80016d8 *)
mov r1 s1;
(* vldmia	r1!, {s4-s18}                            #! EA = L0x8006fbc; PC = 0x80016dc *)
mov s4 L0x8006fbc;
mov s5 L0x8006fc0;
mov s6 L0x8006fc4;
mov s7 L0x8006fc8;
mov s8 L0x8006fcc;
mov s9 L0x8006fd0;
mov s10 L0x8006fd4;
mov s11 L0x8006fd8;
mov s12 L0x8006fdc;
mov s13 L0x8006fe0;
mov s14 L0x8006fe4;
mov s15 L0x8006fe8;
mov s16 L0x8006fec;
mov s17 L0x8006ff0;
mov s18 L0x8006ff4;
(* vmov	s1, r1                                     #! PC = 0x80016e0 *)
mov s1 r1;
(* add.w	lr, r0, #16                               #! PC = 0x80016e4 *)
adds dontcare lr r0 16@uint32;
(* vmov	s3, lr                                     #! PC = 0x80016e8 *)
mov s3 lr;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200196e0; Value = 0xfff48aa1; PC = 0x80016ec *)
mov r4 L0x200196e0;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019700; Value = 0x000342cc; PC = 0x80016f0 *)
mov r5 L0x20019700;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019720; Value = 0x0000bf2e; PC = 0x80016f4 *)
mov r6 L0x20019720;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019740; Value = 0x0005ec21; PC = 0x80016f8 *)
mov r7 L0x20019740;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019760; Value = 0x0000a894; PC = 0x80016fc *)
mov r8 L0x20019760;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019780; Value = 0x0003539c; PC = 0x8001700 *)
mov r9 L0x20019780;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200197a0; Value = 0x000a1abf; PC = 0x8001704 *)
mov r10 L0x200197a0;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200197c0; Value = 0xfffe684d; PC = 0x8001708 *)
mov r11 L0x200197c0;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200196d0; Value = 0xfffecb8f; PC = 0x8001890 *)
mov r4 L0x200196d0;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200196f0; Value = 0x0001b836; PC = 0x8001894 *)
mov r5 L0x200196f0;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019710; Value = 0xfffe0ffa; PC = 0x8001898 *)
mov r6 L0x20019710;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019730; Value = 0x0005037b; PC = 0x800189c *)
mov r7 L0x20019730;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019750; Value = 0xffffe6c5; PC = 0x80018a0 *)
mov r8 L0x20019750;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019770; Value = 0xfff6d7a6; PC = 0x80018a4 *)
mov r9 L0x20019770;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019790; Value = 0x00056002; PC = 0x80018a8 *)
mov r10 L0x20019790;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200197b0; Value = 0x0000559c; PC = 0x80018ac *)
mov r11 L0x200197b0;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019750; PC = 0x80019cc *)
mov L0x20019750 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019760; PC = 0x80019d0 *)
mov L0x20019760 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019770; PC = 0x80019d4 *)
mov L0x20019770 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019780; PC = 0x80019d8 *)
mov L0x20019780 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019790; PC = 0x80019dc *)
mov L0x20019790 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200197a0; PC = 0x80019e0 *)
mov L0x200197a0 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200197b0; PC = 0x80019e4 *)
mov L0x200197b0 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200197c0; PC = 0x80019e8 *)
mov L0x200197c0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019710; PC = 0x8001a14 *)
mov L0x20019710 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019720; PC = 0x8001a18 *)
mov L0x20019720 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019730; PC = 0x8001a1c *)
mov L0x20019730 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019740; PC = 0x8001a20 *)
mov L0x20019740 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200196e0; PC = 0x8001a24 *)
mov L0x200196e0 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200196f0; PC = 0x8001a28 *)
mov L0x200196f0 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019700; PC = 0x8001a2c *)
mov L0x20019700 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200196d0; PC = 0x8001a30 *)
mov L0x200196d0 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200196e4; Value = 0xfffd46cc; PC = 0x80016ec *)
mov r4 L0x200196e4;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019704; Value = 0x00021322; PC = 0x80016f0 *)
mov r5 L0x20019704;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019724; Value = 0x0003376b; PC = 0x80016f4 *)
mov r6 L0x20019724;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019744; Value = 0xfff667a1; PC = 0x80016f8 *)
mov r7 L0x20019744;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019764; Value = 0xfffc094c; PC = 0x80016fc *)
mov r8 L0x20019764;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019784; Value = 0x000d8929; PC = 0x8001700 *)
mov r9 L0x20019784;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200197a4; Value = 0x00038033; PC = 0x8001704 *)
mov r10 L0x200197a4;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200197c4; Value = 0xffff0788; PC = 0x8001708 *)
mov r11 L0x200197c4;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200196d4; Value = 0xfff6426f; PC = 0x8001890 *)
mov r4 L0x200196d4;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200196f4; Value = 0x0007e458; PC = 0x8001894 *)
mov r5 L0x200196f4;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019714; Value = 0xfffd1cc9; PC = 0x8001898 *)
mov r6 L0x20019714;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019734; Value = 0x00071b75; PC = 0x800189c *)
mov r7 L0x20019734;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019754; Value = 0x0008288b; PC = 0x80018a0 *)
mov r8 L0x20019754;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019774; Value = 0xffffd0cf; PC = 0x80018a4 *)
mov r9 L0x20019774;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019794; Value = 0x0006c1ff; PC = 0x80018a8 *)
mov r10 L0x20019794;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200197b4; Value = 0x0005535b; PC = 0x80018ac *)
mov r11 L0x200197b4;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019754; PC = 0x80019cc *)
mov L0x20019754 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019764; PC = 0x80019d0 *)
mov L0x20019764 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019774; PC = 0x80019d4 *)
mov L0x20019774 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019784; PC = 0x80019d8 *)
mov L0x20019784 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019794; PC = 0x80019dc *)
mov L0x20019794 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200197a4; PC = 0x80019e0 *)
mov L0x200197a4 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200197b4; PC = 0x80019e4 *)
mov L0x200197b4 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200197c4; PC = 0x80019e8 *)
mov L0x200197c4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019714; PC = 0x8001a14 *)
mov L0x20019714 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019724; PC = 0x8001a18 *)
mov L0x20019724 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019734; PC = 0x8001a1c *)
mov L0x20019734 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019744; PC = 0x8001a20 *)
mov L0x20019744 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200196e4; PC = 0x8001a24 *)
mov L0x200196e4 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200196f4; PC = 0x8001a28 *)
mov L0x200196f4 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019704; PC = 0x8001a2c *)
mov L0x20019704 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200196d4; PC = 0x8001a30 *)
mov L0x200196d4 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200196e8; Value = 0x0002f63d; PC = 0x80016ec *)
mov r4 L0x200196e8;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019708; Value = 0xfffd712e; PC = 0x80016f0 *)
mov r5 L0x20019708;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019728; Value = 0x0006d084; PC = 0x80016f4 *)
mov r6 L0x20019728;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019748; Value = 0xfffe56a4; PC = 0x80016f8 *)
mov r7 L0x20019748;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019768; Value = 0x0000e02b; PC = 0x80016fc *)
mov r8 L0x20019768;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019788; Value = 0x0002409e; PC = 0x8001700 *)
mov r9 L0x20019788;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200197a8; Value = 0xfff92be7; PC = 0x8001704 *)
mov r10 L0x200197a8;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200197c8; Value = 0xfff31c5a; PC = 0x8001708 *)
mov r11 L0x200197c8;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200196d8; Value = 0xfffcde32; PC = 0x8001890 *)
mov r4 L0x200196d8;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200196f8; Value = 0xfff63358; PC = 0x8001894 *)
mov r5 L0x200196f8;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019718; Value = 0x000816fd; PC = 0x8001898 *)
mov r6 L0x20019718;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019738; Value = 0x0003779d; PC = 0x800189c *)
mov r7 L0x20019738;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019758; Value = 0xfff87afc; PC = 0x80018a0 *)
mov r8 L0x20019758;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019778; Value = 0xfffef752; PC = 0x80018a4 *)
mov r9 L0x20019778;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019798; Value = 0xffff9760; PC = 0x80018a8 *)
mov r10 L0x20019798;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200197b8; Value = 0xfffa255c; PC = 0x80018ac *)
mov r11 L0x200197b8;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019758; PC = 0x80019cc *)
mov L0x20019758 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019768; PC = 0x80019d0 *)
mov L0x20019768 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019778; PC = 0x80019d4 *)
mov L0x20019778 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019788; PC = 0x80019d8 *)
mov L0x20019788 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019798; PC = 0x80019dc *)
mov L0x20019798 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200197a8; PC = 0x80019e0 *)
mov L0x200197a8 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200197b8; PC = 0x80019e4 *)
mov L0x200197b8 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200197c8; PC = 0x80019e8 *)
mov L0x200197c8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019718; PC = 0x8001a14 *)
mov L0x20019718 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019728; PC = 0x8001a18 *)
mov L0x20019728 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019738; PC = 0x8001a1c *)
mov L0x20019738 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019748; PC = 0x8001a20 *)
mov L0x20019748 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200196e8; PC = 0x8001a24 *)
mov L0x200196e8 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200196f8; PC = 0x8001a28 *)
mov L0x200196f8 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019708; PC = 0x8001a2c *)
mov L0x20019708 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200196d8; PC = 0x8001a30 *)
mov L0x200196d8 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200196ec; Value = 0x0000278d; PC = 0x80016ec *)
mov r4 L0x200196ec;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x2001970c; Value = 0xfff6eaac; PC = 0x80016f0 *)
mov r5 L0x2001970c;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x2001972c; Value = 0xfffe157f; PC = 0x80016f4 *)
mov r6 L0x2001972c;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x2001974c; Value = 0x0008fe04; PC = 0x80016f8 *)
mov r7 L0x2001974c;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x2001976c; Value = 0x0003fb60; PC = 0x80016fc *)
mov r8 L0x2001976c;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x2001978c; Value = 0x0002641f; PC = 0x8001700 *)
mov r9 L0x2001978c;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200197ac; Value = 0xfffddb01; PC = 0x8001704 *)
mov r10 L0x200197ac;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200197cc; Value = 0xfffd72e2; PC = 0x8001708 *)
mov r11 L0x200197cc;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200196dc; Value = 0x000b7330; PC = 0x8001890 *)
mov r4 L0x200196dc;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200196fc; Value = 0x0003618e; PC = 0x8001894 *)
mov r5 L0x200196fc;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x2001971c; Value = 0xfffeb888; PC = 0x8001898 *)
mov r6 L0x2001971c;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x2001973c; Value = 0x000430e6; PC = 0x800189c *)
mov r7 L0x2001973c;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x2001975c; Value = 0xffffda5a; PC = 0x80018a0 *)
mov r8 L0x2001975c;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x2001977c; Value = 0x00089e4b; PC = 0x80018a4 *)
mov r9 L0x2001977c;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x2001979c; Value = 0xfff6d522; PC = 0x80018a8 *)
mov r10 L0x2001979c;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200197bc; Value = 0x0002ea6d; PC = 0x80018ac *)
mov r11 L0x200197bc;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x2001975c; PC = 0x80019cc *)
mov L0x2001975c r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x2001976c; PC = 0x80019d0 *)
mov L0x2001976c r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x2001977c; PC = 0x80019d4 *)
mov L0x2001977c r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x2001978c; PC = 0x80019d8 *)
mov L0x2001978c r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x2001979c; PC = 0x80019dc *)
mov L0x2001979c r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200197ac; PC = 0x80019e0 *)
mov L0x200197ac r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200197bc; PC = 0x80019e4 *)
mov L0x200197bc r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200197cc; PC = 0x80019e8 *)
mov L0x200197cc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x2001971c; PC = 0x8001a14 *)
mov L0x2001971c r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x2001972c; PC = 0x8001a18 *)
mov L0x2001972c r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x2001973c; PC = 0x8001a1c *)
mov L0x2001973c r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x2001974c; PC = 0x8001a20 *)
mov L0x2001974c r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200196ec; PC = 0x8001a24 *)
mov L0x200196ec r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200196fc; PC = 0x8001a28 *)
mov L0x200196fc r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x2001970c; PC = 0x8001a2c *)
mov L0x2001970c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200196dc; PC = 0x8001a30 *)
mov L0x200196dc r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* add.w	r0, r0, #240	; 0xf0                       #! PC = 0x8001a40 *)
adds dontcare r0 r0 240@uint32;
(* vmov	r12, s2                                    #! PC = 0x8001a44 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8001a48 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x80016d8 <_4_5_6_7>                     #! PC = 0x8001a4c *)
#bne.w	0x80016d8 <_4_5_6_7>                     #! 0x8001a4c = 0x8001a4c;
(* vmov	r1, s1                                     #! PC = 0x80016d8 *)
mov r1 s1;
(* vldmia	r1!, {s4-s18}                            #! EA = L0x8006ff8; PC = 0x80016dc *)
mov s4 L0x8006ff8;
mov s5 L0x8006ffc;
mov s6 L0x8007000;
mov s7 L0x8007004;
mov s8 L0x8007008;
mov s9 L0x800700c;
mov s10 L0x8007010;
mov s11 L0x8007014;
mov s12 L0x8007018;
mov s13 L0x800701c;
mov s14 L0x8007020;
mov s15 L0x8007024;
mov s16 L0x8007028;
mov s17 L0x800702c;
mov s18 L0x8007030;
(* vmov	s1, r1                                     #! PC = 0x80016e0 *)
mov s1 r1;
(* add.w	lr, r0, #16                               #! PC = 0x80016e4 *)
adds dontcare lr r0 16@uint32;
(* vmov	s3, lr                                     #! PC = 0x80016e8 *)
mov s3 lr;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200197e0; Value = 0x00060f19; PC = 0x80016ec *)
mov r4 L0x200197e0;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019800; Value = 0xfffcaddb; PC = 0x80016f0 *)
mov r5 L0x20019800;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019820; Value = 0xfff6c77f; PC = 0x80016f4 *)
mov r6 L0x20019820;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019840; Value = 0xfffcad29; PC = 0x80016f8 *)
mov r7 L0x20019840;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019860; Value = 0xfffbbbc0; PC = 0x80016fc *)
mov r8 L0x20019860;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019880; Value = 0x0004d550; PC = 0x8001700 *)
mov r9 L0x20019880;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200198a0; Value = 0xffff4051; PC = 0x8001704 *)
mov r10 L0x200198a0;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200198c0; Value = 0xfff3f255; PC = 0x8001708 *)
mov r11 L0x200198c0;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200197d0; Value = 0x000b70d8; PC = 0x8001890 *)
mov r4 L0x200197d0;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200197f0; Value = 0x00014984; PC = 0x8001894 *)
mov r5 L0x200197f0;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019810; Value = 0x0005009d; PC = 0x8001898 *)
mov r6 L0x20019810;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019830; Value = 0xfff9c119; PC = 0x800189c *)
mov r7 L0x20019830;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019850; Value = 0x000770ae; PC = 0x80018a0 *)
mov r8 L0x20019850;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019870; Value = 0x00069120; PC = 0x80018a4 *)
mov r9 L0x20019870;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019890; Value = 0x00013c88; PC = 0x80018a8 *)
mov r10 L0x20019890;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200198b0; Value = 0xfff80c47; PC = 0x80018ac *)
mov r11 L0x200198b0;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019850; PC = 0x80019cc *)
mov L0x20019850 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019860; PC = 0x80019d0 *)
mov L0x20019860 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019870; PC = 0x80019d4 *)
mov L0x20019870 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019880; PC = 0x80019d8 *)
mov L0x20019880 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019890; PC = 0x80019dc *)
mov L0x20019890 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200198a0; PC = 0x80019e0 *)
mov L0x200198a0 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200198b0; PC = 0x80019e4 *)
mov L0x200198b0 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200198c0; PC = 0x80019e8 *)
mov L0x200198c0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019810; PC = 0x8001a14 *)
mov L0x20019810 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019820; PC = 0x8001a18 *)
mov L0x20019820 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019830; PC = 0x8001a1c *)
mov L0x20019830 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019840; PC = 0x8001a20 *)
mov L0x20019840 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200197e0; PC = 0x8001a24 *)
mov L0x200197e0 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200197f0; PC = 0x8001a28 *)
mov L0x200197f0 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019800; PC = 0x8001a2c *)
mov L0x20019800 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200197d0; PC = 0x8001a30 *)
mov L0x200197d0 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200197e4; Value = 0x0001cb78; PC = 0x80016ec *)
mov r4 L0x200197e4;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019804; Value = 0xfffa4752; PC = 0x80016f0 *)
mov r5 L0x20019804;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019824; Value = 0x00038801; PC = 0x80016f4 *)
mov r6 L0x20019824;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019844; Value = 0x00096ed4; PC = 0x80016f8 *)
mov r7 L0x20019844;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019864; Value = 0x000b6eff; PC = 0x80016fc *)
mov r8 L0x20019864;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019884; Value = 0xfff77e39; PC = 0x8001700 *)
mov r9 L0x20019884;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200198a4; Value = 0xfff5e3fc; PC = 0x8001704 *)
mov r10 L0x200198a4;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200198c4; Value = 0x000a2e48; PC = 0x8001708 *)
mov r11 L0x200198c4;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200197d4; Value = 0x00041456; PC = 0x8001890 *)
mov r4 L0x200197d4;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200197f4; Value = 0x0004fcee; PC = 0x8001894 *)
mov r5 L0x200197f4;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019814; Value = 0xfffd1f8b; PC = 0x8001898 *)
mov r6 L0x20019814;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019834; Value = 0x00089739; PC = 0x800189c *)
mov r7 L0x20019834;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019854; Value = 0x00084cab; PC = 0x80018a0 *)
mov r8 L0x20019854;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019874; Value = 0x000508e4; PC = 0x80018a4 *)
mov r9 L0x20019874;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019894; Value = 0x00005961; PC = 0x80018a8 *)
mov r10 L0x20019894;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200198b4; Value = 0x0000d400; PC = 0x80018ac *)
mov r11 L0x200198b4;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019854; PC = 0x80019cc *)
mov L0x20019854 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019864; PC = 0x80019d0 *)
mov L0x20019864 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019874; PC = 0x80019d4 *)
mov L0x20019874 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019884; PC = 0x80019d8 *)
mov L0x20019884 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019894; PC = 0x80019dc *)
mov L0x20019894 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200198a4; PC = 0x80019e0 *)
mov L0x200198a4 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200198b4; PC = 0x80019e4 *)
mov L0x200198b4 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200198c4; PC = 0x80019e8 *)
mov L0x200198c4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019814; PC = 0x8001a14 *)
mov L0x20019814 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019824; PC = 0x8001a18 *)
mov L0x20019824 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019834; PC = 0x8001a1c *)
mov L0x20019834 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019844; PC = 0x8001a20 *)
mov L0x20019844 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200197e4; PC = 0x8001a24 *)
mov L0x200197e4 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200197f4; PC = 0x8001a28 *)
mov L0x200197f4 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019804; PC = 0x8001a2c *)
mov L0x20019804 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200197d4; PC = 0x8001a30 *)
mov L0x200197d4 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200197e8; Value = 0xfffaf07d; PC = 0x80016ec *)
mov r4 L0x200197e8;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019808; Value = 0xfff52eac; PC = 0x80016f0 *)
mov r5 L0x20019808;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019828; Value = 0x00092f51; PC = 0x80016f4 *)
mov r6 L0x20019828;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019848; Value = 0x00066a68; PC = 0x80016f8 *)
mov r7 L0x20019848;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019868; Value = 0xffef56af; PC = 0x80016fc *)
mov r8 L0x20019868;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019888; Value = 0x00020bb5; PC = 0x8001700 *)
mov r9 L0x20019888;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200198a8; Value = 0x00053ee5; PC = 0x8001704 *)
mov r10 L0x200198a8;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200198c8; Value = 0xfffe1468; PC = 0x8001708 *)
mov r11 L0x200198c8;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200197d8; Value = 0xfff9756c; PC = 0x8001890 *)
mov r4 L0x200197d8;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200197f8; Value = 0xfff48a1a; PC = 0x8001894 *)
mov r5 L0x200197f8;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019818; Value = 0x00046a65; PC = 0x8001898 *)
mov r6 L0x20019818;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019838; Value = 0x00034549; PC = 0x800189c *)
mov r7 L0x20019838;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019858; Value = 0xfffa3009; PC = 0x80018a0 *)
mov r8 L0x20019858;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019878; Value = 0xfffb837d; PC = 0x80018a4 *)
mov r9 L0x20019878;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019898; Value = 0xfff7b1c1; PC = 0x80018a8 *)
mov r10 L0x20019898;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200198b8; Value = 0xfff8f1aa; PC = 0x80018ac *)
mov r11 L0x200198b8;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019858; PC = 0x80019cc *)
mov L0x20019858 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019868; PC = 0x80019d0 *)
mov L0x20019868 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019878; PC = 0x80019d4 *)
mov L0x20019878 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019888; PC = 0x80019d8 *)
mov L0x20019888 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019898; PC = 0x80019dc *)
mov L0x20019898 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200198a8; PC = 0x80019e0 *)
mov L0x200198a8 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200198b8; PC = 0x80019e4 *)
mov L0x200198b8 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200198c8; PC = 0x80019e8 *)
mov L0x200198c8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019818; PC = 0x8001a14 *)
mov L0x20019818 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019828; PC = 0x8001a18 *)
mov L0x20019828 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019838; PC = 0x8001a1c *)
mov L0x20019838 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019848; PC = 0x8001a20 *)
mov L0x20019848 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200197e8; PC = 0x8001a24 *)
mov L0x200197e8 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200197f8; PC = 0x8001a28 *)
mov L0x200197f8 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019808; PC = 0x8001a2c *)
mov L0x20019808 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200197d8; PC = 0x8001a30 *)
mov L0x200197d8 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200197ec; Value = 0xfffe64d0; PC = 0x80016ec *)
mov r4 L0x200197ec;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x2001980c; Value = 0x0004cdc8; PC = 0x80016f0 *)
mov r5 L0x2001980c;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x2001982c; Value = 0xfff7ce6f; PC = 0x80016f4 *)
mov r6 L0x2001982c;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x2001984c; Value = 0xfffe3c8a; PC = 0x80016f8 *)
mov r7 L0x2001984c;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x2001986c; Value = 0x00009986; PC = 0x80016fc *)
mov r8 L0x2001986c;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x2001988c; Value = 0x0005cb58; PC = 0x8001700 *)
mov r9 L0x2001988c;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200198ac; Value = 0x0006d9ee; PC = 0x8001704 *)
mov r10 L0x200198ac;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200198cc; Value = 0xfffd534f; PC = 0x8001708 *)
mov r11 L0x200198cc;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200197dc; Value = 0xfffc6dc0; PC = 0x8001890 *)
mov r4 L0x200197dc;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200197fc; Value = 0xfff2d9e4; PC = 0x8001894 *)
mov r5 L0x200197fc;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x2001981c; Value = 0xfff57f5e; PC = 0x8001898 *)
mov r6 L0x2001981c;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x2001983c; Value = 0xfff9faa3; PC = 0x800189c *)
mov r7 L0x2001983c;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x2001985c; Value = 0x0008635d; PC = 0x80018a0 *)
mov r8 L0x2001985c;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x2001987c; Value = 0x00053ee0; PC = 0x80018a4 *)
mov r9 L0x2001987c;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x2001989c; Value = 0xfff570e1; PC = 0x80018a8 *)
mov r10 L0x2001989c;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200198bc; Value = 0xfff43ccb; PC = 0x80018ac *)
mov r11 L0x200198bc;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x2001985c; PC = 0x80019cc *)
mov L0x2001985c r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x2001986c; PC = 0x80019d0 *)
mov L0x2001986c r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x2001987c; PC = 0x80019d4 *)
mov L0x2001987c r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x2001988c; PC = 0x80019d8 *)
mov L0x2001988c r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x2001989c; PC = 0x80019dc *)
mov L0x2001989c r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200198ac; PC = 0x80019e0 *)
mov L0x200198ac r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200198bc; PC = 0x80019e4 *)
mov L0x200198bc r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200198cc; PC = 0x80019e8 *)
mov L0x200198cc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x2001981c; PC = 0x8001a14 *)
mov L0x2001981c r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x2001982c; PC = 0x8001a18 *)
mov L0x2001982c r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x2001983c; PC = 0x8001a1c *)
mov L0x2001983c r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x2001984c; PC = 0x8001a20 *)
mov L0x2001984c r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200197ec; PC = 0x8001a24 *)
mov L0x200197ec r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200197fc; PC = 0x8001a28 *)
mov L0x200197fc r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x2001980c; PC = 0x8001a2c *)
mov L0x2001980c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200197dc; PC = 0x8001a30 *)
mov L0x200197dc r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* add.w	r0, r0, #240	; 0xf0                       #! PC = 0x8001a40 *)
adds dontcare r0 r0 240@uint32;
(* vmov	r12, s2                                    #! PC = 0x8001a44 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8001a48 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x80016d8 <_4_5_6_7>                     #! PC = 0x8001a4c *)
#bne.w	0x80016d8 <_4_5_6_7>                     #! 0x8001a4c = 0x8001a4c;
(* vmov	r1, s1                                     #! PC = 0x80016d8 *)
mov r1 s1;
(* vldmia	r1!, {s4-s18}                            #! EA = L0x8007034; PC = 0x80016dc *)
mov s4 L0x8007034;
mov s5 L0x8007038;
mov s6 L0x800703c;
mov s7 L0x8007040;
mov s8 L0x8007044;
mov s9 L0x8007048;
mov s10 L0x800704c;
mov s11 L0x8007050;
mov s12 L0x8007054;
mov s13 L0x8007058;
mov s14 L0x800705c;
mov s15 L0x8007060;
mov s16 L0x8007064;
mov s17 L0x8007068;
mov s18 L0x800706c;
(* vmov	s1, r1                                     #! PC = 0x80016e0 *)
mov s1 r1;
(* add.w	lr, r0, #16                               #! PC = 0x80016e4 *)
adds dontcare lr r0 16@uint32;
(* vmov	s3, lr                                     #! PC = 0x80016e8 *)
mov s3 lr;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200198e0; Value = 0x00080ac9; PC = 0x80016ec *)
mov r4 L0x200198e0;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019900; Value = 0xfff23975; PC = 0x80016f0 *)
mov r5 L0x20019900;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019920; Value = 0x00059577; PC = 0x80016f4 *)
mov r6 L0x20019920;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019940; Value = 0xfff4c945; PC = 0x80016f8 *)
mov r7 L0x20019940;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019960; Value = 0xfff20ca6; PC = 0x80016fc *)
mov r8 L0x20019960;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019980; Value = 0xfffd635c; PC = 0x8001700 *)
mov r9 L0x20019980;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200199a0; Value = 0x00016203; PC = 0x8001704 *)
mov r10 L0x200199a0;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200199c0; Value = 0xfffc8071; PC = 0x8001708 *)
mov r11 L0x200199c0;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200198d0; Value = 0x00121b20; PC = 0x8001890 *)
mov r4 L0x200198d0;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200198f0; Value = 0xfff81d20; PC = 0x8001894 *)
mov r5 L0x200198f0;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019910; Value = 0x00018103; PC = 0x8001898 *)
mov r6 L0x20019910;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019930; Value = 0x000504f7; PC = 0x800189c *)
mov r7 L0x20019930;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019950; Value = 0xfffb1e14; PC = 0x80018a0 *)
mov r8 L0x20019950;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019970; Value = 0xfffbb866; PC = 0x80018a4 *)
mov r9 L0x20019970;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019990; Value = 0x0000d824; PC = 0x80018a8 *)
mov r10 L0x20019990;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200199b0; Value = 0xfff6f63d; PC = 0x80018ac *)
mov r11 L0x200199b0;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019950; PC = 0x80019cc *)
mov L0x20019950 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019960; PC = 0x80019d0 *)
mov L0x20019960 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019970; PC = 0x80019d4 *)
mov L0x20019970 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019980; PC = 0x80019d8 *)
mov L0x20019980 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019990; PC = 0x80019dc *)
mov L0x20019990 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200199a0; PC = 0x80019e0 *)
mov L0x200199a0 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200199b0; PC = 0x80019e4 *)
mov L0x200199b0 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200199c0; PC = 0x80019e8 *)
mov L0x200199c0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019910; PC = 0x8001a14 *)
mov L0x20019910 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019920; PC = 0x8001a18 *)
mov L0x20019920 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019930; PC = 0x8001a1c *)
mov L0x20019930 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019940; PC = 0x8001a20 *)
mov L0x20019940 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200198e0; PC = 0x8001a24 *)
mov L0x200198e0 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200198f0; PC = 0x8001a28 *)
mov L0x200198f0 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019900; PC = 0x8001a2c *)
mov L0x20019900 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200198d0; PC = 0x8001a30 *)
mov L0x200198d0 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200198e4; Value = 0x00049750; PC = 0x80016ec *)
mov r4 L0x200198e4;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019904; Value = 0xfffe1014; PC = 0x80016f0 *)
mov r5 L0x20019904;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019924; Value = 0x0001f8d3; PC = 0x80016f4 *)
mov r6 L0x20019924;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019944; Value = 0xfffe7126; PC = 0x80016f8 *)
mov r7 L0x20019944;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019964; Value = 0x00002d71; PC = 0x80016fc *)
mov r8 L0x20019964;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019984; Value = 0x00025f49; PC = 0x8001700 *)
mov r9 L0x20019984;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200199a4; Value = 0xfff7fb6c; PC = 0x8001704 *)
mov r10 L0x200199a4;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200199c4; Value = 0x000c05b4; PC = 0x8001708 *)
mov r11 L0x200199c4;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200198d4; Value = 0x000c83aa; PC = 0x8001890 *)
mov r4 L0x200198d4;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200198f4; Value = 0x0004584a; PC = 0x8001894 *)
mov r5 L0x200198f4;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019914; Value = 0xfffe2b23; PC = 0x8001898 *)
mov r6 L0x20019914;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019934; Value = 0xfffe68bd; PC = 0x800189c *)
mov r7 L0x20019934;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019954; Value = 0x00030473; PC = 0x80018a0 *)
mov r8 L0x20019954;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019974; Value = 0xfffe21d0; PC = 0x80018a4 *)
mov r9 L0x20019974;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019994; Value = 0x000ebf4d; PC = 0x80018a8 *)
mov r10 L0x20019994;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200199b4; Value = 0x00008ce4; PC = 0x80018ac *)
mov r11 L0x200199b4;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019954; PC = 0x80019cc *)
mov L0x20019954 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019964; PC = 0x80019d0 *)
mov L0x20019964 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019974; PC = 0x80019d4 *)
mov L0x20019974 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019984; PC = 0x80019d8 *)
mov L0x20019984 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019994; PC = 0x80019dc *)
mov L0x20019994 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200199a4; PC = 0x80019e0 *)
mov L0x200199a4 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200199b4; PC = 0x80019e4 *)
mov L0x200199b4 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200199c4; PC = 0x80019e8 *)
mov L0x200199c4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019914; PC = 0x8001a14 *)
mov L0x20019914 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019924; PC = 0x8001a18 *)
mov L0x20019924 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019934; PC = 0x8001a1c *)
mov L0x20019934 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019944; PC = 0x8001a20 *)
mov L0x20019944 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200198e4; PC = 0x8001a24 *)
mov L0x200198e4 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200198f4; PC = 0x8001a28 *)
mov L0x200198f4 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019904; PC = 0x8001a2c *)
mov L0x20019904 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200198d4; PC = 0x8001a30 *)
mov L0x200198d4 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200198e8; Value = 0x000102ef; PC = 0x80016ec *)
mov r4 L0x200198e8;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019908; Value = 0xfffb0264; PC = 0x80016f0 *)
mov r5 L0x20019908;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019928; Value = 0x000ccb1d; PC = 0x80016f4 *)
mov r6 L0x20019928;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019948; Value = 0x000bc8ea; PC = 0x80016f8 *)
mov r7 L0x20019948;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019968; Value = 0xfff39e13; PC = 0x80016fc *)
mov r8 L0x20019968;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019988; Value = 0x0006fc87; PC = 0x8001700 *)
mov r9 L0x20019988;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200199a8; Value = 0x00036dcb; PC = 0x8001704 *)
mov r10 L0x200199a8;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200199c8; Value = 0xfff16ee4; PC = 0x8001708 *)
mov r11 L0x200199c8;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200198d8; Value = 0x0001c688; PC = 0x8001890 *)
mov r4 L0x200198d8;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200198f8; Value = 0xfffcb524; PC = 0x8001894 *)
mov r5 L0x200198f8;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019918; Value = 0x000fba1f; PC = 0x8001898 *)
mov r6 L0x20019918;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019938; Value = 0x000ee877; PC = 0x800189c *)
mov r7 L0x20019938;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019958; Value = 0xfff0ff45; PC = 0x80018a0 *)
mov r8 L0x20019958;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019978; Value = 0x00098473; PC = 0x80018a4 *)
mov r9 L0x20019978;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019998; Value = 0x0003a617; PC = 0x80018a8 *)
mov r10 L0x20019998;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200199b8; Value = 0xfff27ec4; PC = 0x80018ac *)
mov r11 L0x200199b8;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019958; PC = 0x80019cc *)
mov L0x20019958 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019968; PC = 0x80019d0 *)
mov L0x20019968 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019978; PC = 0x80019d4 *)
mov L0x20019978 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019988; PC = 0x80019d8 *)
mov L0x20019988 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019998; PC = 0x80019dc *)
mov L0x20019998 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200199a8; PC = 0x80019e0 *)
mov L0x200199a8 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200199b8; PC = 0x80019e4 *)
mov L0x200199b8 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200199c8; PC = 0x80019e8 *)
mov L0x200199c8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019918; PC = 0x8001a14 *)
mov L0x20019918 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019928; PC = 0x8001a18 *)
mov L0x20019928 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019938; PC = 0x8001a1c *)
mov L0x20019938 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019948; PC = 0x8001a20 *)
mov L0x20019948 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200198e8; PC = 0x8001a24 *)
mov L0x200198e8 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200198f8; PC = 0x8001a28 *)
mov L0x200198f8 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019908; PC = 0x8001a2c *)
mov L0x20019908 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200198d8; PC = 0x8001a30 *)
mov L0x200198d8 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200198ec; Value = 0x0001c5c4; PC = 0x80016ec *)
mov r4 L0x200198ec;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x2001990c; Value = 0x0013f01a; PC = 0x80016f0 *)
mov r5 L0x2001990c;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x2001992c; Value = 0x0002d0cb; PC = 0x80016f4 *)
mov r6 L0x2001992c;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x2001994c; Value = 0xfff9f05c; PC = 0x80016f8 *)
mov r7 L0x2001994c;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x2001996c; Value = 0xfffb719e; PC = 0x80016fc *)
mov r8 L0x2001996c;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x2001998c; Value = 0x00098b62; PC = 0x8001700 *)
mov r9 L0x2001998c;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x200199ac; Value = 0x000001c0; PC = 0x8001704 *)
mov r10 L0x200199ac;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x200199cc; Value = 0xffef42d7; PC = 0x8001708 *)
mov r11 L0x200199cc;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200198dc; Value = 0xfff0421a; PC = 0x8001890 *)
mov r4 L0x200198dc;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200198fc; Value = 0xffee67a6; PC = 0x8001894 *)
mov r5 L0x200198fc;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x2001991c; Value = 0x0002b35c; PC = 0x8001898 *)
mov r6 L0x2001991c;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x2001993c; Value = 0x0002e2af; PC = 0x800189c *)
mov r7 L0x2001993c;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x2001995c; Value = 0x00065721; PC = 0x80018a0 *)
mov r8 L0x2001995c;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x2001997c; Value = 0x0002c4b0; PC = 0x80018a4 *)
mov r9 L0x2001997c;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x2001999c; Value = 0x00031c79; PC = 0x80018a8 *)
mov r10 L0x2001999c;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200199bc; Value = 0xfff0b30b; PC = 0x80018ac *)
mov r11 L0x200199bc;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x2001995c; PC = 0x80019cc *)
mov L0x2001995c r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x2001996c; PC = 0x80019d0 *)
mov L0x2001996c r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x2001997c; PC = 0x80019d4 *)
mov L0x2001997c r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x2001998c; PC = 0x80019d8 *)
mov L0x2001998c r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x2001999c; PC = 0x80019dc *)
mov L0x2001999c r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x200199ac; PC = 0x80019e0 *)
mov L0x200199ac r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x200199bc; PC = 0x80019e4 *)
mov L0x200199bc r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x200199cc; PC = 0x80019e8 *)
mov L0x200199cc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x2001991c; PC = 0x8001a14 *)
mov L0x2001991c r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x2001992c; PC = 0x8001a18 *)
mov L0x2001992c r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x2001993c; PC = 0x8001a1c *)
mov L0x2001993c r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x2001994c; PC = 0x8001a20 *)
mov L0x2001994c r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200198ec; PC = 0x8001a24 *)
mov L0x200198ec r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200198fc; PC = 0x8001a28 *)
mov L0x200198fc r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x2001990c; PC = 0x8001a2c *)
mov L0x2001990c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200198dc; PC = 0x8001a30 *)
mov L0x200198dc r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* add.w	r0, r0, #240	; 0xf0                       #! PC = 0x8001a40 *)
adds dontcare r0 r0 240@uint32;
(* vmov	r12, s2                                    #! PC = 0x8001a44 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8001a48 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x80016d8 <_4_5_6_7>                     #! PC = 0x8001a4c *)
#bne.w	0x80016d8 <_4_5_6_7>                     #! 0x8001a4c = 0x8001a4c;
(* vmov	r1, s1                                     #! PC = 0x80016d8 *)
mov r1 s1;
(* vldmia	r1!, {s4-s18}                            #! EA = L0x8007070; PC = 0x80016dc *)
mov s4 L0x8007070;
mov s5 L0x8007074;
mov s6 L0x8007078;
mov s7 L0x800707c;
mov s8 L0x8007080;
mov s9 L0x8007084;
mov s10 L0x8007088;
mov s11 L0x800708c;
mov s12 L0x8007090;
mov s13 L0x8007094;
mov s14 L0x8007098;
mov s15 L0x800709c;
mov s16 L0x80070a0;
mov s17 L0x80070a4;
mov s18 L0x80070a8;
(* vmov	s1, r1                                     #! PC = 0x80016e0 *)
mov s1 r1;
(* add.w	lr, r0, #16                               #! PC = 0x80016e4 *)
adds dontcare lr r0 16@uint32;
(* vmov	s3, lr                                     #! PC = 0x80016e8 *)
mov s3 lr;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200199e0; Value = 0xfffdd47d; PC = 0x80016ec *)
mov r4 L0x200199e0;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019a00; Value = 0xfff594c4; PC = 0x80016f0 *)
mov r5 L0x20019a00;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019a20; Value = 0xfff77ed1; PC = 0x80016f4 *)
mov r6 L0x20019a20;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019a40; Value = 0x000256a1; PC = 0x80016f8 *)
mov r7 L0x20019a40;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019a60; Value = 0xfff3aca3; PC = 0x80016fc *)
mov r8 L0x20019a60;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019a80; Value = 0xfffd3904; PC = 0x8001700 *)
mov r9 L0x20019a80;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019aa0; Value = 0x0000b8cf; PC = 0x8001704 *)
mov r10 L0x20019aa0;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019ac0; Value = 0x00074d05; PC = 0x8001708 *)
mov r11 L0x20019ac0;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200199d0; Value = 0x0000f580; PC = 0x8001890 *)
mov r4 L0x200199d0;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200199f0; Value = 0x00015c6a; PC = 0x8001894 *)
mov r5 L0x200199f0;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019a10; Value = 0xfff70354; PC = 0x8001898 *)
mov r6 L0x20019a10;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019a30; Value = 0x000a7ded; PC = 0x800189c *)
mov r7 L0x20019a30;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019a50; Value = 0xfff75b0a; PC = 0x80018a0 *)
mov r8 L0x20019a50;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019a70; Value = 0x0002b335; PC = 0x80018a4 *)
mov r9 L0x20019a70;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019a90; Value = 0x0009e993; PC = 0x80018a8 *)
mov r10 L0x20019a90;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019ab0; Value = 0x000d47d7; PC = 0x80018ac *)
mov r11 L0x20019ab0;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019a50; PC = 0x80019cc *)
mov L0x20019a50 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019a60; PC = 0x80019d0 *)
mov L0x20019a60 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019a70; PC = 0x80019d4 *)
mov L0x20019a70 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019a80; PC = 0x80019d8 *)
mov L0x20019a80 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019a90; PC = 0x80019dc *)
mov L0x20019a90 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019aa0; PC = 0x80019e0 *)
mov L0x20019aa0 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019ab0; PC = 0x80019e4 *)
mov L0x20019ab0 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019ac0; PC = 0x80019e8 *)
mov L0x20019ac0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019a10; PC = 0x8001a14 *)
mov L0x20019a10 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019a20; PC = 0x8001a18 *)
mov L0x20019a20 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019a30; PC = 0x8001a1c *)
mov L0x20019a30 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019a40; PC = 0x8001a20 *)
mov L0x20019a40 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200199e0; PC = 0x8001a24 *)
mov L0x200199e0 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200199f0; PC = 0x8001a28 *)
mov L0x200199f0 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019a00; PC = 0x8001a2c *)
mov L0x20019a00 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200199d0; PC = 0x8001a30 *)
mov L0x200199d0 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200199e4; Value = 0xfff9f96c; PC = 0x80016ec *)
mov r4 L0x200199e4;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019a04; Value = 0xfffdbc85; PC = 0x80016f0 *)
mov r5 L0x20019a04;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019a24; Value = 0xfffb7a96; PC = 0x80016f4 *)
mov r6 L0x20019a24;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019a44; Value = 0x00132fbc; PC = 0x80016f8 *)
mov r7 L0x20019a44;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019a64; Value = 0xfffac8ef; PC = 0x80016fc *)
mov r8 L0x20019a64;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019a84; Value = 0xfff9ac9a; PC = 0x8001700 *)
mov r9 L0x20019a84;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019aa4; Value = 0xfff8562c; PC = 0x8001704 *)
mov r10 L0x20019aa4;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019ac4; Value = 0x00002b82; PC = 0x8001708 *)
mov r11 L0x20019ac4;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200199d4; Value = 0xfffd4d16; PC = 0x8001890 *)
mov r4 L0x200199d4;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200199f4; Value = 0xfff94480; PC = 0x8001894 *)
mov r5 L0x200199f4;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019a14; Value = 0x000e8ede; PC = 0x8001898 *)
mov r6 L0x20019a14;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019a34; Value = 0xfffe0da1; PC = 0x800189c *)
mov r7 L0x20019a34;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019a54; Value = 0xfffad1a6; PC = 0x80018a0 *)
mov r8 L0x20019a54;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019a74; Value = 0xfffac00a; PC = 0x80018a4 *)
mov r9 L0x20019a74;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019a94; Value = 0x0007cc38; PC = 0x80018a8 *)
mov r10 L0x20019a94;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019ab4; Value = 0xfffb70e9; PC = 0x80018ac *)
mov r11 L0x20019ab4;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019a54; PC = 0x80019cc *)
mov L0x20019a54 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019a64; PC = 0x80019d0 *)
mov L0x20019a64 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019a74; PC = 0x80019d4 *)
mov L0x20019a74 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019a84; PC = 0x80019d8 *)
mov L0x20019a84 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019a94; PC = 0x80019dc *)
mov L0x20019a94 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019aa4; PC = 0x80019e0 *)
mov L0x20019aa4 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019ab4; PC = 0x80019e4 *)
mov L0x20019ab4 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019ac4; PC = 0x80019e8 *)
mov L0x20019ac4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019a14; PC = 0x8001a14 *)
mov L0x20019a14 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019a24; PC = 0x8001a18 *)
mov L0x20019a24 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019a34; PC = 0x8001a1c *)
mov L0x20019a34 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019a44; PC = 0x8001a20 *)
mov L0x20019a44 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200199e4; PC = 0x8001a24 *)
mov L0x200199e4 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200199f4; PC = 0x8001a28 *)
mov L0x200199f4 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019a04; PC = 0x8001a2c *)
mov L0x20019a04 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200199d4; PC = 0x8001a30 *)
mov L0x200199d4 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200199e8; Value = 0x0000c89b; PC = 0x80016ec *)
mov r4 L0x200199e8;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019a08; Value = 0xfffa652a; PC = 0x80016f0 *)
mov r5 L0x20019a08;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019a28; Value = 0x000a3fa2; PC = 0x80016f4 *)
mov r6 L0x20019a28;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019a48; Value = 0x0006cc77; PC = 0x80016f8 *)
mov r7 L0x20019a48;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019a68; Value = 0x0006ad22; PC = 0x80016fc *)
mov r8 L0x20019a68;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019a88; Value = 0xfff3888f; PC = 0x8001700 *)
mov r9 L0x20019a88;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019aa8; Value = 0x0002a1e5; PC = 0x8001704 *)
mov r10 L0x20019aa8;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019ac8; Value = 0xfffb0545; PC = 0x8001708 *)
mov r11 L0x20019ac8;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200199d8; Value = 0xfff97832; PC = 0x8001890 *)
mov r4 L0x200199d8;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200199f8; Value = 0x00062868; PC = 0x8001894 *)
mov r5 L0x200199f8;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019a18; Value = 0x0001e543; PC = 0x8001898 *)
mov r6 L0x20019a18;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019a38; Value = 0xfff905c9; PC = 0x800189c *)
mov r7 L0x20019a38;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019a58; Value = 0x000749f1; PC = 0x80018a0 *)
mov r8 L0x20019a58;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019a78; Value = 0x0002ccb8; PC = 0x80018a4 *)
mov r9 L0x20019a78;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019a98; Value = 0x000ac32e; PC = 0x80018a8 *)
mov r10 L0x20019a98;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019ab8; Value = 0xfffef081; PC = 0x80018ac *)
mov r11 L0x20019ab8;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019a58; PC = 0x80019cc *)
mov L0x20019a58 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019a68; PC = 0x80019d0 *)
mov L0x20019a68 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019a78; PC = 0x80019d4 *)
mov L0x20019a78 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019a88; PC = 0x80019d8 *)
mov L0x20019a88 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019a98; PC = 0x80019dc *)
mov L0x20019a98 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019aa8; PC = 0x80019e0 *)
mov L0x20019aa8 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019ab8; PC = 0x80019e4 *)
mov L0x20019ab8 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019ac8; PC = 0x80019e8 *)
mov L0x20019ac8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019a18; PC = 0x8001a14 *)
mov L0x20019a18 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019a28; PC = 0x8001a18 *)
mov L0x20019a28 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019a38; PC = 0x8001a1c *)
mov L0x20019a38 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019a48; PC = 0x8001a20 *)
mov L0x20019a48 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200199e8; PC = 0x8001a24 *)
mov L0x200199e8 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200199f8; PC = 0x8001a28 *)
mov L0x200199f8 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019a08; PC = 0x8001a2c *)
mov L0x20019a08 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200199d8; PC = 0x8001a30 *)
mov L0x200199d8 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x200199ec; Value = 0xfffb98b6; PC = 0x80016ec *)
mov r4 L0x200199ec;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019a0c; Value = 0xfff9d1f5; PC = 0x80016f0 *)
mov r5 L0x20019a0c;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019a2c; Value = 0x0005bcac; PC = 0x80016f4 *)
mov r6 L0x20019a2c;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019a4c; Value = 0x000543c6; PC = 0x80016f8 *)
mov r7 L0x20019a4c;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019a6c; Value = 0xfff72013; PC = 0x80016fc *)
mov r8 L0x20019a6c;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019a8c; Value = 0x000ccb7d; PC = 0x8001700 *)
mov r9 L0x20019a8c;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019aac; Value = 0x000124e5; PC = 0x8001704 *)
mov r10 L0x20019aac;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019acc; Value = 0x00058ba1; PC = 0x8001708 *)
mov r11 L0x20019acc;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x200199dc; Value = 0xffff79d8; PC = 0x8001890 *)
mov r4 L0x200199dc;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x200199fc; Value = 0x00023307; PC = 0x8001894 *)
mov r5 L0x200199fc;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019a1c; Value = 0xfffbf089; PC = 0x8001898 *)
mov r6 L0x20019a1c;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019a3c; Value = 0xfff8b8aa; PC = 0x800189c *)
mov r7 L0x20019a3c;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019a5c; Value = 0xfffd21ad; PC = 0x80018a0 *)
mov r8 L0x20019a5c;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019a7c; Value = 0xfff58c39; PC = 0x80018a4 *)
mov r9 L0x20019a7c;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019a9c; Value = 0x000f0d52; PC = 0x80018a8 *)
mov r10 L0x20019a9c;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019abc; Value = 0x000678e7; PC = 0x80018ac *)
mov r11 L0x20019abc;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019a5c; PC = 0x80019cc *)
mov L0x20019a5c r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019a6c; PC = 0x80019d0 *)
mov L0x20019a6c r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019a7c; PC = 0x80019d4 *)
mov L0x20019a7c r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019a8c; PC = 0x80019d8 *)
mov L0x20019a8c r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019a9c; PC = 0x80019dc *)
mov L0x20019a9c r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019aac; PC = 0x80019e0 *)
mov L0x20019aac r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019abc; PC = 0x80019e4 *)
mov L0x20019abc r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019acc; PC = 0x80019e8 *)
mov L0x20019acc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019a1c; PC = 0x8001a14 *)
mov L0x20019a1c r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019a2c; PC = 0x8001a18 *)
mov L0x20019a2c r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019a3c; PC = 0x8001a1c *)
mov L0x20019a3c r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019a4c; PC = 0x8001a20 *)
mov L0x20019a4c r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x200199ec; PC = 0x8001a24 *)
mov L0x200199ec r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x200199fc; PC = 0x8001a28 *)
mov L0x200199fc r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019a0c; PC = 0x8001a2c *)
mov L0x20019a0c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200199dc; PC = 0x8001a30 *)
mov L0x200199dc r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* add.w	r0, r0, #240	; 0xf0                       #! PC = 0x8001a40 *)
adds dontcare r0 r0 240@uint32;
(* vmov	r12, s2                                    #! PC = 0x8001a44 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8001a48 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x80016d8 <_4_5_6_7>                     #! PC = 0x8001a4c *)
#bne.w	0x80016d8 <_4_5_6_7>                     #! 0x8001a4c = 0x8001a4c;
(* vmov	r1, s1                                     #! PC = 0x80016d8 *)
mov r1 s1;
(* vldmia	r1!, {s4-s18}                            #! EA = L0x80070ac; PC = 0x80016dc *)
mov s4 L0x80070ac;
mov s5 L0x80070b0;
mov s6 L0x80070b4;
mov s7 L0x80070b8;
mov s8 L0x80070bc;
mov s9 L0x80070c0;
mov s10 L0x80070c4;
mov s11 L0x80070c8;
mov s12 L0x80070cc;
mov s13 L0x80070d0;
mov s14 L0x80070d4;
mov s15 L0x80070d8;
mov s16 L0x80070dc;
mov s17 L0x80070e0;
mov s18 L0x80070e4;
(* vmov	s1, r1                                     #! PC = 0x80016e0 *)
mov s1 r1;
(* add.w	lr, r0, #16                               #! PC = 0x80016e4 *)
adds dontcare lr r0 16@uint32;
(* vmov	s3, lr                                     #! PC = 0x80016e8 *)
mov s3 lr;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20019ae0; Value = 0xfff9eab9; PC = 0x80016ec *)
mov r4 L0x20019ae0;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019b00; Value = 0xffffdce0; PC = 0x80016f0 *)
mov r5 L0x20019b00;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019b20; Value = 0xfff610cd; PC = 0x80016f4 *)
mov r6 L0x20019b20;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019b40; Value = 0xfff48611; PC = 0x80016f8 *)
mov r7 L0x20019b40;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019b60; Value = 0x00007f8b; PC = 0x80016fc *)
mov r8 L0x20019b60;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019b80; Value = 0xfff118dc; PC = 0x8001700 *)
mov r9 L0x20019b80;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019ba0; Value = 0x0007e7b9; PC = 0x8001704 *)
mov r10 L0x20019ba0;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019bc0; Value = 0xfffe7be1; PC = 0x8001708 *)
mov r11 L0x20019bc0;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20019ad0; Value = 0xfffd4e9c; PC = 0x8001890 *)
mov r4 L0x20019ad0;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20019af0; Value = 0x000bdb1a; PC = 0x8001894 *)
mov r5 L0x20019af0;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019b10; Value = 0x00023308; PC = 0x8001898 *)
mov r6 L0x20019b10;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019b30; Value = 0x0011ffd3; PC = 0x800189c *)
mov r7 L0x20019b30;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019b50; Value = 0xfff24af4; PC = 0x80018a0 *)
mov r8 L0x20019b50;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019b70; Value = 0x001110f5; PC = 0x80018a4 *)
mov r9 L0x20019b70;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019b90; Value = 0xfffa6c05; PC = 0x80018a8 *)
mov r10 L0x20019b90;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019bb0; Value = 0xfffed83d; PC = 0x80018ac *)
mov r11 L0x20019bb0;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019b50; PC = 0x80019cc *)
mov L0x20019b50 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019b60; PC = 0x80019d0 *)
mov L0x20019b60 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019b70; PC = 0x80019d4 *)
mov L0x20019b70 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019b80; PC = 0x80019d8 *)
mov L0x20019b80 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019b90; PC = 0x80019dc *)
mov L0x20019b90 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019ba0; PC = 0x80019e0 *)
mov L0x20019ba0 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019bb0; PC = 0x80019e4 *)
mov L0x20019bb0 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019bc0; PC = 0x80019e8 *)
mov L0x20019bc0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019b10; PC = 0x8001a14 *)
mov L0x20019b10 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019b20; PC = 0x8001a18 *)
mov L0x20019b20 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019b30; PC = 0x8001a1c *)
mov L0x20019b30 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019b40; PC = 0x8001a20 *)
mov L0x20019b40 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20019ae0; PC = 0x8001a24 *)
mov L0x20019ae0 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20019af0; PC = 0x8001a28 *)
mov L0x20019af0 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019b00; PC = 0x8001a2c *)
mov L0x20019b00 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019ad0; PC = 0x8001a30 *)
mov L0x20019ad0 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20019ae4; Value = 0xffef38f0; PC = 0x80016ec *)
mov r4 L0x20019ae4;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019b04; Value = 0xfff25011; PC = 0x80016f0 *)
mov r5 L0x20019b04;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019b24; Value = 0xfff0bc56; PC = 0x80016f4 *)
mov r6 L0x20019b24;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019b44; Value = 0x00041b66; PC = 0x80016f8 *)
mov r7 L0x20019b44;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019b64; Value = 0xfffb4e81; PC = 0x80016fc *)
mov r8 L0x20019b64;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019b84; Value = 0xfff084c8; PC = 0x8001700 *)
mov r9 L0x20019b84;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019ba4; Value = 0x0003de38; PC = 0x8001704 *)
mov r10 L0x20019ba4;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019bc4; Value = 0xfffb829a; PC = 0x8001708 *)
mov r11 L0x20019bc4;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20019ad4; Value = 0xfffd0246; PC = 0x8001890 *)
mov r4 L0x20019ad4;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20019af4; Value = 0xfff6837c; PC = 0x8001894 *)
mov r5 L0x20019af4;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019b14; Value = 0x00048774; PC = 0x8001898 *)
mov r6 L0x20019b14;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019b34; Value = 0xfff90c39; PC = 0x800189c *)
mov r7 L0x20019b34;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019b54; Value = 0x00093734; PC = 0x80018a0 *)
mov r8 L0x20019b54;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019b74; Value = 0x00034742; PC = 0x80018a4 *)
mov r9 L0x20019b74;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019b94; Value = 0xfffff97a; PC = 0x80018a8 *)
mov r10 L0x20019b94;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019bb4; Value = 0xffebb01f; PC = 0x80018ac *)
mov r11 L0x20019bb4;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019b54; PC = 0x80019cc *)
mov L0x20019b54 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019b64; PC = 0x80019d0 *)
mov L0x20019b64 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019b74; PC = 0x80019d4 *)
mov L0x20019b74 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019b84; PC = 0x80019d8 *)
mov L0x20019b84 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019b94; PC = 0x80019dc *)
mov L0x20019b94 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019ba4; PC = 0x80019e0 *)
mov L0x20019ba4 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019bb4; PC = 0x80019e4 *)
mov L0x20019bb4 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019bc4; PC = 0x80019e8 *)
mov L0x20019bc4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019b14; PC = 0x8001a14 *)
mov L0x20019b14 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019b24; PC = 0x8001a18 *)
mov L0x20019b24 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019b34; PC = 0x8001a1c *)
mov L0x20019b34 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019b44; PC = 0x8001a20 *)
mov L0x20019b44 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20019ae4; PC = 0x8001a24 *)
mov L0x20019ae4 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20019af4; PC = 0x8001a28 *)
mov L0x20019af4 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019b04; PC = 0x8001a2c *)
mov L0x20019b04 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019ad4; PC = 0x8001a30 *)
mov L0x20019ad4 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20019ae8; Value = 0x00080305; PC = 0x80016ec *)
mov r4 L0x20019ae8;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019b08; Value = 0xffffda56; PC = 0x80016f0 *)
mov r5 L0x20019b08;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019b28; Value = 0xffffa204; PC = 0x80016f4 *)
mov r6 L0x20019b28;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019b48; Value = 0x00023547; PC = 0x80016f8 *)
mov r7 L0x20019b48;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019b68; Value = 0xfff92a28; PC = 0x80016fc *)
mov r8 L0x20019b68;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019b88; Value = 0xfffa4cb1; PC = 0x8001700 *)
mov r9 L0x20019b88;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019ba8; Value = 0x000cbd67; PC = 0x8001704 *)
mov r10 L0x20019ba8;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019bc8; Value = 0x000140f7; PC = 0x8001708 *)
mov r11 L0x20019bc8;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20019ad8; Value = 0x0002df52; PC = 0x8001890 *)
mov r4 L0x20019ad8;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20019af8; Value = 0xfff7672a; PC = 0x8001894 *)
mov r5 L0x20019af8;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019b18; Value = 0xfffe8c25; PC = 0x8001898 *)
mov r6 L0x20019b18;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019b38; Value = 0xfffa243f; PC = 0x800189c *)
mov r7 L0x20019b38;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019b58; Value = 0xfff8d2cd; PC = 0x80018a0 *)
mov r8 L0x20019b58;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019b78; Value = 0x000fe1b0; PC = 0x80018a4 *)
mov r9 L0x20019b78;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019b98; Value = 0x000ea242; PC = 0x80018a8 *)
mov r10 L0x20019b98;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019bb8; Value = 0xfffb06e5; PC = 0x80018ac *)
mov r11 L0x20019bb8;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019b58; PC = 0x80019cc *)
mov L0x20019b58 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019b68; PC = 0x80019d0 *)
mov L0x20019b68 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019b78; PC = 0x80019d4 *)
mov L0x20019b78 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019b88; PC = 0x80019d8 *)
mov L0x20019b88 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019b98; PC = 0x80019dc *)
mov L0x20019b98 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019ba8; PC = 0x80019e0 *)
mov L0x20019ba8 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019bb8; PC = 0x80019e4 *)
mov L0x20019bb8 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019bc8; PC = 0x80019e8 *)
mov L0x20019bc8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019b18; PC = 0x8001a14 *)
mov L0x20019b18 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019b28; PC = 0x8001a18 *)
mov L0x20019b28 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019b38; PC = 0x8001a1c *)
mov L0x20019b38 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019b48; PC = 0x8001a20 *)
mov L0x20019b48 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20019ae8; PC = 0x8001a24 *)
mov L0x20019ae8 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20019af8; PC = 0x8001a28 *)
mov L0x20019af8 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019b08; PC = 0x8001a2c *)
mov L0x20019b08 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019ad8; PC = 0x8001a30 *)
mov L0x20019ad8 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20019aec; Value = 0xfffcc03a; PC = 0x80016ec *)
mov r4 L0x20019aec;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019b0c; Value = 0x000846b9; PC = 0x80016f0 *)
mov r5 L0x20019b0c;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019b2c; Value = 0xfff7baaa; PC = 0x80016f4 *)
mov r6 L0x20019b2c;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019b4c; Value = 0x0008e050; PC = 0x80016f8 *)
mov r7 L0x20019b4c;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019b6c; Value = 0xfff13561; PC = 0x80016fc *)
mov r8 L0x20019b6c;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019b8c; Value = 0xfffec861; PC = 0x8001700 *)
mov r9 L0x20019b8c;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019bac; Value = 0x00026261; PC = 0x8001704 *)
mov r10 L0x20019bac;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019bcc; Value = 0x0000ef1d; PC = 0x8001708 *)
mov r11 L0x20019bcc;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20019adc; Value = 0x000b5552; PC = 0x8001890 *)
mov r4 L0x20019adc;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20019afc; Value = 0xfffed6bb; PC = 0x8001894 *)
mov r5 L0x20019afc;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019b1c; Value = 0x00071c41; PC = 0x8001898 *)
mov r6 L0x20019b1c;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019b3c; Value = 0xffefcf20; PC = 0x800189c *)
mov r7 L0x20019b3c;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019b5c; Value = 0xfff971b9; PC = 0x80018a0 *)
mov r8 L0x20019b5c;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019b7c; Value = 0xfff5da2b; PC = 0x80018a4 *)
mov r9 L0x20019b7c;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019b9c; Value = 0x00050230; PC = 0x80018a8 *)
mov r10 L0x20019b9c;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019bbc; Value = 0xfffdf09f; PC = 0x80018ac *)
mov r11 L0x20019bbc;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019b5c; PC = 0x80019cc *)
mov L0x20019b5c r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019b6c; PC = 0x80019d0 *)
mov L0x20019b6c r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019b7c; PC = 0x80019d4 *)
mov L0x20019b7c r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019b8c; PC = 0x80019d8 *)
mov L0x20019b8c r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019b9c; PC = 0x80019dc *)
mov L0x20019b9c r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019bac; PC = 0x80019e0 *)
mov L0x20019bac r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019bbc; PC = 0x80019e4 *)
mov L0x20019bbc r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019bcc; PC = 0x80019e8 *)
mov L0x20019bcc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019b1c; PC = 0x8001a14 *)
mov L0x20019b1c r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019b2c; PC = 0x8001a18 *)
mov L0x20019b2c r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019b3c; PC = 0x8001a1c *)
mov L0x20019b3c r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019b4c; PC = 0x8001a20 *)
mov L0x20019b4c r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20019aec; PC = 0x8001a24 *)
mov L0x20019aec r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20019afc; PC = 0x8001a28 *)
mov L0x20019afc r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019b0c; PC = 0x8001a2c *)
mov L0x20019b0c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019adc; PC = 0x8001a30 *)
mov L0x20019adc r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* add.w	r0, r0, #240	; 0xf0                       #! PC = 0x8001a40 *)
adds dontcare r0 r0 240@uint32;
(* vmov	r12, s2                                    #! PC = 0x8001a44 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8001a48 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x80016d8 <_4_5_6_7>                     #! PC = 0x8001a4c *)
#bne.w	0x80016d8 <_4_5_6_7>                     #! 0x8001a4c = 0x8001a4c;
(* vmov	r1, s1                                     #! PC = 0x80016d8 *)
mov r1 s1;
(* vldmia	r1!, {s4-s18}                            #! EA = L0x80070e8; PC = 0x80016dc *)
mov s4 L0x80070e8;
mov s5 L0x80070ec;
mov s6 L0x80070f0;
mov s7 L0x80070f4;
mov s8 L0x80070f8;
mov s9 L0x80070fc;
mov s10 L0x8007100;
mov s11 L0x8007104;
mov s12 L0x8007108;
mov s13 L0x800710c;
mov s14 L0x8007110;
mov s15 L0x8007114;
mov s16 L0x8007118;
mov s17 L0x800711c;
mov s18 L0x8007120;
(* vmov	s1, r1                                     #! PC = 0x80016e0 *)
mov s1 r1;
(* add.w	lr, r0, #16                               #! PC = 0x80016e4 *)
adds dontcare lr r0 16@uint32;
(* vmov	s3, lr                                     #! PC = 0x80016e8 *)
mov s3 lr;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20019be0; Value = 0x00082788; PC = 0x80016ec *)
mov r4 L0x20019be0;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019c00; Value = 0x000aa0a0; PC = 0x80016f0 *)
mov r5 L0x20019c00;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019c20; Value = 0xfffec215; PC = 0x80016f4 *)
mov r6 L0x20019c20;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019c40; Value = 0x00006295; PC = 0x80016f8 *)
mov r7 L0x20019c40;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019c60; Value = 0x000a5bdc; PC = 0x80016fc *)
mov r8 L0x20019c60;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019c80; Value = 0x000ebf40; PC = 0x8001700 *)
mov r9 L0x20019c80;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019ca0; Value = 0xfffd52b0; PC = 0x8001704 *)
mov r10 L0x20019ca0;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019cc0; Value = 0x0000b3e4; PC = 0x8001708 *)
mov r11 L0x20019cc0;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20019bd0; Value = 0xfff728a3; PC = 0x8001890 *)
mov r4 L0x20019bd0;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20019bf0; Value = 0x0002764b; PC = 0x8001894 *)
mov r5 L0x20019bf0;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019c10; Value = 0xfff79de0; PC = 0x8001898 *)
mov r6 L0x20019c10;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019c30; Value = 0xfff2b60b; PC = 0x800189c *)
mov r7 L0x20019c30;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019c50; Value = 0x00085347; PC = 0x80018a0 *)
mov r8 L0x20019c50;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019c70; Value = 0xfffa0bb3; PC = 0x80018a4 *)
mov r9 L0x20019c70;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019c90; Value = 0xfff47f34; PC = 0x80018a8 *)
mov r10 L0x20019c90;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019cb0; Value = 0xfff7997d; PC = 0x80018ac *)
mov r11 L0x20019cb0;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019c50; PC = 0x80019cc *)
mov L0x20019c50 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019c60; PC = 0x80019d0 *)
mov L0x20019c60 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019c70; PC = 0x80019d4 *)
mov L0x20019c70 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019c80; PC = 0x80019d8 *)
mov L0x20019c80 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019c90; PC = 0x80019dc *)
mov L0x20019c90 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019ca0; PC = 0x80019e0 *)
mov L0x20019ca0 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019cb0; PC = 0x80019e4 *)
mov L0x20019cb0 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019cc0; PC = 0x80019e8 *)
mov L0x20019cc0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019c10; PC = 0x8001a14 *)
mov L0x20019c10 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019c20; PC = 0x8001a18 *)
mov L0x20019c20 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019c30; PC = 0x8001a1c *)
mov L0x20019c30 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019c40; PC = 0x8001a20 *)
mov L0x20019c40 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20019be0; PC = 0x8001a24 *)
mov L0x20019be0 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20019bf0; PC = 0x8001a28 *)
mov L0x20019bf0 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019c00; PC = 0x8001a2c *)
mov L0x20019c00 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019bd0; PC = 0x8001a30 *)
mov L0x20019bd0 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20019be4; Value = 0x00084c4d; PC = 0x80016ec *)
mov r4 L0x20019be4;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019c04; Value = 0x000a75db; PC = 0x80016f0 *)
mov r5 L0x20019c04;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019c24; Value = 0x00096640; PC = 0x80016f4 *)
mov r6 L0x20019c24;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019c44; Value = 0x00067a94; PC = 0x80016f8 *)
mov r7 L0x20019c44;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019c64; Value = 0x0003b8a5; PC = 0x80016fc *)
mov r8 L0x20019c64;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019c84; Value = 0x000e26ba; PC = 0x8001700 *)
mov r9 L0x20019c84;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019ca4; Value = 0x00068613; PC = 0x8001704 *)
mov r10 L0x20019ca4;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019cc4; Value = 0xfff73f09; PC = 0x8001708 *)
mov r11 L0x20019cc4;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20019bd4; Value = 0x0003135c; PC = 0x8001890 *)
mov r4 L0x20019bd4;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20019bf4; Value = 0x00031bdd; PC = 0x8001894 *)
mov r5 L0x20019bf4;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019c14; Value = 0x0002d1b5; PC = 0x8001898 *)
mov r6 L0x20019c14;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019c34; Value = 0x0008cf6d; PC = 0x800189c *)
mov r7 L0x20019c34;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019c54; Value = 0xfff83b43; PC = 0x80018a0 *)
mov r8 L0x20019c54;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019c74; Value = 0xffff56d9; PC = 0x80018a4 *)
mov r9 L0x20019c74;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019c94; Value = 0xffeffd28; PC = 0x80018a8 *)
mov r10 L0x20019c94;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019cb4; Value = 0x0000d37e; PC = 0x80018ac *)
mov r11 L0x20019cb4;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019c54; PC = 0x80019cc *)
mov L0x20019c54 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019c64; PC = 0x80019d0 *)
mov L0x20019c64 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019c74; PC = 0x80019d4 *)
mov L0x20019c74 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019c84; PC = 0x80019d8 *)
mov L0x20019c84 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019c94; PC = 0x80019dc *)
mov L0x20019c94 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019ca4; PC = 0x80019e0 *)
mov L0x20019ca4 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019cb4; PC = 0x80019e4 *)
mov L0x20019cb4 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019cc4; PC = 0x80019e8 *)
mov L0x20019cc4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019c14; PC = 0x8001a14 *)
mov L0x20019c14 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019c24; PC = 0x8001a18 *)
mov L0x20019c24 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019c34; PC = 0x8001a1c *)
mov L0x20019c34 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019c44; PC = 0x8001a20 *)
mov L0x20019c44 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20019be4; PC = 0x8001a24 *)
mov L0x20019be4 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20019bf4; PC = 0x8001a28 *)
mov L0x20019bf4 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019c04; PC = 0x8001a2c *)
mov L0x20019c04 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019bd4; PC = 0x8001a30 *)
mov L0x20019bd4 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20019be8; Value = 0x0000af33; PC = 0x80016ec *)
mov r4 L0x20019be8;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019c08; Value = 0x000be2cc; PC = 0x80016f0 *)
mov r5 L0x20019c08;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019c28; Value = 0xfffba2d7; PC = 0x80016f4 *)
mov r6 L0x20019c28;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019c48; Value = 0xfff52ebe; PC = 0x80016f8 *)
mov r7 L0x20019c48;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019c68; Value = 0xfff9f7ab; PC = 0x80016fc *)
mov r8 L0x20019c68;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019c88; Value = 0xfffd4ef8; PC = 0x8001700 *)
mov r9 L0x20019c88;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019ca8; Value = 0xfff8226f; PC = 0x8001704 *)
mov r10 L0x20019ca8;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019cc8; Value = 0x00036c95; PC = 0x8001708 *)
mov r11 L0x20019cc8;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20019bd8; Value = 0x000b18ba; PC = 0x8001890 *)
mov r4 L0x20019bd8;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20019bf8; Value = 0x00077a72; PC = 0x8001894 *)
mov r5 L0x20019bf8;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019c18; Value = 0xfffdbcbc; PC = 0x8001898 *)
mov r6 L0x20019c18;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019c38; Value = 0x0000bdd0; PC = 0x800189c *)
mov r7 L0x20019c38;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019c58; Value = 0x00035e81; PC = 0x80018a0 *)
mov r8 L0x20019c58;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019c78; Value = 0x0005a465; PC = 0x80018a4 *)
mov r9 L0x20019c78;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019c98; Value = 0xfff75a62; PC = 0x80018a8 *)
mov r10 L0x20019c98;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019cb8; Value = 0x000b7a63; PC = 0x80018ac *)
mov r11 L0x20019cb8;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019c58; PC = 0x80019cc *)
mov L0x20019c58 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019c68; PC = 0x80019d0 *)
mov L0x20019c68 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019c78; PC = 0x80019d4 *)
mov L0x20019c78 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019c88; PC = 0x80019d8 *)
mov L0x20019c88 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019c98; PC = 0x80019dc *)
mov L0x20019c98 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019ca8; PC = 0x80019e0 *)
mov L0x20019ca8 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019cb8; PC = 0x80019e4 *)
mov L0x20019cb8 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019cc8; PC = 0x80019e8 *)
mov L0x20019cc8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019c18; PC = 0x8001a14 *)
mov L0x20019c18 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019c28; PC = 0x8001a18 *)
mov L0x20019c28 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019c38; PC = 0x8001a1c *)
mov L0x20019c38 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019c48; PC = 0x8001a20 *)
mov L0x20019c48 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20019be8; PC = 0x8001a24 *)
mov L0x20019be8 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20019bf8; PC = 0x8001a28 *)
mov L0x20019bf8 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019c08; PC = 0x8001a2c *)
mov L0x20019c08 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019bd8; PC = 0x8001a30 *)
mov L0x20019bd8 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20019bec; Value = 0x0001edb5; PC = 0x80016ec *)
mov r4 L0x20019bec;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019c0c; Value = 0x00043649; PC = 0x80016f0 *)
mov r5 L0x20019c0c;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019c2c; Value = 0x000df17d; PC = 0x80016f4 *)
mov r6 L0x20019c2c;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019c4c; Value = 0x00014a64; PC = 0x80016f8 *)
mov r7 L0x20019c4c;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019c6c; Value = 0xffffcbbd; PC = 0x80016fc *)
mov r8 L0x20019c6c;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019c8c; Value = 0xffee094a; PC = 0x8001700 *)
mov r9 L0x20019c8c;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019cac; Value = 0xfff421a2; PC = 0x8001704 *)
mov r10 L0x20019cac;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019ccc; Value = 0x0000320e; PC = 0x8001708 *)
mov r11 L0x20019ccc;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20019bdc; Value = 0xfff51f17; PC = 0x8001890 *)
mov r4 L0x20019bdc;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20019bfc; Value = 0x0007009e; PC = 0x8001894 *)
mov r5 L0x20019bfc;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019c1c; Value = 0x000146b4; PC = 0x8001898 *)
mov r6 L0x20019c1c;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019c3c; Value = 0x0008ac84; PC = 0x800189c *)
mov r7 L0x20019c3c;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019c5c; Value = 0xfffa92ec; PC = 0x80018a0 *)
mov r8 L0x20019c5c;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019c7c; Value = 0xffff021f; PC = 0x80018a4 *)
mov r9 L0x20019c7c;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019c9c; Value = 0xfff1aa6f; PC = 0x80018a8 *)
mov r10 L0x20019c9c;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019cbc; Value = 0x00090423; PC = 0x80018ac *)
mov r11 L0x20019cbc;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019c5c; PC = 0x80019cc *)
mov L0x20019c5c r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019c6c; PC = 0x80019d0 *)
mov L0x20019c6c r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019c7c; PC = 0x80019d4 *)
mov L0x20019c7c r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019c8c; PC = 0x80019d8 *)
mov L0x20019c8c r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019c9c; PC = 0x80019dc *)
mov L0x20019c9c r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019cac; PC = 0x80019e0 *)
mov L0x20019cac r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019cbc; PC = 0x80019e4 *)
mov L0x20019cbc r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019ccc; PC = 0x80019e8 *)
mov L0x20019ccc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019c1c; PC = 0x8001a14 *)
mov L0x20019c1c r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019c2c; PC = 0x8001a18 *)
mov L0x20019c2c r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019c3c; PC = 0x8001a1c *)
mov L0x20019c3c r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019c4c; PC = 0x8001a20 *)
mov L0x20019c4c r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20019bec; PC = 0x8001a24 *)
mov L0x20019bec r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20019bfc; PC = 0x8001a28 *)
mov L0x20019bfc r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019c0c; PC = 0x8001a2c *)
mov L0x20019c0c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019bdc; PC = 0x8001a30 *)
mov L0x20019bdc r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* add.w	r0, r0, #240	; 0xf0                       #! PC = 0x8001a40 *)
adds dontcare r0 r0 240@uint32;
(* vmov	r12, s2                                    #! PC = 0x8001a44 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8001a48 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x80016d8 <_4_5_6_7>                     #! PC = 0x8001a4c *)
#bne.w	0x80016d8 <_4_5_6_7>                     #! 0x8001a4c = 0x8001a4c;
(* vmov	r1, s1                                     #! PC = 0x80016d8 *)
mov r1 s1;
(* vldmia	r1!, {s4-s18}                            #! EA = L0x8007124; PC = 0x80016dc *)
mov s4 L0x8007124;
mov s5 L0x8007128;
mov s6 L0x800712c;
mov s7 L0x8007130;
mov s8 L0x8007134;
mov s9 L0x8007138;
mov s10 L0x800713c;
mov s11 L0x8007140;
mov s12 L0x8007144;
mov s13 L0x8007148;
mov s14 L0x800714c;
mov s15 L0x8007150;
mov s16 L0x8007154;
mov s17 L0x8007158;
mov s18 L0x800715c;
(* vmov	s1, r1                                     #! PC = 0x80016e0 *)
mov s1 r1;
(* add.w	lr, r0, #16                               #! PC = 0x80016e4 *)
adds dontcare lr r0 16@uint32;
(* vmov	s3, lr                                     #! PC = 0x80016e8 *)
mov s3 lr;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20019ce0; Value = 0xfffdaa12; PC = 0x80016ec *)
mov r4 L0x20019ce0;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019d00; Value = 0xfffc448e; PC = 0x80016f0 *)
mov r5 L0x20019d00;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019d20; Value = 0x000ad927; PC = 0x80016f4 *)
mov r6 L0x20019d20;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019d40; Value = 0x00009adf; PC = 0x80016f8 *)
mov r7 L0x20019d40;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019d60; Value = 0x00112faa; PC = 0x80016fc *)
mov r8 L0x20019d60;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019d80; Value = 0x00052e3e; PC = 0x8001700 *)
mov r9 L0x20019d80;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019da0; Value = 0xffefe0d2; PC = 0x8001704 *)
mov r10 L0x20019da0;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019dc0; Value = 0x0003ef50; PC = 0x8001708 *)
mov r11 L0x20019dc0;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20019cd0; Value = 0xfff14de3; PC = 0x8001890 *)
mov r4 L0x20019cd0;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20019cf0; Value = 0x00058d23; PC = 0x8001894 *)
mov r5 L0x20019cf0;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019d10; Value = 0x00049884; PC = 0x8001898 *)
mov r6 L0x20019d10;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019d30; Value = 0x000292b5; PC = 0x800189c *)
mov r7 L0x20019d30;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019d50; Value = 0xfffa9ccb; PC = 0x80018a0 *)
mov r8 L0x20019d50;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019d70; Value = 0xfff2ff2f; PC = 0x80018a4 *)
mov r9 L0x20019d70;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019d90; Value = 0xffffaf1c; PC = 0x80018a8 *)
mov r10 L0x20019d90;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019db0; Value = 0xffff4ed5; PC = 0x80018ac *)
mov r11 L0x20019db0;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019d50; PC = 0x80019cc *)
mov L0x20019d50 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019d60; PC = 0x80019d0 *)
mov L0x20019d60 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019d70; PC = 0x80019d4 *)
mov L0x20019d70 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019d80; PC = 0x80019d8 *)
mov L0x20019d80 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019d90; PC = 0x80019dc *)
mov L0x20019d90 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019da0; PC = 0x80019e0 *)
mov L0x20019da0 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019db0; PC = 0x80019e4 *)
mov L0x20019db0 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019dc0; PC = 0x80019e8 *)
mov L0x20019dc0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019d10; PC = 0x8001a14 *)
mov L0x20019d10 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019d20; PC = 0x8001a18 *)
mov L0x20019d20 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019d30; PC = 0x8001a1c *)
mov L0x20019d30 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019d40; PC = 0x8001a20 *)
mov L0x20019d40 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20019ce0; PC = 0x8001a24 *)
mov L0x20019ce0 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20019cf0; PC = 0x8001a28 *)
mov L0x20019cf0 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019d00; PC = 0x8001a2c *)
mov L0x20019d00 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019cd0; PC = 0x8001a30 *)
mov L0x20019cd0 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20019ce4; Value = 0x000aaddb; PC = 0x80016ec *)
mov r4 L0x20019ce4;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019d04; Value = 0x0007a2a7; PC = 0x80016f0 *)
mov r5 L0x20019d04;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019d24; Value = 0x000bde34; PC = 0x80016f4 *)
mov r6 L0x20019d24;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019d44; Value = 0xfff98c98; PC = 0x80016f8 *)
mov r7 L0x20019d44;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019d64; Value = 0x000afd9b; PC = 0x80016fc *)
mov r8 L0x20019d64;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019d84; Value = 0x00003934; PC = 0x8001700 *)
mov r9 L0x20019d84;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019da4; Value = 0xfff8c117; PC = 0x8001704 *)
mov r10 L0x20019da4;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019dc4; Value = 0xffffadb1; PC = 0x8001708 *)
mov r11 L0x20019dc4;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20019cd4; Value = 0x0005ee78; PC = 0x8001890 *)
mov r4 L0x20019cd4;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20019cf4; Value = 0xffff0e31; PC = 0x8001894 *)
mov r5 L0x20019cf4;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019d14; Value = 0x00055f47; PC = 0x8001898 *)
mov r6 L0x20019d14;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019d34; Value = 0x0004c881; PC = 0x800189c *)
mov r7 L0x20019d34;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019d54; Value = 0x0003a265; PC = 0x80018a0 *)
mov r8 L0x20019d54;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019d74; Value = 0xfff7a363; PC = 0x80018a4 *)
mov r9 L0x20019d74;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019d94; Value = 0xfff8bc72; PC = 0x80018a8 *)
mov r10 L0x20019d94;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019db4; Value = 0x000a59e0; PC = 0x80018ac *)
mov r11 L0x20019db4;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019d54; PC = 0x80019cc *)
mov L0x20019d54 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019d64; PC = 0x80019d0 *)
mov L0x20019d64 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019d74; PC = 0x80019d4 *)
mov L0x20019d74 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019d84; PC = 0x80019d8 *)
mov L0x20019d84 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019d94; PC = 0x80019dc *)
mov L0x20019d94 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019da4; PC = 0x80019e0 *)
mov L0x20019da4 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019db4; PC = 0x80019e4 *)
mov L0x20019db4 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019dc4; PC = 0x80019e8 *)
mov L0x20019dc4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019d14; PC = 0x8001a14 *)
mov L0x20019d14 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019d24; PC = 0x8001a18 *)
mov L0x20019d24 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019d34; PC = 0x8001a1c *)
mov L0x20019d34 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019d44; PC = 0x8001a20 *)
mov L0x20019d44 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20019ce4; PC = 0x8001a24 *)
mov L0x20019ce4 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20019cf4; PC = 0x8001a28 *)
mov L0x20019cf4 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019d04; PC = 0x8001a2c *)
mov L0x20019d04 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019cd4; PC = 0x8001a30 *)
mov L0x20019cd4 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20019ce8; Value = 0xfff2057b; PC = 0x80016ec *)
mov r4 L0x20019ce8;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019d08; Value = 0x00023f00; PC = 0x80016f0 *)
mov r5 L0x20019d08;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019d28; Value = 0xfff3d0ef; PC = 0x80016f4 *)
mov r6 L0x20019d28;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019d48; Value = 0x00035be8; PC = 0x80016f8 *)
mov r7 L0x20019d48;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019d68; Value = 0x0007ce27; PC = 0x80016fc *)
mov r8 L0x20019d68;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019d88; Value = 0x0006aaa0; PC = 0x8001700 *)
mov r9 L0x20019d88;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019da8; Value = 0xfff6ec33; PC = 0x8001704 *)
mov r10 L0x20019da8;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019dc8; Value = 0x000b8401; PC = 0x8001708 *)
mov r11 L0x20019dc8;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20019cd8; Value = 0xfffc3cb6; PC = 0x8001890 *)
mov r4 L0x20019cd8;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20019cf8; Value = 0x00043f16; PC = 0x8001894 *)
mov r5 L0x20019cf8;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019d18; Value = 0xfff1d186; PC = 0x8001898 *)
mov r6 L0x20019d18;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019d38; Value = 0xfff99fc4; PC = 0x800189c *)
mov r7 L0x20019d38;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019d58; Value = 0x001163a1; PC = 0x80018a0 *)
mov r8 L0x20019d58;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019d78; Value = 0xfff8e425; PC = 0x80018a4 *)
mov r9 L0x20019d78;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019d98; Value = 0xfff94dfe; PC = 0x80018a8 *)
mov r10 L0x20019d98;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019db8; Value = 0x0007e4c3; PC = 0x80018ac *)
mov r11 L0x20019db8;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019d58; PC = 0x80019cc *)
mov L0x20019d58 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019d68; PC = 0x80019d0 *)
mov L0x20019d68 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019d78; PC = 0x80019d4 *)
mov L0x20019d78 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019d88; PC = 0x80019d8 *)
mov L0x20019d88 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019d98; PC = 0x80019dc *)
mov L0x20019d98 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019da8; PC = 0x80019e0 *)
mov L0x20019da8 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019db8; PC = 0x80019e4 *)
mov L0x20019db8 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019dc8; PC = 0x80019e8 *)
mov L0x20019dc8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019d18; PC = 0x8001a14 *)
mov L0x20019d18 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019d28; PC = 0x8001a18 *)
mov L0x20019d28 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019d38; PC = 0x8001a1c *)
mov L0x20019d38 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019d48; PC = 0x8001a20 *)
mov L0x20019d48 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20019ce8; PC = 0x8001a24 *)
mov L0x20019ce8 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20019cf8; PC = 0x8001a28 *)
mov L0x20019cf8 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019d08; PC = 0x8001a2c *)
mov L0x20019d08 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019cd8; PC = 0x8001a30 *)
mov L0x20019cd8 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20019cec; Value = 0x00006763; PC = 0x80016ec *)
mov r4 L0x20019cec;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019d0c; Value = 0xfffcbdfd; PC = 0x80016f0 *)
mov r5 L0x20019d0c;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019d2c; Value = 0x0001553b; PC = 0x80016f4 *)
mov r6 L0x20019d2c;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019d4c; Value = 0x00081516; PC = 0x80016f8 *)
mov r7 L0x20019d4c;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019d6c; Value = 0xffff77cb; PC = 0x80016fc *)
mov r8 L0x20019d6c;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019d8c; Value = 0xfff50700; PC = 0x8001700 *)
mov r9 L0x20019d8c;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019dac; Value = 0xfff9c39e; PC = 0x8001704 *)
mov r10 L0x20019dac;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019dcc; Value = 0x000cd9de; PC = 0x8001708 *)
mov r11 L0x20019dcc;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20019cdc; Value = 0xffffba79; PC = 0x8001890 *)
mov r4 L0x20019cdc;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20019cfc; Value = 0xfffca820; PC = 0x8001894 *)
mov r5 L0x20019cfc;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019d1c; Value = 0xfffd9b92; PC = 0x8001898 *)
mov r6 L0x20019d1c;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019d3c; Value = 0x0013e43e; PC = 0x800189c *)
mov r7 L0x20019d3c;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019d5c; Value = 0xfff8b6fe; PC = 0x80018a0 *)
mov r8 L0x20019d5c;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019d7c; Value = 0x000bc0a3; PC = 0x80018a4 *)
mov r9 L0x20019d7c;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019d9c; Value = 0xfff94223; PC = 0x80018a8 *)
mov r10 L0x20019d9c;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019dbc; Value = 0xfffcecb7; PC = 0x80018ac *)
mov r11 L0x20019dbc;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019d5c; PC = 0x80019cc *)
mov L0x20019d5c r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019d6c; PC = 0x80019d0 *)
mov L0x20019d6c r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019d7c; PC = 0x80019d4 *)
mov L0x20019d7c r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019d8c; PC = 0x80019d8 *)
mov L0x20019d8c r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019d9c; PC = 0x80019dc *)
mov L0x20019d9c r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019dac; PC = 0x80019e0 *)
mov L0x20019dac r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019dbc; PC = 0x80019e4 *)
mov L0x20019dbc r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019dcc; PC = 0x80019e8 *)
mov L0x20019dcc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019d1c; PC = 0x8001a14 *)
mov L0x20019d1c r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019d2c; PC = 0x8001a18 *)
mov L0x20019d2c r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019d3c; PC = 0x8001a1c *)
mov L0x20019d3c r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019d4c; PC = 0x8001a20 *)
mov L0x20019d4c r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20019cec; PC = 0x8001a24 *)
mov L0x20019cec r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20019cfc; PC = 0x8001a28 *)
mov L0x20019cfc r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019d0c; PC = 0x8001a2c *)
mov L0x20019d0c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019cdc; PC = 0x8001a30 *)
mov L0x20019cdc r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* add.w	r0, r0, #240	; 0xf0                       #! PC = 0x8001a40 *)
adds dontcare r0 r0 240@uint32;
(* vmov	r12, s2                                    #! PC = 0x8001a44 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8001a48 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x80016d8 <_4_5_6_7>                     #! PC = 0x8001a4c *)
#bne.w	0x80016d8 <_4_5_6_7>                     #! 0x8001a4c = 0x8001a4c;
(* vmov	r1, s1                                     #! PC = 0x80016d8 *)
mov r1 s1;
(* vldmia	r1!, {s4-s18}                            #! EA = L0x8007160; PC = 0x80016dc *)
mov s4 L0x8007160;
mov s5 L0x8007164;
mov s6 L0x8007168;
mov s7 L0x800716c;
mov s8 L0x8007170;
mov s9 L0x8007174;
mov s10 L0x8007178;
mov s11 L0x800717c;
mov s12 L0x8007180;
mov s13 L0x8007184;
mov s14 L0x8007188;
mov s15 L0x800718c;
mov s16 L0x8007190;
mov s17 L0x8007194;
mov s18 L0x8007198;
(* vmov	s1, r1                                     #! PC = 0x80016e0 *)
mov s1 r1;
(* add.w	lr, r0, #16                               #! PC = 0x80016e4 *)
adds dontcare lr r0 16@uint32;
(* vmov	s3, lr                                     #! PC = 0x80016e8 *)
mov s3 lr;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20019de0; Value = 0xfff64489; PC = 0x80016ec *)
mov r4 L0x20019de0;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019e00; Value = 0x00034792; PC = 0x80016f0 *)
mov r5 L0x20019e00;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019e20; Value = 0x000a1fc3; PC = 0x80016f4 *)
mov r6 L0x20019e20;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019e40; Value = 0x00107841; PC = 0x80016f8 *)
mov r7 L0x20019e40;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019e60; Value = 0xfffff39b; PC = 0x80016fc *)
mov r8 L0x20019e60;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019e80; Value = 0xfffdb375; PC = 0x8001700 *)
mov r9 L0x20019e80;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019ea0; Value = 0x000a7d48; PC = 0x8001704 *)
mov r10 L0x20019ea0;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019ec0; Value = 0x000a6cf1; PC = 0x8001708 *)
mov r11 L0x20019ec0;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20019dd0; Value = 0xfffb0ab3; PC = 0x8001890 *)
mov r4 L0x20019dd0;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20019df0; Value = 0xfffbccbf; PC = 0x8001894 *)
mov r5 L0x20019df0;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019e10; Value = 0xffff1754; PC = 0x8001898 *)
mov r6 L0x20019e10;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019e30; Value = 0xfffae6ef; PC = 0x800189c *)
mov r7 L0x20019e30;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019e50; Value = 0x00099f12; PC = 0x80018a0 *)
mov r8 L0x20019e50;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019e70; Value = 0xfffb0fdf; PC = 0x80018a4 *)
mov r9 L0x20019e70;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019e90; Value = 0x0003aa35; PC = 0x80018a8 *)
mov r10 L0x20019e90;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019eb0; Value = 0x00087c79; PC = 0x80018ac *)
mov r11 L0x20019eb0;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019e50; PC = 0x80019cc *)
mov L0x20019e50 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019e60; PC = 0x80019d0 *)
mov L0x20019e60 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019e70; PC = 0x80019d4 *)
mov L0x20019e70 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019e80; PC = 0x80019d8 *)
mov L0x20019e80 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019e90; PC = 0x80019dc *)
mov L0x20019e90 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019ea0; PC = 0x80019e0 *)
mov L0x20019ea0 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019eb0; PC = 0x80019e4 *)
mov L0x20019eb0 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019ec0; PC = 0x80019e8 *)
mov L0x20019ec0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019e10; PC = 0x8001a14 *)
mov L0x20019e10 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019e20; PC = 0x8001a18 *)
mov L0x20019e20 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019e30; PC = 0x8001a1c *)
mov L0x20019e30 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019e40; PC = 0x8001a20 *)
mov L0x20019e40 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20019de0; PC = 0x8001a24 *)
mov L0x20019de0 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20019df0; PC = 0x8001a28 *)
mov L0x20019df0 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019e00; PC = 0x8001a2c *)
mov L0x20019e00 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019dd0; PC = 0x8001a30 *)
mov L0x20019dd0 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20019de4; Value = 0xfffee7d1; PC = 0x80016ec *)
mov r4 L0x20019de4;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019e04; Value = 0x0008871e; PC = 0x80016f0 *)
mov r5 L0x20019e04;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019e24; Value = 0xfff6e390; PC = 0x80016f4 *)
mov r6 L0x20019e24;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019e44; Value = 0xffed0566; PC = 0x80016f8 *)
mov r7 L0x20019e44;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019e64; Value = 0xfffdcdc0; PC = 0x80016fc *)
mov r8 L0x20019e64;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019e84; Value = 0x00053360; PC = 0x8001700 *)
mov r9 L0x20019e84;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019ea4; Value = 0x0005ed68; PC = 0x8001704 *)
mov r10 L0x20019ea4;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019ec4; Value = 0xfff84aec; PC = 0x8001708 *)
mov r11 L0x20019ec4;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20019dd4; Value = 0xfffa266d; PC = 0x8001890 *)
mov r4 L0x20019dd4;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20019df4; Value = 0xffffdbca; PC = 0x8001894 *)
mov r5 L0x20019df4;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019e14; Value = 0xfff3bd37; PC = 0x8001898 *)
mov r6 L0x20019e14;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019e34; Value = 0xfff84a64; PC = 0x800189c *)
mov r7 L0x20019e34;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019e54; Value = 0xfff8e4c3; PC = 0x80018a0 *)
mov r8 L0x20019e54;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019e74; Value = 0xfffe595d; PC = 0x80018a4 *)
mov r9 L0x20019e74;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019e94; Value = 0xfffae9c8; PC = 0x80018a8 *)
mov r10 L0x20019e94;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019eb4; Value = 0x00090d85; PC = 0x80018ac *)
mov r11 L0x20019eb4;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019e54; PC = 0x80019cc *)
mov L0x20019e54 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019e64; PC = 0x80019d0 *)
mov L0x20019e64 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019e74; PC = 0x80019d4 *)
mov L0x20019e74 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019e84; PC = 0x80019d8 *)
mov L0x20019e84 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019e94; PC = 0x80019dc *)
mov L0x20019e94 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019ea4; PC = 0x80019e0 *)
mov L0x20019ea4 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019eb4; PC = 0x80019e4 *)
mov L0x20019eb4 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019ec4; PC = 0x80019e8 *)
mov L0x20019ec4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019e14; PC = 0x8001a14 *)
mov L0x20019e14 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019e24; PC = 0x8001a18 *)
mov L0x20019e24 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019e34; PC = 0x8001a1c *)
mov L0x20019e34 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019e44; PC = 0x8001a20 *)
mov L0x20019e44 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20019de4; PC = 0x8001a24 *)
mov L0x20019de4 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20019df4; PC = 0x8001a28 *)
mov L0x20019df4 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019e04; PC = 0x8001a2c *)
mov L0x20019e04 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019dd4; PC = 0x8001a30 *)
mov L0x20019dd4 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20019de8; Value = 0x00020b21; PC = 0x80016ec *)
mov r4 L0x20019de8;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019e08; Value = 0x00028e36; PC = 0x80016f0 *)
mov r5 L0x20019e08;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019e28; Value = 0xfffa9112; PC = 0x80016f4 *)
mov r6 L0x20019e28;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019e48; Value = 0xfff371a2; PC = 0x80016f8 *)
mov r7 L0x20019e48;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019e68; Value = 0x00132347; PC = 0x80016fc *)
mov r8 L0x20019e68;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019e88; Value = 0x0002624a; PC = 0x8001700 *)
mov r9 L0x20019e88;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019ea8; Value = 0xfff5fbdd; PC = 0x8001704 *)
mov r10 L0x20019ea8;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019ec8; Value = 0xfffba21a; PC = 0x8001708 *)
mov r11 L0x20019ec8;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20019dd8; Value = 0x00002c06; PC = 0x8001890 *)
mov r4 L0x20019dd8;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20019df8; Value = 0x000892a4; PC = 0x8001894 *)
mov r5 L0x20019df8;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019e18; Value = 0xfff65672; PC = 0x8001898 *)
mov r6 L0x20019e18;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019e38; Value = 0xfffb2e05; PC = 0x800189c *)
mov r7 L0x20019e38;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019e58; Value = 0xfffb8cc7; PC = 0x80018a0 *)
mov r8 L0x20019e58;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019e78; Value = 0xfff45a16; PC = 0x80018a4 *)
mov r9 L0x20019e78;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019e98; Value = 0xfff79403; PC = 0x80018a8 *)
mov r10 L0x20019e98;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019eb8; Value = 0x00022aac; PC = 0x80018ac *)
mov r11 L0x20019eb8;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019e58; PC = 0x80019cc *)
mov L0x20019e58 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019e68; PC = 0x80019d0 *)
mov L0x20019e68 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019e78; PC = 0x80019d4 *)
mov L0x20019e78 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019e88; PC = 0x80019d8 *)
mov L0x20019e88 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019e98; PC = 0x80019dc *)
mov L0x20019e98 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019ea8; PC = 0x80019e0 *)
mov L0x20019ea8 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019eb8; PC = 0x80019e4 *)
mov L0x20019eb8 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019ec8; PC = 0x80019e8 *)
mov L0x20019ec8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019e18; PC = 0x8001a14 *)
mov L0x20019e18 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019e28; PC = 0x8001a18 *)
mov L0x20019e28 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019e38; PC = 0x8001a1c *)
mov L0x20019e38 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019e48; PC = 0x8001a20 *)
mov L0x20019e48 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20019de8; PC = 0x8001a24 *)
mov L0x20019de8 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20019df8; PC = 0x8001a28 *)
mov L0x20019df8 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019e08; PC = 0x8001a2c *)
mov L0x20019e08 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019dd8; PC = 0x8001a30 *)
mov L0x20019dd8 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20019dec; Value = 0x00038429; PC = 0x80016ec *)
mov r4 L0x20019dec;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019e0c; Value = 0xfff959b2; PC = 0x80016f0 *)
mov r5 L0x20019e0c;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019e2c; Value = 0xfffe6e29; PC = 0x80016f4 *)
mov r6 L0x20019e2c;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019e4c; Value = 0xfff9c387; PC = 0x80016f8 *)
mov r7 L0x20019e4c;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019e6c; Value = 0x000c9809; PC = 0x80016fc *)
mov r8 L0x20019e6c;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019e8c; Value = 0x000095e9; PC = 0x8001700 *)
mov r9 L0x20019e8c;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019eac; Value = 0x0006f86a; PC = 0x8001704 *)
mov r10 L0x20019eac;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019ecc; Value = 0x00019e0a; PC = 0x8001708 *)
mov r11 L0x20019ecc;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20019ddc; Value = 0x00079fac; PC = 0x8001890 *)
mov r4 L0x20019ddc;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20019dfc; Value = 0x0013b37c; PC = 0x8001894 *)
mov r5 L0x20019dfc;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019e1c; Value = 0x000715e5; PC = 0x8001898 *)
mov r6 L0x20019e1c;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019e3c; Value = 0x0001f610; PC = 0x800189c *)
mov r7 L0x20019e3c;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019e5c; Value = 0x000abec8; PC = 0x80018a0 *)
mov r8 L0x20019e5c;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019e7c; Value = 0xffffd128; PC = 0x80018a4 *)
mov r9 L0x20019e7c;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019e9c; Value = 0x0004ae54; PC = 0x80018a8 *)
mov r10 L0x20019e9c;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019ebc; Value = 0x0001f7b2; PC = 0x80018ac *)
mov r11 L0x20019ebc;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019e5c; PC = 0x80019cc *)
mov L0x20019e5c r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019e6c; PC = 0x80019d0 *)
mov L0x20019e6c r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019e7c; PC = 0x80019d4 *)
mov L0x20019e7c r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019e8c; PC = 0x80019d8 *)
mov L0x20019e8c r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019e9c; PC = 0x80019dc *)
mov L0x20019e9c r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019eac; PC = 0x80019e0 *)
mov L0x20019eac r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019ebc; PC = 0x80019e4 *)
mov L0x20019ebc r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019ecc; PC = 0x80019e8 *)
mov L0x20019ecc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019e1c; PC = 0x8001a14 *)
mov L0x20019e1c r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019e2c; PC = 0x8001a18 *)
mov L0x20019e2c r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019e3c; PC = 0x8001a1c *)
mov L0x20019e3c r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019e4c; PC = 0x8001a20 *)
mov L0x20019e4c r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20019dec; PC = 0x8001a24 *)
mov L0x20019dec r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20019dfc; PC = 0x8001a28 *)
mov L0x20019dfc r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019e0c; PC = 0x8001a2c *)
mov L0x20019e0c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019ddc; PC = 0x8001a30 *)
mov L0x20019ddc r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* add.w	r0, r0, #240	; 0xf0                       #! PC = 0x8001a40 *)
adds dontcare r0 r0 240@uint32;
(* vmov	r12, s2                                    #! PC = 0x8001a44 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8001a48 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x80016d8 <_4_5_6_7>                     #! PC = 0x8001a4c *)
#bne.w	0x80016d8 <_4_5_6_7>                     #! 0x8001a4c = 0x8001a4c;
(* vmov	r1, s1                                     #! PC = 0x80016d8 *)
mov r1 s1;
(* vldmia	r1!, {s4-s18}                            #! EA = L0x800719c; PC = 0x80016dc *)
mov s4 L0x800719c;
mov s5 L0x80071a0;
mov s6 L0x80071a4;
mov s7 L0x80071a8;
mov s8 L0x80071ac;
mov s9 L0x80071b0;
mov s10 L0x80071b4;
mov s11 L0x80071b8;
mov s12 L0x80071bc;
mov s13 L0x80071c0;
mov s14 L0x80071c4;
mov s15 L0x80071c8;
mov s16 L0x80071cc;
mov s17 L0x80071d0;
mov s18 L0x80071d4;
(* vmov	s1, r1                                     #! PC = 0x80016e0 *)
mov s1 r1;
(* add.w	lr, r0, #16                               #! PC = 0x80016e4 *)
adds dontcare lr r0 16@uint32;
(* vmov	s3, lr                                     #! PC = 0x80016e8 *)
mov s3 lr;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20019ee0; Value = 0xfffe08e5; PC = 0x80016ec *)
mov r4 L0x20019ee0;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019f00; Value = 0x001183a4; PC = 0x80016f0 *)
mov r5 L0x20019f00;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019f20; Value = 0x00025d95; PC = 0x80016f4 *)
mov r6 L0x20019f20;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019f40; Value = 0x0006544b; PC = 0x80016f8 *)
mov r7 L0x20019f40;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019f60; Value = 0x0002998b; PC = 0x80016fc *)
mov r8 L0x20019f60;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019f80; Value = 0xfffde6a1; PC = 0x8001700 *)
mov r9 L0x20019f80;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019fa0; Value = 0xffff250a; PC = 0x8001704 *)
mov r10 L0x20019fa0;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019fc0; Value = 0xfffaa8ef; PC = 0x8001708 *)
mov r11 L0x20019fc0;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20019ed0; Value = 0x0000b01b; PC = 0x8001890 *)
mov r4 L0x20019ed0;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20019ef0; Value = 0xfff574eb; PC = 0x8001894 *)
mov r5 L0x20019ef0;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019f10; Value = 0x0005161c; PC = 0x8001898 *)
mov r6 L0x20019f10;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019f30; Value = 0xfff492a1; PC = 0x800189c *)
mov r7 L0x20019f30;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019f50; Value = 0x00072f0c; PC = 0x80018a0 *)
mov r8 L0x20019f50;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019f70; Value = 0x0001e427; PC = 0x80018a4 *)
mov r9 L0x20019f70;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019f90; Value = 0x0001c74f; PC = 0x80018a8 *)
mov r10 L0x20019f90;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019fb0; Value = 0x00057455; PC = 0x80018ac *)
mov r11 L0x20019fb0;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019f50; PC = 0x80019cc *)
mov L0x20019f50 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019f60; PC = 0x80019d0 *)
mov L0x20019f60 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019f70; PC = 0x80019d4 *)
mov L0x20019f70 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019f80; PC = 0x80019d8 *)
mov L0x20019f80 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019f90; PC = 0x80019dc *)
mov L0x20019f90 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019fa0; PC = 0x80019e0 *)
mov L0x20019fa0 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019fb0; PC = 0x80019e4 *)
mov L0x20019fb0 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019fc0; PC = 0x80019e8 *)
mov L0x20019fc0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019f10; PC = 0x8001a14 *)
mov L0x20019f10 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019f20; PC = 0x8001a18 *)
mov L0x20019f20 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019f30; PC = 0x8001a1c *)
mov L0x20019f30 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019f40; PC = 0x8001a20 *)
mov L0x20019f40 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20019ee0; PC = 0x8001a24 *)
mov L0x20019ee0 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20019ef0; PC = 0x8001a28 *)
mov L0x20019ef0 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019f00; PC = 0x8001a2c *)
mov L0x20019f00 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019ed0; PC = 0x8001a30 *)
mov L0x20019ed0 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20019ee4; Value = 0xfffe9683; PC = 0x80016ec *)
mov r4 L0x20019ee4;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019f04; Value = 0xfffcf3ec; PC = 0x80016f0 *)
mov r5 L0x20019f04;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019f24; Value = 0x00020af4; PC = 0x80016f4 *)
mov r6 L0x20019f24;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019f44; Value = 0xfff3af9a; PC = 0x80016f8 *)
mov r7 L0x20019f44;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019f64; Value = 0xfff1aa90; PC = 0x80016fc *)
mov r8 L0x20019f64;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019f84; Value = 0x000854c6; PC = 0x8001700 *)
mov r9 L0x20019f84;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019fa4; Value = 0x0010c3ea; PC = 0x8001704 *)
mov r10 L0x20019fa4;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019fc4; Value = 0xfffefa82; PC = 0x8001708 *)
mov r11 L0x20019fc4;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20019ed4; Value = 0xfff1f6db; PC = 0x8001890 *)
mov r4 L0x20019ed4;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20019ef4; Value = 0x0004f22c; PC = 0x8001894 *)
mov r5 L0x20019ef4;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019f14; Value = 0xfff5cd7d; PC = 0x8001898 *)
mov r6 L0x20019f14;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019f34; Value = 0xfffc2156; PC = 0x800189c *)
mov r7 L0x20019f34;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019f54; Value = 0xfffbc49d; PC = 0x80018a0 *)
mov r8 L0x20019f54;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019f74; Value = 0x0009938f; PC = 0x80018a4 *)
mov r9 L0x20019f74;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019f94; Value = 0x00059a16; PC = 0x80018a8 *)
mov r10 L0x20019f94;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019fb4; Value = 0x00035961; PC = 0x80018ac *)
mov r11 L0x20019fb4;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019f54; PC = 0x80019cc *)
mov L0x20019f54 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019f64; PC = 0x80019d0 *)
mov L0x20019f64 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019f74; PC = 0x80019d4 *)
mov L0x20019f74 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019f84; PC = 0x80019d8 *)
mov L0x20019f84 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019f94; PC = 0x80019dc *)
mov L0x20019f94 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019fa4; PC = 0x80019e0 *)
mov L0x20019fa4 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019fb4; PC = 0x80019e4 *)
mov L0x20019fb4 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019fc4; PC = 0x80019e8 *)
mov L0x20019fc4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019f14; PC = 0x8001a14 *)
mov L0x20019f14 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019f24; PC = 0x8001a18 *)
mov L0x20019f24 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019f34; PC = 0x8001a1c *)
mov L0x20019f34 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019f44; PC = 0x8001a20 *)
mov L0x20019f44 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20019ee4; PC = 0x8001a24 *)
mov L0x20019ee4 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20019ef4; PC = 0x8001a28 *)
mov L0x20019ef4 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019f04; PC = 0x8001a2c *)
mov L0x20019f04 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019ed4; PC = 0x8001a30 *)
mov L0x20019ed4 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20019ee8; Value = 0x00069ce5; PC = 0x80016ec *)
mov r4 L0x20019ee8;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019f08; Value = 0x0004cf4e; PC = 0x80016f0 *)
mov r5 L0x20019f08;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019f28; Value = 0xfff63ab4; PC = 0x80016f4 *)
mov r6 L0x20019f28;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019f48; Value = 0xfff8e160; PC = 0x80016f8 *)
mov r7 L0x20019f48;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019f68; Value = 0x0008680b; PC = 0x80016fc *)
mov r8 L0x20019f68;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019f88; Value = 0x0002d16a; PC = 0x8001700 *)
mov r9 L0x20019f88;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019fa8; Value = 0x0002ce6d; PC = 0x8001704 *)
mov r10 L0x20019fa8;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019fc8; Value = 0x0009b370; PC = 0x8001708 *)
mov r11 L0x20019fc8;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20019ed8; Value = 0x0000fd42; PC = 0x8001890 *)
mov r4 L0x20019ed8;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20019ef8; Value = 0xfffd04fc; PC = 0x8001894 *)
mov r5 L0x20019ef8;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019f18; Value = 0x000595a8; PC = 0x8001898 *)
mov r6 L0x20019f18;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019f38; Value = 0x00050547; PC = 0x800189c *)
mov r7 L0x20019f38;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019f58; Value = 0x00046ce3; PC = 0x80018a0 *)
mov r8 L0x20019f58;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019f78; Value = 0xfff569b8; PC = 0x80018a4 *)
mov r9 L0x20019f78;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019f98; Value = 0x00030445; PC = 0x80018a8 *)
mov r10 L0x20019f98;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019fb8; Value = 0x000507d2; PC = 0x80018ac *)
mov r11 L0x20019fb8;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019f58; PC = 0x80019cc *)
mov L0x20019f58 r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019f68; PC = 0x80019d0 *)
mov L0x20019f68 r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019f78; PC = 0x80019d4 *)
mov L0x20019f78 r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019f88; PC = 0x80019d8 *)
mov L0x20019f88 r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019f98; PC = 0x80019dc *)
mov L0x20019f98 r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019fa8; PC = 0x80019e0 *)
mov L0x20019fa8 r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019fb8; PC = 0x80019e4 *)
mov L0x20019fb8 r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019fc8; PC = 0x80019e8 *)
mov L0x20019fc8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019f18; PC = 0x8001a14 *)
mov L0x20019f18 r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019f28; PC = 0x8001a18 *)
mov L0x20019f28 r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019f38; PC = 0x8001a1c *)
mov L0x20019f38 r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019f48; PC = 0x8001a20 *)
mov L0x20019f48 r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20019ee8; PC = 0x8001a24 *)
mov L0x20019ee8 r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20019ef8; PC = 0x8001a28 *)
mov L0x20019ef8 r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019f08; PC = 0x8001a2c *)
mov L0x20019f08 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019ed8; PC = 0x8001a30 *)
mov L0x20019ed8 r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* ldr.w	r4, [r0, #16]                             #! EA = L0x20019eec; Value = 0x000191fb; PC = 0x80016ec *)
mov r4 L0x20019eec;
(* ldr.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019f0c; Value = 0xffeafa40; PC = 0x80016f0 *)
mov r5 L0x20019f0c;
(* ldr.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019f2c; Value = 0xfffa2faf; PC = 0x80016f4 *)
mov r6 L0x20019f2c;
(* ldr.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019f4c; Value = 0xfff6aadb; PC = 0x80016f8 *)
mov r7 L0x20019f4c;
(* ldr.w	r8, [r0, #144]	; 0x90                     #! EA = L0x20019f6c; Value = 0x000fdc77; PC = 0x80016fc *)
mov r8 L0x20019f6c;
(* ldr.w	r9, [r0, #176]	; 0xb0                     #! EA = L0x20019f8c; Value = 0x00017b9d; PC = 0x8001700 *)
mov r9 L0x20019f8c;
(* ldr.w	r10, [r0, #208]	; 0xd0                    #! EA = L0x20019fac; Value = 0x0000a352; PC = 0x8001704 *)
mov r10 L0x20019fac;
(* ldr.w	r11, [r0, #240]	; 0xf0                    #! EA = L0x20019fcc; Value = 0xfffe32c6; PC = 0x8001708 *)
mov r11 L0x20019fcc;
(* vmov	r1, s4                                     #! PC = 0x800170c *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001710 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001714 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8001718 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x800171c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001720 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001724 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001728 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800172c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001730 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001734 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001738 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800173c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x8001740 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x8001742 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x8001744 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x8001746 *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x8001748 *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x800174c *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x8001750 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x8001754 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x8001758 *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x800175c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001760 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001764 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001768 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800176c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001770 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001774 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001778 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800177c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001780 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001784 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001788 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800178c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001790 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001792 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001794 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x8001796 *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8001798 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x800179c *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x80017a0 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x80017a4 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x80017a8 *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x80017ac *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017b0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80017b4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x80017b8 *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x80017bc *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017c0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x80017c4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80017c8 *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x80017cc *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80017d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x80017d8 *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x80017dc *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80017e0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80017e4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x80017e8 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x80017ea *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x80017ec *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x80017ee *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80017f0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80017f4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80017f8 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80017fc *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8001800 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8001804 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001808 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x800180c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8001810 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001814 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001818 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x800181c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8001820 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001824 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001828 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x800182c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8001830 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001834 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001838 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x800183c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8001840 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8001844 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001848 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x800184c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8001850 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001854 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001858 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x800185c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8001860 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8001864 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001868 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x800186c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8001870 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001874 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001878 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x800187c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8001880 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8001884 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8001888 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x800188c *)
mov s26 r10;
mov s27 r11;
(* ldr.w	r4, [r0]                                  #! EA = L0x20019edc; Value = 0x000bf108; PC = 0x8001890 *)
mov r4 L0x20019edc;
(* ldr.w	r5, [r0, #32]                             #! EA = L0x20019efc; Value = 0x00065a72; PC = 0x8001894 *)
mov r5 L0x20019efc;
(* ldr.w	r6, [r0, #64]	; 0x40                      #! EA = L0x20019f1c; Value = 0xfffea9b1; PC = 0x8001898 *)
mov r6 L0x20019f1c;
(* ldr.w	r7, [r0, #96]	; 0x60                      #! EA = L0x20019f3c; Value = 0xfffc2552; PC = 0x800189c *)
mov r7 L0x20019f3c;
(* ldr.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019f5c; Value = 0xfffc8b2a; PC = 0x80018a0 *)
mov r8 L0x20019f5c;
(* ldr.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019f7c; Value = 0x00021342; PC = 0x80018a4 *)
mov r9 L0x20019f7c;
(* ldr.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019f9c; Value = 0x0003c4be; PC = 0x80018a8 *)
mov r10 L0x20019f9c;
(* ldr.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019fbc; Value = 0x000ed748; PC = 0x80018ac *)
mov r11 L0x20019fbc;
(* vmov	r1, s4                                     #! PC = 0x80018b0 *)
mov r1 s4;
(* smull	r12, r8, r8, r1                           #! PC = 0x80018b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80018bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80018c0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018c4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80018c8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r10, r10, r1                         #! PC = 0x80018cc *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018d0 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x80018d4 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x80018d8 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80018dc *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x80018e0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r8                                      #! PC = 0x80018e4 *)
add r4 r4 r8;
(* add	r5, r9                                      #! PC = 0x80018e6 *)
add r5 r5 r9;
(* add	r6, r10                                     #! PC = 0x80018e8 *)
add r6 r6 r10;
(* add	r7, r11                                     #! PC = 0x80018ea *)
add r7 r7 r11;
(* sub.w	r8, r4, r8, lsl #1                        #! PC = 0x80018ec *)
shl tmpx2 r8 1;
sub r8 r4 tmpx2;
(* sub.w	r9, r5, r9, lsl #1                        #! PC = 0x80018f0 *)
shl tmpx2 r9 1;
sub r9 r5 tmpx2;
(* sub.w	r10, r6, r10, lsl #1                      #! PC = 0x80018f4 *)
shl tmpx2 r10 1;
sub r10 r6 tmpx2;
(* sub.w	r11, r7, r11, lsl #1                      #! PC = 0x80018f8 *)
shl tmpx2 r11 1;
sub r11 r7 tmpx2;
(* vmov	r1, s5                                     #! PC = 0x80018fc *)
mov r1 s5;
(* smull	r12, r6, r6, r1                           #! PC = 0x8001900 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001904 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8001908 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r7, r7, r1                           #! PC = 0x800190c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001910 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001914 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8001918 *)
mov r1 s6;
(* smull	r12, r10, r10, r1                         #! PC = 0x800191c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001920 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8001924 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001928 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x800192c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001930 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r6                                      #! PC = 0x8001934 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8001936 *)
add r5 r5 r7;
(* add	r8, r10                                     #! PC = 0x8001938 *)
add r8 r8 r10;
(* add	r9, r11                                     #! PC = 0x800193a *)
add r9 r9 r11;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x800193c *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8001940 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8001944 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* sub.w	r11, r9, r11, lsl #1                      #! PC = 0x8001948 *)
shl tmpx2 r11 1;
sub r11 r9 tmpx2;
(* vmov	r1, s7                                     #! PC = 0x800194c *)
mov r1 s7;
(* smull	r12, r5, r5, r1                           #! PC = 0x8001950 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001954 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8001958 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s8                                     #! PC = 0x800195c *)
mov r1 s8;
(* smull	r12, r7, r7, r1                           #! PC = 0x8001960 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001964 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8001968 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x800196c *)
mov r1 s9;
(* smull	r12, r9, r9, r1                           #! PC = 0x8001970 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001974 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8001978 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s10                                    #! PC = 0x800197c *)
mov r1 s10;
(* smull	r12, r11, r11, r1                         #! PC = 0x8001980 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8001984 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8001988 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x800198c *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x800198e *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8001990 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8001992 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8001994 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8001998 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x800199c *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x80019a0 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x80019a4 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x80019a8 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x80019ac *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x80019b0 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x80019b4 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019b6 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x80019b8 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x80019ba *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x80019bc *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x80019c0 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x80019c4 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x80019c8 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #128]	; 0x80                     #! EA = L0x20019f5c; PC = 0x80019cc *)
mov L0x20019f5c r8;
(* str.w	r4, [r0, #144]	; 0x90                     #! EA = L0x20019f6c; PC = 0x80019d0 *)
mov L0x20019f6c r4;
(* str.w	r9, [r0, #160]	; 0xa0                     #! EA = L0x20019f7c; PC = 0x80019d4 *)
mov L0x20019f7c r9;
(* str.w	r5, [r0, #176]	; 0xb0                     #! EA = L0x20019f8c; PC = 0x80019d8 *)
mov L0x20019f8c r5;
(* str.w	r10, [r0, #192]	; 0xc0                    #! EA = L0x20019f9c; PC = 0x80019dc *)
mov L0x20019f9c r10;
(* str.w	r6, [r0, #208]	; 0xd0                     #! EA = L0x20019fac; PC = 0x80019e0 *)
mov L0x20019fac r6;
(* str.w	r11, [r0, #224]	; 0xe0                    #! EA = L0x20019fbc; PC = 0x80019e4 *)
mov L0x20019fbc r11;
(* str.w	r7, [r0, #240]	; 0xf0                     #! EA = L0x20019fcc; PC = 0x80019e8 *)
mov L0x20019fcc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x80019ec *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x80019f0 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x80019f4 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x80019f8 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x80019fc *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x80019fe *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8001a00 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8001a02 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8001a04 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8001a08 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8001a0c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8001a10 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #64]	; 0x40                     #! EA = L0x20019f1c; PC = 0x8001a14 *)
mov L0x20019f1c r10;
(* str.w	r6, [r0, #80]	; 0x50                      #! EA = L0x20019f2c; PC = 0x8001a18 *)
mov L0x20019f2c r6;
(* str.w	r11, [r0, #96]	; 0x60                     #! EA = L0x20019f3c; PC = 0x8001a1c *)
mov L0x20019f3c r11;
(* str.w	r7, [r0, #112]	; 0x70                     #! EA = L0x20019f4c; PC = 0x8001a20 *)
mov L0x20019f4c r7;
(* str.w	r4, [r0, #16]                             #! EA = L0x20019eec; PC = 0x8001a24 *)
mov L0x20019eec r4;
(* str.w	r9, [r0, #32]                             #! EA = L0x20019efc; PC = 0x8001a28 *)
mov L0x20019efc r9;
(* str.w	r5, [r0, #48]	; 0x30                      #! EA = L0x20019f0c; PC = 0x8001a2c *)
mov L0x20019f0c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019edc; PC = 0x8001a30 *)
mov L0x20019edc r8;
(* vmov	lr, s3                                     #! PC = 0x8001a34 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8001a38 *)
(* cmp.w r0, lr *)
nop;
(* #bne.w	0x80016ec <_4_5_6_7_inner>               #! PC = 0x8001a3c *)
#bne.w	0x80016ec <_4_5_6_7_inner>               #! 0x8001a3c = 0x8001a3c;
(* add.w	r0, r0, #240	; 0xf0                       #! PC = 0x8001a40 *)
adds dontcare r0 r0 240@uint32;
(* vmov	r12, s2                                    #! PC = 0x8001a44 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8001a48 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x80016d8 <_4_5_6_7>                     #! PC = 0x8001a4c *)
#bne.w	0x80016d8 <_4_5_6_7>                     #! 0x8001a4c = 0x8001a4c;
(* #vpop	{s16-s31}                                  #! PC = 0x8001a50 *)
#vpop	{%%s16-%%s31}                                  #! 0x8001a50 = 0x8001a50;
(* #ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, r10, r11, r12, pc}#! EA = L0x20016f88; Value = 0x20016fd0; PC = 0x8001a54 *)
#ldmia.w	sp!, {%%r4, %%r5, %%r6, %%r7, %%r8, %%r9, %%r10, %%r11, %%r12, pc}#! L0x20016f88 = L0x20016f88; 0x20016fd0 = 0x20016fd0; 0x8001a54 = 0x8001a54;
(* #add.w	r5, sp, #16384	; 0x4000                   #! PC = 0x800032c *)
#add.w	%%r5, sp, #16384	; 0x4000                   #! 0x800032c = 0x800032c;

{
and [
eqmod (cinp_poly**2)
    (L0x20018fd0*(x** 0)+L0x20018fd4*(x** 1)+L0x20018fd8*(x** 2)+
     L0x20018fdc*(x** 3))
    [1043969, x**4 - 1],
eqmod (cinp_poly**2)
    (L0x20018fe0*(x** 0)+L0x20018fe4*(x** 1)+L0x20018fe8*(x** 2)+
     L0x20018fec*(x** 3))
    [1043969, x**4 - 1043968],
eqmod (cinp_poly**2)
    (L0x20018ff0*(x** 0)+L0x20018ff4*(x** 1)+L0x20018ff8*(x** 2)+
     L0x20018ffc*(x** 3))
    [1043969, x**4 - 554923],
eqmod (cinp_poly**2)
    (L0x20019000*(x** 0)+L0x20019004*(x** 1)+L0x20019008*(x** 2)+
     L0x2001900c*(x** 3))
    [1043969, x**4 - 489046],
eqmod (cinp_poly**2)
    (L0x20019010*(x** 0)+L0x20019014*(x** 1)+L0x20019018*(x** 2)+
     L0x2001901c*(x** 3))
    [1043969, x**4 - 287998],
eqmod (cinp_poly**2)
    (L0x20019020*(x** 0)+L0x20019024*(x** 1)+L0x20019028*(x** 2)+
     L0x2001902c*(x** 3))
    [1043969, x**4 - 755971],
eqmod (cinp_poly**2)
    (L0x20019030*(x** 0)+L0x20019034*(x** 1)+L0x20019038*(x** 2)+
     L0x2001903c*(x** 3))
    [1043969, x**4 - 719789],
eqmod (cinp_poly**2)
    (L0x20019040*(x** 0)+L0x20019044*(x** 1)+L0x20019048*(x** 2)+
     L0x2001904c*(x** 3))
    [1043969, x**4 - 324180],
eqmod (cinp_poly**2)
    (L0x20019050*(x** 0)+L0x20019054*(x** 1)+L0x20019058*(x** 2)+
     L0x2001905c*(x** 3))
    [1043969, x**4 - 29512],
eqmod (cinp_poly**2)
    (L0x20019060*(x** 0)+L0x20019064*(x** 1)+L0x20019068*(x** 2)+
     L0x2001906c*(x** 3))
    [1043969, x**4 - 1014457],
eqmod (cinp_poly**2)
    (L0x20019070*(x** 0)+L0x20019074*(x** 1)+L0x20019078*(x** 2)+
     L0x2001907c*(x** 3))
    [1043969, x**4 - 145873],
eqmod (cinp_poly**2)
    (L0x20019080*(x** 0)+L0x20019084*(x** 1)+L0x20019088*(x** 2)+
     L0x2001908c*(x** 3))
    [1043969, x**4 - 898096],
eqmod (cinp_poly**2)
    (L0x20019090*(x** 0)+L0x20019094*(x** 1)+L0x20019098*(x** 2)+
     L0x2001909c*(x** 3))
    [1043969, x**4 - 445347],
eqmod (cinp_poly**2)
    (L0x200190a0*(x** 0)+L0x200190a4*(x** 1)+L0x200190a8*(x** 2)+
     L0x200190ac*(x** 3))
    [1043969, x**4 - 598622],
eqmod (cinp_poly**2)
    (L0x200190b0*(x** 0)+L0x200190b4*(x** 1)+L0x200190b8*(x** 2)+
     L0x200190bc*(x** 3))
    [1043969, x**4 - 775725],
eqmod (cinp_poly**2)
    (L0x200190c0*(x** 0)+L0x200190c4*(x** 1)+L0x200190c8*(x** 2)+
     L0x200190cc*(x** 3))
    [1043969, x**4 - 268244],
eqmod (cinp_poly**2)
    (L0x200190d0*(x** 0)+L0x200190d4*(x** 1)+L0x200190d8*(x** 2)+
     L0x200190dc*(x** 3))
    [1043969, x**4 - 754540],
eqmod (cinp_poly**2)
    (L0x200190e0*(x** 0)+L0x200190e4*(x** 1)+L0x200190e8*(x** 2)+
     L0x200190ec*(x** 3))
    [1043969, x**4 - 289429],
eqmod (cinp_poly**2)
    (L0x200190f0*(x** 0)+L0x200190f4*(x** 1)+L0x200190f8*(x** 2)+
     L0x200190fc*(x** 3))
    [1043969, x**4 - 689776],
eqmod (cinp_poly**2)
    (L0x20019100*(x** 0)+L0x20019104*(x** 1)+L0x20019108*(x** 2)+
     L0x2001910c*(x** 3))
    [1043969, x**4 - 354193],
eqmod (cinp_poly**2)
    (L0x20019110*(x** 0)+L0x20019114*(x** 1)+L0x20019118*(x** 2)+
     L0x2001911c*(x** 3))
    [1043969, x**4 - 731663],
eqmod (cinp_poly**2)
    (L0x20019120*(x** 0)+L0x20019124*(x** 1)+L0x20019128*(x** 2)+
     L0x2001912c*(x** 3))
    [1043969, x**4 - 312306],
eqmod (cinp_poly**2)
    (L0x20019130*(x** 0)+L0x20019134*(x** 1)+L0x20019138*(x** 2)+
     L0x2001913c*(x** 3))
    [1043969, x**4 - 379345],
eqmod (cinp_poly**2)
    (L0x20019140*(x** 0)+L0x20019144*(x** 1)+L0x20019148*(x** 2)+
     L0x2001914c*(x** 3))
    [1043969, x**4 - 664624],
eqmod (cinp_poly**2)
    (L0x20019150*(x** 0)+L0x20019154*(x** 1)+L0x20019158*(x** 2)+
     L0x2001915c*(x** 3))
    [1043969, x**4 - 125710],
eqmod (cinp_poly**2)
    (L0x20019160*(x** 0)+L0x20019164*(x** 1)+L0x20019168*(x** 2)+
     L0x2001916c*(x** 3))
    [1043969, x**4 - 918259],
eqmod (cinp_poly**2)
    (L0x20019170*(x** 0)+L0x20019174*(x** 1)+L0x20019178*(x** 2)+
     L0x2001917c*(x** 3))
    [1043969, x**4 - 317781],
eqmod (cinp_poly**2)
    (L0x20019180*(x** 0)+L0x20019184*(x** 1)+L0x20019188*(x** 2)+
     L0x2001918c*(x** 3))
    [1043969, x**4 - 726188],
eqmod (cinp_poly**2)
    (L0x20019190*(x** 0)+L0x20019194*(x** 1)+L0x20019198*(x** 2)+
     L0x2001919c*(x** 3))
    [1043969, x**4 - 427629],
eqmod (cinp_poly**2)
    (L0x200191a0*(x** 0)+L0x200191a4*(x** 1)+L0x200191a8*(x** 2)+
     L0x200191ac*(x** 3))
    [1043969, x**4 - 616340],
eqmod (cinp_poly**2)
    (L0x200191b0*(x** 0)+L0x200191b4*(x** 1)+L0x200191b8*(x** 2)+
     L0x200191bc*(x** 3))
    [1043969, x**4 - 750053],
eqmod (cinp_poly**2)
    (L0x200191c0*(x** 0)+L0x200191c4*(x** 1)+L0x200191c8*(x** 2)+
     L0x200191cc*(x** 3))
    [1043969, x**4 - 293916],
eqmod (cinp_poly**2)
    (L0x200191d0*(x** 0)+L0x200191d4*(x** 1)+L0x200191d8*(x** 2)+
     L0x200191dc*(x** 3))
    [1043969, x**4 - 587782],
eqmod (cinp_poly**2)
    (L0x200191e0*(x** 0)+L0x200191e4*(x** 1)+L0x200191e8*(x** 2)+
     L0x200191ec*(x** 3))
    [1043969, x**4 - 456187],
eqmod (cinp_poly**2)
    (L0x200191f0*(x** 0)+L0x200191f4*(x** 1)+L0x200191f8*(x** 2)+
     L0x200191fc*(x** 3))
    [1043969, x**4 - 252302],
eqmod (cinp_poly**2)
    (L0x20019200*(x** 0)+L0x20019204*(x** 1)+L0x20019208*(x** 2)+
     L0x2001920c*(x** 3))
    [1043969, x**4 - 791667],
eqmod (cinp_poly**2)
    (L0x20019210*(x** 0)+L0x20019214*(x** 1)+L0x20019218*(x** 2)+
     L0x2001921c*(x** 3))
    [1043969, x**4 - 467086],
eqmod (cinp_poly**2)
    (L0x20019220*(x** 0)+L0x20019224*(x** 1)+L0x20019228*(x** 2)+
     L0x2001922c*(x** 3))
    [1043969, x**4 - 576883],
eqmod (cinp_poly**2)
    (L0x20019230*(x** 0)+L0x20019234*(x** 1)+L0x20019238*(x** 2)+
     L0x2001923c*(x** 3))
    [1043969, x**4 - 141058],
eqmod (cinp_poly**2)
    (L0x20019240*(x** 0)+L0x20019244*(x** 1)+L0x20019248*(x** 2)+
     L0x2001924c*(x** 3))
    [1043969, x**4 - 902911],
eqmod (cinp_poly**2)
    (L0x20019250*(x** 0)+L0x20019254*(x** 1)+L0x20019258*(x** 2)+
     L0x2001925c*(x** 3))
    [1043969, x**4 - 33480],
eqmod (cinp_poly**2)
    (L0x20019260*(x** 0)+L0x20019264*(x** 1)+L0x20019268*(x** 2)+
     L0x2001926c*(x** 3))
    [1043969, x**4 - 1010489],
eqmod (cinp_poly**2)
    (L0x20019270*(x** 0)+L0x20019274*(x** 1)+L0x20019278*(x** 2)+
     L0x2001927c*(x** 3))
    [1043969, x**4 - 349716],
eqmod (cinp_poly**2)
    (L0x20019280*(x** 0)+L0x20019284*(x** 1)+L0x20019288*(x** 2)+
     L0x2001928c*(x** 3))
    [1043969, x**4 - 694253],
eqmod (cinp_poly**2)
    (L0x20019290*(x** 0)+L0x20019294*(x** 1)+L0x20019298*(x** 2)+
     L0x2001929c*(x** 3))
    [1043969, x**4 - 75356],
eqmod (cinp_poly**2)
    (L0x200192a0*(x** 0)+L0x200192a4*(x** 1)+L0x200192a8*(x** 2)+
     L0x200192ac*(x** 3))
    [1043969, x**4 - 968613],
eqmod (cinp_poly**2)
    (L0x200192b0*(x** 0)+L0x200192b4*(x** 1)+L0x200192b8*(x** 2)+
     L0x200192bc*(x** 3))
    [1043969, x**4 - 599293],
eqmod (cinp_poly**2)
    (L0x200192c0*(x** 0)+L0x200192c4*(x** 1)+L0x200192c8*(x** 2)+
     L0x200192cc*(x** 3))
    [1043969, x**4 - 444676],
eqmod (cinp_poly**2)
    (L0x200192d0*(x** 0)+L0x200192d4*(x** 1)+L0x200192d8*(x** 2)+
     L0x200192dc*(x** 3))
    [1043969, x**4 - 899855],
eqmod (cinp_poly**2)
    (L0x200192e0*(x** 0)+L0x200192e4*(x** 1)+L0x200192e8*(x** 2)+
     L0x200192ec*(x** 3))
    [1043969, x**4 - 144114],
eqmod (cinp_poly**2)
    (L0x200192f0*(x** 0)+L0x200192f4*(x** 1)+L0x200192f8*(x** 2)+
     L0x200192fc*(x** 3))
    [1043969, x**4 - 28054],
eqmod (cinp_poly**2)
    (L0x20019300*(x** 0)+L0x20019304*(x** 1)+L0x20019308*(x** 2)+
     L0x2001930c*(x** 3))
    [1043969, x**4 - 1015915],
eqmod (cinp_poly**2)
    (L0x20019310*(x** 0)+L0x20019314*(x** 1)+L0x20019318*(x** 2)+
     L0x2001931c*(x** 3))
    [1043969, x**4 - 531761],
eqmod (cinp_poly**2)
    (L0x20019320*(x** 0)+L0x20019324*(x** 1)+L0x20019328*(x** 2)+
     L0x2001932c*(x** 3))
    [1043969, x**4 - 512208],
eqmod (cinp_poly**2)
    (L0x20019330*(x** 0)+L0x20019334*(x** 1)+L0x20019338*(x** 2)+
     L0x2001933c*(x** 3))
    [1043969, x**4 - 219801],
eqmod (cinp_poly**2)
    (L0x20019340*(x** 0)+L0x20019344*(x** 1)+L0x20019348*(x** 2)+
     L0x2001934c*(x** 3))
    [1043969, x**4 - 824168],
eqmod (cinp_poly**2)
    (L0x20019350*(x** 0)+L0x20019354*(x** 1)+L0x20019358*(x** 2)+
     L0x2001935c*(x** 3))
    [1043969, x**4 - 37338],
eqmod (cinp_poly**2)
    (L0x20019360*(x** 0)+L0x20019364*(x** 1)+L0x20019368*(x** 2)+
     L0x2001936c*(x** 3))
    [1043969, x**4 - 1006631],
eqmod (cinp_poly**2)
    (L0x20019370*(x** 0)+L0x20019374*(x** 1)+L0x20019378*(x** 2)+
     L0x2001937c*(x** 3))
    [1043969, x**4 - 62231],
eqmod (cinp_poly**2)
    (L0x20019380*(x** 0)+L0x20019384*(x** 1)+L0x20019388*(x** 2)+
     L0x2001938c*(x** 3))
    [1043969, x**4 - 981738],
eqmod (cinp_poly**2)
    (L0x20019390*(x** 0)+L0x20019394*(x** 1)+L0x20019398*(x** 2)+
     L0x2001939c*(x** 3))
    [1043969, x**4 - 388624],
eqmod (cinp_poly**2)
    (L0x200193a0*(x** 0)+L0x200193a4*(x** 1)+L0x200193a8*(x** 2)+
     L0x200193ac*(x** 3))
    [1043969, x**4 - 655345],
eqmod (cinp_poly**2)
    (L0x200193b0*(x** 0)+L0x200193b4*(x** 1)+L0x200193b8*(x** 2)+
     L0x200193bc*(x** 3))
    [1043969, x**4 - 587715],
eqmod (cinp_poly**2)
    (L0x200193c0*(x** 0)+L0x200193c4*(x** 1)+L0x200193c8*(x** 2)+
     L0x200193cc*(x** 3))
    [1043969, x**4 - 456254],
eqmod (cinp_poly**2)
    (L0x200193d0*(x** 0)+L0x200193d4*(x** 1)+L0x200193d8*(x** 2)+
     L0x200193dc*(x** 3))
    [1043969, x**4 - 1013205],
eqmod (cinp_poly**2)
    (L0x200193e0*(x** 0)+L0x200193e4*(x** 1)+L0x200193e8*(x** 2)+
     L0x200193ec*(x** 3))
    [1043969, x**4 - 30764],
eqmod (cinp_poly**2)
    (L0x200193f0*(x** 0)+L0x200193f4*(x** 1)+L0x200193f8*(x** 2)+
     L0x200193fc*(x** 3))
    [1043969, x**4 - 373885],
eqmod (cinp_poly**2)
    (L0x20019400*(x** 0)+L0x20019404*(x** 1)+L0x20019408*(x** 2)+
     L0x2001940c*(x** 3))
    [1043969, x**4 - 670084],
eqmod (cinp_poly**2)
    (L0x20019410*(x** 0)+L0x20019414*(x** 1)+L0x20019418*(x** 2)+
     L0x2001941c*(x** 3))
    [1043969, x**4 - 194431],
eqmod (cinp_poly**2)
    (L0x20019420*(x** 0)+L0x20019424*(x** 1)+L0x20019428*(x** 2)+
     L0x2001942c*(x** 3))
    [1043969, x**4 - 849538],
eqmod (cinp_poly**2)
    (L0x20019430*(x** 0)+L0x20019434*(x** 1)+L0x20019438*(x** 2)+
     L0x2001943c*(x** 3))
    [1043969, x**4 - 37663],
eqmod (cinp_poly**2)
    (L0x20019440*(x** 0)+L0x20019444*(x** 1)+L0x20019448*(x** 2)+
     L0x2001944c*(x** 3))
    [1043969, x**4 - 1006306],
eqmod (cinp_poly**2)
    (L0x20019450*(x** 0)+L0x20019454*(x** 1)+L0x20019458*(x** 2)+
     L0x2001945c*(x** 3))
    [1043969, x**4 - 345862],
eqmod (cinp_poly**2)
    (L0x20019460*(x** 0)+L0x20019464*(x** 1)+L0x20019468*(x** 2)+
     L0x2001946c*(x** 3))
    [1043969, x**4 - 698107],
eqmod (cinp_poly**2)
    (L0x20019470*(x** 0)+L0x20019474*(x** 1)+L0x20019478*(x** 2)+
     L0x2001947c*(x** 3))
    [1043969, x**4 - 385759],
eqmod (cinp_poly**2)
    (L0x20019480*(x** 0)+L0x20019484*(x** 1)+L0x20019488*(x** 2)+
     L0x2001948c*(x** 3))
    [1043969, x**4 - 658210],
eqmod (cinp_poly**2)
    (L0x20019490*(x** 0)+L0x20019494*(x** 1)+L0x20019498*(x** 2)+
     L0x2001949c*(x** 3))
    [1043969, x**4 - 394048],
eqmod (cinp_poly**2)
    (L0x200194a0*(x** 0)+L0x200194a4*(x** 1)+L0x200194a8*(x** 2)+
     L0x200194ac*(x** 3))
    [1043969, x**4 - 649921],
eqmod (cinp_poly**2)
    (L0x200194b0*(x** 0)+L0x200194b4*(x** 1)+L0x200194b8*(x** 2)+
     L0x200194bc*(x** 3))
    [1043969, x**4 - 727440],
eqmod (cinp_poly**2)
    (L0x200194c0*(x** 0)+L0x200194c4*(x** 1)+L0x200194c8*(x** 2)+
     L0x200194cc*(x** 3))
    [1043969, x**4 - 316529],
eqmod (cinp_poly**2)
    (L0x200194d0*(x** 0)+L0x200194d4*(x** 1)+L0x200194d8*(x** 2)+
     L0x200194dc*(x** 3))
    [1043969, x**4 - 1026124],
eqmod (cinp_poly**2)
    (L0x200194e0*(x** 0)+L0x200194e4*(x** 1)+L0x200194e8*(x** 2)+
     L0x200194ec*(x** 3))
    [1043969, x**4 - 17845],
eqmod (cinp_poly**2)
    (L0x200194f0*(x** 0)+L0x200194f4*(x** 1)+L0x200194f8*(x** 2)+
     L0x200194fc*(x** 3))
    [1043969, x**4 - 488999],
eqmod (cinp_poly**2)
    (L0x20019500*(x** 0)+L0x20019504*(x** 1)+L0x20019508*(x** 2)+
     L0x2001950c*(x** 3))
    [1043969, x**4 - 554970],
eqmod (cinp_poly**2)
    (L0x20019510*(x** 0)+L0x20019514*(x** 1)+L0x20019518*(x** 2)+
     L0x2001951c*(x** 3))
    [1043969, x**4 - 135077],
eqmod (cinp_poly**2)
    (L0x20019520*(x** 0)+L0x20019524*(x** 1)+L0x20019528*(x** 2)+
     L0x2001952c*(x** 3))
    [1043969, x**4 - 908892],
eqmod (cinp_poly**2)
    (L0x20019530*(x** 0)+L0x20019534*(x** 1)+L0x20019538*(x** 2)+
     L0x2001953c*(x** 3))
    [1043969, x**4 - 359871],
eqmod (cinp_poly**2)
    (L0x20019540*(x** 0)+L0x20019544*(x** 1)+L0x20019548*(x** 2)+
     L0x2001954c*(x** 3))
    [1043969, x**4 - 684098],
eqmod (cinp_poly**2)
    (L0x20019550*(x** 0)+L0x20019554*(x** 1)+L0x20019558*(x** 2)+
     L0x2001955c*(x** 3))
    [1043969, x**4 - 562705],
eqmod (cinp_poly**2)
    (L0x20019560*(x** 0)+L0x20019564*(x** 1)+L0x20019568*(x** 2)+
     L0x2001956c*(x** 3))
    [1043969, x**4 - 481264],
eqmod (cinp_poly**2)
    (L0x20019570*(x** 0)+L0x20019574*(x** 1)+L0x20019578*(x** 2)+
     L0x2001957c*(x** 3))
    [1043969, x**4 - 555001],
eqmod (cinp_poly**2)
    (L0x20019580*(x** 0)+L0x20019584*(x** 1)+L0x20019588*(x** 2)+
     L0x2001958c*(x** 3))
    [1043969, x**4 - 488968],
eqmod (cinp_poly**2)
    (L0x20019590*(x** 0)+L0x20019594*(x** 1)+L0x20019598*(x** 2)+
     L0x2001959c*(x** 3))
    [1043969, x**4 - 518782],
eqmod (cinp_poly**2)
    (L0x200195a0*(x** 0)+L0x200195a4*(x** 1)+L0x200195a8*(x** 2)+
     L0x200195ac*(x** 3))
    [1043969, x**4 - 525187],
eqmod (cinp_poly**2)
    (L0x200195b0*(x** 0)+L0x200195b4*(x** 1)+L0x200195b8*(x** 2)+
     L0x200195bc*(x** 3))
    [1043969, x**4 - 216315],
eqmod (cinp_poly**2)
    (L0x200195c0*(x** 0)+L0x200195c4*(x** 1)+L0x200195c8*(x** 2)+
     L0x200195cc*(x** 3))
    [1043969, x**4 - 827654],
eqmod (cinp_poly**2)
    (L0x200195d0*(x** 0)+L0x200195d4*(x** 1)+L0x200195d8*(x** 2)+
     L0x200195dc*(x** 3))
    [1043969, x**4 - 61601],
eqmod (cinp_poly**2)
    (L0x200195e0*(x** 0)+L0x200195e4*(x** 1)+L0x200195e8*(x** 2)+
     L0x200195ec*(x** 3))
    [1043969, x**4 - 982368],
eqmod (cinp_poly**2)
    (L0x200195f0*(x** 0)+L0x200195f4*(x** 1)+L0x200195f8*(x** 2)+
     L0x200195fc*(x** 3))
    [1043969, x**4 - 90787],
eqmod (cinp_poly**2)
    (L0x20019600*(x** 0)+L0x20019604*(x** 1)+L0x20019608*(x** 2)+
     L0x2001960c*(x** 3))
    [1043969, x**4 - 953182],
eqmod (cinp_poly**2)
    (L0x20019610*(x** 0)+L0x20019614*(x** 1)+L0x20019618*(x** 2)+
     L0x2001961c*(x** 3))
    [1043969, x**4 - 799581],
eqmod (cinp_poly**2)
    (L0x20019620*(x** 0)+L0x20019624*(x** 1)+L0x20019628*(x** 2)+
     L0x2001962c*(x** 3))
    [1043969, x**4 - 244388],
eqmod (cinp_poly**2)
    (L0x20019630*(x** 0)+L0x20019634*(x** 1)+L0x20019638*(x** 2)+
     L0x2001963c*(x** 3))
    [1043969, x**4 - 270821],
eqmod (cinp_poly**2)
    (L0x20019640*(x** 0)+L0x20019644*(x** 1)+L0x20019648*(x** 2)+
     L0x2001964c*(x** 3))
    [1043969, x**4 - 773148],
eqmod (cinp_poly**2)
    (L0x20019650*(x** 0)+L0x20019654*(x** 1)+L0x20019658*(x** 2)+
     L0x2001965c*(x** 3))
    [1043969, x**4 - 418683],
eqmod (cinp_poly**2)
    (L0x20019660*(x** 0)+L0x20019664*(x** 1)+L0x20019668*(x** 2)+
     L0x2001966c*(x** 3))
    [1043969, x**4 - 625286],
eqmod (cinp_poly**2)
    (L0x20019670*(x** 0)+L0x20019674*(x** 1)+L0x20019678*(x** 2)+
     L0x2001967c*(x** 3))
    [1043969, x**4 - 481490],
eqmod (cinp_poly**2)
    (L0x20019680*(x** 0)+L0x20019684*(x** 1)+L0x20019688*(x** 2)+
     L0x2001968c*(x** 3))
    [1043969, x**4 - 562479],
eqmod (cinp_poly**2)
    (L0x20019690*(x** 0)+L0x20019694*(x** 1)+L0x20019698*(x** 2)+
     L0x2001969c*(x** 3))
    [1043969, x**4 - 403165],
eqmod (cinp_poly**2)
    (L0x200196a0*(x** 0)+L0x200196a4*(x** 1)+L0x200196a8*(x** 2)+
     L0x200196ac*(x** 3))
    [1043969, x**4 - 640804],
eqmod (cinp_poly**2)
    (L0x200196b0*(x** 0)+L0x200196b4*(x** 1)+L0x200196b8*(x** 2)+
     L0x200196bc*(x** 3))
    [1043969, x**4 - 886657],
eqmod (cinp_poly**2)
    (L0x200196c0*(x** 0)+L0x200196c4*(x** 1)+L0x200196c8*(x** 2)+
     L0x200196cc*(x** 3))
    [1043969, x**4 - 157312],
eqmod (cinp_poly**2)
    (L0x200196d0*(x** 0)+L0x200196d4*(x** 1)+L0x200196d8*(x** 2)+
     L0x200196dc*(x** 3))
    [1043969, x**4 - 830722],
eqmod (cinp_poly**2)
    (L0x200196e0*(x** 0)+L0x200196e4*(x** 1)+L0x200196e8*(x** 2)+
     L0x200196ec*(x** 3))
    [1043969, x**4 - 213247],
eqmod (cinp_poly**2)
    (L0x200196f0*(x** 0)+L0x200196f4*(x** 1)+L0x200196f8*(x** 2)+
     L0x200196fc*(x** 3))
    [1043969, x**4 - 309107],
eqmod (cinp_poly**2)
    (L0x20019700*(x** 0)+L0x20019704*(x** 1)+L0x20019708*(x** 2)+
     L0x2001970c*(x** 3))
    [1043969, x**4 - 734862],
eqmod (cinp_poly**2)
    (L0x20019710*(x** 0)+L0x20019714*(x** 1)+L0x20019718*(x** 2)+
     L0x2001971c*(x** 3))
    [1043969, x**4 - 942795],
eqmod (cinp_poly**2)
    (L0x20019720*(x** 0)+L0x20019724*(x** 1)+L0x20019728*(x** 2)+
     L0x2001972c*(x** 3))
    [1043969, x**4 - 101174],
eqmod (cinp_poly**2)
    (L0x20019730*(x** 0)+L0x20019734*(x** 1)+L0x20019738*(x** 2)+
     L0x2001973c*(x** 3))
    [1043969, x**4 - 873218],
eqmod (cinp_poly**2)
    (L0x20019740*(x** 0)+L0x20019744*(x** 1)+L0x20019748*(x** 2)+
     L0x2001974c*(x** 3))
    [1043969, x**4 - 170751],
eqmod (cinp_poly**2)
    (L0x20019750*(x** 0)+L0x20019754*(x** 1)+L0x20019758*(x** 2)+
     L0x2001975c*(x** 3))
    [1043969, x**4 - 743637],
eqmod (cinp_poly**2)
    (L0x20019760*(x** 0)+L0x20019764*(x** 1)+L0x20019768*(x** 2)+
     L0x2001976c*(x** 3))
    [1043969, x**4 - 300332],
eqmod (cinp_poly**2)
    (L0x20019770*(x** 0)+L0x20019774*(x** 1)+L0x20019778*(x** 2)+
     L0x2001977c*(x** 3))
    [1043969, x**4 - 164662],
eqmod (cinp_poly**2)
    (L0x20019780*(x** 0)+L0x20019784*(x** 1)+L0x20019788*(x** 2)+
     L0x2001978c*(x** 3))
    [1043969, x**4 - 879307],
eqmod (cinp_poly**2)
    (L0x20019790*(x** 0)+L0x20019794*(x** 1)+L0x20019798*(x** 2)+
     L0x2001979c*(x** 3))
    [1043969, x**4 - 948221],
eqmod (cinp_poly**2)
    (L0x200197a0*(x** 0)+L0x200197a4*(x** 1)+L0x200197a8*(x** 2)+
     L0x200197ac*(x** 3))
    [1043969, x**4 - 95748],
eqmod (cinp_poly**2)
    (L0x200197b0*(x** 0)+L0x200197b4*(x** 1)+L0x200197b8*(x** 2)+
     L0x200197bc*(x** 3))
    [1043969, x**4 - 34851],
eqmod (cinp_poly**2)
    (L0x200197c0*(x** 0)+L0x200197c4*(x** 1)+L0x200197c8*(x** 2)+
     L0x200197cc*(x** 3))
    [1043969, x**4 - 1009118],
eqmod (cinp_poly**2)
    (L0x200197d0*(x** 0)+L0x200197d4*(x** 1)+L0x200197d8*(x** 2)+
     L0x200197dc*(x** 3))
    [1043969, x**4 - 941631],
eqmod (cinp_poly**2)
    (L0x200197e0*(x** 0)+L0x200197e4*(x** 1)+L0x200197e8*(x** 2)+
     L0x200197ec*(x** 3))
    [1043969, x**4 - 102338],
eqmod (cinp_poly**2)
    (L0x200197f0*(x** 0)+L0x200197f4*(x** 1)+L0x200197f8*(x** 2)+
     L0x200197fc*(x** 3))
    [1043969, x**4 - 115688],
eqmod (cinp_poly**2)
    (L0x20019800*(x** 0)+L0x20019804*(x** 1)+L0x20019808*(x** 2)+
     L0x2001980c*(x** 3))
    [1043969, x**4 - 928281],
eqmod (cinp_poly**2)
    (L0x20019810*(x** 0)+L0x20019814*(x** 1)+L0x20019818*(x** 2)+
     L0x2001981c*(x** 3))
    [1043969, x**4 - 193484],
eqmod (cinp_poly**2)
    (L0x20019820*(x** 0)+L0x20019824*(x** 1)+L0x20019828*(x** 2)+
     L0x2001982c*(x** 3))
    [1043969, x**4 - 850485],
eqmod (cinp_poly**2)
    (L0x20019830*(x** 0)+L0x20019834*(x** 1)+L0x20019838*(x** 2)+
     L0x2001983c*(x** 3))
    [1043969, x**4 - 685958],
eqmod (cinp_poly**2)
    (L0x20019840*(x** 0)+L0x20019844*(x** 1)+L0x20019848*(x** 2)+
     L0x2001984c*(x** 3))
    [1043969, x**4 - 358011],
eqmod (cinp_poly**2)
    (L0x20019850*(x** 0)+L0x20019854*(x** 1)+L0x20019858*(x** 2)+
     L0x2001985c*(x** 3))
    [1043969, x**4 - 3261],
eqmod (cinp_poly**2)
    (L0x20019860*(x** 0)+L0x20019864*(x** 1)+L0x20019868*(x** 2)+
     L0x2001986c*(x** 3))
    [1043969, x**4 - 1040708],
eqmod (cinp_poly**2)
    (L0x20019870*(x** 0)+L0x20019874*(x** 1)+L0x20019878*(x** 2)+
     L0x2001987c*(x** 3))
    [1043969, x**4 - 405626],
eqmod (cinp_poly**2)
    (L0x20019880*(x** 0)+L0x20019884*(x** 1)+L0x20019888*(x** 2)+
     L0x2001988c*(x** 3))
    [1043969, x**4 - 638343],
eqmod (cinp_poly**2)
    (L0x20019890*(x** 0)+L0x20019894*(x** 1)+L0x20019898*(x** 2)+
     L0x2001989c*(x** 3))
    [1043969, x**4 - 633347],
eqmod (cinp_poly**2)
    (L0x200198a0*(x** 0)+L0x200198a4*(x** 1)+L0x200198a8*(x** 2)+
     L0x200198ac*(x** 3))
    [1043969, x**4 - 410622],
eqmod (cinp_poly**2)
    (L0x200198b0*(x** 0)+L0x200198b4*(x** 1)+L0x200198b8*(x** 2)+
     L0x200198bc*(x** 3))
    [1043969, x**4 - 389617],
eqmod (cinp_poly**2)
    (L0x200198c0*(x** 0)+L0x200198c4*(x** 1)+L0x200198c8*(x** 2)+
     L0x200198cc*(x** 3))
    [1043969, x**4 - 654352],
eqmod (cinp_poly**2)
    (L0x200198d0*(x** 0)+L0x200198d4*(x** 1)+L0x200198d8*(x** 2)+
     L0x200198dc*(x** 3))
    [1043969, x**4 - 96534],
eqmod (cinp_poly**2)
    (L0x200198e0*(x** 0)+L0x200198e4*(x** 1)+L0x200198e8*(x** 2)+
     L0x200198ec*(x** 3))
    [1043969, x**4 - 947435],
eqmod (cinp_poly**2)
    (L0x200198f0*(x** 0)+L0x200198f4*(x** 1)+L0x200198f8*(x** 2)+
     L0x200198fc*(x** 3))
    [1043969, x**4 - 799554],
eqmod (cinp_poly**2)
    (L0x20019900*(x** 0)+L0x20019904*(x** 1)+L0x20019908*(x** 2)+
     L0x2001990c*(x** 3))
    [1043969, x**4 - 244415],
eqmod (cinp_poly**2)
    (L0x20019910*(x** 0)+L0x20019914*(x** 1)+L0x20019918*(x** 2)+
     L0x2001991c*(x** 3))
    [1043969, x**4 - 704462],
eqmod (cinp_poly**2)
    (L0x20019920*(x** 0)+L0x20019924*(x** 1)+L0x20019928*(x** 2)+
     L0x2001992c*(x** 3))
    [1043969, x**4 - 339507],
eqmod (cinp_poly**2)
    (L0x20019930*(x** 0)+L0x20019934*(x** 1)+L0x20019938*(x** 2)+
     L0x2001993c*(x** 3))
    [1043969, x**4 - 666593],
eqmod (cinp_poly**2)
    (L0x20019940*(x** 0)+L0x20019944*(x** 1)+L0x20019948*(x** 2)+
     L0x2001994c*(x** 3))
    [1043969, x**4 - 377376],
eqmod (cinp_poly**2)
    (L0x20019950*(x** 0)+L0x20019954*(x** 1)+L0x20019958*(x** 2)+
     L0x2001995c*(x** 3))
    [1043969, x**4 - 963976],
eqmod (cinp_poly**2)
    (L0x20019960*(x** 0)+L0x20019964*(x** 1)+L0x20019968*(x** 2)+
     L0x2001996c*(x** 3))
    [1043969, x**4 - 79993],
eqmod (cinp_poly**2)
    (L0x20019970*(x** 0)+L0x20019974*(x** 1)+L0x20019978*(x** 2)+
     L0x2001997c*(x** 3))
    [1043969, x**4 - 650310],
eqmod (cinp_poly**2)
    (L0x20019980*(x** 0)+L0x20019984*(x** 1)+L0x20019988*(x** 2)+
     L0x2001998c*(x** 3))
    [1043969, x**4 - 393659],
eqmod (cinp_poly**2)
    (L0x20019990*(x** 0)+L0x20019994*(x** 1)+L0x20019998*(x** 2)+
     L0x2001999c*(x** 3))
    [1043969, x**4 - 483878],
eqmod (cinp_poly**2)
    (L0x200199a0*(x** 0)+L0x200199a4*(x** 1)+L0x200199a8*(x** 2)+
     L0x200199ac*(x** 3))
    [1043969, x**4 - 560091],
eqmod (cinp_poly**2)
    (L0x200199b0*(x** 0)+L0x200199b4*(x** 1)+L0x200199b8*(x** 2)+
     L0x200199bc*(x** 3))
    [1043969, x**4 - 984749],
eqmod (cinp_poly**2)
    (L0x200199c0*(x** 0)+L0x200199c4*(x** 1)+L0x200199c8*(x** 2)+
     L0x200199cc*(x** 3))
    [1043969, x**4 - 59220],
eqmod (cinp_poly**2)
    (L0x200199d0*(x** 0)+L0x200199d4*(x** 1)+L0x200199d8*(x** 2)+
     L0x200199dc*(x** 3))
    [1043969, x**4 - 15495],
eqmod (cinp_poly**2)
    (L0x200199e0*(x** 0)+L0x200199e4*(x** 1)+L0x200199e8*(x** 2)+
     L0x200199ec*(x** 3))
    [1043969, x**4 - 1028474],
eqmod (cinp_poly**2)
    (L0x200199f0*(x** 0)+L0x200199f4*(x** 1)+L0x200199f8*(x** 2)+
     L0x200199fc*(x** 3))
    [1043969, x**4 - 403201],
eqmod (cinp_poly**2)
    (L0x20019a00*(x** 0)+L0x20019a04*(x** 1)+L0x20019a08*(x** 2)+
     L0x20019a0c*(x** 3))
    [1043969, x**4 - 640768],
eqmod (cinp_poly**2)
    (L0x20019a10*(x** 0)+L0x20019a14*(x** 1)+L0x20019a18*(x** 2)+
     L0x20019a1c*(x** 3))
    [1043969, x**4 - 605504],
eqmod (cinp_poly**2)
    (L0x20019a20*(x** 0)+L0x20019a24*(x** 1)+L0x20019a28*(x** 2)+
     L0x20019a2c*(x** 3))
    [1043969, x**4 - 438465],
eqmod (cinp_poly**2)
    (L0x20019a30*(x** 0)+L0x20019a34*(x** 1)+L0x20019a38*(x** 2)+
     L0x20019a3c*(x** 3))
    [1043969, x**4 - 409728],
eqmod (cinp_poly**2)
    (L0x20019a40*(x** 0)+L0x20019a44*(x** 1)+L0x20019a48*(x** 2)+
     L0x20019a4c*(x** 3))
    [1043969, x**4 - 634241],
eqmod (cinp_poly**2)
    (L0x20019a50*(x** 0)+L0x20019a54*(x** 1)+L0x20019a58*(x** 2)+
     L0x20019a5c*(x** 3))
    [1043969, x**4 - 30018],
eqmod (cinp_poly**2)
    (L0x20019a60*(x** 0)+L0x20019a64*(x** 1)+L0x20019a68*(x** 2)+
     L0x20019a6c*(x** 3))
    [1043969, x**4 - 1013951],
eqmod (cinp_poly**2)
    (L0x20019a70*(x** 0)+L0x20019a74*(x** 1)+L0x20019a78*(x** 2)+
     L0x20019a7c*(x** 3))
    [1043969, x**4 - 109250],
eqmod (cinp_poly**2)
    (L0x20019a80*(x** 0)+L0x20019a84*(x** 1)+L0x20019a88*(x** 2)+
     L0x20019a8c*(x** 3))
    [1043969, x**4 - 934719],
eqmod (cinp_poly**2)
    (L0x20019a90*(x** 0)+L0x20019a94*(x** 1)+L0x20019a98*(x** 2)+
     L0x20019a9c*(x** 3))
    [1043969, x**4 - 16675],
eqmod (cinp_poly**2)
    (L0x20019aa0*(x** 0)+L0x20019aa4*(x** 1)+L0x20019aa8*(x** 2)+
     L0x20019aac*(x** 3))
    [1043969, x**4 - 1027294],
eqmod (cinp_poly**2)
    (L0x20019ab0*(x** 0)+L0x20019ab4*(x** 1)+L0x20019ab8*(x** 2)+
     L0x20019abc*(x** 3))
    [1043969, x**4 - 643778],
eqmod (cinp_poly**2)
    (L0x20019ac0*(x** 0)+L0x20019ac4*(x** 1)+L0x20019ac8*(x** 2)+
     L0x20019acc*(x** 3))
    [1043969, x**4 - 400191],
eqmod (cinp_poly**2)
    (L0x20019ad0*(x** 0)+L0x20019ad4*(x** 1)+L0x20019ad8*(x** 2)+
     L0x20019adc*(x** 3))
    [1043969, x**4 - 188469],
eqmod (cinp_poly**2)
    (L0x20019ae0*(x** 0)+L0x20019ae4*(x** 1)+L0x20019ae8*(x** 2)+
     L0x20019aec*(x** 3))
    [1043969, x**4 - 855500],
eqmod (cinp_poly**2)
    (L0x20019af0*(x** 0)+L0x20019af4*(x** 1)+L0x20019af8*(x** 2)+
     L0x20019afc*(x** 3))
    [1043969, x**4 - 968467],
eqmod (cinp_poly**2)
    (L0x20019b00*(x** 0)+L0x20019b04*(x** 1)+L0x20019b08*(x** 2)+
     L0x20019b0c*(x** 3))
    [1043969, x**4 - 75502],
eqmod (cinp_poly**2)
    (L0x20019b10*(x** 0)+L0x20019b14*(x** 1)+L0x20019b18*(x** 2)+
     L0x20019b1c*(x** 3))
    [1043969, x**4 - 658814],
eqmod (cinp_poly**2)
    (L0x20019b20*(x** 0)+L0x20019b24*(x** 1)+L0x20019b28*(x** 2)+
     L0x20019b2c*(x** 3))
    [1043969, x**4 - 385155],
eqmod (cinp_poly**2)
    (L0x20019b30*(x** 0)+L0x20019b34*(x** 1)+L0x20019b38*(x** 2)+
     L0x20019b3c*(x** 3))
    [1043969, x**4 - 405305],
eqmod (cinp_poly**2)
    (L0x20019b40*(x** 0)+L0x20019b44*(x** 1)+L0x20019b48*(x** 2)+
     L0x20019b4c*(x** 3))
    [1043969, x**4 - 638664],
eqmod (cinp_poly**2)
    (L0x20019b50*(x** 0)+L0x20019b54*(x** 1)+L0x20019b58*(x** 2)+
     L0x20019b5c*(x** 3))
    [1043969, x**4 - 874265],
eqmod (cinp_poly**2)
    (L0x20019b60*(x** 0)+L0x20019b64*(x** 1)+L0x20019b68*(x** 2)+
     L0x20019b6c*(x** 3))
    [1043969, x**4 - 169704],
eqmod (cinp_poly**2)
    (L0x20019b70*(x** 0)+L0x20019b74*(x** 1)+L0x20019b78*(x** 2)+
     L0x20019b7c*(x** 3))
    [1043969, x**4 - 658791],
eqmod (cinp_poly**2)
    (L0x20019b80*(x** 0)+L0x20019b84*(x** 1)+L0x20019b88*(x** 2)+
     L0x20019b8c*(x** 3))
    [1043969, x**4 - 385178],
eqmod (cinp_poly**2)
    (L0x20019b90*(x** 0)+L0x20019b94*(x** 1)+L0x20019b98*(x** 2)+
     L0x20019b9c*(x** 3))
    [1043969, x**4 - 40112],
eqmod (cinp_poly**2)
    (L0x20019ba0*(x** 0)+L0x20019ba4*(x** 1)+L0x20019ba8*(x** 2)+
     L0x20019bac*(x** 3))
    [1043969, x**4 - 1003857],
eqmod (cinp_poly**2)
    (L0x20019bb0*(x** 0)+L0x20019bb4*(x** 1)+L0x20019bb8*(x** 2)+
     L0x20019bbc*(x** 3))
    [1043969, x**4 - 608327],
eqmod (cinp_poly**2)
    (L0x20019bc0*(x** 0)+L0x20019bc4*(x** 1)+L0x20019bc8*(x** 2)+
     L0x20019bcc*(x** 3))
    [1043969, x**4 - 435642],
eqmod (cinp_poly**2)
    (L0x20019bd0*(x** 0)+L0x20019bd4*(x** 1)+L0x20019bd8*(x** 2)+
     L0x20019bdc*(x** 3))
    [1043969, x**4 - 759697],
eqmod (cinp_poly**2)
    (L0x20019be0*(x** 0)+L0x20019be4*(x** 1)+L0x20019be8*(x** 2)+
     L0x20019bec*(x** 3))
    [1043969, x**4 - 284272],
eqmod (cinp_poly**2)
    (L0x20019bf0*(x** 0)+L0x20019bf4*(x** 1)+L0x20019bf8*(x** 2)+
     L0x20019bfc*(x** 3))
    [1043969, x**4 - 908658],
eqmod (cinp_poly**2)
    (L0x20019c00*(x** 0)+L0x20019c04*(x** 1)+L0x20019c08*(x** 2)+
     L0x20019c0c*(x** 3))
    [1043969, x**4 - 135311],
eqmod (cinp_poly**2)
    (L0x20019c10*(x** 0)+L0x20019c14*(x** 1)+L0x20019c18*(x** 2)+
     L0x20019c1c*(x** 3))
    [1043969, x**4 - 369462],
eqmod (cinp_poly**2)
    (L0x20019c20*(x** 0)+L0x20019c24*(x** 1)+L0x20019c28*(x** 2)+
     L0x20019c2c*(x** 3))
    [1043969, x**4 - 674507],
eqmod (cinp_poly**2)
    (L0x20019c30*(x** 0)+L0x20019c34*(x** 1)+L0x20019c38*(x** 2)+
     L0x20019c3c*(x** 3))
    [1043969, x**4 - 1021423],
eqmod (cinp_poly**2)
    (L0x20019c40*(x** 0)+L0x20019c44*(x** 1)+L0x20019c48*(x** 2)+
     L0x20019c4c*(x** 3))
    [1043969, x**4 - 22546],
eqmod (cinp_poly**2)
    (L0x20019c50*(x** 0)+L0x20019c54*(x** 1)+L0x20019c58*(x** 2)+
     L0x20019c5c*(x** 3))
    [1043969, x**4 - 943589],
eqmod (cinp_poly**2)
    (L0x20019c60*(x** 0)+L0x20019c64*(x** 1)+L0x20019c68*(x** 2)+
     L0x20019c6c*(x** 3))
    [1043969, x**4 - 100380],
eqmod (cinp_poly**2)
    (L0x20019c70*(x** 0)+L0x20019c74*(x** 1)+L0x20019c78*(x** 2)+
     L0x20019c7c*(x** 3))
    [1043969, x**4 - 927162],
eqmod (cinp_poly**2)
    (L0x20019c80*(x** 0)+L0x20019c84*(x** 1)+L0x20019c88*(x** 2)+
     L0x20019c8c*(x** 3))
    [1043969, x**4 - 116807],
eqmod (cinp_poly**2)
    (L0x20019c90*(x** 0)+L0x20019c94*(x** 1)+L0x20019c98*(x** 2)+
     L0x20019c9c*(x** 3))
    [1043969, x**4 - 350308],
eqmod (cinp_poly**2)
    (L0x20019ca0*(x** 0)+L0x20019ca4*(x** 1)+L0x20019ca8*(x** 2)+
     L0x20019cac*(x** 3))
    [1043969, x**4 - 693661],
eqmod (cinp_poly**2)
    (L0x20019cb0*(x** 0)+L0x20019cb4*(x** 1)+L0x20019cb8*(x** 2)+
     L0x20019cbc*(x** 3))
    [1043969, x**4 - 674670],
eqmod (cinp_poly**2)
    (L0x20019cc0*(x** 0)+L0x20019cc4*(x** 1)+L0x20019cc8*(x** 2)+
     L0x20019ccc*(x** 3))
    [1043969, x**4 - 369299],
eqmod (cinp_poly**2)
    (L0x20019cd0*(x** 0)+L0x20019cd4*(x** 1)+L0x20019cd8*(x** 2)+
     L0x20019cdc*(x** 3))
    [1043969, x**4 - 319829],
eqmod (cinp_poly**2)
    (L0x20019ce0*(x** 0)+L0x20019ce4*(x** 1)+L0x20019ce8*(x** 2)+
     L0x20019cec*(x** 3))
    [1043969, x**4 - 724140],
eqmod (cinp_poly**2)
    (L0x20019cf0*(x** 0)+L0x20019cf4*(x** 1)+L0x20019cf8*(x** 2)+
     L0x20019cfc*(x** 3))
    [1043969, x**4 - 518322],
eqmod (cinp_poly**2)
    (L0x20019d00*(x** 0)+L0x20019d04*(x** 1)+L0x20019d08*(x** 2)+
     L0x20019d0c*(x** 3))
    [1043969, x**4 - 525647],
eqmod (cinp_poly**2)
    (L0x20019d10*(x** 0)+L0x20019d14*(x** 1)+L0x20019d18*(x** 2)+
     L0x20019d1c*(x** 3))
    [1043969, x**4 - 727472],
eqmod (cinp_poly**2)
    (L0x20019d20*(x** 0)+L0x20019d24*(x** 1)+L0x20019d28*(x** 2)+
     L0x20019d2c*(x** 3))
    [1043969, x**4 - 316497],
eqmod (cinp_poly**2)
    (L0x20019d30*(x** 0)+L0x20019d34*(x** 1)+L0x20019d38*(x** 2)+
     L0x20019d3c*(x** 3))
    [1043969, x**4 - 659984],
eqmod (cinp_poly**2)
    (L0x20019d40*(x** 0)+L0x20019d44*(x** 1)+L0x20019d48*(x** 2)+
     L0x20019d4c*(x** 3))
    [1043969, x**4 - 383985],
eqmod (cinp_poly**2)
    (L0x20019d50*(x** 0)+L0x20019d54*(x** 1)+L0x20019d58*(x** 2)+
     L0x20019d5c*(x** 3))
    [1043969, x**4 - 269719],
eqmod (cinp_poly**2)
    (L0x20019d60*(x** 0)+L0x20019d64*(x** 1)+L0x20019d68*(x** 2)+
     L0x20019d6c*(x** 3))
    [1043969, x**4 - 774250],
eqmod (cinp_poly**2)
    (L0x20019d70*(x** 0)+L0x20019d74*(x** 1)+L0x20019d78*(x** 2)+
     L0x20019d7c*(x** 3))
    [1043969, x**4 - 485076],
eqmod (cinp_poly**2)
    (L0x20019d80*(x** 0)+L0x20019d84*(x** 1)+L0x20019d88*(x** 2)+
     L0x20019d8c*(x** 3))
    [1043969, x**4 - 558893],
eqmod (cinp_poly**2)
    (L0x20019d90*(x** 0)+L0x20019d94*(x** 1)+L0x20019d98*(x** 2)+
     L0x20019d9c*(x** 3))
    [1043969, x**4 - 975148],
eqmod (cinp_poly**2)
    (L0x20019da0*(x** 0)+L0x20019da4*(x** 1)+L0x20019da8*(x** 2)+
     L0x20019dac*(x** 3))
    [1043969, x**4 - 68821],
eqmod (cinp_poly**2)
    (L0x20019db0*(x** 0)+L0x20019db4*(x** 1)+L0x20019db8*(x** 2)+
     L0x20019dbc*(x** 3))
    [1043969, x**4 - 118175],
eqmod (cinp_poly**2)
    (L0x20019dc0*(x** 0)+L0x20019dc4*(x** 1)+L0x20019dc8*(x** 2)+
     L0x20019dcc*(x** 3))
    [1043969, x**4 - 925794],
eqmod (cinp_poly**2)
    (L0x20019dd0*(x** 0)+L0x20019dd4*(x** 1)+L0x20019dd8*(x** 2)+
     L0x20019ddc*(x** 3))
    [1043969, x**4 - 405653],
eqmod (cinp_poly**2)
    (L0x20019de0*(x** 0)+L0x20019de4*(x** 1)+L0x20019de8*(x** 2)+
     L0x20019dec*(x** 3))
    [1043969, x**4 - 638316],
eqmod (cinp_poly**2)
    (L0x20019df0*(x** 0)+L0x20019df4*(x** 1)+L0x20019df8*(x** 2)+
     L0x20019dfc*(x** 3))
    [1043969, x**4 - 364094],
eqmod (cinp_poly**2)
    (L0x20019e00*(x** 0)+L0x20019e04*(x** 1)+L0x20019e08*(x** 2)+
     L0x20019e0c*(x** 3))
    [1043969, x**4 - 679875],
eqmod (cinp_poly**2)
    (L0x20019e10*(x** 0)+L0x20019e14*(x** 1)+L0x20019e18*(x** 2)+
     L0x20019e1c*(x** 3))
    [1043969, x**4 - 857780],
eqmod (cinp_poly**2)
    (L0x20019e20*(x** 0)+L0x20019e24*(x** 1)+L0x20019e28*(x** 2)+
     L0x20019e2c*(x** 3))
    [1043969, x**4 - 186189],
eqmod (cinp_poly**2)
    (L0x20019e30*(x** 0)+L0x20019e34*(x** 1)+L0x20019e38*(x** 2)+
     L0x20019e3c*(x** 3))
    [1043969, x**4 - 9514],
eqmod (cinp_poly**2)
    (L0x20019e40*(x** 0)+L0x20019e44*(x** 1)+L0x20019e48*(x** 2)+
     L0x20019e4c*(x** 3))
    [1043969, x**4 - 1034455],
eqmod (cinp_poly**2)
    (L0x20019e50*(x** 0)+L0x20019e54*(x** 1)+L0x20019e58*(x** 2)+
     L0x20019e5c*(x** 3))
    [1043969, x**4 - 438813],
eqmod (cinp_poly**2)
    (L0x20019e60*(x** 0)+L0x20019e64*(x** 1)+L0x20019e68*(x** 2)+
     L0x20019e6c*(x** 3))
    [1043969, x**4 - 605156],
eqmod (cinp_poly**2)
    (L0x20019e70*(x** 0)+L0x20019e74*(x** 1)+L0x20019e78*(x** 2)+
     L0x20019e7c*(x** 3))
    [1043969, x**4 - 613180],
eqmod (cinp_poly**2)
    (L0x20019e80*(x** 0)+L0x20019e84*(x** 1)+L0x20019e88*(x** 2)+
     L0x20019e8c*(x** 3))
    [1043969, x**4 - 430789],
eqmod (cinp_poly**2)
    (L0x20019e90*(x** 0)+L0x20019e94*(x** 1)+L0x20019e98*(x** 2)+
     L0x20019e9c*(x** 3))
    [1043969, x**4 - 643048],
eqmod (cinp_poly**2)
    (L0x20019ea0*(x** 0)+L0x20019ea4*(x** 1)+L0x20019ea8*(x** 2)+
     L0x20019eac*(x** 3))
    [1043969, x**4 - 400921],
eqmod (cinp_poly**2)
    (L0x20019eb0*(x** 0)+L0x20019eb4*(x** 1)+L0x20019eb8*(x** 2)+
     L0x20019ebc*(x** 3))
    [1043969, x**4 - 993476],
eqmod (cinp_poly**2)
    (L0x20019ec0*(x** 0)+L0x20019ec4*(x** 1)+L0x20019ec8*(x** 2)+
     L0x20019ecc*(x** 3))
    [1043969, x**4 - 50493],
eqmod (cinp_poly**2)
    (L0x20019ed0*(x** 0)+L0x20019ed4*(x** 1)+L0x20019ed8*(x** 2)+
     L0x20019edc*(x** 3))
    [1043969, x**4 - 143510],
eqmod (cinp_poly**2)
    (L0x20019ee0*(x** 0)+L0x20019ee4*(x** 1)+L0x20019ee8*(x** 2)+
     L0x20019eec*(x** 3))
    [1043969, x**4 - 900459],
eqmod (cinp_poly**2)
    (L0x20019ef0*(x** 0)+L0x20019ef4*(x** 1)+L0x20019ef8*(x** 2)+
     L0x20019efc*(x** 3))
    [1043969, x**4 - 956472],
eqmod (cinp_poly**2)
    (L0x20019f00*(x** 0)+L0x20019f04*(x** 1)+L0x20019f08*(x** 2)+
     L0x20019f0c*(x** 3))
    [1043969, x**4 - 87497],
eqmod (cinp_poly**2)
    (L0x20019f10*(x** 0)+L0x20019f14*(x** 1)+L0x20019f18*(x** 2)+
     L0x20019f1c*(x** 3))
    [1043969, x**4 - 904239],
eqmod (cinp_poly**2)
    (L0x20019f20*(x** 0)+L0x20019f24*(x** 1)+L0x20019f28*(x** 2)+
     L0x20019f2c*(x** 3))
    [1043969, x**4 - 139730],
eqmod (cinp_poly**2)
    (L0x20019f30*(x** 0)+L0x20019f34*(x** 1)+L0x20019f38*(x** 2)+
     L0x20019f3c*(x** 3))
    [1043969, x**4 - 362716],
eqmod (cinp_poly**2)
    (L0x20019f40*(x** 0)+L0x20019f44*(x** 1)+L0x20019f48*(x** 2)+
     L0x20019f4c*(x** 3))
    [1043969, x**4 - 681253],
eqmod (cinp_poly**2)
    (L0x20019f50*(x** 0)+L0x20019f54*(x** 1)+L0x20019f58*(x** 2)+
     L0x20019f5c*(x** 3))
    [1043969, x**4 - 928856],
eqmod (cinp_poly**2)
    (L0x20019f60*(x** 0)+L0x20019f64*(x** 1)+L0x20019f68*(x** 2)+
     L0x20019f6c*(x** 3))
    [1043969, x**4 - 115113],
eqmod (cinp_poly**2)
    (L0x20019f70*(x** 0)+L0x20019f74*(x** 1)+L0x20019f78*(x** 2)+
     L0x20019f7c*(x** 3))
    [1043969, x**4 - 567842],
eqmod (cinp_poly**2)
    (L0x20019f80*(x** 0)+L0x20019f84*(x** 1)+L0x20019f88*(x** 2)+
     L0x20019f8c*(x** 3))
    [1043969, x**4 - 476127],
eqmod (cinp_poly**2)
    (L0x20019f90*(x** 0)+L0x20019f94*(x** 1)+L0x20019f98*(x** 2)+
     L0x20019f9c*(x** 3))
    [1043969, x**4 - 1009759],
eqmod (cinp_poly**2)
    (L0x20019fa0*(x** 0)+L0x20019fa4*(x** 1)+L0x20019fa8*(x** 2)+
     L0x20019fac*(x** 3))
    [1043969, x**4 - 34210],
eqmod (cinp_poly**2)
    (L0x20019fb0*(x** 0)+L0x20019fb4*(x** 1)+L0x20019fb8*(x** 2)+
     L0x20019fbc*(x** 3))
    [1043969, x**4 - 660435],
eqmod (cinp_poly**2)
    (L0x20019fc0*(x** 0)+L0x20019fc4*(x** 1)+L0x20019fc8*(x** 2)+
     L0x20019fcc*(x** 3))
    [1043969, x**4 - 383534]
] && and [
(-128)@32*1043969@32 <s L0x20018fd0, L0x20018fd0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20018fd4, L0x20018fd4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20018fd8, L0x20018fd8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20018fdc, L0x20018fdc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20018fe0, L0x20018fe0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20018fe4, L0x20018fe4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20018fe8, L0x20018fe8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20018fec, L0x20018fec <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20018ff0, L0x20018ff0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20018ff4, L0x20018ff4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20018ff8, L0x20018ff8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20018ffc, L0x20018ffc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019000, L0x20019000 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019004, L0x20019004 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019008, L0x20019008 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001900c, L0x2001900c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019010, L0x20019010 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019014, L0x20019014 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019018, L0x20019018 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001901c, L0x2001901c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019020, L0x20019020 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019024, L0x20019024 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019028, L0x20019028 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001902c, L0x2001902c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019030, L0x20019030 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019034, L0x20019034 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019038, L0x20019038 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001903c, L0x2001903c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019040, L0x20019040 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019044, L0x20019044 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019048, L0x20019048 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001904c, L0x2001904c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019050, L0x20019050 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019054, L0x20019054 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019058, L0x20019058 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001905c, L0x2001905c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019060, L0x20019060 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019064, L0x20019064 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019068, L0x20019068 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001906c, L0x2001906c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019070, L0x20019070 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019074, L0x20019074 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019078, L0x20019078 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001907c, L0x2001907c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019080, L0x20019080 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019084, L0x20019084 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019088, L0x20019088 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001908c, L0x2001908c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019090, L0x20019090 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019094, L0x20019094 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019098, L0x20019098 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001909c, L0x2001909c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190a0, L0x200190a0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190a4, L0x200190a4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190a8, L0x200190a8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190ac, L0x200190ac <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190b0, L0x200190b0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190b4, L0x200190b4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190b8, L0x200190b8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190bc, L0x200190bc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190c0, L0x200190c0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190c4, L0x200190c4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190c8, L0x200190c8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190cc, L0x200190cc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190d0, L0x200190d0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190d4, L0x200190d4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190d8, L0x200190d8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190dc, L0x200190dc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190e0, L0x200190e0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190e4, L0x200190e4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190e8, L0x200190e8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190ec, L0x200190ec <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190f0, L0x200190f0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190f4, L0x200190f4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190f8, L0x200190f8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200190fc, L0x200190fc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019100, L0x20019100 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019104, L0x20019104 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019108, L0x20019108 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001910c, L0x2001910c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019110, L0x20019110 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019114, L0x20019114 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019118, L0x20019118 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001911c, L0x2001911c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019120, L0x20019120 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019124, L0x20019124 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019128, L0x20019128 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001912c, L0x2001912c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019130, L0x20019130 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019134, L0x20019134 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019138, L0x20019138 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001913c, L0x2001913c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019140, L0x20019140 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019144, L0x20019144 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019148, L0x20019148 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001914c, L0x2001914c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019150, L0x20019150 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019154, L0x20019154 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019158, L0x20019158 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001915c, L0x2001915c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019160, L0x20019160 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019164, L0x20019164 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019168, L0x20019168 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001916c, L0x2001916c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019170, L0x20019170 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019174, L0x20019174 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019178, L0x20019178 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001917c, L0x2001917c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019180, L0x20019180 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019184, L0x20019184 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019188, L0x20019188 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001918c, L0x2001918c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019190, L0x20019190 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019194, L0x20019194 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019198, L0x20019198 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001919c, L0x2001919c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191a0, L0x200191a0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191a4, L0x200191a4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191a8, L0x200191a8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191ac, L0x200191ac <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191b0, L0x200191b0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191b4, L0x200191b4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191b8, L0x200191b8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191bc, L0x200191bc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191c0, L0x200191c0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191c4, L0x200191c4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191c8, L0x200191c8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191cc, L0x200191cc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191d0, L0x200191d0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191d4, L0x200191d4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191d8, L0x200191d8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191dc, L0x200191dc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191e0, L0x200191e0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191e4, L0x200191e4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191e8, L0x200191e8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191ec, L0x200191ec <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191f0, L0x200191f0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191f4, L0x200191f4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191f8, L0x200191f8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200191fc, L0x200191fc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019200, L0x20019200 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019204, L0x20019204 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019208, L0x20019208 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001920c, L0x2001920c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019210, L0x20019210 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019214, L0x20019214 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019218, L0x20019218 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001921c, L0x2001921c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019220, L0x20019220 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019224, L0x20019224 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019228, L0x20019228 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001922c, L0x2001922c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019230, L0x20019230 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019234, L0x20019234 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019238, L0x20019238 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001923c, L0x2001923c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019240, L0x20019240 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019244, L0x20019244 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019248, L0x20019248 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001924c, L0x2001924c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019250, L0x20019250 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019254, L0x20019254 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019258, L0x20019258 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001925c, L0x2001925c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019260, L0x20019260 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019264, L0x20019264 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019268, L0x20019268 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001926c, L0x2001926c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019270, L0x20019270 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019274, L0x20019274 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019278, L0x20019278 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001927c, L0x2001927c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019280, L0x20019280 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019284, L0x20019284 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019288, L0x20019288 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001928c, L0x2001928c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019290, L0x20019290 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019294, L0x20019294 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019298, L0x20019298 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001929c, L0x2001929c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192a0, L0x200192a0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192a4, L0x200192a4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192a8, L0x200192a8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192ac, L0x200192ac <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192b0, L0x200192b0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192b4, L0x200192b4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192b8, L0x200192b8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192bc, L0x200192bc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192c0, L0x200192c0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192c4, L0x200192c4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192c8, L0x200192c8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192cc, L0x200192cc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192d0, L0x200192d0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192d4, L0x200192d4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192d8, L0x200192d8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192dc, L0x200192dc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192e0, L0x200192e0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192e4, L0x200192e4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192e8, L0x200192e8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192ec, L0x200192ec <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192f0, L0x200192f0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192f4, L0x200192f4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192f8, L0x200192f8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200192fc, L0x200192fc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019300, L0x20019300 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019304, L0x20019304 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019308, L0x20019308 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001930c, L0x2001930c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019310, L0x20019310 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019314, L0x20019314 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019318, L0x20019318 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001931c, L0x2001931c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019320, L0x20019320 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019324, L0x20019324 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019328, L0x20019328 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001932c, L0x2001932c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019330, L0x20019330 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019334, L0x20019334 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019338, L0x20019338 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001933c, L0x2001933c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019340, L0x20019340 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019344, L0x20019344 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019348, L0x20019348 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001934c, L0x2001934c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019350, L0x20019350 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019354, L0x20019354 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019358, L0x20019358 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001935c, L0x2001935c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019360, L0x20019360 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019364, L0x20019364 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019368, L0x20019368 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001936c, L0x2001936c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019370, L0x20019370 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019374, L0x20019374 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019378, L0x20019378 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001937c, L0x2001937c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019380, L0x20019380 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019384, L0x20019384 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019388, L0x20019388 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001938c, L0x2001938c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019390, L0x20019390 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019394, L0x20019394 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019398, L0x20019398 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001939c, L0x2001939c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193a0, L0x200193a0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193a4, L0x200193a4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193a8, L0x200193a8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193ac, L0x200193ac <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193b0, L0x200193b0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193b4, L0x200193b4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193b8, L0x200193b8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193bc, L0x200193bc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193c0, L0x200193c0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193c4, L0x200193c4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193c8, L0x200193c8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193cc, L0x200193cc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193d0, L0x200193d0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193d4, L0x200193d4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193d8, L0x200193d8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193dc, L0x200193dc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193e0, L0x200193e0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193e4, L0x200193e4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193e8, L0x200193e8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193ec, L0x200193ec <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193f0, L0x200193f0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193f4, L0x200193f4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193f8, L0x200193f8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200193fc, L0x200193fc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019400, L0x20019400 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019404, L0x20019404 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019408, L0x20019408 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001940c, L0x2001940c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019410, L0x20019410 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019414, L0x20019414 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019418, L0x20019418 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001941c, L0x2001941c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019420, L0x20019420 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019424, L0x20019424 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019428, L0x20019428 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001942c, L0x2001942c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019430, L0x20019430 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019434, L0x20019434 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019438, L0x20019438 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001943c, L0x2001943c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019440, L0x20019440 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019444, L0x20019444 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019448, L0x20019448 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001944c, L0x2001944c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019450, L0x20019450 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019454, L0x20019454 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019458, L0x20019458 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001945c, L0x2001945c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019460, L0x20019460 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019464, L0x20019464 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019468, L0x20019468 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001946c, L0x2001946c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019470, L0x20019470 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019474, L0x20019474 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019478, L0x20019478 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001947c, L0x2001947c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019480, L0x20019480 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019484, L0x20019484 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019488, L0x20019488 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001948c, L0x2001948c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019490, L0x20019490 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019494, L0x20019494 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019498, L0x20019498 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001949c, L0x2001949c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194a0, L0x200194a0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194a4, L0x200194a4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194a8, L0x200194a8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194ac, L0x200194ac <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194b0, L0x200194b0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194b4, L0x200194b4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194b8, L0x200194b8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194bc, L0x200194bc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194c0, L0x200194c0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194c4, L0x200194c4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194c8, L0x200194c8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194cc, L0x200194cc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194d0, L0x200194d0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194d4, L0x200194d4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194d8, L0x200194d8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194dc, L0x200194dc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194e0, L0x200194e0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194e4, L0x200194e4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194e8, L0x200194e8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194ec, L0x200194ec <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194f0, L0x200194f0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194f4, L0x200194f4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194f8, L0x200194f8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200194fc, L0x200194fc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019500, L0x20019500 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019504, L0x20019504 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019508, L0x20019508 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001950c, L0x2001950c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019510, L0x20019510 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019514, L0x20019514 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019518, L0x20019518 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001951c, L0x2001951c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019520, L0x20019520 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019524, L0x20019524 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019528, L0x20019528 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001952c, L0x2001952c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019530, L0x20019530 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019534, L0x20019534 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019538, L0x20019538 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001953c, L0x2001953c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019540, L0x20019540 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019544, L0x20019544 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019548, L0x20019548 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001954c, L0x2001954c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019550, L0x20019550 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019554, L0x20019554 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019558, L0x20019558 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001955c, L0x2001955c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019560, L0x20019560 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019564, L0x20019564 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019568, L0x20019568 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001956c, L0x2001956c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019570, L0x20019570 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019574, L0x20019574 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019578, L0x20019578 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001957c, L0x2001957c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019580, L0x20019580 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019584, L0x20019584 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019588, L0x20019588 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001958c, L0x2001958c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019590, L0x20019590 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019594, L0x20019594 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019598, L0x20019598 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001959c, L0x2001959c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195a0, L0x200195a0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195a4, L0x200195a4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195a8, L0x200195a8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195ac, L0x200195ac <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195b0, L0x200195b0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195b4, L0x200195b4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195b8, L0x200195b8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195bc, L0x200195bc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195c0, L0x200195c0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195c4, L0x200195c4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195c8, L0x200195c8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195cc, L0x200195cc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195d0, L0x200195d0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195d4, L0x200195d4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195d8, L0x200195d8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195dc, L0x200195dc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195e0, L0x200195e0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195e4, L0x200195e4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195e8, L0x200195e8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195ec, L0x200195ec <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195f0, L0x200195f0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195f4, L0x200195f4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195f8, L0x200195f8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200195fc, L0x200195fc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019600, L0x20019600 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019604, L0x20019604 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019608, L0x20019608 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001960c, L0x2001960c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019610, L0x20019610 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019614, L0x20019614 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019618, L0x20019618 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001961c, L0x2001961c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019620, L0x20019620 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019624, L0x20019624 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019628, L0x20019628 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001962c, L0x2001962c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019630, L0x20019630 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019634, L0x20019634 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019638, L0x20019638 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001963c, L0x2001963c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019640, L0x20019640 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019644, L0x20019644 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019648, L0x20019648 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001964c, L0x2001964c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019650, L0x20019650 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019654, L0x20019654 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019658, L0x20019658 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001965c, L0x2001965c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019660, L0x20019660 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019664, L0x20019664 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019668, L0x20019668 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001966c, L0x2001966c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019670, L0x20019670 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019674, L0x20019674 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019678, L0x20019678 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001967c, L0x2001967c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019680, L0x20019680 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019684, L0x20019684 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019688, L0x20019688 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001968c, L0x2001968c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019690, L0x20019690 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019694, L0x20019694 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019698, L0x20019698 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001969c, L0x2001969c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196a0, L0x200196a0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196a4, L0x200196a4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196a8, L0x200196a8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196ac, L0x200196ac <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196b0, L0x200196b0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196b4, L0x200196b4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196b8, L0x200196b8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196bc, L0x200196bc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196c0, L0x200196c0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196c4, L0x200196c4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196c8, L0x200196c8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196cc, L0x200196cc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196d0, L0x200196d0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196d4, L0x200196d4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196d8, L0x200196d8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196dc, L0x200196dc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196e0, L0x200196e0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196e4, L0x200196e4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196e8, L0x200196e8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196ec, L0x200196ec <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196f0, L0x200196f0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196f4, L0x200196f4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196f8, L0x200196f8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200196fc, L0x200196fc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019700, L0x20019700 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019704, L0x20019704 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019708, L0x20019708 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001970c, L0x2001970c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019710, L0x20019710 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019714, L0x20019714 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019718, L0x20019718 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001971c, L0x2001971c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019720, L0x20019720 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019724, L0x20019724 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019728, L0x20019728 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001972c, L0x2001972c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019730, L0x20019730 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019734, L0x20019734 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019738, L0x20019738 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001973c, L0x2001973c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019740, L0x20019740 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019744, L0x20019744 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019748, L0x20019748 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001974c, L0x2001974c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019750, L0x20019750 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019754, L0x20019754 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019758, L0x20019758 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001975c, L0x2001975c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019760, L0x20019760 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019764, L0x20019764 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019768, L0x20019768 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001976c, L0x2001976c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019770, L0x20019770 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019774, L0x20019774 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019778, L0x20019778 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001977c, L0x2001977c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019780, L0x20019780 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019784, L0x20019784 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019788, L0x20019788 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001978c, L0x2001978c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019790, L0x20019790 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019794, L0x20019794 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019798, L0x20019798 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001979c, L0x2001979c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197a0, L0x200197a0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197a4, L0x200197a4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197a8, L0x200197a8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197ac, L0x200197ac <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197b0, L0x200197b0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197b4, L0x200197b4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197b8, L0x200197b8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197bc, L0x200197bc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197c0, L0x200197c0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197c4, L0x200197c4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197c8, L0x200197c8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197cc, L0x200197cc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197d0, L0x200197d0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197d4, L0x200197d4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197d8, L0x200197d8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197dc, L0x200197dc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197e0, L0x200197e0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197e4, L0x200197e4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197e8, L0x200197e8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197ec, L0x200197ec <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197f0, L0x200197f0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197f4, L0x200197f4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197f8, L0x200197f8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200197fc, L0x200197fc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019800, L0x20019800 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019804, L0x20019804 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019808, L0x20019808 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001980c, L0x2001980c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019810, L0x20019810 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019814, L0x20019814 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019818, L0x20019818 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001981c, L0x2001981c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019820, L0x20019820 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019824, L0x20019824 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019828, L0x20019828 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001982c, L0x2001982c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019830, L0x20019830 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019834, L0x20019834 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019838, L0x20019838 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001983c, L0x2001983c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019840, L0x20019840 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019844, L0x20019844 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019848, L0x20019848 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001984c, L0x2001984c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019850, L0x20019850 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019854, L0x20019854 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019858, L0x20019858 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001985c, L0x2001985c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019860, L0x20019860 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019864, L0x20019864 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019868, L0x20019868 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001986c, L0x2001986c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019870, L0x20019870 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019874, L0x20019874 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019878, L0x20019878 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001987c, L0x2001987c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019880, L0x20019880 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019884, L0x20019884 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019888, L0x20019888 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001988c, L0x2001988c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019890, L0x20019890 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019894, L0x20019894 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019898, L0x20019898 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001989c, L0x2001989c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198a0, L0x200198a0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198a4, L0x200198a4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198a8, L0x200198a8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198ac, L0x200198ac <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198b0, L0x200198b0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198b4, L0x200198b4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198b8, L0x200198b8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198bc, L0x200198bc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198c0, L0x200198c0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198c4, L0x200198c4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198c8, L0x200198c8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198cc, L0x200198cc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198d0, L0x200198d0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198d4, L0x200198d4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198d8, L0x200198d8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198dc, L0x200198dc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198e0, L0x200198e0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198e4, L0x200198e4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198e8, L0x200198e8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198ec, L0x200198ec <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198f0, L0x200198f0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198f4, L0x200198f4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198f8, L0x200198f8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200198fc, L0x200198fc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019900, L0x20019900 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019904, L0x20019904 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019908, L0x20019908 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001990c, L0x2001990c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019910, L0x20019910 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019914, L0x20019914 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019918, L0x20019918 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001991c, L0x2001991c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019920, L0x20019920 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019924, L0x20019924 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019928, L0x20019928 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001992c, L0x2001992c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019930, L0x20019930 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019934, L0x20019934 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019938, L0x20019938 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001993c, L0x2001993c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019940, L0x20019940 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019944, L0x20019944 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019948, L0x20019948 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001994c, L0x2001994c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019950, L0x20019950 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019954, L0x20019954 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019958, L0x20019958 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001995c, L0x2001995c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019960, L0x20019960 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019964, L0x20019964 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019968, L0x20019968 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001996c, L0x2001996c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019970, L0x20019970 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019974, L0x20019974 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019978, L0x20019978 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001997c, L0x2001997c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019980, L0x20019980 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019984, L0x20019984 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019988, L0x20019988 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001998c, L0x2001998c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019990, L0x20019990 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019994, L0x20019994 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019998, L0x20019998 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x2001999c, L0x2001999c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199a0, L0x200199a0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199a4, L0x200199a4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199a8, L0x200199a8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199ac, L0x200199ac <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199b0, L0x200199b0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199b4, L0x200199b4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199b8, L0x200199b8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199bc, L0x200199bc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199c0, L0x200199c0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199c4, L0x200199c4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199c8, L0x200199c8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199cc, L0x200199cc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199d0, L0x200199d0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199d4, L0x200199d4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199d8, L0x200199d8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199dc, L0x200199dc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199e0, L0x200199e0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199e4, L0x200199e4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199e8, L0x200199e8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199ec, L0x200199ec <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199f0, L0x200199f0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199f4, L0x200199f4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199f8, L0x200199f8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x200199fc, L0x200199fc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a00, L0x20019a00 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a04, L0x20019a04 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a08, L0x20019a08 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a0c, L0x20019a0c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a10, L0x20019a10 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a14, L0x20019a14 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a18, L0x20019a18 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a1c, L0x20019a1c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a20, L0x20019a20 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a24, L0x20019a24 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a28, L0x20019a28 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a2c, L0x20019a2c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a30, L0x20019a30 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a34, L0x20019a34 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a38, L0x20019a38 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a3c, L0x20019a3c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a40, L0x20019a40 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a44, L0x20019a44 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a48, L0x20019a48 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a4c, L0x20019a4c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a50, L0x20019a50 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a54, L0x20019a54 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a58, L0x20019a58 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a5c, L0x20019a5c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a60, L0x20019a60 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a64, L0x20019a64 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a68, L0x20019a68 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a6c, L0x20019a6c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a70, L0x20019a70 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a74, L0x20019a74 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a78, L0x20019a78 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a7c, L0x20019a7c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a80, L0x20019a80 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a84, L0x20019a84 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a88, L0x20019a88 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a8c, L0x20019a8c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a90, L0x20019a90 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a94, L0x20019a94 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a98, L0x20019a98 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019a9c, L0x20019a9c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019aa0, L0x20019aa0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019aa4, L0x20019aa4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019aa8, L0x20019aa8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019aac, L0x20019aac <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ab0, L0x20019ab0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ab4, L0x20019ab4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ab8, L0x20019ab8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019abc, L0x20019abc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ac0, L0x20019ac0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ac4, L0x20019ac4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ac8, L0x20019ac8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019acc, L0x20019acc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ad0, L0x20019ad0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ad4, L0x20019ad4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ad8, L0x20019ad8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019adc, L0x20019adc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ae0, L0x20019ae0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ae4, L0x20019ae4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ae8, L0x20019ae8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019aec, L0x20019aec <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019af0, L0x20019af0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019af4, L0x20019af4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019af8, L0x20019af8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019afc, L0x20019afc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b00, L0x20019b00 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b04, L0x20019b04 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b08, L0x20019b08 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b0c, L0x20019b0c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b10, L0x20019b10 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b14, L0x20019b14 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b18, L0x20019b18 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b1c, L0x20019b1c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b20, L0x20019b20 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b24, L0x20019b24 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b28, L0x20019b28 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b2c, L0x20019b2c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b30, L0x20019b30 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b34, L0x20019b34 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b38, L0x20019b38 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b3c, L0x20019b3c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b40, L0x20019b40 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b44, L0x20019b44 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b48, L0x20019b48 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b4c, L0x20019b4c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b50, L0x20019b50 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b54, L0x20019b54 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b58, L0x20019b58 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b5c, L0x20019b5c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b60, L0x20019b60 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b64, L0x20019b64 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b68, L0x20019b68 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b6c, L0x20019b6c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b70, L0x20019b70 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b74, L0x20019b74 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b78, L0x20019b78 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b7c, L0x20019b7c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b80, L0x20019b80 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b84, L0x20019b84 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b88, L0x20019b88 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b8c, L0x20019b8c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b90, L0x20019b90 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b94, L0x20019b94 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b98, L0x20019b98 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019b9c, L0x20019b9c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ba0, L0x20019ba0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ba4, L0x20019ba4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ba8, L0x20019ba8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019bac, L0x20019bac <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019bb0, L0x20019bb0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019bb4, L0x20019bb4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019bb8, L0x20019bb8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019bbc, L0x20019bbc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019bc0, L0x20019bc0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019bc4, L0x20019bc4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019bc8, L0x20019bc8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019bcc, L0x20019bcc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019bd0, L0x20019bd0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019bd4, L0x20019bd4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019bd8, L0x20019bd8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019bdc, L0x20019bdc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019be0, L0x20019be0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019be4, L0x20019be4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019be8, L0x20019be8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019bec, L0x20019bec <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019bf0, L0x20019bf0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019bf4, L0x20019bf4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019bf8, L0x20019bf8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019bfc, L0x20019bfc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c00, L0x20019c00 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c04, L0x20019c04 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c08, L0x20019c08 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c0c, L0x20019c0c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c10, L0x20019c10 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c14, L0x20019c14 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c18, L0x20019c18 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c1c, L0x20019c1c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c20, L0x20019c20 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c24, L0x20019c24 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c28, L0x20019c28 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c2c, L0x20019c2c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c30, L0x20019c30 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c34, L0x20019c34 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c38, L0x20019c38 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c3c, L0x20019c3c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c40, L0x20019c40 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c44, L0x20019c44 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c48, L0x20019c48 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c4c, L0x20019c4c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c50, L0x20019c50 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c54, L0x20019c54 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c58, L0x20019c58 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c5c, L0x20019c5c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c60, L0x20019c60 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c64, L0x20019c64 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c68, L0x20019c68 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c6c, L0x20019c6c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c70, L0x20019c70 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c74, L0x20019c74 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c78, L0x20019c78 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c7c, L0x20019c7c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c80, L0x20019c80 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c84, L0x20019c84 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c88, L0x20019c88 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c8c, L0x20019c8c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c90, L0x20019c90 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c94, L0x20019c94 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c98, L0x20019c98 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019c9c, L0x20019c9c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ca0, L0x20019ca0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ca4, L0x20019ca4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ca8, L0x20019ca8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019cac, L0x20019cac <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019cb0, L0x20019cb0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019cb4, L0x20019cb4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019cb8, L0x20019cb8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019cbc, L0x20019cbc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019cc0, L0x20019cc0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019cc4, L0x20019cc4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019cc8, L0x20019cc8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ccc, L0x20019ccc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019cd0, L0x20019cd0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019cd4, L0x20019cd4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019cd8, L0x20019cd8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019cdc, L0x20019cdc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ce0, L0x20019ce0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ce4, L0x20019ce4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ce8, L0x20019ce8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019cec, L0x20019cec <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019cf0, L0x20019cf0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019cf4, L0x20019cf4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019cf8, L0x20019cf8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019cfc, L0x20019cfc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d00, L0x20019d00 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d04, L0x20019d04 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d08, L0x20019d08 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d0c, L0x20019d0c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d10, L0x20019d10 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d14, L0x20019d14 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d18, L0x20019d18 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d1c, L0x20019d1c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d20, L0x20019d20 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d24, L0x20019d24 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d28, L0x20019d28 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d2c, L0x20019d2c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d30, L0x20019d30 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d34, L0x20019d34 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d38, L0x20019d38 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d3c, L0x20019d3c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d40, L0x20019d40 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d44, L0x20019d44 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d48, L0x20019d48 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d4c, L0x20019d4c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d50, L0x20019d50 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d54, L0x20019d54 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d58, L0x20019d58 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d5c, L0x20019d5c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d60, L0x20019d60 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d64, L0x20019d64 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d68, L0x20019d68 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d6c, L0x20019d6c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d70, L0x20019d70 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d74, L0x20019d74 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d78, L0x20019d78 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d7c, L0x20019d7c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d80, L0x20019d80 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d84, L0x20019d84 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d88, L0x20019d88 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d8c, L0x20019d8c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d90, L0x20019d90 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d94, L0x20019d94 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d98, L0x20019d98 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019d9c, L0x20019d9c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019da0, L0x20019da0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019da4, L0x20019da4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019da8, L0x20019da8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019dac, L0x20019dac <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019db0, L0x20019db0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019db4, L0x20019db4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019db8, L0x20019db8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019dbc, L0x20019dbc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019dc0, L0x20019dc0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019dc4, L0x20019dc4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019dc8, L0x20019dc8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019dcc, L0x20019dcc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019dd0, L0x20019dd0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019dd4, L0x20019dd4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019dd8, L0x20019dd8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ddc, L0x20019ddc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019de0, L0x20019de0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019de4, L0x20019de4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019de8, L0x20019de8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019dec, L0x20019dec <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019df0, L0x20019df0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019df4, L0x20019df4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019df8, L0x20019df8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019dfc, L0x20019dfc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e00, L0x20019e00 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e04, L0x20019e04 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e08, L0x20019e08 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e0c, L0x20019e0c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e10, L0x20019e10 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e14, L0x20019e14 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e18, L0x20019e18 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e1c, L0x20019e1c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e20, L0x20019e20 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e24, L0x20019e24 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e28, L0x20019e28 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e2c, L0x20019e2c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e30, L0x20019e30 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e34, L0x20019e34 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e38, L0x20019e38 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e3c, L0x20019e3c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e40, L0x20019e40 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e44, L0x20019e44 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e48, L0x20019e48 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e4c, L0x20019e4c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e50, L0x20019e50 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e54, L0x20019e54 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e58, L0x20019e58 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e5c, L0x20019e5c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e60, L0x20019e60 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e64, L0x20019e64 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e68, L0x20019e68 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e6c, L0x20019e6c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e70, L0x20019e70 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e74, L0x20019e74 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e78, L0x20019e78 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e7c, L0x20019e7c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e80, L0x20019e80 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e84, L0x20019e84 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e88, L0x20019e88 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e8c, L0x20019e8c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e90, L0x20019e90 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e94, L0x20019e94 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e98, L0x20019e98 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019e9c, L0x20019e9c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ea0, L0x20019ea0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ea4, L0x20019ea4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ea8, L0x20019ea8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019eac, L0x20019eac <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019eb0, L0x20019eb0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019eb4, L0x20019eb4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019eb8, L0x20019eb8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ebc, L0x20019ebc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ec0, L0x20019ec0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ec4, L0x20019ec4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ec8, L0x20019ec8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ecc, L0x20019ecc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ed0, L0x20019ed0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ed4, L0x20019ed4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ed8, L0x20019ed8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019edc, L0x20019edc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ee0, L0x20019ee0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ee4, L0x20019ee4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ee8, L0x20019ee8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019eec, L0x20019eec <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ef0, L0x20019ef0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ef4, L0x20019ef4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019ef8, L0x20019ef8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019efc, L0x20019efc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f00, L0x20019f00 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f04, L0x20019f04 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f08, L0x20019f08 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f0c, L0x20019f0c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f10, L0x20019f10 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f14, L0x20019f14 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f18, L0x20019f18 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f1c, L0x20019f1c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f20, L0x20019f20 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f24, L0x20019f24 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f28, L0x20019f28 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f2c, L0x20019f2c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f30, L0x20019f30 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f34, L0x20019f34 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f38, L0x20019f38 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f3c, L0x20019f3c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f40, L0x20019f40 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f44, L0x20019f44 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f48, L0x20019f48 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f4c, L0x20019f4c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f50, L0x20019f50 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f54, L0x20019f54 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f58, L0x20019f58 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f5c, L0x20019f5c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f60, L0x20019f60 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f64, L0x20019f64 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f68, L0x20019f68 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f6c, L0x20019f6c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f70, L0x20019f70 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f74, L0x20019f74 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f78, L0x20019f78 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f7c, L0x20019f7c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f80, L0x20019f80 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f84, L0x20019f84 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f88, L0x20019f88 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f8c, L0x20019f8c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f90, L0x20019f90 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f94, L0x20019f94 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f98, L0x20019f98 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019f9c, L0x20019f9c <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019fa0, L0x20019fa0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019fa4, L0x20019fa4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019fa8, L0x20019fa8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019fac, L0x20019fac <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019fb0, L0x20019fb0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019fb4, L0x20019fb4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019fb8, L0x20019fb8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019fbc, L0x20019fbc <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019fc0, L0x20019fc0 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019fc4, L0x20019fc4 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019fc8, L0x20019fc8 <s 128@32*1043969@32,
(-128)@32*1043969@32 <s L0x20019fcc, L0x20019fcc <s 128@32*1043969@32
]
}

