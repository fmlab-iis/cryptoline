(* frege: -v -isafety -slicing -no_carry_constraint -jobs 32 __asm_ntt_0_1_2_3_32.cl
Parsing Cryptoline file:                [OK]            0.292005 seconds
Checking well-formedness:               [OK]            0.135788 seconds
Transforming to SSA form:               [OK]            0.045081 seconds
Normalizing specification:              [OK]            0.000635 seconds
Rewriting assignments:                  [OK]            0.037358 seconds
Verifying program safety:               [OK]            1459.781852 seconds
Verifying range assertions:             [OK]            181.322094 seconds
Verifying range specification:          [OK]            1436.832006 seconds
Rewriting value-preserved casting:      [OK]            0.002095 seconds
Verifying algebraic assertions:         [OK]            257.602608 seconds
Verifying algebraic specification:      [OK]            601.714996 seconds
Verification result:                    [OK]            3937.788839 seconds
*)

proc main (
sint32 f000, sint32 f001, sint32 f002, sint32 f003, sint32 f004,
sint32 f005, sint32 f006, sint32 f007, sint32 f008, sint32 f009,
sint32 f010, sint32 f011, sint32 f012, sint32 f013, sint32 f014,
sint32 f015, sint32 f016, sint32 f017, sint32 f018, sint32 f019,
sint32 f020, sint32 f021, sint32 f022, sint32 f023, sint32 f024,
sint32 f025, sint32 f026, sint32 f027, sint32 f028, sint32 f029,
sint32 f030, sint32 f031, sint32 f032, sint32 f033, sint32 f034,
sint32 f035, sint32 f036, sint32 f037, sint32 f038, sint32 f039,
sint32 f040, sint32 f041, sint32 f042, sint32 f043, sint32 f044,
sint32 f045, sint32 f046, sint32 f047, sint32 f048, sint32 f049,
sint32 f050, sint32 f051, sint32 f052, sint32 f053, sint32 f054,
sint32 f055, sint32 f056, sint32 f057, sint32 f058, sint32 f059,
sint32 f060, sint32 f061, sint32 f062, sint32 f063, sint32 f064,
sint32 f065, sint32 f066, sint32 f067, sint32 f068, sint32 f069,
sint32 f070, sint32 f071, sint32 f072, sint32 f073, sint32 f074,
sint32 f075, sint32 f076, sint32 f077, sint32 f078, sint32 f079,
sint32 f080, sint32 f081, sint32 f082, sint32 f083, sint32 f084,
sint32 f085, sint32 f086, sint32 f087, sint32 f088, sint32 f089,
sint32 f090, sint32 f091, sint32 f092, sint32 f093, sint32 f094,
sint32 f095, sint32 f096, sint32 f097, sint32 f098, sint32 f099,
sint32 f100, sint32 f101, sint32 f102, sint32 f103, sint32 f104,
sint32 f105, sint32 f106, sint32 f107, sint32 f108, sint32 f109,
sint32 f110, sint32 f111, sint32 f112, sint32 f113, sint32 f114,
sint32 f115, sint32 f116, sint32 f117, sint32 f118, sint32 f119,
sint32 f120, sint32 f121, sint32 f122, sint32 f123, sint32 f124,
sint32 f125, sint32 f126, sint32 f127, sint32 f128, sint32 f129,
sint32 f130, sint32 f131, sint32 f132, sint32 f133, sint32 f134,
sint32 f135, sint32 f136, sint32 f137, sint32 f138, sint32 f139,
sint32 f140, sint32 f141, sint32 f142, sint32 f143, sint32 f144,
sint32 f145, sint32 f146, sint32 f147, sint32 f148, sint32 f149,
sint32 f150, sint32 f151, sint32 f152, sint32 f153, sint32 f154,
sint32 f155, sint32 f156, sint32 f157, sint32 f158, sint32 f159,
sint32 f160, sint32 f161, sint32 f162, sint32 f163, sint32 f164,
sint32 f165, sint32 f166, sint32 f167, sint32 f168, sint32 f169,
sint32 f170, sint32 f171, sint32 f172, sint32 f173, sint32 f174,
sint32 f175, sint32 f176, sint32 f177, sint32 f178, sint32 f179,
sint32 f180, sint32 f181, sint32 f182, sint32 f183, sint32 f184,
sint32 f185, sint32 f186, sint32 f187, sint32 f188, sint32 f189,
sint32 f190, sint32 f191, sint32 f192, sint32 f193, sint32 f194,
sint32 f195, sint32 f196, sint32 f197, sint32 f198, sint32 f199,
sint32 f200, sint32 f201, sint32 f202, sint32 f203, sint32 f204,
sint32 f205, sint32 f206, sint32 f207, sint32 f208, sint32 f209,
sint32 f210, sint32 f211, sint32 f212, sint32 f213, sint32 f214,
sint32 f215, sint32 f216, sint32 f217, sint32 f218, sint32 f219,
sint32 f220, sint32 f221, sint32 f222, sint32 f223, sint32 f224,
sint32 f225, sint32 f226, sint32 f227, sint32 f228, sint32 f229,
sint32 f230, sint32 f231, sint32 f232, sint32 f233, sint32 f234,
sint32 f235, sint32 f236, sint32 f237, sint32 f238, sint32 f239,
sint32 f240, sint32 f241, sint32 f242, sint32 f243, sint32 f244,
sint32 f245, sint32 f246, sint32 f247, sint32 f248, sint32 f249,
sint32 f250, sint32 f251, sint32 f252, sint32 f253, sint32 f254,
sint32 f255, sint32 f256, sint32 f257, sint32 f258, sint32 f259,
sint32 f260, sint32 f261, sint32 f262, sint32 f263, sint32 f264,
sint32 f265, sint32 f266, sint32 f267, sint32 f268, sint32 f269,
sint32 f270, sint32 f271, sint32 f272, sint32 f273, sint32 f274,
sint32 f275, sint32 f276, sint32 f277, sint32 f278, sint32 f279,
sint32 f280, sint32 f281, sint32 f282, sint32 f283, sint32 f284,
sint32 f285, sint32 f286, sint32 f287, sint32 f288, sint32 f289,
sint32 f290, sint32 f291, sint32 f292, sint32 f293, sint32 f294,
sint32 f295, sint32 f296, sint32 f297, sint32 f298, sint32 f299,
sint32 f300, sint32 f301, sint32 f302, sint32 f303, sint32 f304,
sint32 f305, sint32 f306, sint32 f307, sint32 f308, sint32 f309,
sint32 f310, sint32 f311, sint32 f312, sint32 f313, sint32 f314,
sint32 f315, sint32 f316, sint32 f317, sint32 f318, sint32 f319,
sint32 f320, sint32 f321, sint32 f322, sint32 f323, sint32 f324,
sint32 f325, sint32 f326, sint32 f327, sint32 f328, sint32 f329,
sint32 f330, sint32 f331, sint32 f332, sint32 f333, sint32 f334,
sint32 f335, sint32 f336, sint32 f337, sint32 f338, sint32 f339,
sint32 f340, sint32 f341, sint32 f342, sint32 f343, sint32 f344,
sint32 f345, sint32 f346, sint32 f347, sint32 f348, sint32 f349,
sint32 f350, sint32 f351, sint32 f352, sint32 f353, sint32 f354,
sint32 f355, sint32 f356, sint32 f357, sint32 f358, sint32 f359,
sint32 f360, sint32 f361, sint32 f362, sint32 f363, sint32 f364,
sint32 f365, sint32 f366, sint32 f367, sint32 f368, sint32 f369,
sint32 f370, sint32 f371, sint32 f372, sint32 f373, sint32 f374,
sint32 f375, sint32 f376, sint32 f377, sint32 f378, sint32 f379,
sint32 f380, sint32 f381, sint32 f382, sint32 f383, sint32 f384,
sint32 f385, sint32 f386, sint32 f387, sint32 f388, sint32 f389,
sint32 f390, sint32 f391, sint32 f392, sint32 f393, sint32 f394,
sint32 f395, sint32 f396, sint32 f397, sint32 f398, sint32 f399,
sint32 f400, sint32 f401, sint32 f402, sint32 f403, sint32 f404,
sint32 f405, sint32 f406, sint32 f407, sint32 f408, sint32 f409,
sint32 f410, sint32 f411, sint32 f412, sint32 f413, sint32 f414,
sint32 f415, sint32 f416, sint32 f417, sint32 f418, sint32 f419,
sint32 f420, sint32 f421, sint32 f422, sint32 f423, sint32 f424,
sint32 f425, sint32 f426, sint32 f427, sint32 f428, sint32 f429,
sint32 f430, sint32 f431, sint32 f432, sint32 f433, sint32 f434,
sint32 f435, sint32 f436, sint32 f437, sint32 f438, sint32 f439,
sint32 f440, sint32 f441, sint32 f442, sint32 f443, sint32 f444,
sint32 f445, sint32 f446, sint32 f447, sint32 f448, sint32 f449,
sint32 f450, sint32 f451, sint32 f452, sint32 f453, sint32 f454,
sint32 f455, sint32 f456, sint32 f457, sint32 f458, sint32 f459,
sint32 f460, sint32 f461, sint32 f462, sint32 f463, sint32 f464,
sint32 f465, sint32 f466, sint32 f467, sint32 f468, sint32 f469,
sint32 f470, sint32 f471, sint32 f472, sint32 f473, sint32 f474,
sint32 f475, sint32 f476, sint32 f477, sint32 f478, sint32 f479,
sint32 f480, sint32 f481, sint32 f482, sint32 f483, sint32 f484,
sint32 f485, sint32 f486, sint32 f487, sint32 f488, sint32 f489,
sint32 f490, sint32 f491, sint32 f492, sint32 f493, sint32 f494,
sint32 f495, sint32 f496, sint32 f497, sint32 f498, sint32 f499,
sint32 f500, sint32 f501, sint32 f502, sint32 f503, sint32 f504,
sint32 f505, sint32 f506, sint32 f507, sint32 f508
) =
{
true && and [
(-2048)@32 <=s f000, f000 <s 2048@32, (-2048)@32 <=s f001, f001 <s 2048@32,
(-2048)@32 <=s f002, f002 <s 2048@32, (-2048)@32 <=s f003, f003 <s 2048@32,
(-2048)@32 <=s f004, f004 <s 2048@32, (-2048)@32 <=s f005, f005 <s 2048@32,
(-2048)@32 <=s f006, f006 <s 2048@32, (-2048)@32 <=s f007, f007 <s 2048@32,
(-2048)@32 <=s f008, f008 <s 2048@32, (-2048)@32 <=s f009, f009 <s 2048@32,
(-2048)@32 <=s f010, f010 <s 2048@32, (-2048)@32 <=s f011, f011 <s 2048@32,
(-2048)@32 <=s f012, f012 <s 2048@32, (-2048)@32 <=s f013, f013 <s 2048@32,
(-2048)@32 <=s f014, f014 <s 2048@32, (-2048)@32 <=s f015, f015 <s 2048@32,
(-2048)@32 <=s f016, f016 <s 2048@32, (-2048)@32 <=s f017, f017 <s 2048@32,
(-2048)@32 <=s f018, f018 <s 2048@32, (-2048)@32 <=s f019, f019 <s 2048@32,
(-2048)@32 <=s f020, f020 <s 2048@32, (-2048)@32 <=s f021, f021 <s 2048@32,
(-2048)@32 <=s f022, f022 <s 2048@32, (-2048)@32 <=s f023, f023 <s 2048@32,
(-2048)@32 <=s f024, f024 <s 2048@32, (-2048)@32 <=s f025, f025 <s 2048@32,
(-2048)@32 <=s f026, f026 <s 2048@32, (-2048)@32 <=s f027, f027 <s 2048@32,
(-2048)@32 <=s f028, f028 <s 2048@32, (-2048)@32 <=s f029, f029 <s 2048@32,
(-2048)@32 <=s f030, f030 <s 2048@32, (-2048)@32 <=s f031, f031 <s 2048@32,
(-2048)@32 <=s f032, f032 <s 2048@32, (-2048)@32 <=s f033, f033 <s 2048@32,
(-2048)@32 <=s f034, f034 <s 2048@32, (-2048)@32 <=s f035, f035 <s 2048@32,
(-2048)@32 <=s f036, f036 <s 2048@32, (-2048)@32 <=s f037, f037 <s 2048@32,
(-2048)@32 <=s f038, f038 <s 2048@32, (-2048)@32 <=s f039, f039 <s 2048@32,
(-2048)@32 <=s f040, f040 <s 2048@32, (-2048)@32 <=s f041, f041 <s 2048@32,
(-2048)@32 <=s f042, f042 <s 2048@32, (-2048)@32 <=s f043, f043 <s 2048@32,
(-2048)@32 <=s f044, f044 <s 2048@32, (-2048)@32 <=s f045, f045 <s 2048@32,
(-2048)@32 <=s f046, f046 <s 2048@32, (-2048)@32 <=s f047, f047 <s 2048@32,
(-2048)@32 <=s f048, f048 <s 2048@32, (-2048)@32 <=s f049, f049 <s 2048@32,
(-2048)@32 <=s f050, f050 <s 2048@32, (-2048)@32 <=s f051, f051 <s 2048@32,
(-2048)@32 <=s f052, f052 <s 2048@32, (-2048)@32 <=s f053, f053 <s 2048@32,
(-2048)@32 <=s f054, f054 <s 2048@32, (-2048)@32 <=s f055, f055 <s 2048@32,
(-2048)@32 <=s f056, f056 <s 2048@32, (-2048)@32 <=s f057, f057 <s 2048@32,
(-2048)@32 <=s f058, f058 <s 2048@32, (-2048)@32 <=s f059, f059 <s 2048@32,
(-2048)@32 <=s f060, f060 <s 2048@32, (-2048)@32 <=s f061, f061 <s 2048@32,
(-2048)@32 <=s f062, f062 <s 2048@32, (-2048)@32 <=s f063, f063 <s 2048@32,
(-2048)@32 <=s f064, f064 <s 2048@32, (-2048)@32 <=s f065, f065 <s 2048@32,
(-2048)@32 <=s f066, f066 <s 2048@32, (-2048)@32 <=s f067, f067 <s 2048@32,
(-2048)@32 <=s f068, f068 <s 2048@32, (-2048)@32 <=s f069, f069 <s 2048@32,
(-2048)@32 <=s f070, f070 <s 2048@32, (-2048)@32 <=s f071, f071 <s 2048@32,
(-2048)@32 <=s f072, f072 <s 2048@32, (-2048)@32 <=s f073, f073 <s 2048@32,
(-2048)@32 <=s f074, f074 <s 2048@32, (-2048)@32 <=s f075, f075 <s 2048@32,
(-2048)@32 <=s f076, f076 <s 2048@32, (-2048)@32 <=s f077, f077 <s 2048@32,
(-2048)@32 <=s f078, f078 <s 2048@32, (-2048)@32 <=s f079, f079 <s 2048@32,
(-2048)@32 <=s f080, f080 <s 2048@32, (-2048)@32 <=s f081, f081 <s 2048@32,
(-2048)@32 <=s f082, f082 <s 2048@32, (-2048)@32 <=s f083, f083 <s 2048@32,
(-2048)@32 <=s f084, f084 <s 2048@32, (-2048)@32 <=s f085, f085 <s 2048@32,
(-2048)@32 <=s f086, f086 <s 2048@32, (-2048)@32 <=s f087, f087 <s 2048@32,
(-2048)@32 <=s f088, f088 <s 2048@32, (-2048)@32 <=s f089, f089 <s 2048@32,
(-2048)@32 <=s f090, f090 <s 2048@32, (-2048)@32 <=s f091, f091 <s 2048@32,
(-2048)@32 <=s f092, f092 <s 2048@32, (-2048)@32 <=s f093, f093 <s 2048@32,
(-2048)@32 <=s f094, f094 <s 2048@32, (-2048)@32 <=s f095, f095 <s 2048@32,
(-2048)@32 <=s f096, f096 <s 2048@32, (-2048)@32 <=s f097, f097 <s 2048@32,
(-2048)@32 <=s f098, f098 <s 2048@32, (-2048)@32 <=s f099, f099 <s 2048@32,
(-2048)@32 <=s f100, f100 <s 2048@32, (-2048)@32 <=s f101, f101 <s 2048@32,
(-2048)@32 <=s f102, f102 <s 2048@32, (-2048)@32 <=s f103, f103 <s 2048@32,
(-2048)@32 <=s f104, f104 <s 2048@32, (-2048)@32 <=s f105, f105 <s 2048@32,
(-2048)@32 <=s f106, f106 <s 2048@32, (-2048)@32 <=s f107, f107 <s 2048@32,
(-2048)@32 <=s f108, f108 <s 2048@32, (-2048)@32 <=s f109, f109 <s 2048@32,
(-2048)@32 <=s f110, f110 <s 2048@32, (-2048)@32 <=s f111, f111 <s 2048@32,
(-2048)@32 <=s f112, f112 <s 2048@32, (-2048)@32 <=s f113, f113 <s 2048@32,
(-2048)@32 <=s f114, f114 <s 2048@32, (-2048)@32 <=s f115, f115 <s 2048@32,
(-2048)@32 <=s f116, f116 <s 2048@32, (-2048)@32 <=s f117, f117 <s 2048@32,
(-2048)@32 <=s f118, f118 <s 2048@32, (-2048)@32 <=s f119, f119 <s 2048@32,
(-2048)@32 <=s f120, f120 <s 2048@32, (-2048)@32 <=s f121, f121 <s 2048@32,
(-2048)@32 <=s f122, f122 <s 2048@32, (-2048)@32 <=s f123, f123 <s 2048@32,
(-2048)@32 <=s f124, f124 <s 2048@32, (-2048)@32 <=s f125, f125 <s 2048@32,
(-2048)@32 <=s f126, f126 <s 2048@32, (-2048)@32 <=s f127, f127 <s 2048@32,
(-2048)@32 <=s f128, f128 <s 2048@32, (-2048)@32 <=s f129, f129 <s 2048@32,
(-2048)@32 <=s f130, f130 <s 2048@32, (-2048)@32 <=s f131, f131 <s 2048@32,
(-2048)@32 <=s f132, f132 <s 2048@32, (-2048)@32 <=s f133, f133 <s 2048@32,
(-2048)@32 <=s f134, f134 <s 2048@32, (-2048)@32 <=s f135, f135 <s 2048@32,
(-2048)@32 <=s f136, f136 <s 2048@32, (-2048)@32 <=s f137, f137 <s 2048@32,
(-2048)@32 <=s f138, f138 <s 2048@32, (-2048)@32 <=s f139, f139 <s 2048@32,
(-2048)@32 <=s f140, f140 <s 2048@32, (-2048)@32 <=s f141, f141 <s 2048@32,
(-2048)@32 <=s f142, f142 <s 2048@32, (-2048)@32 <=s f143, f143 <s 2048@32,
(-2048)@32 <=s f144, f144 <s 2048@32, (-2048)@32 <=s f145, f145 <s 2048@32,
(-2048)@32 <=s f146, f146 <s 2048@32, (-2048)@32 <=s f147, f147 <s 2048@32,
(-2048)@32 <=s f148, f148 <s 2048@32, (-2048)@32 <=s f149, f149 <s 2048@32,
(-2048)@32 <=s f150, f150 <s 2048@32, (-2048)@32 <=s f151, f151 <s 2048@32,
(-2048)@32 <=s f152, f152 <s 2048@32, (-2048)@32 <=s f153, f153 <s 2048@32,
(-2048)@32 <=s f154, f154 <s 2048@32, (-2048)@32 <=s f155, f155 <s 2048@32,
(-2048)@32 <=s f156, f156 <s 2048@32, (-2048)@32 <=s f157, f157 <s 2048@32,
(-2048)@32 <=s f158, f158 <s 2048@32, (-2048)@32 <=s f159, f159 <s 2048@32,
(-2048)@32 <=s f160, f160 <s 2048@32, (-2048)@32 <=s f161, f161 <s 2048@32,
(-2048)@32 <=s f162, f162 <s 2048@32, (-2048)@32 <=s f163, f163 <s 2048@32,
(-2048)@32 <=s f164, f164 <s 2048@32, (-2048)@32 <=s f165, f165 <s 2048@32,
(-2048)@32 <=s f166, f166 <s 2048@32, (-2048)@32 <=s f167, f167 <s 2048@32,
(-2048)@32 <=s f168, f168 <s 2048@32, (-2048)@32 <=s f169, f169 <s 2048@32,
(-2048)@32 <=s f170, f170 <s 2048@32, (-2048)@32 <=s f171, f171 <s 2048@32,
(-2048)@32 <=s f172, f172 <s 2048@32, (-2048)@32 <=s f173, f173 <s 2048@32,
(-2048)@32 <=s f174, f174 <s 2048@32, (-2048)@32 <=s f175, f175 <s 2048@32,
(-2048)@32 <=s f176, f176 <s 2048@32, (-2048)@32 <=s f177, f177 <s 2048@32,
(-2048)@32 <=s f178, f178 <s 2048@32, (-2048)@32 <=s f179, f179 <s 2048@32,
(-2048)@32 <=s f180, f180 <s 2048@32, (-2048)@32 <=s f181, f181 <s 2048@32,
(-2048)@32 <=s f182, f182 <s 2048@32, (-2048)@32 <=s f183, f183 <s 2048@32,
(-2048)@32 <=s f184, f184 <s 2048@32, (-2048)@32 <=s f185, f185 <s 2048@32,
(-2048)@32 <=s f186, f186 <s 2048@32, (-2048)@32 <=s f187, f187 <s 2048@32,
(-2048)@32 <=s f188, f188 <s 2048@32, (-2048)@32 <=s f189, f189 <s 2048@32,
(-2048)@32 <=s f190, f190 <s 2048@32, (-2048)@32 <=s f191, f191 <s 2048@32,
(-2048)@32 <=s f192, f192 <s 2048@32, (-2048)@32 <=s f193, f193 <s 2048@32,
(-2048)@32 <=s f194, f194 <s 2048@32, (-2048)@32 <=s f195, f195 <s 2048@32,
(-2048)@32 <=s f196, f196 <s 2048@32, (-2048)@32 <=s f197, f197 <s 2048@32,
(-2048)@32 <=s f198, f198 <s 2048@32, (-2048)@32 <=s f199, f199 <s 2048@32,
(-2048)@32 <=s f200, f200 <s 2048@32, (-2048)@32 <=s f201, f201 <s 2048@32,
(-2048)@32 <=s f202, f202 <s 2048@32, (-2048)@32 <=s f203, f203 <s 2048@32,
(-2048)@32 <=s f204, f204 <s 2048@32, (-2048)@32 <=s f205, f205 <s 2048@32,
(-2048)@32 <=s f206, f206 <s 2048@32, (-2048)@32 <=s f207, f207 <s 2048@32,
(-2048)@32 <=s f208, f208 <s 2048@32, (-2048)@32 <=s f209, f209 <s 2048@32,
(-2048)@32 <=s f210, f210 <s 2048@32, (-2048)@32 <=s f211, f211 <s 2048@32,
(-2048)@32 <=s f212, f212 <s 2048@32, (-2048)@32 <=s f213, f213 <s 2048@32,
(-2048)@32 <=s f214, f214 <s 2048@32, (-2048)@32 <=s f215, f215 <s 2048@32,
(-2048)@32 <=s f216, f216 <s 2048@32, (-2048)@32 <=s f217, f217 <s 2048@32,
(-2048)@32 <=s f218, f218 <s 2048@32, (-2048)@32 <=s f219, f219 <s 2048@32,
(-2048)@32 <=s f220, f220 <s 2048@32, (-2048)@32 <=s f221, f221 <s 2048@32,
(-2048)@32 <=s f222, f222 <s 2048@32, (-2048)@32 <=s f223, f223 <s 2048@32,
(-2048)@32 <=s f224, f224 <s 2048@32, (-2048)@32 <=s f225, f225 <s 2048@32,
(-2048)@32 <=s f226, f226 <s 2048@32, (-2048)@32 <=s f227, f227 <s 2048@32,
(-2048)@32 <=s f228, f228 <s 2048@32, (-2048)@32 <=s f229, f229 <s 2048@32,
(-2048)@32 <=s f230, f230 <s 2048@32, (-2048)@32 <=s f231, f231 <s 2048@32,
(-2048)@32 <=s f232, f232 <s 2048@32, (-2048)@32 <=s f233, f233 <s 2048@32,
(-2048)@32 <=s f234, f234 <s 2048@32, (-2048)@32 <=s f235, f235 <s 2048@32,
(-2048)@32 <=s f236, f236 <s 2048@32, (-2048)@32 <=s f237, f237 <s 2048@32,
(-2048)@32 <=s f238, f238 <s 2048@32, (-2048)@32 <=s f239, f239 <s 2048@32,
(-2048)@32 <=s f240, f240 <s 2048@32, (-2048)@32 <=s f241, f241 <s 2048@32,
(-2048)@32 <=s f242, f242 <s 2048@32, (-2048)@32 <=s f243, f243 <s 2048@32,
(-2048)@32 <=s f244, f244 <s 2048@32, (-2048)@32 <=s f245, f245 <s 2048@32,
(-2048)@32 <=s f246, f246 <s 2048@32, (-2048)@32 <=s f247, f247 <s 2048@32,
(-2048)@32 <=s f248, f248 <s 2048@32, (-2048)@32 <=s f249, f249 <s 2048@32,
(-2048)@32 <=s f250, f250 <s 2048@32, (-2048)@32 <=s f251, f251 <s 2048@32,
(-2048)@32 <=s f252, f252 <s 2048@32, (-2048)@32 <=s f253, f253 <s 2048@32,
(-2048)@32 <=s f254, f254 <s 2048@32, (-2048)@32 <=s f255, f255 <s 2048@32,
(-2048)@32 <=s f256, f256 <s 2048@32, (-2048)@32 <=s f257, f257 <s 2048@32,
(-2048)@32 <=s f258, f258 <s 2048@32, (-2048)@32 <=s f259, f259 <s 2048@32,
(-2048)@32 <=s f260, f260 <s 2048@32, (-2048)@32 <=s f261, f261 <s 2048@32,
(-2048)@32 <=s f262, f262 <s 2048@32, (-2048)@32 <=s f263, f263 <s 2048@32,
(-2048)@32 <=s f264, f264 <s 2048@32, (-2048)@32 <=s f265, f265 <s 2048@32,
(-2048)@32 <=s f266, f266 <s 2048@32, (-2048)@32 <=s f267, f267 <s 2048@32,
(-2048)@32 <=s f268, f268 <s 2048@32, (-2048)@32 <=s f269, f269 <s 2048@32,
(-2048)@32 <=s f270, f270 <s 2048@32, (-2048)@32 <=s f271, f271 <s 2048@32,
(-2048)@32 <=s f272, f272 <s 2048@32, (-2048)@32 <=s f273, f273 <s 2048@32,
(-2048)@32 <=s f274, f274 <s 2048@32, (-2048)@32 <=s f275, f275 <s 2048@32,
(-2048)@32 <=s f276, f276 <s 2048@32, (-2048)@32 <=s f277, f277 <s 2048@32,
(-2048)@32 <=s f278, f278 <s 2048@32, (-2048)@32 <=s f279, f279 <s 2048@32,
(-2048)@32 <=s f280, f280 <s 2048@32, (-2048)@32 <=s f281, f281 <s 2048@32,
(-2048)@32 <=s f282, f282 <s 2048@32, (-2048)@32 <=s f283, f283 <s 2048@32,
(-2048)@32 <=s f284, f284 <s 2048@32, (-2048)@32 <=s f285, f285 <s 2048@32,
(-2048)@32 <=s f286, f286 <s 2048@32, (-2048)@32 <=s f287, f287 <s 2048@32,
(-2048)@32 <=s f288, f288 <s 2048@32, (-2048)@32 <=s f289, f289 <s 2048@32,
(-2048)@32 <=s f290, f290 <s 2048@32, (-2048)@32 <=s f291, f291 <s 2048@32,
(-2048)@32 <=s f292, f292 <s 2048@32, (-2048)@32 <=s f293, f293 <s 2048@32,
(-2048)@32 <=s f294, f294 <s 2048@32, (-2048)@32 <=s f295, f295 <s 2048@32,
(-2048)@32 <=s f296, f296 <s 2048@32, (-2048)@32 <=s f297, f297 <s 2048@32,
(-2048)@32 <=s f298, f298 <s 2048@32, (-2048)@32 <=s f299, f299 <s 2048@32,
(-2048)@32 <=s f300, f300 <s 2048@32, (-2048)@32 <=s f301, f301 <s 2048@32,
(-2048)@32 <=s f302, f302 <s 2048@32, (-2048)@32 <=s f303, f303 <s 2048@32,
(-2048)@32 <=s f304, f304 <s 2048@32, (-2048)@32 <=s f305, f305 <s 2048@32,
(-2048)@32 <=s f306, f306 <s 2048@32, (-2048)@32 <=s f307, f307 <s 2048@32,
(-2048)@32 <=s f308, f308 <s 2048@32, (-2048)@32 <=s f309, f309 <s 2048@32,
(-2048)@32 <=s f310, f310 <s 2048@32, (-2048)@32 <=s f311, f311 <s 2048@32,
(-2048)@32 <=s f312, f312 <s 2048@32, (-2048)@32 <=s f313, f313 <s 2048@32,
(-2048)@32 <=s f314, f314 <s 2048@32, (-2048)@32 <=s f315, f315 <s 2048@32,
(-2048)@32 <=s f316, f316 <s 2048@32, (-2048)@32 <=s f317, f317 <s 2048@32,
(-2048)@32 <=s f318, f318 <s 2048@32, (-2048)@32 <=s f319, f319 <s 2048@32,
(-2048)@32 <=s f320, f320 <s 2048@32, (-2048)@32 <=s f321, f321 <s 2048@32,
(-2048)@32 <=s f322, f322 <s 2048@32, (-2048)@32 <=s f323, f323 <s 2048@32,
(-2048)@32 <=s f324, f324 <s 2048@32, (-2048)@32 <=s f325, f325 <s 2048@32,
(-2048)@32 <=s f326, f326 <s 2048@32, (-2048)@32 <=s f327, f327 <s 2048@32,
(-2048)@32 <=s f328, f328 <s 2048@32, (-2048)@32 <=s f329, f329 <s 2048@32,
(-2048)@32 <=s f330, f330 <s 2048@32, (-2048)@32 <=s f331, f331 <s 2048@32,
(-2048)@32 <=s f332, f332 <s 2048@32, (-2048)@32 <=s f333, f333 <s 2048@32,
(-2048)@32 <=s f334, f334 <s 2048@32, (-2048)@32 <=s f335, f335 <s 2048@32,
(-2048)@32 <=s f336, f336 <s 2048@32, (-2048)@32 <=s f337, f337 <s 2048@32,
(-2048)@32 <=s f338, f338 <s 2048@32, (-2048)@32 <=s f339, f339 <s 2048@32,
(-2048)@32 <=s f340, f340 <s 2048@32, (-2048)@32 <=s f341, f341 <s 2048@32,
(-2048)@32 <=s f342, f342 <s 2048@32, (-2048)@32 <=s f343, f343 <s 2048@32,
(-2048)@32 <=s f344, f344 <s 2048@32, (-2048)@32 <=s f345, f345 <s 2048@32,
(-2048)@32 <=s f346, f346 <s 2048@32, (-2048)@32 <=s f347, f347 <s 2048@32,
(-2048)@32 <=s f348, f348 <s 2048@32, (-2048)@32 <=s f349, f349 <s 2048@32,
(-2048)@32 <=s f350, f350 <s 2048@32, (-2048)@32 <=s f351, f351 <s 2048@32,
(-2048)@32 <=s f352, f352 <s 2048@32, (-2048)@32 <=s f353, f353 <s 2048@32,
(-2048)@32 <=s f354, f354 <s 2048@32, (-2048)@32 <=s f355, f355 <s 2048@32,
(-2048)@32 <=s f356, f356 <s 2048@32, (-2048)@32 <=s f357, f357 <s 2048@32,
(-2048)@32 <=s f358, f358 <s 2048@32, (-2048)@32 <=s f359, f359 <s 2048@32,
(-2048)@32 <=s f360, f360 <s 2048@32, (-2048)@32 <=s f361, f361 <s 2048@32,
(-2048)@32 <=s f362, f362 <s 2048@32, (-2048)@32 <=s f363, f363 <s 2048@32,
(-2048)@32 <=s f364, f364 <s 2048@32, (-2048)@32 <=s f365, f365 <s 2048@32,
(-2048)@32 <=s f366, f366 <s 2048@32, (-2048)@32 <=s f367, f367 <s 2048@32,
(-2048)@32 <=s f368, f368 <s 2048@32, (-2048)@32 <=s f369, f369 <s 2048@32,
(-2048)@32 <=s f370, f370 <s 2048@32, (-2048)@32 <=s f371, f371 <s 2048@32,
(-2048)@32 <=s f372, f372 <s 2048@32, (-2048)@32 <=s f373, f373 <s 2048@32,
(-2048)@32 <=s f374, f374 <s 2048@32, (-2048)@32 <=s f375, f375 <s 2048@32,
(-2048)@32 <=s f376, f376 <s 2048@32, (-2048)@32 <=s f377, f377 <s 2048@32,
(-2048)@32 <=s f378, f378 <s 2048@32, (-2048)@32 <=s f379, f379 <s 2048@32,
(-2048)@32 <=s f380, f380 <s 2048@32, (-2048)@32 <=s f381, f381 <s 2048@32,
(-2048)@32 <=s f382, f382 <s 2048@32, (-2048)@32 <=s f383, f383 <s 2048@32,
(-2048)@32 <=s f384, f384 <s 2048@32, (-2048)@32 <=s f385, f385 <s 2048@32,
(-2048)@32 <=s f386, f386 <s 2048@32, (-2048)@32 <=s f387, f387 <s 2048@32,
(-2048)@32 <=s f388, f388 <s 2048@32, (-2048)@32 <=s f389, f389 <s 2048@32,
(-2048)@32 <=s f390, f390 <s 2048@32, (-2048)@32 <=s f391, f391 <s 2048@32,
(-2048)@32 <=s f392, f392 <s 2048@32, (-2048)@32 <=s f393, f393 <s 2048@32,
(-2048)@32 <=s f394, f394 <s 2048@32, (-2048)@32 <=s f395, f395 <s 2048@32,
(-2048)@32 <=s f396, f396 <s 2048@32, (-2048)@32 <=s f397, f397 <s 2048@32,
(-2048)@32 <=s f398, f398 <s 2048@32, (-2048)@32 <=s f399, f399 <s 2048@32,
(-2048)@32 <=s f400, f400 <s 2048@32, (-2048)@32 <=s f401, f401 <s 2048@32,
(-2048)@32 <=s f402, f402 <s 2048@32, (-2048)@32 <=s f403, f403 <s 2048@32,
(-2048)@32 <=s f404, f404 <s 2048@32, (-2048)@32 <=s f405, f405 <s 2048@32,
(-2048)@32 <=s f406, f406 <s 2048@32, (-2048)@32 <=s f407, f407 <s 2048@32,
(-2048)@32 <=s f408, f408 <s 2048@32, (-2048)@32 <=s f409, f409 <s 2048@32,
(-2048)@32 <=s f410, f410 <s 2048@32, (-2048)@32 <=s f411, f411 <s 2048@32,
(-2048)@32 <=s f412, f412 <s 2048@32, (-2048)@32 <=s f413, f413 <s 2048@32,
(-2048)@32 <=s f414, f414 <s 2048@32, (-2048)@32 <=s f415, f415 <s 2048@32,
(-2048)@32 <=s f416, f416 <s 2048@32, (-2048)@32 <=s f417, f417 <s 2048@32,
(-2048)@32 <=s f418, f418 <s 2048@32, (-2048)@32 <=s f419, f419 <s 2048@32,
(-2048)@32 <=s f420, f420 <s 2048@32, (-2048)@32 <=s f421, f421 <s 2048@32,
(-2048)@32 <=s f422, f422 <s 2048@32, (-2048)@32 <=s f423, f423 <s 2048@32,
(-2048)@32 <=s f424, f424 <s 2048@32, (-2048)@32 <=s f425, f425 <s 2048@32,
(-2048)@32 <=s f426, f426 <s 2048@32, (-2048)@32 <=s f427, f427 <s 2048@32,
(-2048)@32 <=s f428, f428 <s 2048@32, (-2048)@32 <=s f429, f429 <s 2048@32,
(-2048)@32 <=s f430, f430 <s 2048@32, (-2048)@32 <=s f431, f431 <s 2048@32,
(-2048)@32 <=s f432, f432 <s 2048@32, (-2048)@32 <=s f433, f433 <s 2048@32,
(-2048)@32 <=s f434, f434 <s 2048@32, (-2048)@32 <=s f435, f435 <s 2048@32,
(-2048)@32 <=s f436, f436 <s 2048@32, (-2048)@32 <=s f437, f437 <s 2048@32,
(-2048)@32 <=s f438, f438 <s 2048@32, (-2048)@32 <=s f439, f439 <s 2048@32,
(-2048)@32 <=s f440, f440 <s 2048@32, (-2048)@32 <=s f441, f441 <s 2048@32,
(-2048)@32 <=s f442, f442 <s 2048@32, (-2048)@32 <=s f443, f443 <s 2048@32,
(-2048)@32 <=s f444, f444 <s 2048@32, (-2048)@32 <=s f445, f445 <s 2048@32,
(-2048)@32 <=s f446, f446 <s 2048@32, (-2048)@32 <=s f447, f447 <s 2048@32,
(-2048)@32 <=s f448, f448 <s 2048@32, (-2048)@32 <=s f449, f449 <s 2048@32,
(-2048)@32 <=s f450, f450 <s 2048@32, (-2048)@32 <=s f451, f451 <s 2048@32,
(-2048)@32 <=s f452, f452 <s 2048@32, (-2048)@32 <=s f453, f453 <s 2048@32,
(-2048)@32 <=s f454, f454 <s 2048@32, (-2048)@32 <=s f455, f455 <s 2048@32,
(-2048)@32 <=s f456, f456 <s 2048@32, (-2048)@32 <=s f457, f457 <s 2048@32,
(-2048)@32 <=s f458, f458 <s 2048@32, (-2048)@32 <=s f459, f459 <s 2048@32,
(-2048)@32 <=s f460, f460 <s 2048@32, (-2048)@32 <=s f461, f461 <s 2048@32,
(-2048)@32 <=s f462, f462 <s 2048@32, (-2048)@32 <=s f463, f463 <s 2048@32,
(-2048)@32 <=s f464, f464 <s 2048@32, (-2048)@32 <=s f465, f465 <s 2048@32,
(-2048)@32 <=s f466, f466 <s 2048@32, (-2048)@32 <=s f467, f467 <s 2048@32,
(-2048)@32 <=s f468, f468 <s 2048@32, (-2048)@32 <=s f469, f469 <s 2048@32,
(-2048)@32 <=s f470, f470 <s 2048@32, (-2048)@32 <=s f471, f471 <s 2048@32,
(-2048)@32 <=s f472, f472 <s 2048@32, (-2048)@32 <=s f473, f473 <s 2048@32,
(-2048)@32 <=s f474, f474 <s 2048@32, (-2048)@32 <=s f475, f475 <s 2048@32,
(-2048)@32 <=s f476, f476 <s 2048@32, (-2048)@32 <=s f477, f477 <s 2048@32,
(-2048)@32 <=s f478, f478 <s 2048@32, (-2048)@32 <=s f479, f479 <s 2048@32,
(-2048)@32 <=s f480, f480 <s 2048@32, (-2048)@32 <=s f481, f481 <s 2048@32,
(-2048)@32 <=s f482, f482 <s 2048@32, (-2048)@32 <=s f483, f483 <s 2048@32,
(-2048)@32 <=s f484, f484 <s 2048@32, (-2048)@32 <=s f485, f485 <s 2048@32,
(-2048)@32 <=s f486, f486 <s 2048@32, (-2048)@32 <=s f487, f487 <s 2048@32,
(-2048)@32 <=s f488, f488 <s 2048@32, (-2048)@32 <=s f489, f489 <s 2048@32,
(-2048)@32 <=s f490, f490 <s 2048@32, (-2048)@32 <=s f491, f491 <s 2048@32,
(-2048)@32 <=s f492, f492 <s 2048@32, (-2048)@32 <=s f493, f493 <s 2048@32,
(-2048)@32 <=s f494, f494 <s 2048@32, (-2048)@32 <=s f495, f495 <s 2048@32,
(-2048)@32 <=s f496, f496 <s 2048@32, (-2048)@32 <=s f497, f497 <s 2048@32,
(-2048)@32 <=s f498, f498 <s 2048@32, (-2048)@32 <=s f499, f499 <s 2048@32,
(-2048)@32 <=s f500, f500 <s 2048@32, (-2048)@32 <=s f501, f501 <s 2048@32,
(-2048)@32 <=s f502, f502 <s 2048@32, (-2048)@32 <=s f503, f503 <s 2048@32,
(-2048)@32 <=s f504, f504 <s 2048@32, (-2048)@32 <=s f505, f505 <s 2048@32,
(-2048)@32 <=s f506, f506 <s 2048@32, (-2048)@32 <=s f507, f507 <s 2048@32,
(-2048)@32 <=s f508, f508 <s 2048@32
]
}

(**************** initialization ****************)

mov r2 1993076223@uint32; mov r3 1043969@sint32;
mov L0x20016fd0 f000; mov L0x20016fd2 f001; mov L0x20016fd4 f002;
mov L0x20016fd6 f003; mov L0x20016fd8 f004; mov L0x20016fda f005;
mov L0x20016fdc f006; mov L0x20016fde f007; mov L0x20016fe0 f008;
mov L0x20016fe2 f009; mov L0x20016fe4 f010; mov L0x20016fe6 f011;
mov L0x20016fe8 f012; mov L0x20016fea f013; mov L0x20016fec f014;
mov L0x20016fee f015; mov L0x20016ff0 f016; mov L0x20016ff2 f017;
mov L0x20016ff4 f018; mov L0x20016ff6 f019; mov L0x20016ff8 f020;
mov L0x20016ffa f021; mov L0x20016ffc f022; mov L0x20016ffe f023;
mov L0x20017000 f024; mov L0x20017002 f025; mov L0x20017004 f026;
mov L0x20017006 f027; mov L0x20017008 f028; mov L0x2001700a f029;
mov L0x2001700c f030; mov L0x2001700e f031; mov L0x20017010 f032;
mov L0x20017012 f033; mov L0x20017014 f034; mov L0x20017016 f035;
mov L0x20017018 f036; mov L0x2001701a f037; mov L0x2001701c f038;
mov L0x2001701e f039; mov L0x20017020 f040; mov L0x20017022 f041;
mov L0x20017024 f042; mov L0x20017026 f043; mov L0x20017028 f044;
mov L0x2001702a f045; mov L0x2001702c f046; mov L0x2001702e f047;
mov L0x20017030 f048; mov L0x20017032 f049; mov L0x20017034 f050;
mov L0x20017036 f051; mov L0x20017038 f052; mov L0x2001703a f053;
mov L0x2001703c f054; mov L0x2001703e f055; mov L0x20017040 f056;
mov L0x20017042 f057; mov L0x20017044 f058; mov L0x20017046 f059;
mov L0x20017048 f060; mov L0x2001704a f061; mov L0x2001704c f062;
mov L0x2001704e f063; mov L0x20017050 f064; mov L0x20017052 f065;
mov L0x20017054 f066; mov L0x20017056 f067; mov L0x20017058 f068;
mov L0x2001705a f069; mov L0x2001705c f070; mov L0x2001705e f071;
mov L0x20017060 f072; mov L0x20017062 f073; mov L0x20017064 f074;
mov L0x20017066 f075; mov L0x20017068 f076; mov L0x2001706a f077;
mov L0x2001706c f078; mov L0x2001706e f079; mov L0x20017070 f080;
mov L0x20017072 f081; mov L0x20017074 f082; mov L0x20017076 f083;
mov L0x20017078 f084; mov L0x2001707a f085; mov L0x2001707c f086;
mov L0x2001707e f087; mov L0x20017080 f088; mov L0x20017082 f089;
mov L0x20017084 f090; mov L0x20017086 f091; mov L0x20017088 f092;
mov L0x2001708a f093; mov L0x2001708c f094; mov L0x2001708e f095;
mov L0x20017090 f096; mov L0x20017092 f097; mov L0x20017094 f098;
mov L0x20017096 f099; mov L0x20017098 f100; mov L0x2001709a f101;
mov L0x2001709c f102; mov L0x2001709e f103; mov L0x200170a0 f104;
mov L0x200170a2 f105; mov L0x200170a4 f106; mov L0x200170a6 f107;
mov L0x200170a8 f108; mov L0x200170aa f109; mov L0x200170ac f110;
mov L0x200170ae f111; mov L0x200170b0 f112; mov L0x200170b2 f113;
mov L0x200170b4 f114; mov L0x200170b6 f115; mov L0x200170b8 f116;
mov L0x200170ba f117; mov L0x200170bc f118; mov L0x200170be f119;
mov L0x200170c0 f120; mov L0x200170c2 f121; mov L0x200170c4 f122;
mov L0x200170c6 f123; mov L0x200170c8 f124; mov L0x200170ca f125;
mov L0x200170cc f126; mov L0x200170ce f127; mov L0x200170d0 f128;
mov L0x200170d2 f129; mov L0x200170d4 f130; mov L0x200170d6 f131;
mov L0x200170d8 f132; mov L0x200170da f133; mov L0x200170dc f134;
mov L0x200170de f135; mov L0x200170e0 f136; mov L0x200170e2 f137;
mov L0x200170e4 f138; mov L0x200170e6 f139; mov L0x200170e8 f140;
mov L0x200170ea f141; mov L0x200170ec f142; mov L0x200170ee f143;
mov L0x200170f0 f144; mov L0x200170f2 f145; mov L0x200170f4 f146;
mov L0x200170f6 f147; mov L0x200170f8 f148; mov L0x200170fa f149;
mov L0x200170fc f150; mov L0x200170fe f151; mov L0x20017100 f152;
mov L0x20017102 f153; mov L0x20017104 f154; mov L0x20017106 f155;
mov L0x20017108 f156; mov L0x2001710a f157; mov L0x2001710c f158;
mov L0x2001710e f159; mov L0x20017110 f160; mov L0x20017112 f161;
mov L0x20017114 f162; mov L0x20017116 f163; mov L0x20017118 f164;
mov L0x2001711a f165; mov L0x2001711c f166; mov L0x2001711e f167;
mov L0x20017120 f168; mov L0x20017122 f169; mov L0x20017124 f170;
mov L0x20017126 f171; mov L0x20017128 f172; mov L0x2001712a f173;
mov L0x2001712c f174; mov L0x2001712e f175; mov L0x20017130 f176;
mov L0x20017132 f177; mov L0x20017134 f178; mov L0x20017136 f179;
mov L0x20017138 f180; mov L0x2001713a f181; mov L0x2001713c f182;
mov L0x2001713e f183; mov L0x20017140 f184; mov L0x20017142 f185;
mov L0x20017144 f186; mov L0x20017146 f187; mov L0x20017148 f188;
mov L0x2001714a f189; mov L0x2001714c f190; mov L0x2001714e f191;
mov L0x20017150 f192; mov L0x20017152 f193; mov L0x20017154 f194;
mov L0x20017156 f195; mov L0x20017158 f196; mov L0x2001715a f197;
mov L0x2001715c f198; mov L0x2001715e f199; mov L0x20017160 f200;
mov L0x20017162 f201; mov L0x20017164 f202; mov L0x20017166 f203;
mov L0x20017168 f204; mov L0x2001716a f205; mov L0x2001716c f206;
mov L0x2001716e f207; mov L0x20017170 f208; mov L0x20017172 f209;
mov L0x20017174 f210; mov L0x20017176 f211; mov L0x20017178 f212;
mov L0x2001717a f213; mov L0x2001717c f214; mov L0x2001717e f215;
mov L0x20017180 f216; mov L0x20017182 f217; mov L0x20017184 f218;
mov L0x20017186 f219; mov L0x20017188 f220; mov L0x2001718a f221;
mov L0x2001718c f222; mov L0x2001718e f223; mov L0x20017190 f224;
mov L0x20017192 f225; mov L0x20017194 f226; mov L0x20017196 f227;
mov L0x20017198 f228; mov L0x2001719a f229; mov L0x2001719c f230;
mov L0x2001719e f231; mov L0x200171a0 f232; mov L0x200171a2 f233;
mov L0x200171a4 f234; mov L0x200171a6 f235; mov L0x200171a8 f236;
mov L0x200171aa f237; mov L0x200171ac f238; mov L0x200171ae f239;
mov L0x200171b0 f240; mov L0x200171b2 f241; mov L0x200171b4 f242;
mov L0x200171b6 f243; mov L0x200171b8 f244; mov L0x200171ba f245;
mov L0x200171bc f246; mov L0x200171be f247; mov L0x200171c0 f248;
mov L0x200171c2 f249; mov L0x200171c4 f250; mov L0x200171c6 f251;
mov L0x200171c8 f252; mov L0x200171ca f253; mov L0x200171cc f254;
mov L0x200171ce f255; mov L0x200171d0 f256; mov L0x200171d2 f257;
mov L0x200171d4 f258; mov L0x200171d6 f259; mov L0x200171d8 f260;
mov L0x200171da f261; mov L0x200171dc f262; mov L0x200171de f263;
mov L0x200171e0 f264; mov L0x200171e2 f265; mov L0x200171e4 f266;
mov L0x200171e6 f267; mov L0x200171e8 f268; mov L0x200171ea f269;
mov L0x200171ec f270; mov L0x200171ee f271; mov L0x200171f0 f272;
mov L0x200171f2 f273; mov L0x200171f4 f274; mov L0x200171f6 f275;
mov L0x200171f8 f276; mov L0x200171fa f277; mov L0x200171fc f278;
mov L0x200171fe f279; mov L0x20017200 f280; mov L0x20017202 f281;
mov L0x20017204 f282; mov L0x20017206 f283; mov L0x20017208 f284;
mov L0x2001720a f285; mov L0x2001720c f286; mov L0x2001720e f287;
mov L0x20017210 f288; mov L0x20017212 f289; mov L0x20017214 f290;
mov L0x20017216 f291; mov L0x20017218 f292; mov L0x2001721a f293;
mov L0x2001721c f294; mov L0x2001721e f295; mov L0x20017220 f296;
mov L0x20017222 f297; mov L0x20017224 f298; mov L0x20017226 f299;
mov L0x20017228 f300; mov L0x2001722a f301; mov L0x2001722c f302;
mov L0x2001722e f303; mov L0x20017230 f304; mov L0x20017232 f305;
mov L0x20017234 f306; mov L0x20017236 f307; mov L0x20017238 f308;
mov L0x2001723a f309; mov L0x2001723c f310; mov L0x2001723e f311;
mov L0x20017240 f312; mov L0x20017242 f313; mov L0x20017244 f314;
mov L0x20017246 f315; mov L0x20017248 f316; mov L0x2001724a f317;
mov L0x2001724c f318; mov L0x2001724e f319; mov L0x20017250 f320;
mov L0x20017252 f321; mov L0x20017254 f322; mov L0x20017256 f323;
mov L0x20017258 f324; mov L0x2001725a f325; mov L0x2001725c f326;
mov L0x2001725e f327; mov L0x20017260 f328; mov L0x20017262 f329;
mov L0x20017264 f330; mov L0x20017266 f331; mov L0x20017268 f332;
mov L0x2001726a f333; mov L0x2001726c f334; mov L0x2001726e f335;
mov L0x20017270 f336; mov L0x20017272 f337; mov L0x20017274 f338;
mov L0x20017276 f339; mov L0x20017278 f340; mov L0x2001727a f341;
mov L0x2001727c f342; mov L0x2001727e f343; mov L0x20017280 f344;
mov L0x20017282 f345; mov L0x20017284 f346; mov L0x20017286 f347;
mov L0x20017288 f348; mov L0x2001728a f349; mov L0x2001728c f350;
mov L0x2001728e f351; mov L0x20017290 f352; mov L0x20017292 f353;
mov L0x20017294 f354; mov L0x20017296 f355; mov L0x20017298 f356;
mov L0x2001729a f357; mov L0x2001729c f358; mov L0x2001729e f359;
mov L0x200172a0 f360; mov L0x200172a2 f361; mov L0x200172a4 f362;
mov L0x200172a6 f363; mov L0x200172a8 f364; mov L0x200172aa f365;
mov L0x200172ac f366; mov L0x200172ae f367; mov L0x200172b0 f368;
mov L0x200172b2 f369; mov L0x200172b4 f370; mov L0x200172b6 f371;
mov L0x200172b8 f372; mov L0x200172ba f373; mov L0x200172bc f374;
mov L0x200172be f375; mov L0x200172c0 f376; mov L0x200172c2 f377;
mov L0x200172c4 f378; mov L0x200172c6 f379; mov L0x200172c8 f380;
mov L0x200172ca f381; mov L0x200172cc f382; mov L0x200172ce f383;
mov L0x200172d0 f384; mov L0x200172d2 f385; mov L0x200172d4 f386;
mov L0x200172d6 f387; mov L0x200172d8 f388; mov L0x200172da f389;
mov L0x200172dc f390; mov L0x200172de f391; mov L0x200172e0 f392;
mov L0x200172e2 f393; mov L0x200172e4 f394; mov L0x200172e6 f395;
mov L0x200172e8 f396; mov L0x200172ea f397; mov L0x200172ec f398;
mov L0x200172ee f399; mov L0x200172f0 f400; mov L0x200172f2 f401;
mov L0x200172f4 f402; mov L0x200172f6 f403; mov L0x200172f8 f404;
mov L0x200172fa f405; mov L0x200172fc f406; mov L0x200172fe f407;
mov L0x20017300 f408; mov L0x20017302 f409; mov L0x20017304 f410;
mov L0x20017306 f411; mov L0x20017308 f412; mov L0x2001730a f413;
mov L0x2001730c f414; mov L0x2001730e f415; mov L0x20017310 f416;
mov L0x20017312 f417; mov L0x20017314 f418; mov L0x20017316 f419;
mov L0x20017318 f420; mov L0x2001731a f421; mov L0x2001731c f422;
mov L0x2001731e f423; mov L0x20017320 f424; mov L0x20017322 f425;
mov L0x20017324 f426; mov L0x20017326 f427; mov L0x20017328 f428;
mov L0x2001732a f429; mov L0x2001732c f430; mov L0x2001732e f431;
mov L0x20017330 f432; mov L0x20017332 f433; mov L0x20017334 f434;
mov L0x20017336 f435; mov L0x20017338 f436; mov L0x2001733a f437;
mov L0x2001733c f438; mov L0x2001733e f439; mov L0x20017340 f440;
mov L0x20017342 f441; mov L0x20017344 f442; mov L0x20017346 f443;
mov L0x20017348 f444; mov L0x2001734a f445; mov L0x2001734c f446;
mov L0x2001734e f447; mov L0x20017350 f448; mov L0x20017352 f449;
mov L0x20017354 f450; mov L0x20017356 f451; mov L0x20017358 f452;
mov L0x2001735a f453; mov L0x2001735c f454; mov L0x2001735e f455;
mov L0x20017360 f456; mov L0x20017362 f457; mov L0x20017364 f458;
mov L0x20017366 f459; mov L0x20017368 f460; mov L0x2001736a f461;
mov L0x2001736c f462; mov L0x2001736e f463; mov L0x20017370 f464;
mov L0x20017372 f465; mov L0x20017374 f466; mov L0x20017376 f467;
mov L0x20017378 f468; mov L0x2001737a f469; mov L0x2001737c f470;
mov L0x2001737e f471; mov L0x20017380 f472; mov L0x20017382 f473;
mov L0x20017384 f474; mov L0x20017386 f475; mov L0x20017388 f476;
mov L0x2001738a f477; mov L0x2001738c f478; mov L0x2001738e f479;
mov L0x20017390 f480; mov L0x20017392 f481; mov L0x20017394 f482;
mov L0x20017396 f483; mov L0x20017398 f484; mov L0x2001739a f485;
mov L0x2001739c f486; mov L0x2001739e f487; mov L0x200173a0 f488;
mov L0x200173a2 f489; mov L0x200173a4 f490; mov L0x200173a6 f491;
mov L0x200173a8 f492; mov L0x200173aa f493; mov L0x200173ac f494;
mov L0x200173ae f495; mov L0x200173b0 f496; mov L0x200173b2 f497;
mov L0x200173b4 f498; mov L0x200173b6 f499; mov L0x200173b8 f500;
mov L0x200173ba f501; mov L0x200173bc f502; mov L0x200173be f503;
mov L0x200173c0 f504; mov L0x200173c2 f505; mov L0x200173c4 f506;
mov L0x200173c6 f507; mov L0x200173c8 f508;



(**************** pointers ****************)

nondet lr@uint32; nondet r0@uint32;



(**************** input poly ****************)

ghost x@bit, inp_poly@bit,
inp_poly_0@bit, inp_poly_1@bit, inp_poly_2@bit, inp_poly_3@bit,
inp_poly_4@bit, inp_poly_5@bit, inp_poly_6@bit, inp_poly_7@bit :
and [
inp_poly_0**2 = 
f000*(x**0)+f001*(x**1)+f002*(x**2)+f003*(x**3)+f004*(x**4)+
f005*(x**5)+f006*(x**6)+f007*(x**7)+f008*(x**8)+f009*(x**9)+
f010*(x**10)+f011*(x**11)+f012*(x**12)+f013*(x**13)+f014*(x**14)+
f015*(x**15)+f016*(x**16)+f017*(x**17)+f018*(x**18)+f019*(x**19)+
f020*(x**20)+f021*(x**21)+f022*(x**22)+f023*(x**23)+f024*(x**24)+
f025*(x**25)+f026*(x**26)+f027*(x**27)+f028*(x**28)+f029*(x**29)+
f030*(x**30)+f031*(x**31)+f032*(x**32)+f033*(x**33)+f034*(x**34)+
f035*(x**35)+f036*(x**36)+f037*(x**37)+f038*(x**38)+f039*(x**39)+
f040*(x**40)+f041*(x**41)+f042*(x**42)+f043*(x**43)+f044*(x**44)+
f045*(x**45)+f046*(x**46)+f047*(x**47)+f048*(x**48)+f049*(x**49)+
f050*(x**50)+f051*(x**51)+f052*(x**52)+f053*(x**53)+f054*(x**54)+
f055*(x**55)+f056*(x**56)+f057*(x**57)+f058*(x**58)+f059*(x**59)+
f060*(x**60)+f061*(x**61)+f062*(x**62)+f063*(x**63),
inp_poly_1**2 = 
f064*(x**0)+f065*(x**1)+f066*(x**2)+f067*(x**3)+f068*(x**4)+
f069*(x**5)+f070*(x**6)+f071*(x**7)+f072*(x**8)+f073*(x**9)+
f074*(x**10)+f075*(x**11)+f076*(x**12)+f077*(x**13)+f078*(x**14)+
f079*(x**15)+f080*(x**16)+f081*(x**17)+f082*(x**18)+f083*(x**19)+
f084*(x**20)+f085*(x**21)+f086*(x**22)+f087*(x**23)+f088*(x**24)+
f089*(x**25)+f090*(x**26)+f091*(x**27)+f092*(x**28)+f093*(x**29)+
f094*(x**30)+f095*(x**31)+f096*(x**32)+f097*(x**33)+f098*(x**34)+
f099*(x**35)+f100*(x**36)+f101*(x**37)+f102*(x**38)+f103*(x**39)+
f104*(x**40)+f105*(x**41)+f106*(x**42)+f107*(x**43)+f108*(x**44)+
f109*(x**45)+f110*(x**46)+f111*(x**47)+f112*(x**48)+f113*(x**49)+
f114*(x**50)+f115*(x**51)+f116*(x**52)+f117*(x**53)+f118*(x**54)+
f119*(x**55)+f120*(x**56)+f121*(x**57)+f122*(x**58)+f123*(x**59)+
f124*(x**60)+f125*(x**61)+f126*(x**62)+f127*(x**63),
inp_poly_2**2 = 
f128*(x**0)+f129*(x**1)+f130*(x**2)+f131*(x**3)+f132*(x**4)+
f133*(x**5)+f134*(x**6)+f135*(x**7)+f136*(x**8)+f137*(x**9)+
f138*(x**10)+f139*(x**11)+f140*(x**12)+f141*(x**13)+f142*(x**14)+
f143*(x**15)+f144*(x**16)+f145*(x**17)+f146*(x**18)+f147*(x**19)+
f148*(x**20)+f149*(x**21)+f150*(x**22)+f151*(x**23)+f152*(x**24)+
f153*(x**25)+f154*(x**26)+f155*(x**27)+f156*(x**28)+f157*(x**29)+
f158*(x**30)+f159*(x**31)+f160*(x**32)+f161*(x**33)+f162*(x**34)+
f163*(x**35)+f164*(x**36)+f165*(x**37)+f166*(x**38)+f167*(x**39)+
f168*(x**40)+f169*(x**41)+f170*(x**42)+f171*(x**43)+f172*(x**44)+
f173*(x**45)+f174*(x**46)+f175*(x**47)+f176*(x**48)+f177*(x**49)+
f178*(x**50)+f179*(x**51)+f180*(x**52)+f181*(x**53)+f182*(x**54)+
f183*(x**55)+f184*(x**56)+f185*(x**57)+f186*(x**58)+f187*(x**59)+
f188*(x**60)+f189*(x**61)+f190*(x**62)+f191*(x**63),
inp_poly_3**2 = 
f192*(x**0)+f193*(x**1)+f194*(x**2)+f195*(x**3)+f196*(x**4)+
f197*(x**5)+f198*(x**6)+f199*(x**7)+f200*(x**8)+f201*(x**9)+
f202*(x**10)+f203*(x**11)+f204*(x**12)+f205*(x**13)+f206*(x**14)+
f207*(x**15)+f208*(x**16)+f209*(x**17)+f210*(x**18)+f211*(x**19)+
f212*(x**20)+f213*(x**21)+f214*(x**22)+f215*(x**23)+f216*(x**24)+
f217*(x**25)+f218*(x**26)+f219*(x**27)+f220*(x**28)+f221*(x**29)+
f222*(x**30)+f223*(x**31)+f224*(x**32)+f225*(x**33)+f226*(x**34)+
f227*(x**35)+f228*(x**36)+f229*(x**37)+f230*(x**38)+f231*(x**39)+
f232*(x**40)+f233*(x**41)+f234*(x**42)+f235*(x**43)+f236*(x**44)+
f237*(x**45)+f238*(x**46)+f239*(x**47)+f240*(x**48)+f241*(x**49)+
f242*(x**50)+f243*(x**51)+f244*(x**52)+f245*(x**53)+f246*(x**54)+
f247*(x**55)+f248*(x**56)+f249*(x**57)+f250*(x**58)+f251*(x**59)+
f252*(x**60)+f253*(x**61)+f254*(x**62)+f255*(x**63),
inp_poly_4**2 = 
f256*(x**0)+f257*(x**1)+f258*(x**2)+f259*(x**3)+f260*(x**4)+
f261*(x**5)+f262*(x**6)+f263*(x**7)+f264*(x**8)+f265*(x**9)+
f266*(x**10)+f267*(x**11)+f268*(x**12)+f269*(x**13)+f270*(x**14)+
f271*(x**15)+f272*(x**16)+f273*(x**17)+f274*(x**18)+f275*(x**19)+
f276*(x**20)+f277*(x**21)+f278*(x**22)+f279*(x**23)+f280*(x**24)+
f281*(x**25)+f282*(x**26)+f283*(x**27)+f284*(x**28)+f285*(x**29)+
f286*(x**30)+f287*(x**31)+f288*(x**32)+f289*(x**33)+f290*(x**34)+
f291*(x**35)+f292*(x**36)+f293*(x**37)+f294*(x**38)+f295*(x**39)+
f296*(x**40)+f297*(x**41)+f298*(x**42)+f299*(x**43)+f300*(x**44)+
f301*(x**45)+f302*(x**46)+f303*(x**47)+f304*(x**48)+f305*(x**49)+
f306*(x**50)+f307*(x**51)+f308*(x**52)+f309*(x**53)+f310*(x**54)+
f311*(x**55)+f312*(x**56)+f313*(x**57)+f314*(x**58)+f315*(x**59)+
f316*(x**60)+f317*(x**61)+f318*(x**62)+f319*(x**63),
inp_poly_5**2 = 
f320*(x**0)+f321*(x**1)+f322*(x**2)+f323*(x**3)+f324*(x**4)+
f325*(x**5)+f326*(x**6)+f327*(x**7)+f328*(x**8)+f329*(x**9)+
f330*(x**10)+f331*(x**11)+f332*(x**12)+f333*(x**13)+f334*(x**14)+
f335*(x**15)+f336*(x**16)+f337*(x**17)+f338*(x**18)+f339*(x**19)+
f340*(x**20)+f341*(x**21)+f342*(x**22)+f343*(x**23)+f344*(x**24)+
f345*(x**25)+f346*(x**26)+f347*(x**27)+f348*(x**28)+f349*(x**29)+
f350*(x**30)+f351*(x**31)+f352*(x**32)+f353*(x**33)+f354*(x**34)+
f355*(x**35)+f356*(x**36)+f357*(x**37)+f358*(x**38)+f359*(x**39)+
f360*(x**40)+f361*(x**41)+f362*(x**42)+f363*(x**43)+f364*(x**44)+
f365*(x**45)+f366*(x**46)+f367*(x**47)+f368*(x**48)+f369*(x**49)+
f370*(x**50)+f371*(x**51)+f372*(x**52)+f373*(x**53)+f374*(x**54)+
f375*(x**55)+f376*(x**56)+f377*(x**57)+f378*(x**58)+f379*(x**59)+
f380*(x**60)+f381*(x**61)+f382*(x**62)+f383*(x**63),
inp_poly_6**2 = 
f384*(x**0)+f385*(x**1)+f386*(x**2)+f387*(x**3)+f388*(x**4)+
f389*(x**5)+f390*(x**6)+f391*(x**7)+f392*(x**8)+f393*(x**9)+
f394*(x**10)+f395*(x**11)+f396*(x**12)+f397*(x**13)+f398*(x**14)+
f399*(x**15)+f400*(x**16)+f401*(x**17)+f402*(x**18)+f403*(x**19)+
f404*(x**20)+f405*(x**21)+f406*(x**22)+f407*(x**23)+f408*(x**24)+
f409*(x**25)+f410*(x**26)+f411*(x**27)+f412*(x**28)+f413*(x**29)+
f414*(x**30)+f415*(x**31)+f416*(x**32)+f417*(x**33)+f418*(x**34)+
f419*(x**35)+f420*(x**36)+f421*(x**37)+f422*(x**38)+f423*(x**39)+
f424*(x**40)+f425*(x**41)+f426*(x**42)+f427*(x**43)+f428*(x**44)+
f429*(x**45)+f430*(x**46)+f431*(x**47)+f432*(x**48)+f433*(x**49)+
f434*(x**50)+f435*(x**51)+f436*(x**52)+f437*(x**53)+f438*(x**54)+
f439*(x**55)+f440*(x**56)+f441*(x**57)+f442*(x**58)+f443*(x**59)+
f444*(x**60)+f445*(x**61)+f446*(x**62)+f447*(x**63),
inp_poly_7**2 = 
f448*(x**0)+f449*(x**1)+f450*(x**2)+f451*(x**3)+f452*(x**4)+
f453*(x**5)+f454*(x**6)+f455*(x**7)+f456*(x**8)+f457*(x**9)+
f458*(x**10)+f459*(x**11)+f460*(x**12)+f461*(x**13)+f462*(x**14)+
f463*(x**15)+f464*(x**16)+f465*(x**17)+f466*(x**18)+f467*(x**19)+
f468*(x**20)+f469*(x**21)+f470*(x**22)+f471*(x**23)+f472*(x**24)+
f473*(x**25)+f474*(x**26)+f475*(x**27)+f476*(x**28)+f477*(x**29)+
f478*(x**30)+f479*(x**31)+f480*(x**32)+f481*(x**33)+f482*(x**34)+
f483*(x**35)+f484*(x**36)+f485*(x**37)+f486*(x**38)+f487*(x**39)+
f488*(x**40)+f489*(x**41)+f490*(x**42)+f491*(x**43)+f492*(x**44)+
f493*(x**45)+f494*(x**46)+f495*(x**47)+f496*(x**48)+f497*(x**49)+
f498*(x**50)+f499*(x**51)+f500*(x**52)+f501*(x**53)+f502*(x**54)+
f503*(x**55)+f504*(x**56)+f505*(x**57)+f506*(x**58)+f507*(x**59)+
f508*(x**60),
inp_poly**2 = 
(inp_poly_0**2)*(x**0)+(inp_poly_1**2)*(x**64)+(inp_poly_2**2)*(x**128)+
(inp_poly_3**2)*(x**192)+(inp_poly_4**2)*(x**256)+(inp_poly_5**2)*(x**320)+
(inp_poly_6**2)*(x**384)+(inp_poly_7**2)*(x**448)
] && true;



(**************** constants ****************)

mov L0x8006ddc (  78830)@sint32; mov L0x8006de0 (  78830)@sint32;
mov L0x8006de4 ( 191052)@sint32; mov L0x8006de8 (  78830)@sint32;
mov L0x8006dec ( 191052)@sint32; mov L0x8006df0 (-311503)@sint32;
mov L0x8006df4 ( 207751)@sint32; mov L0x8006df8 (  78830)@sint32;
mov L0x8006dfc ( 191052)@sint32; mov L0x8006e00 (-311503)@sint32;
mov L0x8006e04 ( 207751)@sint32; mov L0x8006e08 ( 468028)@sint32;
mov L0x8006e0c (-149945)@sint32; mov L0x8006e10 ( 114478)@sint32;
mov L0x8006e14 ( -82425)@sint32; mov L0x8006e18 (  78830)@sint32;
mov L0x8006e1c (  78830)@sint32; mov L0x8006e20 ( 191052)@sint32;
mov L0x8006e24 (  78830)@sint32; mov L0x8006e28 ( 191052)@sint32;
mov L0x8006e2c (-311503)@sint32; mov L0x8006e30 ( 207751)@sint32;
mov L0x8006e34 (  78830)@sint32; mov L0x8006e38 ( 191052)@sint32;
mov L0x8006e3c (-311503)@sint32; mov L0x8006e40 ( 207751)@sint32;
mov L0x8006e44 ( 468028)@sint32; mov L0x8006e48 (-149945)@sint32;
mov L0x8006e4c ( 114478)@sint32; mov L0x8006e50 ( -82425)@sint32;
mov L0x8006e54 ( 191052)@sint32; mov L0x8006e58 (-311503)@sint32;
mov L0x8006e5c ( 207751)@sint32; mov L0x8006e60 ( 468028)@sint32;
mov L0x8006e64 (-149945)@sint32; mov L0x8006e68 ( 114478)@sint32;
mov L0x8006e6c ( -82425)@sint32; mov L0x8006e70 ( 254425)@sint32;
mov L0x8006e74 ( -83285)@sint32; mov L0x8006e78 (-205022)@sint32;
mov L0x8006e7c ( 318314)@sint32; mov L0x8006e80 ( 365552)@sint32;
mov L0x8006e84 (-403894)@sint32; mov L0x8006e88 ( 235060)@sint32;
mov L0x8006e8c ( 449706)@sint32; mov L0x8006e90 (-311503)@sint32;
mov L0x8006e94 ( 468028)@sint32; mov L0x8006e98 (-149945)@sint32;
mov L0x8006e9c ( 254425)@sint32; mov L0x8006ea0 ( -83285)@sint32;
mov L0x8006ea4 (-205022)@sint32; mov L0x8006ea8 ( 318314)@sint32;
mov L0x8006eac ( 378933)@sint32; mov L0x8006eb0 ( 313241)@sint32;
mov L0x8006eb4 (-397250)@sint32; mov L0x8006eb8 ( 288321)@sint32;
mov L0x8006ebc (  74768)@sint32; mov L0x8006ec0 (  22897)@sint32;
mov L0x8006ec4 ( 129870)@sint32; mov L0x8006ec8 (-461967)@sint32;
mov L0x8006ecc ( 207751)@sint32; mov L0x8006ed0 ( 114478)@sint32;
mov L0x8006ed4 ( -82425)@sint32; mov L0x8006ed8 ( 365552)@sint32;
mov L0x8006edc (-403894)@sint32; mov L0x8006ee0 ( 235060)@sint32;
mov L0x8006ee4 ( 449706)@sint32; mov L0x8006ee8 ( -35962)@sint32;
mov L0x8006eec ( 370478)@sint32; mov L0x8006ef0 ( 232373)@sint32;
mov L0x8006ef4 ( 159337)@sint32; mov L0x8006ef8 ( 405929)@sint32;
mov L0x8006efc (  59399)@sint32; mov L0x8006f00 ( -40385)@sint32;
mov L0x8006f04 ( 317168)@sint32; mov L0x8006f08 ( 468028)@sint32;
mov L0x8006f0c ( 254425)@sint32; mov L0x8006f10 ( -83285)@sint32;
mov L0x8006f14 ( 378933)@sint32; mov L0x8006f18 ( 313241)@sint32;
mov L0x8006f1c (-397250)@sint32; mov L0x8006f20 ( 288321)@sint32;
mov L0x8006f24 (  13867)@sint32; mov L0x8006f28 (  21742)@sint32;
mov L0x8006f2c ( 486841)@sint32; mov L0x8006f30 ( -73546)@sint32;
mov L0x8006f34 (   7056)@sint32; mov L0x8006f38 (-391031)@sint32;
mov L0x8006f3c (-493755)@sint32; mov L0x8006f40 ( -78001)@sint32;
mov L0x8006f44 (-149945)@sint32; mov L0x8006f48 (-205022)@sint32;
mov L0x8006f4c ( 318314)@sint32; mov L0x8006f50 (  74768)@sint32;
mov L0x8006f54 (  22897)@sint32; mov L0x8006f58 ( 129870)@sint32;
mov L0x8006f5c (-461967)@sint32; mov L0x8006f60 (-495107)@sint32;
mov L0x8006f64 ( 279814)@sint32; mov L0x8006f68 (-363890)@sint32;
mov L0x8006f6c (-182676)@sint32; mov L0x8006f70 (-207660)@sint32;
mov L0x8006f74 (  75978)@sint32; mov L0x8006f78 ( 187423)@sint32;
mov L0x8006f7c ( -78196)@sint32; mov L0x8006f80 ( 114478)@sint32;
mov L0x8006f84 ( 365552)@sint32; mov L0x8006f88 (-403894)@sint32;
mov L0x8006f8c ( -35962)@sint32; mov L0x8006f90 ( 370478)@sint32;
mov L0x8006f94 ( 232373)@sint32; mov L0x8006f98 ( 159337)@sint32;
mov L0x8006f9c ( 507011)@sint32; mov L0x8006fa0 ( 331715)@sint32;
mov L0x8006fa4 ( 297886)@sint32; mov L0x8006fa8 (-346620)@sint32;
mov L0x8006fac (-299045)@sint32; mov L0x8006fb0 ( 275767)@sint32;
mov L0x8006fb4 ( -51317)@sint32; mov L0x8006fb8 ( 402791)@sint32;
mov L0x8006fbc ( -82425)@sint32; mov L0x8006fc0 ( 235060)@sint32;
mov L0x8006fc4 ( 449706)@sint32; mov L0x8006fc8 ( 405929)@sint32;
mov L0x8006fcc (  59399)@sint32; mov L0x8006fd0 ( -40385)@sint32;
mov L0x8006fd4 ( 317168)@sint32; mov L0x8006fd8 (-272172)@sint32;
mov L0x8006fdc (-375619)@sint32; mov L0x8006fe0 ( 376740)@sint32;
mov L0x8006fe4 (-409013)@sint32; mov L0x8006fe8 ( -42578)@sint32;
mov L0x8006fec (-405086)@sint32; mov L0x8006ff0 (  81030)@sint32;
mov L0x8006ff4 (-422078)@sint32; mov L0x8006ff8 ( 254425)@sint32;
mov L0x8006ffc ( 378933)@sint32; mov L0x8007000 ( 313241)@sint32;
mov L0x8007004 (  13867)@sint32; mov L0x8007008 (  21742)@sint32;
mov L0x800700c ( 486841)@sint32; mov L0x8007010 ( -73546)@sint32;
mov L0x8007014 ( 487892)@sint32; mov L0x8007018 (-428144)@sint32;
mov L0x800701c ( -43370)@sint32; mov L0x8007020 (-393153)@sint32;
mov L0x8007024 ( 248256)@sint32; mov L0x8007028 (-228921)@sint32;
mov L0x800702c ( -29446)@sint32; mov L0x8007030 ( -59870)@sint32;
mov L0x8007034 ( -83285)@sint32; mov L0x8007038 (-397250)@sint32;
mov L0x800703c ( 288321)@sint32; mov L0x8007040 (   7056)@sint32;
mov L0x8007044 (-391031)@sint32; mov L0x8007048 (-493755)@sint32;
mov L0x800704c ( -78001)@sint32; mov L0x8007050 ( 285179)@sint32;
mov L0x8007054 ( 257414)@sint32; mov L0x8007058 (-147526)@sint32;
mov L0x800705c ( 390544)@sint32; mov L0x8007060 (-275430)@sint32;
mov L0x8007064 (-160445)@sint32; mov L0x8007068 (-436582)@sint32;
mov L0x800706c ( 316768)@sint32; mov L0x8007070 (-205022)@sint32;
mov L0x8007074 (  74768)@sint32; mov L0x8007078 (  22897)@sint32;
mov L0x800707c (-495107)@sint32; mov L0x8007080 ( 279814)@sint32;
mov L0x8007084 (-363890)@sint32; mov L0x8007088 (-182676)@sint32;
mov L0x800708c (  27120)@sint32; mov L0x8007090 (-345344)@sint32;
mov L0x8007094 (-470298)@sint32; mov L0x8007098 (-498651)@sint32;
mov L0x800709c (-358783)@sint32; mov L0x80070a0 ( 477219)@sint32;
mov L0x80070a4 ( 133279)@sint32; mov L0x80070a8 (-401288)@sint32;
mov L0x80070ac ( 318314)@sint32; mov L0x80070b0 ( 129870)@sint32;
mov L0x80070b4 (-461967)@sint32; mov L0x80070b8 (-207660)@sint32;
mov L0x80070bc (  75978)@sint32; mov L0x80070c0 ( 187423)@sint32;
mov L0x80070c4 ( -78196)@sint32; mov L0x80070c8 ( 288431)@sint32;
mov L0x80070cc (-155391)@sint32; mov L0x80070d0 ( -18223)@sint32;
mov L0x80070d4 (-478095)@sint32; mov L0x80070d8 (-347554)@sint32;
mov L0x80070dc ( 256625)@sint32; mov L0x80070e0 (-153141)@sint32;
mov L0x80070e4 (-298605)@sint32; mov L0x80070e8 ( 365552)@sint32;
mov L0x80070ec ( -35962)@sint32; mov L0x80070f0 ( 370478)@sint32;
mov L0x80070f4 ( 507011)@sint32; mov L0x80070f8 ( 331715)@sint32;
mov L0x80070fc ( 297886)@sint32; mov L0x8007100 (-346620)@sint32;
mov L0x8007104 (-367175)@sint32; mov L0x8007108 (-334857)@sint32;
mov L0x800710c (  42298)@sint32; mov L0x8007110 (-465942)@sint32;
mov L0x8007114 ( 329620)@sint32; mov L0x8007118 ( -89230)@sint32;
mov L0x800711c (-288348)@sint32; mov L0x8007120 ( 279364)@sint32;
mov L0x8007124 (-403894)@sint32; mov L0x8007128 ( 232373)@sint32;
mov L0x800712c ( 159337)@sint32; mov L0x8007130 (-299045)@sint32;
mov L0x8007134 ( 275767)@sint32; mov L0x8007138 ( -51317)@sint32;
mov L0x800713c ( 402791)@sint32; mov L0x8007140 ( 268720)@sint32;
mov L0x8007144 ( 464538)@sint32; mov L0x8007148 ( 356621)@sint32;
mov L0x800714c ( 343605)@sint32; mov L0x8007150 ( 476116)@sint32;
mov L0x8007154 (  44548)@sint32; mov L0x8007158 ( 347463)@sint32;
mov L0x800715c ( 399863)@sint32; mov L0x8007160 ( 235060)@sint32;
mov L0x8007164 ( 405929)@sint32; mov L0x8007168 (  59399)@sint32;
mov L0x800716c (-272172)@sint32; mov L0x8007170 (-375619)@sint32;
mov L0x8007174 ( 376740)@sint32; mov L0x8007178 (-409013)@sint32;
mov L0x800717c (-188449)@sint32; mov L0x8007180 (-309697)@sint32;
mov L0x8007184 (-118699)@sint32; mov L0x8007188 ( 418878)@sint32;
mov L0x800718c (-284025)@sint32; mov L0x8007190 ( 170731)@sint32;
mov L0x8007194 ( 515076)@sint32; mov L0x8007198 ( 290607)@sint32;
mov L0x800719c ( 449706)@sint32; mov L0x80071a0 ( -40385)@sint32;
mov L0x80071a4 ( 317168)@sint32; mov L0x80071a8 ( -42578)@sint32;
mov L0x80071ac (-405086)@sint32; mov L0x80071b0 (  81030)@sint32;
mov L0x80071b4 (-422078)@sint32; mov L0x80071b8 ( 445216)@sint32;
mov L0x80071bc ( 114673)@sint32; mov L0x80071c0 (   1019)@sint32;
mov L0x80071c4 (-364661)@sint32; mov L0x80071c8 (-179242)@sint32;
mov L0x80071cc (-317922)@sint32; mov L0x80071d0 (-202373)@sint32;
mov L0x80071d4 ( 400989)@sint32;

(* #! -> SP = 0x20016fb0 *)
#! 0x20016fb0 = 0x20016fb0;
(* #stmdb	sp!, {r4, r5, r6, r7, r8, r9, r10, r11, r12, lr}#! EA = L0x20016fb0; PC = 0x8000b5c *)
#stmdb	sp!, {%%r4, %%r5, %%r6, %%r7, %%r8, %%r9, %%r10, %%r11, %%r12, %%lr}#! L0x20016fb0 = L0x20016fb0; 0x8000b5c = 0x8000b5c;
(* #vpush	{s16-s31}                                 #! PC = 0x8000b60 *)
#vpush	{%%s16-%%s31}                                 #! 0x8000b60 = 0x8000b60;
(* #ldr.w	lr, [sp, #104]	; 0x68                     #! EA = L0x20016fb0; Value = 0x20016fd0; PC = 0x8000b64 *)
#ldr.w	%%lr, [sp, #104]	; 0x68                     #! L0x20016fb0 = L0x20016fb0; 0x20016fd0 = 0x20016fd0; 0x8000b64 = 0x8000b64;
(* vldmia	r1!, {s4-s18}                             #! EA = L0x8006ddc; PC = 0x8000b68 *)
mov s4 L0x8006ddc;
mov s5 L0x8006de0;
mov s6 L0x8006de4;
mov s7 L0x8006de8;
mov s8 L0x8006dec;
mov s9 L0x8006df0;
mov s10 L0x8006df4;
mov s11 L0x8006df8;
mov s12 L0x8006dfc;
mov s13 L0x8006e00;
mov s14 L0x8006e04;
mov s15 L0x8006e08;
mov s16 L0x8006e0c;
mov s17 L0x8006e10;
mov s18 L0x8006e14;
(* vmov	s0, lr                                     #! PC = 0x8000b6c *)
mov s0 lr;
(* add.w	r12, r0, #244	; 0xf4                      #! PC = 0x8000b70 *)
adds dontcare r12 r0 244@uint32;
(* vmov	s2, r12                                    #! PC = 0x8000b74 *)
mov s2 r12;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017050; Value = 0x01d4fe30; PC = 0x8000b7c *)
mov r4 L0x20017050;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017150; Value = 0xfcf20159; PC = 0x8000b80 *)
mov r5 L0x20017150;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017250; Value = 0x00b7fdde; PC = 0x8000b84 *)
mov r6 L0x20017250;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017350; Value = 0xfe94035c; PC = 0x8000b88 *)
mov r7 L0x20017350;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf064@sint32 : and [cf064 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf192@sint32 : and [cf192 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf320@sint32 : and [cf320 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf448@sint32 : and [cf448 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170d0; Value = 0xfcfe008a; PC = 0x8000c9c *)
mov r5 L0x200170d0;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171d0; Value = 0xfe2a00cc; PC = 0x8000ca0 *)
mov r6 L0x200171d0;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172d0; Value = 0x0204ff1a; PC = 0x8000ca4 *)
mov r7 L0x200172d0;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fd0; Value = 0x00cf002d; PC = 0x8000ca8 *)
mov r4 L0x20016fd0;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf000@sint32 : and [cf000 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf128@sint32 : and [cf128 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf256@sint32 : and [cf256 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf384@sint32 : and [cf384 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197d0; PC = 0x8000d54 *)
mov L0x200197d0 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198d0; PC = 0x8000d58 *)
mov L0x200198d0 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199d0; PC = 0x8000d5c *)
mov L0x200199d0 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019ad0; PC = 0x8000d60 *)
mov L0x20019ad0 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019bd0; PC = 0x8000d64 *)
mov L0x20019bd0 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019cd0; PC = 0x8000d68 *)
mov L0x20019cd0 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019dd0; PC = 0x8000d6c *)
mov L0x20019dd0 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019ed0; PC = 0x8000d70 *)
mov L0x20019ed0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193d0; PC = 0x8000d9c *)
mov L0x200193d0 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194d0; PC = 0x8000da0 *)
mov L0x200194d0 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195d0; PC = 0x8000da4 *)
mov L0x200195d0 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196d0; PC = 0x8000da8 *)
mov L0x200196d0 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190d0; PC = 0x8000dac *)
mov L0x200190d0 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191d0; PC = 0x8000db0 *)
mov L0x200191d0 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192d0; PC = 0x8000db4 *)
mov L0x200192d0 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018fd0; PC = 0x8000db8 *)
mov L0x20018fd0 r8;

(**************** CUT   0, - *****************)

ecut and [
eqmod cf000 f000 2**11, eqmod cf064 f064 2**11, eqmod cf128 f128 2**11,
eqmod cf192 f192 2**11, eqmod cf256 f256 2**11, eqmod cf320 f320 2**11,
eqmod cf384 f384 2**11, eqmod cf448 f448 2**11,
eqmod L0x20018fd0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190d0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191d0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192d0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193d0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194d0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195d0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196d0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197d0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198d0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199d0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019ad0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019bd0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019cd0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019dd0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019ed0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  268244 ]
];


(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017052; Value = 0xfd3401d4; PC = 0x8000b7c *)
mov r4 L0x20017052;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017152; Value = 0xfe6ffcf2; PC = 0x8000b80 *)
mov r5 L0x20017152;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017252; Value = 0x015d00b7; PC = 0x8000b84 *)
mov r6 L0x20017252;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017352; Value = 0x0218fe94; PC = 0x8000b88 *)
mov r7 L0x20017352;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf065@sint32 : and [cf065 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf193@sint32 : and [cf193 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf321@sint32 : and [cf321 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf449@sint32 : and [cf449 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170d2; Value = 0x0021fcfe; PC = 0x8000c9c *)
mov r5 L0x200170d2;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171d2; Value = 0xfeaffe2a; PC = 0x8000ca0 *)
mov r6 L0x200171d2;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172d2; Value = 0xffa30204; PC = 0x8000ca4 *)
mov r7 L0x200172d2;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fd2; Value = 0x024600cf; PC = 0x8000ca8 *)
mov r4 L0x20016fd2;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf001@sint32 : and [cf001 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf129@sint32 : and [cf129 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf257@sint32 : and [cf257 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf385@sint32 : and [cf385 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197d4; PC = 0x8000d54 *)
mov L0x200197d4 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198d4; PC = 0x8000d58 *)
mov L0x200198d4 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199d4; PC = 0x8000d5c *)
mov L0x200199d4 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019ad4; PC = 0x8000d60 *)
mov L0x20019ad4 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019bd4; PC = 0x8000d64 *)
mov L0x20019bd4 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019cd4; PC = 0x8000d68 *)
mov L0x20019cd4 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019dd4; PC = 0x8000d6c *)
mov L0x20019dd4 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019ed4; PC = 0x8000d70 *)
mov L0x20019ed4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193d4; PC = 0x8000d9c *)
mov L0x200193d4 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194d4; PC = 0x8000da0 *)
mov L0x200194d4 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195d4; PC = 0x8000da4 *)
mov L0x200195d4 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196d4; PC = 0x8000da8 *)
mov L0x200196d4 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190d4; PC = 0x8000dac *)
mov L0x200190d4 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191d4; PC = 0x8000db0 *)
mov L0x200191d4 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192d4; PC = 0x8000db4 *)
mov L0x200192d4 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018fd4; PC = 0x8000db8 *)
mov L0x20018fd4 r8;



(**************** CUT   1, - *****************)

ecut and [
eqmod cf001 f001 2**11, eqmod cf065 f065 2**11, eqmod cf129 f129 2**11,
eqmod cf193 f193 2**11, eqmod cf257 f257 2**11, eqmod cf321 f321 2**11,
eqmod cf385 f385 2**11, eqmod cf449 f449 2**11,
eqmod L0x20018fd4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190d4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191d4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192d4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193d4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194d4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195d4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196d4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197d4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198d4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199d4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019ad4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019bd4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019cd4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019dd4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019ed4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017054; Value = 0xfc9dfd34; PC = 0x8000b7c *)
mov r4 L0x20017054;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017154; Value = 0x03fafe6f; PC = 0x8000b80 *)
mov r5 L0x20017154;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017254; Value = 0x008b015d; PC = 0x8000b84 *)
mov r6 L0x20017254;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017354; Value = 0x02d70218; PC = 0x8000b88 *)
mov r7 L0x20017354;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf066@sint32 : and [cf066 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf194@sint32 : and [cf194 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf322@sint32 : and [cf322 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf450@sint32 : and [cf450 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170d4; Value = 0x001b0021; PC = 0x8000c9c *)
mov r5 L0x200170d4;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171d4; Value = 0xfeaffeaf; PC = 0x8000ca0 *)
mov r6 L0x200171d4;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172d4; Value = 0x037fffa3; PC = 0x8000ca4 *)
mov r7 L0x200172d4;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fd4; Value = 0xfd290246; PC = 0x8000ca8 *)
mov r4 L0x20016fd4;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf002@sint32 : and [cf002 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf130@sint32 : and [cf130 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf258@sint32 : and [cf258 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf386@sint32 : and [cf386 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197d8; PC = 0x8000d54 *)
mov L0x200197d8 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198d8; PC = 0x8000d58 *)
mov L0x200198d8 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199d8; PC = 0x8000d5c *)
mov L0x200199d8 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019ad8; PC = 0x8000d60 *)
mov L0x20019ad8 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019bd8; PC = 0x8000d64 *)
mov L0x20019bd8 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019cd8; PC = 0x8000d68 *)
mov L0x20019cd8 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019dd8; PC = 0x8000d6c *)
mov L0x20019dd8 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019ed8; PC = 0x8000d70 *)
mov L0x20019ed8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193d8; PC = 0x8000d9c *)
mov L0x200193d8 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194d8; PC = 0x8000da0 *)
mov L0x200194d8 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195d8; PC = 0x8000da4 *)
mov L0x200195d8 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196d8; PC = 0x8000da8 *)
mov L0x200196d8 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190d8; PC = 0x8000dac *)
mov L0x200190d8 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191d8; PC = 0x8000db0 *)
mov L0x200191d8 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192d8; PC = 0x8000db4 *)
mov L0x200192d8 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018fd8; PC = 0x8000db8 *)
mov L0x20018fd8 r8;



(**************** CUT   2, - *****************)

ecut and [
eqmod cf002 f002 2**11, eqmod cf066 f066 2**11, eqmod cf130 f130 2**11,
eqmod cf194 f194 2**11, eqmod cf258 f258 2**11, eqmod cf322 f322 2**11,
eqmod cf386 f386 2**11, eqmod cf450 f450 2**11,
eqmod L0x20018fd8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190d8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191d8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192d8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193d8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194d8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195d8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196d8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197d8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198d8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199d8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019ad8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019bd8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019cd8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019dd8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019ed8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  268244 ]
];



(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017056; Value = 0x003afc9d; PC = 0x8000b7c *)
mov r4 L0x20017056;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017156; Value = 0x02e703fa; PC = 0x8000b80 *)
mov r5 L0x20017156;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017256; Value = 0x0284008b; PC = 0x8000b84 *)
mov r6 L0x20017256;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017356; Value = 0xfc4902d7; PC = 0x8000b88 *)
mov r7 L0x20017356;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf067@sint32 : and [cf067 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf195@sint32 : and [cf195 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf323@sint32 : and [cf323 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf451@sint32 : and [cf451 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170d6; Value = 0xfd19001b; PC = 0x8000c9c *)
mov r5 L0x200170d6;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171d6; Value = 0x0062feaf; PC = 0x8000ca0 *)
mov r6 L0x200171d6;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172d6; Value = 0x0141037f; PC = 0x8000ca4 *)
mov r7 L0x200172d6;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fd6; Value = 0xff04fd29; PC = 0x8000ca8 *)
mov r4 L0x20016fd6;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf003@sint32 : and [cf003 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf131@sint32 : and [cf131 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf259@sint32 : and [cf259 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf387@sint32 : and [cf387 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197dc; PC = 0x8000d54 *)
mov L0x200197dc r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198dc; PC = 0x8000d58 *)
mov L0x200198dc r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199dc; PC = 0x8000d5c *)
mov L0x200199dc r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019adc; PC = 0x8000d60 *)
mov L0x20019adc r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019bdc; PC = 0x8000d64 *)
mov L0x20019bdc r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019cdc; PC = 0x8000d68 *)
mov L0x20019cdc r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019ddc; PC = 0x8000d6c *)
mov L0x20019ddc r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019edc; PC = 0x8000d70 *)
mov L0x20019edc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193dc; PC = 0x8000d9c *)
mov L0x200193dc r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194dc; PC = 0x8000da0 *)
mov L0x200194dc r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195dc; PC = 0x8000da4 *)
mov L0x200195dc r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196dc; PC = 0x8000da8 *)
mov L0x200196dc r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190dc; PC = 0x8000dac *)
mov L0x200190dc r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191dc; PC = 0x8000db0 *)
mov L0x200191dc r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192dc; PC = 0x8000db4 *)
mov L0x200192dc r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018fdc; PC = 0x8000db8 *)
mov L0x20018fdc r8;



(**************** CUT   3, - *****************)

ecut and [
eqmod cf003 f003 2**11, eqmod cf067 f067 2**11, eqmod cf131 f131 2**11,
eqmod cf195 f195 2**11, eqmod cf259 f259 2**11, eqmod cf323 f323 2**11,
eqmod cf387 f387 2**11, eqmod cf451 f451 2**11,
eqmod L0x20018fdc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190dc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191dc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192dc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193dc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194dc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195dc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196dc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197dc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198dc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199dc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019adc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019bdc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019cdc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019ddc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019edc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017058; Value = 0xfc0d003a; PC = 0x8000b7c *)
mov r4 L0x20017058;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017158; Value = 0x00ec02e7; PC = 0x8000b80 *)
mov r5 L0x20017158;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017258; Value = 0xfe360284; PC = 0x8000b84 *)
mov r6 L0x20017258;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017358; Value = 0xff1ffc49; PC = 0x8000b88 *)
mov r7 L0x20017358;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf068@sint32 : and [cf068 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf196@sint32 : and [cf196 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf324@sint32 : and [cf324 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf452@sint32 : and [cf452 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170d8; Value = 0xff7efd19; PC = 0x8000c9c *)
mov r5 L0x200170d8;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171d8; Value = 0xfd0f0062; PC = 0x8000ca0 *)
mov r6 L0x200171d8;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172d8; Value = 0x01ff0141; PC = 0x8000ca4 *)
mov r7 L0x200172d8;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fd8; Value = 0x01b4ff04; PC = 0x8000ca8 *)
mov r4 L0x20016fd8;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf004@sint32 : and [cf004 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf132@sint32 : and [cf132 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf260@sint32 : and [cf260 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf388@sint32 : and [cf388 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197e0; PC = 0x8000d54 *)
mov L0x200197e0 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198e0; PC = 0x8000d58 *)
mov L0x200198e0 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199e0; PC = 0x8000d5c *)
mov L0x200199e0 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019ae0; PC = 0x8000d60 *)
mov L0x20019ae0 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019be0; PC = 0x8000d64 *)
mov L0x20019be0 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019ce0; PC = 0x8000d68 *)
mov L0x20019ce0 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019de0; PC = 0x8000d6c *)
mov L0x20019de0 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019ee0; PC = 0x8000d70 *)
mov L0x20019ee0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193e0; PC = 0x8000d9c *)
mov L0x200193e0 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194e0; PC = 0x8000da0 *)
mov L0x200194e0 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195e0; PC = 0x8000da4 *)
mov L0x200195e0 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196e0; PC = 0x8000da8 *)
mov L0x200196e0 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190e0; PC = 0x8000dac *)
mov L0x200190e0 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191e0; PC = 0x8000db0 *)
mov L0x200191e0 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192e0; PC = 0x8000db4 *)
mov L0x200192e0 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018fe0; PC = 0x8000db8 *)
mov L0x20018fe0 r8;



(**************** CUT   4, - *****************)

ecut and [
eqmod cf004 f004 2**11, eqmod cf068 f068 2**11, eqmod cf132 f132 2**11,
eqmod cf196 f196 2**11, eqmod cf260 f260 2**11, eqmod cf324 f324 2**11,
eqmod cf388 f388 2**11, eqmod cf452 f452 2**11,
eqmod L0x20018fe0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190e0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191e0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192e0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193e0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194e0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195e0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196e0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197e0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198e0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199e0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019ae0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019be0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019ce0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019de0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019ee0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001705a; Value = 0x020ffc0d; PC = 0x8000b7c *)
mov r4 L0x2001705a;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001715a; Value = 0xfca900ec; PC = 0x8000b80 *)
mov r5 L0x2001715a;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001725a; Value = 0xfee0fe36; PC = 0x8000b84 *)
mov r6 L0x2001725a;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001735a; Value = 0x0130ff1f; PC = 0x8000b88 *)
mov r7 L0x2001735a;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf069@sint32 : and [cf069 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf197@sint32 : and [cf197 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf325@sint32 : and [cf325 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf453@sint32 : and [cf453 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170da; Value = 0xfce4ff7e; PC = 0x8000c9c *)
mov r5 L0x200170da;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171da; Value = 0xfe86fd0f; PC = 0x8000ca0 *)
mov r6 L0x200171da;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172da; Value = 0xfc1201ff; PC = 0x8000ca4 *)
mov r7 L0x200172da;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fda; Value = 0x037801b4; PC = 0x8000ca8 *)
mov r4 L0x20016fda;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf005@sint32 : and [cf005 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf133@sint32 : and [cf133 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf261@sint32 : and [cf261 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf389@sint32 : and [cf389 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197e4; PC = 0x8000d54 *)
mov L0x200197e4 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198e4; PC = 0x8000d58 *)
mov L0x200198e4 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199e4; PC = 0x8000d5c *)
mov L0x200199e4 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019ae4; PC = 0x8000d60 *)
mov L0x20019ae4 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019be4; PC = 0x8000d64 *)
mov L0x20019be4 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019ce4; PC = 0x8000d68 *)
mov L0x20019ce4 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019de4; PC = 0x8000d6c *)
mov L0x20019de4 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019ee4; PC = 0x8000d70 *)
mov L0x20019ee4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193e4; PC = 0x8000d9c *)
mov L0x200193e4 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194e4; PC = 0x8000da0 *)
mov L0x200194e4 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195e4; PC = 0x8000da4 *)
mov L0x200195e4 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196e4; PC = 0x8000da8 *)
mov L0x200196e4 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190e4; PC = 0x8000dac *)
mov L0x200190e4 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191e4; PC = 0x8000db0 *)
mov L0x200191e4 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192e4; PC = 0x8000db4 *)
mov L0x200192e4 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018fe4; PC = 0x8000db8 *)
mov L0x20018fe4 r8;
(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;



(**************** CUT   5, - *****************)

ecut and [
eqmod cf005 f005 2**11, eqmod cf069 f069 2**11, eqmod cf133 f133 2**11,
eqmod cf197 f197 2**11, eqmod cf261 f261 2**11, eqmod cf325 f325 2**11,
eqmod cf389 f389 2**11, eqmod cf453 f453 2**11,
eqmod L0x20018fe4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190e4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191e4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192e4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193e4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194e4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195e4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196e4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197e4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198e4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199e4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019ae4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019be4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019ce4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019de4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019ee4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  268244 ]
];




(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001705c; Value = 0x02bd020f; PC = 0x8000b7c *)
mov r4 L0x2001705c;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001715c; Value = 0x02e6fca9; PC = 0x8000b80 *)
mov r5 L0x2001715c;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001725c; Value = 0xffbbfee0; PC = 0x8000b84 *)
mov r6 L0x2001725c;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001735c; Value = 0xfd810130; PC = 0x8000b88 *)
mov r7 L0x2001735c;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf070@sint32 : and [cf070 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf198@sint32 : and [cf198 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf326@sint32 : and [cf326 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf454@sint32 : and [cf454 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170dc; Value = 0xff75fce4; PC = 0x8000c9c *)
mov r5 L0x200170dc;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171dc; Value = 0xfc75fe86; PC = 0x8000ca0 *)
mov r6 L0x200171dc;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172dc; Value = 0x004ffc12; PC = 0x8000ca4 *)
mov r7 L0x200172dc;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fdc; Value = 0xfdd80378; PC = 0x8000ca8 *)
mov r4 L0x20016fdc;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf006@sint32 : and [cf006 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf134@sint32 : and [cf134 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf262@sint32 : and [cf262 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf390@sint32 : and [cf390 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197e8; PC = 0x8000d54 *)
mov L0x200197e8 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198e8; PC = 0x8000d58 *)
mov L0x200198e8 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199e8; PC = 0x8000d5c *)
mov L0x200199e8 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019ae8; PC = 0x8000d60 *)
mov L0x20019ae8 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019be8; PC = 0x8000d64 *)
mov L0x20019be8 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019ce8; PC = 0x8000d68 *)
mov L0x20019ce8 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019de8; PC = 0x8000d6c *)
mov L0x20019de8 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019ee8; PC = 0x8000d70 *)
mov L0x20019ee8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193e8; PC = 0x8000d9c *)
mov L0x200193e8 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194e8; PC = 0x8000da0 *)
mov L0x200194e8 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195e8; PC = 0x8000da4 *)
mov L0x200195e8 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196e8; PC = 0x8000da8 *)
mov L0x200196e8 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190e8; PC = 0x8000dac *)
mov L0x200190e8 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191e8; PC = 0x8000db0 *)
mov L0x200191e8 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192e8; PC = 0x8000db4 *)
mov L0x200192e8 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018fe8; PC = 0x8000db8 *)
mov L0x20018fe8 r8;



(**************** CUT   6, - *****************)

ecut and [
eqmod cf006 f006 2**11, eqmod cf070 f070 2**11, eqmod cf134 f134 2**11,
eqmod cf198 f198 2**11, eqmod cf262 f262 2**11, eqmod cf326 f326 2**11,
eqmod cf390 f390 2**11, eqmod cf454 f454 2**11,
eqmod L0x20018fe8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190e8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191e8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192e8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193e8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194e8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195e8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196e8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197e8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198e8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199e8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019ae8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019be8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019ce8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019de8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019ee8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  268244 ]
];



(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001705e; Value = 0xfe2f02bd; PC = 0x8000b7c *)
mov r4 L0x2001705e;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001715e; Value = 0xfcab02e6; PC = 0x8000b80 *)
mov r5 L0x2001715e;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001725e; Value = 0x01bfffbb; PC = 0x8000b84 *)
mov r6 L0x2001725e;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001735e; Value = 0xfd21fd81; PC = 0x8000b88 *)
mov r7 L0x2001735e;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf071@sint32 : and [cf071 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf199@sint32 : and [cf199 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf327@sint32 : and [cf327 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf455@sint32 : and [cf455 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170de; Value = 0x008eff75; PC = 0x8000c9c *)
mov r5 L0x200170de;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171de; Value = 0xff55fc75; PC = 0x8000ca0 *)
mov r6 L0x200171de;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172de; Value = 0x0242004f; PC = 0x8000ca4 *)
mov r7 L0x200172de;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fde; Value = 0xfc68fdd8; PC = 0x8000ca8 *)
mov r4 L0x20016fde;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf007@sint32 : and [cf007 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf135@sint32 : and [cf135 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf263@sint32 : and [cf263 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf391@sint32 : and [cf391 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197ec; PC = 0x8000d54 *)
mov L0x200197ec r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198ec; PC = 0x8000d58 *)
mov L0x200198ec r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199ec; PC = 0x8000d5c *)
mov L0x200199ec r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019aec; PC = 0x8000d60 *)
mov L0x20019aec r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019bec; PC = 0x8000d64 *)
mov L0x20019bec r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019cec; PC = 0x8000d68 *)
mov L0x20019cec r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019dec; PC = 0x8000d6c *)
mov L0x20019dec r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019eec; PC = 0x8000d70 *)
mov L0x20019eec r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193ec; PC = 0x8000d9c *)
mov L0x200193ec r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194ec; PC = 0x8000da0 *)
mov L0x200194ec r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195ec; PC = 0x8000da4 *)
mov L0x200195ec r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196ec; PC = 0x8000da8 *)
mov L0x200196ec r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190ec; PC = 0x8000dac *)
mov L0x200190ec r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191ec; PC = 0x8000db0 *)
mov L0x200191ec r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192ec; PC = 0x8000db4 *)
mov L0x200192ec r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018fec; PC = 0x8000db8 *)
mov L0x20018fec r8;



(**************** CUT   7, - *****************)

ecut and [
eqmod cf007 f007 2**11, eqmod cf071 f071 2**11, eqmod cf135 f135 2**11,
eqmod cf199 f199 2**11, eqmod cf263 f263 2**11, eqmod cf327 f327 2**11,
eqmod cf391 f391 2**11, eqmod cf455 f455 2**11,
eqmod L0x20018fec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190ec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191ec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192ec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193ec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194ec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195ec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196ec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197ec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198ec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199ec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019aec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019bec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019cec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019dec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019eec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017060; Value = 0x01a1fe2f; PC = 0x8000b7c *)
mov r4 L0x20017060;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017160; Value = 0x03aafcab; PC = 0x8000b80 *)
mov r5 L0x20017160;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017260; Value = 0x009701bf; PC = 0x8000b84 *)
mov r6 L0x20017260;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017360; Value = 0x03cdfd21; PC = 0x8000b88 *)
mov r7 L0x20017360;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf072@sint32 : and [cf072 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf200@sint32 : and [cf200 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf328@sint32 : and [cf328 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf456@sint32 : and [cf456 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170e0; Value = 0x00c6008e; PC = 0x8000c9c *)
mov r5 L0x200170e0;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171e0; Value = 0x0374ff55; PC = 0x8000ca0 *)
mov r6 L0x200171e0;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172e0; Value = 0xfc980242; PC = 0x8000ca4 *)
mov r7 L0x200172e0;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fe0; Value = 0x02a7fc68; PC = 0x8000ca8 *)
mov r4 L0x20016fe0;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf008@sint32 : and [cf008 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf136@sint32 : and [cf136 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf264@sint32 : and [cf264 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf392@sint32 : and [cf392 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197f0; PC = 0x8000d54 *)
mov L0x200197f0 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198f0; PC = 0x8000d58 *)
mov L0x200198f0 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199f0; PC = 0x8000d5c *)
mov L0x200199f0 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019af0; PC = 0x8000d60 *)
mov L0x20019af0 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019bf0; PC = 0x8000d64 *)
mov L0x20019bf0 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019cf0; PC = 0x8000d68 *)
mov L0x20019cf0 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019df0; PC = 0x8000d6c *)
mov L0x20019df0 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019ef0; PC = 0x8000d70 *)
mov L0x20019ef0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193f0; PC = 0x8000d9c *)
mov L0x200193f0 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194f0; PC = 0x8000da0 *)
mov L0x200194f0 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195f0; PC = 0x8000da4 *)
mov L0x200195f0 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196f0; PC = 0x8000da8 *)
mov L0x200196f0 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190f0; PC = 0x8000dac *)
mov L0x200190f0 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191f0; PC = 0x8000db0 *)
mov L0x200191f0 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192f0; PC = 0x8000db4 *)
mov L0x200192f0 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018ff0; PC = 0x8000db8 *)
mov L0x20018ff0 r8;



(**************** CUT   8, - *****************)

ecut and [
eqmod cf008 f008 2**11, eqmod cf072 f072 2**11, eqmod cf136 f136 2**11,
eqmod cf200 f200 2**11, eqmod cf264 f264 2**11, eqmod cf328 f328 2**11,
eqmod cf392 f392 2**11, eqmod cf456 f456 2**11,
eqmod L0x20018ff0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190f0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191f0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192f0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193f0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194f0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195f0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196f0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197f0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198f0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199f0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019af0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019bf0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019cf0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019df0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019ef0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017062; Value = 0x03f701a1; PC = 0x8000b7c *)
mov r4 L0x20017062;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017162; Value = 0xfc3903aa; PC = 0x8000b80 *)
mov r5 L0x20017162;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017262; Value = 0x02850097; PC = 0x8000b84 *)
mov r6 L0x20017262;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017362; Value = 0xff0d03cd; PC = 0x8000b88 *)
mov r7 L0x20017362;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf073@sint32 : and [cf073 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf201@sint32 : and [cf201 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf329@sint32 : and [cf329 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf457@sint32 : and [cf457 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170e2; Value = 0x018700c6; PC = 0x8000c9c *)
mov r5 L0x200170e2;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171e2; Value = 0xfe1c0374; PC = 0x8000ca0 *)
mov r6 L0x200171e2;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172e2; Value = 0xfccbfc98; PC = 0x8000ca4 *)
mov r7 L0x200172e2;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fe2; Value = 0x03ff02a7; PC = 0x8000ca8 *)
mov r4 L0x20016fe2;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf009@sint32 : and [cf009 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf137@sint32 : and [cf137 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf265@sint32 : and [cf265 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf393@sint32 : and [cf393 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197f4; PC = 0x8000d54 *)
mov L0x200197f4 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198f4; PC = 0x8000d58 *)
mov L0x200198f4 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199f4; PC = 0x8000d5c *)
mov L0x200199f4 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019af4; PC = 0x8000d60 *)
mov L0x20019af4 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019bf4; PC = 0x8000d64 *)
mov L0x20019bf4 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019cf4; PC = 0x8000d68 *)
mov L0x20019cf4 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019df4; PC = 0x8000d6c *)
mov L0x20019df4 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019ef4; PC = 0x8000d70 *)
mov L0x20019ef4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193f4; PC = 0x8000d9c *)
mov L0x200193f4 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194f4; PC = 0x8000da0 *)
mov L0x200194f4 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195f4; PC = 0x8000da4 *)
mov L0x200195f4 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196f4; PC = 0x8000da8 *)
mov L0x200196f4 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190f4; PC = 0x8000dac *)
mov L0x200190f4 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191f4; PC = 0x8000db0 *)
mov L0x200191f4 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192f4; PC = 0x8000db4 *)
mov L0x200192f4 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018ff4; PC = 0x8000db8 *)
mov L0x20018ff4 r8;



(**************** CUT   9, - *****************)

ecut and [
eqmod cf009 f009 2**11, eqmod cf073 f073 2**11, eqmod cf137 f137 2**11,
eqmod cf201 f201 2**11, eqmod cf265 f265 2**11, eqmod cf329 f329 2**11,
eqmod cf393 f393 2**11, eqmod cf457 f457 2**11,
eqmod L0x20018ff4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190f4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191f4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192f4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193f4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194f4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195f4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196f4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197f4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198f4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199f4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019af4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019bf4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019cf4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019df4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019ef4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017064; Value = 0xfd0f03f7; PC = 0x8000b7c *)
mov r4 L0x20017064;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017164; Value = 0x02bffc39; PC = 0x8000b80 *)
mov r5 L0x20017164;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017264; Value = 0x02a20285; PC = 0x8000b84 *)
mov r6 L0x20017264;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017364; Value = 0xfcd9ff0d; PC = 0x8000b88 *)
mov r7 L0x20017364;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf074@sint32 : and [cf074 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf202@sint32 : and [cf202 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf330@sint32 : and [cf330 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf458@sint32 : and [cf458 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170e4; Value = 0xfc850187; PC = 0x8000c9c *)
mov r5 L0x200170e4;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171e4; Value = 0x0070fe1c; PC = 0x8000ca0 *)
mov r6 L0x200171e4;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172e4; Value = 0x0273fccb; PC = 0x8000ca4 *)
mov r7 L0x200172e4;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fe4; Value = 0x003f03ff; PC = 0x8000ca8 *)
mov r4 L0x20016fe4;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf010@sint32 : and [cf010 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf138@sint32 : and [cf138 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf266@sint32 : and [cf266 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf394@sint32 : and [cf394 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197f8; PC = 0x8000d54 *)
mov L0x200197f8 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198f8; PC = 0x8000d58 *)
mov L0x200198f8 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199f8; PC = 0x8000d5c *)
mov L0x200199f8 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019af8; PC = 0x8000d60 *)
mov L0x20019af8 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019bf8; PC = 0x8000d64 *)
mov L0x20019bf8 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019cf8; PC = 0x8000d68 *)
mov L0x20019cf8 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019df8; PC = 0x8000d6c *)
mov L0x20019df8 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019ef8; PC = 0x8000d70 *)
mov L0x20019ef8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193f8; PC = 0x8000d9c *)
mov L0x200193f8 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194f8; PC = 0x8000da0 *)
mov L0x200194f8 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195f8; PC = 0x8000da4 *)
mov L0x200195f8 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196f8; PC = 0x8000da8 *)
mov L0x200196f8 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190f8; PC = 0x8000dac *)
mov L0x200190f8 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191f8; PC = 0x8000db0 *)
mov L0x200191f8 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192f8; PC = 0x8000db4 *)
mov L0x200192f8 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018ff8; PC = 0x8000db8 *)
mov L0x20018ff8 r8;



(**************** CUT  10, - *****************)

ecut and [
eqmod cf010 f010 2**11, eqmod cf074 f074 2**11, eqmod cf138 f138 2**11,
eqmod cf202 f202 2**11, eqmod cf266 f266 2**11, eqmod cf330 f330 2**11,
eqmod cf394 f394 2**11, eqmod cf458 f458 2**11,
eqmod L0x20018ff8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190f8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191f8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192f8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193f8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194f8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195f8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196f8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197f8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198f8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199f8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019af8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019bf8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019cf8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019df8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019ef8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017066; Value = 0xfcd9fd0f; PC = 0x8000b7c *)
mov r4 L0x20017066;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017166; Value = 0x03ba02bf; PC = 0x8000b80 *)
mov r5 L0x20017166;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017266; Value = 0x020a02a2; PC = 0x8000b84 *)
mov r6 L0x20017266;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017366; Value = 0x0146fcd9; PC = 0x8000b88 *)
mov r7 L0x20017366;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf075@sint32 : and [cf075 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf203@sint32 : and [cf203 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf331@sint32 : and [cf331 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf459@sint32 : and [cf459 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170e6; Value = 0x0058fc85; PC = 0x8000c9c *)
mov r5 L0x200170e6;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171e6; Value = 0xffa30070; PC = 0x8000ca0 *)
mov r6 L0x200171e6;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172e6; Value = 0x02f60273; PC = 0x8000ca4 *)
mov r7 L0x200172e6;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fe6; Value = 0x012b003f; PC = 0x8000ca8 *)
mov r4 L0x20016fe6;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf011@sint32 : and [cf011 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf139@sint32 : and [cf139 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf267@sint32 : and [cf267 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf395@sint32 : and [cf395 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200197fc; PC = 0x8000d54 *)
mov L0x200197fc r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200198fc; PC = 0x8000d58 *)
mov L0x200198fc r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x200199fc; PC = 0x8000d5c *)
mov L0x200199fc r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019afc; PC = 0x8000d60 *)
mov L0x20019afc r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019bfc; PC = 0x8000d64 *)
mov L0x20019bfc r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019cfc; PC = 0x8000d68 *)
mov L0x20019cfc r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019dfc; PC = 0x8000d6c *)
mov L0x20019dfc r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019efc; PC = 0x8000d70 *)
mov L0x20019efc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200193fc; PC = 0x8000d9c *)
mov L0x200193fc r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200194fc; PC = 0x8000da0 *)
mov L0x200194fc r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200195fc; PC = 0x8000da4 *)
mov L0x200195fc r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200196fc; PC = 0x8000da8 *)
mov L0x200196fc r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200190fc; PC = 0x8000dac *)
mov L0x200190fc r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200191fc; PC = 0x8000db0 *)
mov L0x200191fc r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200192fc; PC = 0x8000db4 *)
mov L0x200192fc r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20018ffc; PC = 0x8000db8 *)
mov L0x20018ffc r8;



(**************** CUT  11, - *****************)

ecut and [
eqmod cf011 f011 2**11, eqmod cf075 f075 2**11, eqmod cf139 f139 2**11,
eqmod cf203 f203 2**11, eqmod cf267 f267 2**11, eqmod cf331 f331 2**11,
eqmod cf395 f395 2**11, eqmod cf459 f459 2**11,
eqmod L0x20018ffc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190fc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191fc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192fc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193fc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194fc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195fc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196fc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197fc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198fc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199fc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019afc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019bfc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019cfc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019dfc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019efc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017068; Value = 0xfd68fcd9; PC = 0x8000b7c *)
mov r4 L0x20017068;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017168; Value = 0x035f03ba; PC = 0x8000b80 *)
mov r5 L0x20017168;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017268; Value = 0xfec1020a; PC = 0x8000b84 *)
mov r6 L0x20017268;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017368; Value = 0xfc710146; PC = 0x8000b88 *)
mov r7 L0x20017368;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf076@sint32 : and [cf076 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf204@sint32 : and [cf204 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf332@sint32 : and [cf332 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf460@sint32 : and [cf460 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170e8; Value = 0x00dd0058; PC = 0x8000c9c *)
mov r5 L0x200170e8;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171e8; Value = 0xfe09ffa3; PC = 0x8000ca0 *)
mov r6 L0x200171e8;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172e8; Value = 0xfdf902f6; PC = 0x8000ca4 *)
mov r7 L0x200172e8;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fe8; Value = 0xfef1012b; PC = 0x8000ca8 *)
mov r4 L0x20016fe8;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf012@sint32 : and [cf012 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf140@sint32 : and [cf140 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf268@sint32 : and [cf268 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf396@sint32 : and [cf396 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019800; PC = 0x8000d54 *)
mov L0x20019800 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019900; PC = 0x8000d58 *)
mov L0x20019900 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a00; PC = 0x8000d5c *)
mov L0x20019a00 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b00; PC = 0x8000d60 *)
mov L0x20019b00 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c00; PC = 0x8000d64 *)
mov L0x20019c00 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d00; PC = 0x8000d68 *)
mov L0x20019d00 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e00; PC = 0x8000d6c *)
mov L0x20019e00 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f00; PC = 0x8000d70 *)
mov L0x20019f00 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019400; PC = 0x8000d9c *)
mov L0x20019400 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019500; PC = 0x8000da0 *)
mov L0x20019500 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019600; PC = 0x8000da4 *)
mov L0x20019600 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019700; PC = 0x8000da8 *)
mov L0x20019700 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019100; PC = 0x8000dac *)
mov L0x20019100 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019200; PC = 0x8000db0 *)
mov L0x20019200 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019300; PC = 0x8000db4 *)
mov L0x20019300 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019000; PC = 0x8000db8 *)
mov L0x20019000 r8;



(**************** CUT  12, - *****************)

ecut and [
eqmod cf012 f012 2**11, eqmod cf076 f076 2**11, eqmod cf140 f140 2**11,
eqmod cf204 f204 2**11, eqmod cf268 f268 2**11, eqmod cf332 f332 2**11,
eqmod cf396 f396 2**11, eqmod cf460 f460 2**11,
eqmod L0x20019000*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019100*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019200*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019300*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019400*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019500*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019600*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019700*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019800*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019900*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a00*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b00*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c00*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d00*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e00*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f00*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001706a; Value = 0xfef4fd68; PC = 0x8000b7c *)
mov r4 L0x2001706a;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001716a; Value = 0x00a5035f; PC = 0x8000b80 *)
mov r5 L0x2001716a;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001726a; Value = 0xfd5afec1; PC = 0x8000b84 *)
mov r6 L0x2001726a;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001736a; Value = 0x01b1fc71; PC = 0x8000b88 *)
mov r7 L0x2001736a;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf077@sint32 : and [cf077 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf205@sint32 : and [cf205 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf333@sint32 : and [cf333 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf461@sint32 : and [cf461 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170ea; Value = 0xfc2400dd; PC = 0x8000c9c *)
mov r5 L0x200170ea;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171ea; Value = 0x00e3fe09; PC = 0x8000ca0 *)
mov r6 L0x200171ea;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172ea; Value = 0xfd54fdf9; PC = 0x8000ca4 *)
mov r7 L0x200172ea;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fea; Value = 0xfdfcfef1; PC = 0x8000ca8 *)
mov r4 L0x20016fea;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf013@sint32 : and [cf013 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf141@sint32 : and [cf141 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf269@sint32 : and [cf269 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf397@sint32 : and [cf397 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019804; PC = 0x8000d54 *)
mov L0x20019804 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019904; PC = 0x8000d58 *)
mov L0x20019904 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a04; PC = 0x8000d5c *)
mov L0x20019a04 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b04; PC = 0x8000d60 *)
mov L0x20019b04 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c04; PC = 0x8000d64 *)
mov L0x20019c04 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d04; PC = 0x8000d68 *)
mov L0x20019d04 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e04; PC = 0x8000d6c *)
mov L0x20019e04 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f04; PC = 0x8000d70 *)
mov L0x20019f04 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019404; PC = 0x8000d9c *)
mov L0x20019404 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019504; PC = 0x8000da0 *)
mov L0x20019504 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019604; PC = 0x8000da4 *)
mov L0x20019604 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019704; PC = 0x8000da8 *)
mov L0x20019704 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019104; PC = 0x8000dac *)
mov L0x20019104 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019204; PC = 0x8000db0 *)
mov L0x20019204 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019304; PC = 0x8000db4 *)
mov L0x20019304 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019004; PC = 0x8000db8 *)
mov L0x20019004 r8;



(**************** CUT  13, - *****************)

ecut and [
eqmod cf013 f013 2**11, eqmod cf077 f077 2**11, eqmod cf141 f141 2**11,
eqmod cf205 f205 2**11, eqmod cf269 f269 2**11, eqmod cf333 f333 2**11,
eqmod cf397 f397 2**11, eqmod cf461 f461 2**11,
eqmod L0x20019004*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019104*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019204*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019304*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019404*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019504*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019604*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019704*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019804*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019904*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a04*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b04*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c04*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d04*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e04*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f04*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001706c; Value = 0xfed9fef4; PC = 0x8000b7c *)
mov r4 L0x2001706c;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001716c; Value = 0xfcb100a5; PC = 0x8000b80 *)
mov r5 L0x2001716c;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001726c; Value = 0xfc48fd5a; PC = 0x8000b84 *)
mov r6 L0x2001726c;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001736c; Value = 0xff9a01b1; PC = 0x8000b88 *)
mov r7 L0x2001736c;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf078@sint32 : and [cf078 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf206@sint32 : and [cf206 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf334@sint32 : and [cf334 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf462@sint32 : and [cf462 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170ec; Value = 0xfce6fc24; PC = 0x8000c9c *)
mov r5 L0x200170ec;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171ec; Value = 0xff5f00e3; PC = 0x8000ca0 *)
mov r6 L0x200171ec;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172ec; Value = 0xfcb3fd54; PC = 0x8000ca4 *)
mov r7 L0x200172ec;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fec; Value = 0x03d9fdfc; PC = 0x8000ca8 *)
mov r4 L0x20016fec;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf014@sint32 : and [cf014 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf142@sint32 : and [cf142 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf270@sint32 : and [cf270 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf398@sint32 : and [cf398 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019808; PC = 0x8000d54 *)
mov L0x20019808 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019908; PC = 0x8000d58 *)
mov L0x20019908 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a08; PC = 0x8000d5c *)
mov L0x20019a08 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b08; PC = 0x8000d60 *)
mov L0x20019b08 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c08; PC = 0x8000d64 *)
mov L0x20019c08 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d08; PC = 0x8000d68 *)
mov L0x20019d08 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e08; PC = 0x8000d6c *)
mov L0x20019e08 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f08; PC = 0x8000d70 *)
mov L0x20019f08 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019408; PC = 0x8000d9c *)
mov L0x20019408 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019508; PC = 0x8000da0 *)
mov L0x20019508 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019608; PC = 0x8000da4 *)
mov L0x20019608 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019708; PC = 0x8000da8 *)
mov L0x20019708 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019108; PC = 0x8000dac *)
mov L0x20019108 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019208; PC = 0x8000db0 *)
mov L0x20019208 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019308; PC = 0x8000db4 *)
mov L0x20019308 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019008; PC = 0x8000db8 *)
mov L0x20019008 r8;



(**************** CUT  14, - *****************)

ecut and [
eqmod cf014 f014 2**11, eqmod cf078 f078 2**11, eqmod cf142 f142 2**11,
eqmod cf206 f206 2**11, eqmod cf270 f270 2**11, eqmod cf334 f334 2**11,
eqmod cf398 f398 2**11, eqmod cf462 f462 2**11,
eqmod L0x20019008*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019108*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019208*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019308*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019408*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019508*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019608*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019708*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019808*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019908*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a08*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b08*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c08*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d08*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e08*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f08*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001706e; Value = 0xfd3cfed9; PC = 0x8000b7c *)
mov r4 L0x2001706e;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001716e; Value = 0xfcaefcb1; PC = 0x8000b80 *)
mov r5 L0x2001716e;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001726e; Value = 0x011cfc48; PC = 0x8000b84 *)
mov r6 L0x2001726e;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001736e; Value = 0xfd72ff9a; PC = 0x8000b88 *)
mov r7 L0x2001736e;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf079@sint32 : and [cf079 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf207@sint32 : and [cf207 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf335@sint32 : and [cf335 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf463@sint32 : and [cf463 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170ee; Value = 0x029dfce6; PC = 0x8000c9c *)
mov r5 L0x200170ee;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171ee; Value = 0x00beff5f; PC = 0x8000ca0 *)
mov r6 L0x200171ee;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172ee; Value = 0xfdd4fcb3; PC = 0x8000ca4 *)
mov r7 L0x200172ee;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016fee; Value = 0x037a03d9; PC = 0x8000ca8 *)
mov r4 L0x20016fee;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf015@sint32 : and [cf015 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf143@sint32 : and [cf143 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf271@sint32 : and [cf271 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf399@sint32 : and [cf399 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x2001980c; PC = 0x8000d54 *)
mov L0x2001980c r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x2001990c; PC = 0x8000d58 *)
mov L0x2001990c r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a0c; PC = 0x8000d5c *)
mov L0x20019a0c r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b0c; PC = 0x8000d60 *)
mov L0x20019b0c r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c0c; PC = 0x8000d64 *)
mov L0x20019c0c r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d0c; PC = 0x8000d68 *)
mov L0x20019d0c r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e0c; PC = 0x8000d6c *)
mov L0x20019e0c r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f0c; PC = 0x8000d70 *)
mov L0x20019f0c r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x2001940c; PC = 0x8000d9c *)
mov L0x2001940c r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x2001950c; PC = 0x8000da0 *)
mov L0x2001950c r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x2001960c; PC = 0x8000da4 *)
mov L0x2001960c r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x2001970c; PC = 0x8000da8 *)
mov L0x2001970c r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x2001910c; PC = 0x8000dac *)
mov L0x2001910c r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x2001920c; PC = 0x8000db0 *)
mov L0x2001920c r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x2001930c; PC = 0x8000db4 *)
mov L0x2001930c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x2001900c; PC = 0x8000db8 *)
mov L0x2001900c r8;



(**************** CUT  15, - *****************)

ecut and [
eqmod cf015 f015 2**11, eqmod cf079 f079 2**11, eqmod cf143 f143 2**11,
eqmod cf207 f207 2**11, eqmod cf271 f271 2**11, eqmod cf335 f335 2**11,
eqmod cf399 f399 2**11, eqmod cf463 f463 2**11,
eqmod L0x2001900c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -       1 ],
eqmod L0x2001910c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x2001920c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  554923 ],
eqmod L0x2001930c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  489046 ],
eqmod L0x2001940c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  287998 ],
eqmod L0x2001950c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  755971 ],
eqmod L0x2001960c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  719789 ],
eqmod L0x2001970c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  324180 ],
eqmod L0x2001980c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -   29512 ],
eqmod L0x2001990c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a0c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b0c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c0c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d0c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e0c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f0c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017070; Value = 0xffa8fd3c; PC = 0x8000b7c *)
mov r4 L0x20017070;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017170; Value = 0xfd24fcae; PC = 0x8000b80 *)
mov r5 L0x20017170;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017270; Value = 0x03c9011c; PC = 0x8000b84 *)
mov r6 L0x20017270;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017370; Value = 0xfc2afd72; PC = 0x8000b88 *)
mov r7 L0x20017370;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf080@sint32 : and [cf080 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf208@sint32 : and [cf208 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf336@sint32 : and [cf336 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf464@sint32 : and [cf464 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170f0; Value = 0x025d029d; PC = 0x8000c9c *)
mov r5 L0x200170f0;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171f0; Value = 0x01b300be; PC = 0x8000ca0 *)
mov r6 L0x200171f0;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172f0; Value = 0x02ecfdd4; PC = 0x8000ca4 *)
mov r7 L0x200172f0;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016ff0; Value = 0x0396037a; PC = 0x8000ca8 *)
mov r4 L0x20016ff0;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf016@sint32 : and [cf016 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf144@sint32 : and [cf144 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf272@sint32 : and [cf272 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf400@sint32 : and [cf400 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019810; PC = 0x8000d54 *)
mov L0x20019810 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019910; PC = 0x8000d58 *)
mov L0x20019910 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a10; PC = 0x8000d5c *)
mov L0x20019a10 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b10; PC = 0x8000d60 *)
mov L0x20019b10 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c10; PC = 0x8000d64 *)
mov L0x20019c10 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d10; PC = 0x8000d68 *)
mov L0x20019d10 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e10; PC = 0x8000d6c *)
mov L0x20019e10 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f10; PC = 0x8000d70 *)
mov L0x20019f10 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019410; PC = 0x8000d9c *)
mov L0x20019410 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019510; PC = 0x8000da0 *)
mov L0x20019510 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019610; PC = 0x8000da4 *)
mov L0x20019610 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019710; PC = 0x8000da8 *)
mov L0x20019710 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019110; PC = 0x8000dac *)
mov L0x20019110 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019210; PC = 0x8000db0 *)
mov L0x20019210 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019310; PC = 0x8000db4 *)
mov L0x20019310 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019010; PC = 0x8000db8 *)
mov L0x20019010 r8;



(**************** CUT  16, - *****************)

ecut and [
eqmod cf016 f016 2**11, eqmod cf080 f080 2**11, eqmod cf144 f144 2**11,
eqmod cf208 f208 2**11, eqmod cf272 f272 2**11, eqmod cf336 f336 2**11,
eqmod cf400 f400 2**11, eqmod cf464 f464 2**11,
eqmod L0x20019010*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019110*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019210*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019310*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019410*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019510*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019610*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019710*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019810*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019910*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a10*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b10*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c10*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d10*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e10*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f10*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017072; Value = 0xfd35ffa8; PC = 0x8000b7c *)
mov r4 L0x20017072;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017172; Value = 0xfe81fd24; PC = 0x8000b80 *)
mov r5 L0x20017172;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017272; Value = 0x00c503c9; PC = 0x8000b84 *)
mov r6 L0x20017272;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017372; Value = 0x0125fc2a; PC = 0x8000b88 *)
mov r7 L0x20017372;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf081@sint32 : and [cf081 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf209@sint32 : and [cf209 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf337@sint32 : and [cf337 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf465@sint32 : and [cf465 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170f2; Value = 0xfe44025d; PC = 0x8000c9c *)
mov r5 L0x200170f2;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171f2; Value = 0xfc1401b3; PC = 0x8000ca0 *)
mov r6 L0x200171f2;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172f2; Value = 0x00b502ec; PC = 0x8000ca4 *)
mov r7 L0x200172f2;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016ff2; Value = 0x02090396; PC = 0x8000ca8 *)
mov r4 L0x20016ff2;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf017@sint32 : and [cf017 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf145@sint32 : and [cf145 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf273@sint32 : and [cf273 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf401@sint32 : and [cf401 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019814; PC = 0x8000d54 *)
mov L0x20019814 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019914; PC = 0x8000d58 *)
mov L0x20019914 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a14; PC = 0x8000d5c *)
mov L0x20019a14 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b14; PC = 0x8000d60 *)
mov L0x20019b14 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c14; PC = 0x8000d64 *)
mov L0x20019c14 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d14; PC = 0x8000d68 *)
mov L0x20019d14 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e14; PC = 0x8000d6c *)
mov L0x20019e14 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f14; PC = 0x8000d70 *)
mov L0x20019f14 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019414; PC = 0x8000d9c *)
mov L0x20019414 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019514; PC = 0x8000da0 *)
mov L0x20019514 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019614; PC = 0x8000da4 *)
mov L0x20019614 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019714; PC = 0x8000da8 *)
mov L0x20019714 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019114; PC = 0x8000dac *)
mov L0x20019114 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019214; PC = 0x8000db0 *)
mov L0x20019214 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019314; PC = 0x8000db4 *)
mov L0x20019314 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019014; PC = 0x8000db8 *)
mov L0x20019014 r8;



(**************** CUT  17, - *****************)

ecut and [
eqmod cf017 f017 2**11, eqmod cf081 f081 2**11, eqmod cf145 f145 2**11,
eqmod cf209 f209 2**11, eqmod cf273 f273 2**11, eqmod cf337 f337 2**11,
eqmod cf401 f401 2**11, eqmod cf465 f465 2**11,
eqmod L0x20019014*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019114*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019214*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019314*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019414*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019514*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019614*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019714*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019814*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019914*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a14*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b14*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c14*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d14*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e14*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f14*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017074; Value = 0x02e5fd35; PC = 0x8000b7c *)
mov r4 L0x20017074;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017174; Value = 0xfd61fe81; PC = 0x8000b80 *)
mov r5 L0x20017174;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017274; Value = 0x018900c5; PC = 0x8000b84 *)
mov r6 L0x20017274;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017374; Value = 0xfd110125; PC = 0x8000b88 *)
mov r7 L0x20017374;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf082@sint32 : and [cf082 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf210@sint32 : and [cf210 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf338@sint32 : and [cf338 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf466@sint32 : and [cf466 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170f4; Value = 0xfc40fe44; PC = 0x8000c9c *)
mov r5 L0x200170f4;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171f4; Value = 0x017afc14; PC = 0x8000ca0 *)
mov r6 L0x200171f4;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172f4; Value = 0xfc1e00b5; PC = 0x8000ca4 *)
mov r7 L0x200172f4;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016ff4; Value = 0xfc2c0209; PC = 0x8000ca8 *)
mov r4 L0x20016ff4;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf018@sint32 : and [cf018 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf146@sint32 : and [cf146 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf274@sint32 : and [cf274 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf402@sint32 : and [cf402 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019818; PC = 0x8000d54 *)
mov L0x20019818 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019918; PC = 0x8000d58 *)
mov L0x20019918 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a18; PC = 0x8000d5c *)
mov L0x20019a18 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b18; PC = 0x8000d60 *)
mov L0x20019b18 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c18; PC = 0x8000d64 *)
mov L0x20019c18 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d18; PC = 0x8000d68 *)
mov L0x20019d18 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e18; PC = 0x8000d6c *)
mov L0x20019e18 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f18; PC = 0x8000d70 *)
mov L0x20019f18 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019418; PC = 0x8000d9c *)
mov L0x20019418 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019518; PC = 0x8000da0 *)
mov L0x20019518 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019618; PC = 0x8000da4 *)
mov L0x20019618 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019718; PC = 0x8000da8 *)
mov L0x20019718 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019118; PC = 0x8000dac *)
mov L0x20019118 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019218; PC = 0x8000db0 *)
mov L0x20019218 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019318; PC = 0x8000db4 *)
mov L0x20019318 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019018; PC = 0x8000db8 *)
mov L0x20019018 r8;



(**************** CUT  18, - *****************)

ecut and [
eqmod cf018 f018 2**11, eqmod cf082 f082 2**11, eqmod cf146 f146 2**11,
eqmod cf210 f210 2**11, eqmod cf274 f274 2**11, eqmod cf338 f338 2**11,
eqmod cf402 f402 2**11, eqmod cf466 f466 2**11,
eqmod L0x20019018*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019118*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019218*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019318*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019418*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019518*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019618*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019718*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019818*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019918*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a18*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b18*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c18*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d18*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e18*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f18*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017076; Value = 0x018f02e5; PC = 0x8000b7c *)
mov r4 L0x20017076;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017176; Value = 0xff8afd61; PC = 0x8000b80 *)
mov r5 L0x20017176;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017276; Value = 0xff690189; PC = 0x8000b84 *)
mov r6 L0x20017276;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017376; Value = 0x038cfd11; PC = 0x8000b88 *)
mov r7 L0x20017376;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf083@sint32 : and [cf083 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf211@sint32 : and [cf211 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf339@sint32 : and [cf339 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf467@sint32 : and [cf467 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170f6; Value = 0x028bfc40; PC = 0x8000c9c *)
mov r5 L0x200170f6;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171f6; Value = 0xff27017a; PC = 0x8000ca0 *)
mov r6 L0x200171f6;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172f6; Value = 0x0282fc1e; PC = 0x8000ca4 *)
mov r7 L0x200172f6;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016ff6; Value = 0x00a5fc2c; PC = 0x8000ca8 *)
mov r4 L0x20016ff6;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf019@sint32 : and [cf019 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf147@sint32 : and [cf147 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf275@sint32 : and [cf275 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf403@sint32 : and [cf403 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x2001981c; PC = 0x8000d54 *)
mov L0x2001981c r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x2001991c; PC = 0x8000d58 *)
mov L0x2001991c r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a1c; PC = 0x8000d5c *)
mov L0x20019a1c r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b1c; PC = 0x8000d60 *)
mov L0x20019b1c r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c1c; PC = 0x8000d64 *)
mov L0x20019c1c r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d1c; PC = 0x8000d68 *)
mov L0x20019d1c r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e1c; PC = 0x8000d6c *)
mov L0x20019e1c r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f1c; PC = 0x8000d70 *)
mov L0x20019f1c r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x2001941c; PC = 0x8000d9c *)
mov L0x2001941c r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x2001951c; PC = 0x8000da0 *)
mov L0x2001951c r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x2001961c; PC = 0x8000da4 *)
mov L0x2001961c r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x2001971c; PC = 0x8000da8 *)
mov L0x2001971c r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x2001911c; PC = 0x8000dac *)
mov L0x2001911c r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x2001921c; PC = 0x8000db0 *)
mov L0x2001921c r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x2001931c; PC = 0x8000db4 *)
mov L0x2001931c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x2001901c; PC = 0x8000db8 *)
mov L0x2001901c r8;



(**************** CUT  19, - *****************)

ecut and [
eqmod cf019 f019 2**11, eqmod cf083 f083 2**11, eqmod cf147 f147 2**11,
eqmod cf211 f211 2**11, eqmod cf275 f275 2**11, eqmod cf339 f339 2**11,
eqmod cf403 f403 2**11, eqmod cf467 f467 2**11,
eqmod L0x2001901c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -       1 ],
eqmod L0x2001911c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x2001921c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  554923 ],
eqmod L0x2001931c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  489046 ],
eqmod L0x2001941c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  287998 ],
eqmod L0x2001951c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  755971 ],
eqmod L0x2001961c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  719789 ],
eqmod L0x2001971c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  324180 ],
eqmod L0x2001981c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -   29512 ],
eqmod L0x2001991c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a1c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b1c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c1c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d1c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e1c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f1c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017078; Value = 0x022a018f; PC = 0x8000b7c *)
mov r4 L0x20017078;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017178; Value = 0x0078ff8a; PC = 0x8000b80 *)
mov r5 L0x20017178;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017278; Value = 0x00d2ff69; PC = 0x8000b84 *)
mov r6 L0x20017278;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017378; Value = 0x0081038c; PC = 0x8000b88 *)
mov r7 L0x20017378;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf084@sint32 : and [cf084 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf212@sint32 : and [cf212 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf340@sint32 : and [cf340 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf468@sint32 : and [cf468 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170f8; Value = 0xfc5a028b; PC = 0x8000c9c *)
mov r5 L0x200170f8;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171f8; Value = 0x0058ff27; PC = 0x8000ca0 *)
mov r6 L0x200171f8;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172f8; Value = 0xff7f0282; PC = 0x8000ca4 *)
mov r7 L0x200172f8;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016ff8; Value = 0xfd5700a5; PC = 0x8000ca8 *)
mov r4 L0x20016ff8;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf020@sint32 : and [cf020 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf148@sint32 : and [cf148 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf276@sint32 : and [cf276 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf404@sint32 : and [cf404 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019820; PC = 0x8000d54 *)
mov L0x20019820 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019920; PC = 0x8000d58 *)
mov L0x20019920 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a20; PC = 0x8000d5c *)
mov L0x20019a20 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b20; PC = 0x8000d60 *)
mov L0x20019b20 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c20; PC = 0x8000d64 *)
mov L0x20019c20 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d20; PC = 0x8000d68 *)
mov L0x20019d20 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e20; PC = 0x8000d6c *)
mov L0x20019e20 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f20; PC = 0x8000d70 *)
mov L0x20019f20 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019420; PC = 0x8000d9c *)
mov L0x20019420 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019520; PC = 0x8000da0 *)
mov L0x20019520 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019620; PC = 0x8000da4 *)
mov L0x20019620 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019720; PC = 0x8000da8 *)
mov L0x20019720 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019120; PC = 0x8000dac *)
mov L0x20019120 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019220; PC = 0x8000db0 *)
mov L0x20019220 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019320; PC = 0x8000db4 *)
mov L0x20019320 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019020; PC = 0x8000db8 *)
mov L0x20019020 r8;



(**************** CUT  20, - *****************)

ecut and [
eqmod cf020 f020 2**11, eqmod cf084 f084 2**11, eqmod cf148 f148 2**11,
eqmod cf212 f212 2**11, eqmod cf276 f276 2**11, eqmod cf340 f340 2**11,
eqmod cf404 f404 2**11, eqmod cf468 f468 2**11,
eqmod L0x20019020*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019120*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019220*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019320*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019420*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019520*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019620*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019720*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019820*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019920*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a20*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b20*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c20*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d20*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e20*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f20*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  268244 ]
];



(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001707a; Value = 0xfdcd022a; PC = 0x8000b7c *)
mov r4 L0x2001707a;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001717a; Value = 0x03450078; PC = 0x8000b80 *)
mov r5 L0x2001717a;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001727a; Value = 0xffaa00d2; PC = 0x8000b84 *)
mov r6 L0x2001727a;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001737a; Value = 0x021a0081; PC = 0x8000b88 *)
mov r7 L0x2001737a;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf085@sint32 : and [cf085 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf213@sint32 : and [cf213 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf341@sint32 : and [cf341 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf469@sint32 : and [cf469 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170fa; Value = 0xfe23fc5a; PC = 0x8000c9c *)
mov r5 L0x200170fa;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171fa; Value = 0xfd270058; PC = 0x8000ca0 *)
mov r6 L0x200171fa;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172fa; Value = 0xfd56ff7f; PC = 0x8000ca4 *)
mov r7 L0x200172fa;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016ffa; Value = 0x0374fd57; PC = 0x8000ca8 *)
mov r4 L0x20016ffa;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf021@sint32 : and [cf021 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf149@sint32 : and [cf149 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf277@sint32 : and [cf277 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf405@sint32 : and [cf405 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019824; PC = 0x8000d54 *)
mov L0x20019824 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019924; PC = 0x8000d58 *)
mov L0x20019924 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a24; PC = 0x8000d5c *)
mov L0x20019a24 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b24; PC = 0x8000d60 *)
mov L0x20019b24 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c24; PC = 0x8000d64 *)
mov L0x20019c24 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d24; PC = 0x8000d68 *)
mov L0x20019d24 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e24; PC = 0x8000d6c *)
mov L0x20019e24 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f24; PC = 0x8000d70 *)
mov L0x20019f24 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019424; PC = 0x8000d9c *)
mov L0x20019424 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019524; PC = 0x8000da0 *)
mov L0x20019524 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019624; PC = 0x8000da4 *)
mov L0x20019624 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019724; PC = 0x8000da8 *)
mov L0x20019724 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019124; PC = 0x8000dac *)
mov L0x20019124 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019224; PC = 0x8000db0 *)
mov L0x20019224 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019324; PC = 0x8000db4 *)
mov L0x20019324 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019024; PC = 0x8000db8 *)
mov L0x20019024 r8;



(**************** CUT  21, - *****************)

ecut and [
eqmod cf021 f021 2**11, eqmod cf085 f085 2**11, eqmod cf149 f149 2**11,
eqmod cf213 f213 2**11, eqmod cf277 f277 2**11, eqmod cf341 f341 2**11,
eqmod cf405 f405 2**11, eqmod cf469 f469 2**11,
eqmod L0x20019024*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019124*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019224*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019324*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019424*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019524*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019624*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019724*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019824*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019924*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a24*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b24*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c24*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d24*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e24*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f24*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001707c; Value = 0x00c2fdcd; PC = 0x8000b7c *)
mov r4 L0x2001707c;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001717c; Value = 0xfde40345; PC = 0x8000b80 *)
mov r5 L0x2001717c;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001727c; Value = 0xfe32ffaa; PC = 0x8000b84 *)
mov r6 L0x2001727c;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001737c; Value = 0xfed4021a; PC = 0x8000b88 *)
mov r7 L0x2001737c;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf086@sint32 : and [cf086 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf214@sint32 : and [cf214 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf342@sint32 : and [cf342 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf470@sint32 : and [cf470 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170fc; Value = 0xfe5efe23; PC = 0x8000c9c *)
mov r5 L0x200170fc;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171fc; Value = 0x006bfd27; PC = 0x8000ca0 *)
mov r6 L0x200171fc;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172fc; Value = 0x0263fd56; PC = 0x8000ca4 *)
mov r7 L0x200172fc;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016ffc; Value = 0xff640374; PC = 0x8000ca8 *)
mov r4 L0x20016ffc;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf022@sint32 : and [cf022 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf150@sint32 : and [cf150 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf278@sint32 : and [cf278 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf406@sint32 : and [cf406 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019828; PC = 0x8000d54 *)
mov L0x20019828 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019928; PC = 0x8000d58 *)
mov L0x20019928 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a28; PC = 0x8000d5c *)
mov L0x20019a28 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b28; PC = 0x8000d60 *)
mov L0x20019b28 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c28; PC = 0x8000d64 *)
mov L0x20019c28 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d28; PC = 0x8000d68 *)
mov L0x20019d28 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e28; PC = 0x8000d6c *)
mov L0x20019e28 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f28; PC = 0x8000d70 *)
mov L0x20019f28 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019428; PC = 0x8000d9c *)
mov L0x20019428 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019528; PC = 0x8000da0 *)
mov L0x20019528 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019628; PC = 0x8000da4 *)
mov L0x20019628 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019728; PC = 0x8000da8 *)
mov L0x20019728 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019128; PC = 0x8000dac *)
mov L0x20019128 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019228; PC = 0x8000db0 *)
mov L0x20019228 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019328; PC = 0x8000db4 *)
mov L0x20019328 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019028; PC = 0x8000db8 *)
mov L0x20019028 r8;



(**************** CUT  22, - *****************)

ecut and [
eqmod cf022 f022 2**11, eqmod cf086 f086 2**11, eqmod cf150 f150 2**11,
eqmod cf214 f214 2**11, eqmod cf278 f278 2**11, eqmod cf342 f342 2**11,
eqmod cf406 f406 2**11, eqmod cf470 f470 2**11,
eqmod L0x20019028*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019128*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019228*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019328*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019428*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019528*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019628*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019728*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019828*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019928*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a28*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b28*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c28*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d28*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e28*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f28*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001707e; Value = 0x004000c2; PC = 0x8000b7c *)
mov r4 L0x2001707e;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001717e; Value = 0x023cfde4; PC = 0x8000b80 *)
mov r5 L0x2001717e;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001727e; Value = 0xfdcffe32; PC = 0x8000b84 *)
mov r6 L0x2001727e;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001737e; Value = 0xff2dfed4; PC = 0x8000b88 *)
mov r7 L0x2001737e;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf087@sint32 : and [cf087 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf215@sint32 : and [cf215 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf343@sint32 : and [cf343 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf471@sint32 : and [cf471 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x200170fe; Value = 0x01a9fe5e; PC = 0x8000c9c *)
mov r5 L0x200170fe;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x200171fe; Value = 0xff10006b; PC = 0x8000ca0 *)
mov r6 L0x200171fe;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x200172fe; Value = 0xfd7b0263; PC = 0x8000ca4 *)
mov r7 L0x200172fe;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20016ffe; Value = 0x00c4ff64; PC = 0x8000ca8 *)
mov r4 L0x20016ffe;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf023@sint32 : and [cf023 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf151@sint32 : and [cf151 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf279@sint32 : and [cf279 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf407@sint32 : and [cf407 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x2001982c; PC = 0x8000d54 *)
mov L0x2001982c r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x2001992c; PC = 0x8000d58 *)
mov L0x2001992c r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a2c; PC = 0x8000d5c *)
mov L0x20019a2c r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b2c; PC = 0x8000d60 *)
mov L0x20019b2c r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c2c; PC = 0x8000d64 *)
mov L0x20019c2c r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d2c; PC = 0x8000d68 *)
mov L0x20019d2c r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e2c; PC = 0x8000d6c *)
mov L0x20019e2c r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f2c; PC = 0x8000d70 *)
mov L0x20019f2c r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x2001942c; PC = 0x8000d9c *)
mov L0x2001942c r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x2001952c; PC = 0x8000da0 *)
mov L0x2001952c r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x2001962c; PC = 0x8000da4 *)
mov L0x2001962c r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x2001972c; PC = 0x8000da8 *)
mov L0x2001972c r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x2001912c; PC = 0x8000dac *)
mov L0x2001912c r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x2001922c; PC = 0x8000db0 *)
mov L0x2001922c r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x2001932c; PC = 0x8000db4 *)
mov L0x2001932c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x2001902c; PC = 0x8000db8 *)
mov L0x2001902c r8;



(**************** CUT  23, - *****************)

ecut and [
eqmod cf023 f023 2**11, eqmod cf087 f087 2**11, eqmod cf151 f151 2**11,
eqmod cf215 f215 2**11, eqmod cf279 f279 2**11, eqmod cf343 f343 2**11,
eqmod cf407 f407 2**11, eqmod cf471 f471 2**11,
eqmod L0x2001902c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -       1 ],
eqmod L0x2001912c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x2001922c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  554923 ],
eqmod L0x2001932c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  489046 ],
eqmod L0x2001942c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  287998 ],
eqmod L0x2001952c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  755971 ],
eqmod L0x2001962c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  719789 ],
eqmod L0x2001972c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  324180 ],
eqmod L0x2001982c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -   29512 ],
eqmod L0x2001992c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a2c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b2c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c2c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d2c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e2c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f2c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017080; Value = 0x03c10040; PC = 0x8000b7c *)
mov r4 L0x20017080;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017180; Value = 0x0247023c; PC = 0x8000b80 *)
mov r5 L0x20017180;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017280; Value = 0xfd71fdcf; PC = 0x8000b84 *)
mov r6 L0x20017280;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017380; Value = 0xfd34ff2d; PC = 0x8000b88 *)
mov r7 L0x20017380;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf088@sint32 : and [cf088 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf216@sint32 : and [cf216 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf344@sint32 : and [cf344 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf472@sint32 : and [cf472 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017100; Value = 0x01ef01a9; PC = 0x8000c9c *)
mov r5 L0x20017100;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017200; Value = 0x02d8ff10; PC = 0x8000ca0 *)
mov r6 L0x20017200;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017300; Value = 0x0160fd7b; PC = 0x8000ca4 *)
mov r7 L0x20017300;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017000; Value = 0x03af00c4; PC = 0x8000ca8 *)
mov r4 L0x20017000;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf024@sint32 : and [cf024 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf152@sint32 : and [cf152 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf280@sint32 : and [cf280 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf408@sint32 : and [cf408 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019830; PC = 0x8000d54 *)
mov L0x20019830 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019930; PC = 0x8000d58 *)
mov L0x20019930 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a30; PC = 0x8000d5c *)
mov L0x20019a30 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b30; PC = 0x8000d60 *)
mov L0x20019b30 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c30; PC = 0x8000d64 *)
mov L0x20019c30 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d30; PC = 0x8000d68 *)
mov L0x20019d30 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e30; PC = 0x8000d6c *)
mov L0x20019e30 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f30; PC = 0x8000d70 *)
mov L0x20019f30 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019430; PC = 0x8000d9c *)
mov L0x20019430 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019530; PC = 0x8000da0 *)
mov L0x20019530 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019630; PC = 0x8000da4 *)
mov L0x20019630 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019730; PC = 0x8000da8 *)
mov L0x20019730 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019130; PC = 0x8000dac *)
mov L0x20019130 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019230; PC = 0x8000db0 *)
mov L0x20019230 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019330; PC = 0x8000db4 *)
mov L0x20019330 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019030; PC = 0x8000db8 *)
mov L0x20019030 r8;



(**************** CUT  24, - *****************)

ecut and [
eqmod cf024 f024 2**11, eqmod cf088 f088 2**11, eqmod cf152 f152 2**11,
eqmod cf216 f216 2**11, eqmod cf280 f280 2**11, eqmod cf344 f344 2**11,
eqmod cf408 f408 2**11, eqmod cf472 f472 2**11,
eqmod L0x20019030*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019130*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019230*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019330*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019430*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019530*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019630*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019730*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019830*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019930*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a30*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b30*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c30*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d30*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e30*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f30*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017082; Value = 0xfecf03c1; PC = 0x8000b7c *)
mov r4 L0x20017082;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017182; Value = 0x00780247; PC = 0x8000b80 *)
mov r5 L0x20017182;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017282; Value = 0xfdc8fd71; PC = 0x8000b84 *)
mov r6 L0x20017282;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017382; Value = 0xfcaafd34; PC = 0x8000b88 *)
mov r7 L0x20017382;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf089@sint32 : and [cf089 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf217@sint32 : and [cf217 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf345@sint32 : and [cf345 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf473@sint32 : and [cf473 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017102; Value = 0xfeec01ef; PC = 0x8000c9c *)
mov r5 L0x20017102;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017202; Value = 0x00b102d8; PC = 0x8000ca0 *)
mov r6 L0x20017202;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017302; Value = 0xfda70160; PC = 0x8000ca4 *)
mov r7 L0x20017302;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017002; Value = 0xfd1503af; PC = 0x8000ca8 *)
mov r4 L0x20017002;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf025@sint32 : and [cf025 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf153@sint32 : and [cf153 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf281@sint32 : and [cf281 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf409@sint32 : and [cf409 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019834; PC = 0x8000d54 *)
mov L0x20019834 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019934; PC = 0x8000d58 *)
mov L0x20019934 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a34; PC = 0x8000d5c *)
mov L0x20019a34 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b34; PC = 0x8000d60 *)
mov L0x20019b34 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c34; PC = 0x8000d64 *)
mov L0x20019c34 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d34; PC = 0x8000d68 *)
mov L0x20019d34 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e34; PC = 0x8000d6c *)
mov L0x20019e34 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f34; PC = 0x8000d70 *)
mov L0x20019f34 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019434; PC = 0x8000d9c *)
mov L0x20019434 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019534; PC = 0x8000da0 *)
mov L0x20019534 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019634; PC = 0x8000da4 *)
mov L0x20019634 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019734; PC = 0x8000da8 *)
mov L0x20019734 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019134; PC = 0x8000dac *)
mov L0x20019134 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019234; PC = 0x8000db0 *)
mov L0x20019234 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019334; PC = 0x8000db4 *)
mov L0x20019334 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019034; PC = 0x8000db8 *)
mov L0x20019034 r8;



(**************** CUT  25, - *****************)

ecut and [
eqmod cf025 f025 2**11, eqmod cf089 f089 2**11, eqmod cf153 f153 2**11,
eqmod cf217 f217 2**11, eqmod cf281 f281 2**11, eqmod cf345 f345 2**11,
eqmod cf409 f409 2**11, eqmod cf473 f473 2**11,
eqmod L0x20019034*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019134*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019234*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019334*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019434*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019534*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019634*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019734*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019834*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019934*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a34*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b34*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c34*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d34*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e34*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f34*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017084; Value = 0x0052fecf; PC = 0x8000b7c *)
mov r4 L0x20017084;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017184; Value = 0x02390078; PC = 0x8000b80 *)
mov r5 L0x20017184;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017284; Value = 0xfdd7fdc8; PC = 0x8000b84 *)
mov r6 L0x20017284;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017384; Value = 0x0368fcaa; PC = 0x8000b88 *)
mov r7 L0x20017384;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf090@sint32 : and [cf090 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf218@sint32 : and [cf218 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf346@sint32 : and [cf346 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf474@sint32 : and [cf474 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017104; Value = 0x023ffeec; PC = 0x8000c9c *)
mov r5 L0x20017104;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017204; Value = 0x011000b1; PC = 0x8000ca0 *)
mov r6 L0x20017204;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017304; Value = 0xfdaafda7; PC = 0x8000ca4 *)
mov r7 L0x20017304;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017004; Value = 0x0228fd15; PC = 0x8000ca8 *)
mov r4 L0x20017004;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf026@sint32 : and [cf026 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf154@sint32 : and [cf154 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf282@sint32 : and [cf282 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf410@sint32 : and [cf410 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019838; PC = 0x8000d54 *)
mov L0x20019838 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019938; PC = 0x8000d58 *)
mov L0x20019938 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a38; PC = 0x8000d5c *)
mov L0x20019a38 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b38; PC = 0x8000d60 *)
mov L0x20019b38 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c38; PC = 0x8000d64 *)
mov L0x20019c38 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d38; PC = 0x8000d68 *)
mov L0x20019d38 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e38; PC = 0x8000d6c *)
mov L0x20019e38 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f38; PC = 0x8000d70 *)
mov L0x20019f38 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019438; PC = 0x8000d9c *)
mov L0x20019438 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019538; PC = 0x8000da0 *)
mov L0x20019538 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019638; PC = 0x8000da4 *)
mov L0x20019638 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019738; PC = 0x8000da8 *)
mov L0x20019738 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019138; PC = 0x8000dac *)
mov L0x20019138 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019238; PC = 0x8000db0 *)
mov L0x20019238 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019338; PC = 0x8000db4 *)
mov L0x20019338 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019038; PC = 0x8000db8 *)
mov L0x20019038 r8;



(**************** CUT  26, - *****************)

ecut and [
eqmod cf026 f026 2**11, eqmod cf090 f090 2**11, eqmod cf154 f154 2**11,
eqmod cf218 f218 2**11, eqmod cf282 f282 2**11, eqmod cf346 f346 2**11,
eqmod cf410 f410 2**11, eqmod cf474 f474 2**11,
eqmod L0x20019038*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019138*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019238*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019338*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019438*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019538*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019638*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019738*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019838*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019938*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a38*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b38*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c38*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d38*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e38*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f38*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017086; Value = 0xfd710052; PC = 0x8000b7c *)
mov r4 L0x20017086;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017186; Value = 0x00470239; PC = 0x8000b80 *)
mov r5 L0x20017186;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017286; Value = 0x0214fdd7; PC = 0x8000b84 *)
mov r6 L0x20017286;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017386; Value = 0xfdac0368; PC = 0x8000b88 *)
mov r7 L0x20017386;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf091@sint32 : and [cf091 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf219@sint32 : and [cf219 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf347@sint32 : and [cf347 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf475@sint32 : and [cf475 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017106; Value = 0x0391023f; PC = 0x8000c9c *)
mov r5 L0x20017106;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017206; Value = 0xfd9a0110; PC = 0x8000ca0 *)
mov r6 L0x20017206;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017306; Value = 0xfd7ffdaa; PC = 0x8000ca4 *)
mov r7 L0x20017306;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017006; Value = 0x03a40228; PC = 0x8000ca8 *)
mov r4 L0x20017006;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf027@sint32 : and [cf027 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf155@sint32 : and [cf155 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf283@sint32 : and [cf283 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf411@sint32 : and [cf411 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x2001983c; PC = 0x8000d54 *)
mov L0x2001983c r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x2001993c; PC = 0x8000d58 *)
mov L0x2001993c r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a3c; PC = 0x8000d5c *)
mov L0x20019a3c r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b3c; PC = 0x8000d60 *)
mov L0x20019b3c r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c3c; PC = 0x8000d64 *)
mov L0x20019c3c r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d3c; PC = 0x8000d68 *)
mov L0x20019d3c r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e3c; PC = 0x8000d6c *)
mov L0x20019e3c r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f3c; PC = 0x8000d70 *)
mov L0x20019f3c r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x2001943c; PC = 0x8000d9c *)
mov L0x2001943c r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x2001953c; PC = 0x8000da0 *)
mov L0x2001953c r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x2001963c; PC = 0x8000da4 *)
mov L0x2001963c r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x2001973c; PC = 0x8000da8 *)
mov L0x2001973c r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x2001913c; PC = 0x8000dac *)
mov L0x2001913c r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x2001923c; PC = 0x8000db0 *)
mov L0x2001923c r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x2001933c; PC = 0x8000db4 *)
mov L0x2001933c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x2001903c; PC = 0x8000db8 *)
mov L0x2001903c r8;



(**************** CUT  27, - *****************)

ecut and [
eqmod cf027 f027 2**11, eqmod cf091 f091 2**11, eqmod cf155 f155 2**11,
eqmod cf219 f219 2**11, eqmod cf283 f283 2**11, eqmod cf347 f347 2**11,
eqmod cf411 f411 2**11, eqmod cf475 f475 2**11,
eqmod L0x2001903c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -       1 ],
eqmod L0x2001913c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x2001923c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  554923 ],
eqmod L0x2001933c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  489046 ],
eqmod L0x2001943c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  287998 ],
eqmod L0x2001953c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  755971 ],
eqmod L0x2001963c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  719789 ],
eqmod L0x2001973c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  324180 ],
eqmod L0x2001983c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -   29512 ],
eqmod L0x2001993c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a3c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b3c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c3c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d3c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e3c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f3c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017088; Value = 0x026afd71; PC = 0x8000b7c *)
mov r4 L0x20017088;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017188; Value = 0x00c70047; PC = 0x8000b80 *)
mov r5 L0x20017188;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017288; Value = 0x02510214; PC = 0x8000b84 *)
mov r6 L0x20017288;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017388; Value = 0x0276fdac; PC = 0x8000b88 *)
mov r7 L0x20017388;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf092@sint32 : and [cf092 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf220@sint32 : and [cf220 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf348@sint32 : and [cf348 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf476@sint32 : and [cf476 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017108; Value = 0xfd9c0391; PC = 0x8000c9c *)
mov r5 L0x20017108;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017208; Value = 0x00aefd9a; PC = 0x8000ca0 *)
mov r6 L0x20017208;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017308; Value = 0x00acfd7f; PC = 0x8000ca4 *)
mov r7 L0x20017308;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017008; Value = 0xfce903a4; PC = 0x8000ca8 *)
mov r4 L0x20017008;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf028@sint32 : and [cf028 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf156@sint32 : and [cf156 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf284@sint32 : and [cf284 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf412@sint32 : and [cf412 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019840; PC = 0x8000d54 *)
mov L0x20019840 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019940; PC = 0x8000d58 *)
mov L0x20019940 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a40; PC = 0x8000d5c *)
mov L0x20019a40 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b40; PC = 0x8000d60 *)
mov L0x20019b40 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c40; PC = 0x8000d64 *)
mov L0x20019c40 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d40; PC = 0x8000d68 *)
mov L0x20019d40 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e40; PC = 0x8000d6c *)
mov L0x20019e40 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f40; PC = 0x8000d70 *)
mov L0x20019f40 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019440; PC = 0x8000d9c *)
mov L0x20019440 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019540; PC = 0x8000da0 *)
mov L0x20019540 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019640; PC = 0x8000da4 *)
mov L0x20019640 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019740; PC = 0x8000da8 *)
mov L0x20019740 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019140; PC = 0x8000dac *)
mov L0x20019140 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019240; PC = 0x8000db0 *)
mov L0x20019240 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019340; PC = 0x8000db4 *)
mov L0x20019340 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019040; PC = 0x8000db8 *)
mov L0x20019040 r8;



(**************** CUT  28, - *****************)

ecut and [
eqmod cf028 f028 2**11, eqmod cf092 f092 2**11, eqmod cf156 f156 2**11,
eqmod cf220 f220 2**11, eqmod cf284 f284 2**11, eqmod cf348 f348 2**11,
eqmod cf412 f412 2**11, eqmod cf476 f476 2**11,
eqmod L0x20019040*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019140*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019240*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019340*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019440*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019540*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019640*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019740*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019840*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019940*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a40*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b40*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c40*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d40*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e40*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f40*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001708a; Value = 0xfe72026a; PC = 0x8000b7c *)
mov r4 L0x2001708a;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001718a; Value = 0x019d00c7; PC = 0x8000b80 *)
mov r5 L0x2001718a;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001728a; Value = 0x00a20251; PC = 0x8000b84 *)
mov r6 L0x2001728a;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001738a; Value = 0xfe720276; PC = 0x8000b88 *)
mov r7 L0x2001738a;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf093@sint32 : and [cf093 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf221@sint32 : and [cf221 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf349@sint32 : and [cf349 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf477@sint32 : and [cf477 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001710a; Value = 0xff7afd9c; PC = 0x8000c9c *)
mov r5 L0x2001710a;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001720a; Value = 0xfd0500ae; PC = 0x8000ca0 *)
mov r6 L0x2001720a;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001730a; Value = 0xfce100ac; PC = 0x8000ca4 *)
mov r7 L0x2001730a;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001700a; Value = 0x0257fce9; PC = 0x8000ca8 *)
mov r4 L0x2001700a;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf029@sint32 : and [cf029 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf157@sint32 : and [cf157 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf285@sint32 : and [cf285 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf413@sint32 : and [cf413 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019844; PC = 0x8000d54 *)
mov L0x20019844 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019944; PC = 0x8000d58 *)
mov L0x20019944 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a44; PC = 0x8000d5c *)
mov L0x20019a44 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b44; PC = 0x8000d60 *)
mov L0x20019b44 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c44; PC = 0x8000d64 *)
mov L0x20019c44 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d44; PC = 0x8000d68 *)
mov L0x20019d44 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e44; PC = 0x8000d6c *)
mov L0x20019e44 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f44; PC = 0x8000d70 *)
mov L0x20019f44 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019444; PC = 0x8000d9c *)
mov L0x20019444 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019544; PC = 0x8000da0 *)
mov L0x20019544 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019644; PC = 0x8000da4 *)
mov L0x20019644 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019744; PC = 0x8000da8 *)
mov L0x20019744 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019144; PC = 0x8000dac *)
mov L0x20019144 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019244; PC = 0x8000db0 *)
mov L0x20019244 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019344; PC = 0x8000db4 *)
mov L0x20019344 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019044; PC = 0x8000db8 *)
mov L0x20019044 r8;



(**************** CUT  29, - *****************)

ecut and [
eqmod cf029 f029 2**11, eqmod cf093 f093 2**11, eqmod cf157 f157 2**11,
eqmod cf221 f221 2**11, eqmod cf285 f285 2**11, eqmod cf349 f349 2**11,
eqmod cf413 f413 2**11, eqmod cf477 f477 2**11,
eqmod L0x20019044*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019144*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019244*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019344*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019444*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019544*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019644*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019744*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019844*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019944*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a44*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b44*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c44*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d44*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e44*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f44*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001708c; Value = 0xff29fe72; PC = 0x8000b7c *)
mov r4 L0x2001708c;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001718c; Value = 0xfdd9019d; PC = 0x8000b80 *)
mov r5 L0x2001718c;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001728c; Value = 0xfe0600a2; PC = 0x8000b84 *)
mov r6 L0x2001728c;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001738c; Value = 0xfc81fe72; PC = 0x8000b88 *)
mov r7 L0x2001738c;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf094@sint32 : and [cf094 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf222@sint32 : and [cf222 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf350@sint32 : and [cf350 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf478@sint32 : and [cf478 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001710c; Value = 0xfe41ff7a; PC = 0x8000c9c *)
mov r5 L0x2001710c;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001720c; Value = 0xfe08fd05; PC = 0x8000ca0 *)
mov r6 L0x2001720c;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001730c; Value = 0xfff0fce1; PC = 0x8000ca4 *)
mov r7 L0x2001730c;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001700c; Value = 0x03db0257; PC = 0x8000ca8 *)
mov r4 L0x2001700c;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf030@sint32 : and [cf030 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf158@sint32 : and [cf158 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf286@sint32 : and [cf286 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf414@sint32 : and [cf414 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019848; PC = 0x8000d54 *)
mov L0x20019848 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019948; PC = 0x8000d58 *)
mov L0x20019948 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a48; PC = 0x8000d5c *)
mov L0x20019a48 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b48; PC = 0x8000d60 *)
mov L0x20019b48 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c48; PC = 0x8000d64 *)
mov L0x20019c48 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d48; PC = 0x8000d68 *)
mov L0x20019d48 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e48; PC = 0x8000d6c *)
mov L0x20019e48 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f48; PC = 0x8000d70 *)
mov L0x20019f48 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019448; PC = 0x8000d9c *)
mov L0x20019448 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019548; PC = 0x8000da0 *)
mov L0x20019548 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019648; PC = 0x8000da4 *)
mov L0x20019648 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019748; PC = 0x8000da8 *)
mov L0x20019748 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019148; PC = 0x8000dac *)
mov L0x20019148 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019248; PC = 0x8000db0 *)
mov L0x20019248 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019348; PC = 0x8000db4 *)
mov L0x20019348 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019048; PC = 0x8000db8 *)
mov L0x20019048 r8;



(**************** CUT  30, - *****************)

ecut and [
eqmod cf030 f030 2**11, eqmod cf094 f094 2**11, eqmod cf158 f158 2**11,
eqmod cf222 f222 2**11, eqmod cf286 f286 2**11, eqmod cf350 f350 2**11,
eqmod cf414 f414 2**11, eqmod cf478 f478 2**11,
eqmod L0x20019048*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019148*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019248*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019348*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019448*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019548*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019648*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019748*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019848*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019948*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a48*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b48*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c48*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d48*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e48*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f48*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001708e; Value = 0xfc23ff29; PC = 0x8000b7c *)
mov r4 L0x2001708e;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001718e; Value = 0x004afdd9; PC = 0x8000b80 *)
mov r5 L0x2001718e;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001728e; Value = 0x028dfe06; PC = 0x8000b84 *)
mov r6 L0x2001728e;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001738e; Value = 0x0187fc81; PC = 0x8000b88 *)
mov r7 L0x2001738e;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf095@sint32 : and [cf095 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf223@sint32 : and [cf223 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf351@sint32 : and [cf351 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf479@sint32 : and [cf479 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001710e; Value = 0x0317fe41; PC = 0x8000c9c *)
mov r5 L0x2001710e;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001720e; Value = 0xff38fe08; PC = 0x8000ca0 *)
mov r6 L0x2001720e;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001730e; Value = 0xfce4fff0; PC = 0x8000ca4 *)
mov r7 L0x2001730e;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001700e; Value = 0xfe5e03db; PC = 0x8000ca8 *)
mov r4 L0x2001700e;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf031@sint32 : and [cf031 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf159@sint32 : and [cf159 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf287@sint32 : and [cf287 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf415@sint32 : and [cf415 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x2001984c; PC = 0x8000d54 *)
mov L0x2001984c r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x2001994c; PC = 0x8000d58 *)
mov L0x2001994c r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a4c; PC = 0x8000d5c *)
mov L0x20019a4c r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b4c; PC = 0x8000d60 *)
mov L0x20019b4c r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c4c; PC = 0x8000d64 *)
mov L0x20019c4c r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d4c; PC = 0x8000d68 *)
mov L0x20019d4c r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e4c; PC = 0x8000d6c *)
mov L0x20019e4c r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f4c; PC = 0x8000d70 *)
mov L0x20019f4c r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x2001944c; PC = 0x8000d9c *)
mov L0x2001944c r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x2001954c; PC = 0x8000da0 *)
mov L0x2001954c r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x2001964c; PC = 0x8000da4 *)
mov L0x2001964c r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x2001974c; PC = 0x8000da8 *)
mov L0x2001974c r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x2001914c; PC = 0x8000dac *)
mov L0x2001914c r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x2001924c; PC = 0x8000db0 *)
mov L0x2001924c r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x2001934c; PC = 0x8000db4 *)
mov L0x2001934c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x2001904c; PC = 0x8000db8 *)
mov L0x2001904c r8;



(**************** CUT  31, - *****************)

ecut and [
eqmod cf031 f031 2**11, eqmod cf095 f095 2**11, eqmod cf159 f159 2**11,
eqmod cf223 f223 2**11, eqmod cf287 f287 2**11, eqmod cf351 f351 2**11,
eqmod cf415 f415 2**11, eqmod cf479 f479 2**11,
eqmod L0x2001904c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -       1 ],
eqmod L0x2001914c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x2001924c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  554923 ],
eqmod L0x2001934c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  489046 ],
eqmod L0x2001944c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  287998 ],
eqmod L0x2001954c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  755971 ],
eqmod L0x2001964c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  719789 ],
eqmod L0x2001974c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  324180 ],
eqmod L0x2001984c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -   29512 ],
eqmod L0x2001994c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a4c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b4c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c4c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d4c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e4c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f4c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017090; Value = 0x01b0fc23; PC = 0x8000b7c *)
mov r4 L0x20017090;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017190; Value = 0x03af004a; PC = 0x8000b80 *)
mov r5 L0x20017190;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017290; Value = 0xff1d028d; PC = 0x8000b84 *)
mov r6 L0x20017290;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017390; Value = 0xfc8b0187; PC = 0x8000b88 *)
mov r7 L0x20017390;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf096@sint32 : and [cf096 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf224@sint32 : and [cf224 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf352@sint32 : and [cf352 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf480@sint32 : and [cf480 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017110; Value = 0xff180317; PC = 0x8000c9c *)
mov r5 L0x20017110;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017210; Value = 0x0184ff38; PC = 0x8000ca0 *)
mov r6 L0x20017210;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017310; Value = 0x0138fce4; PC = 0x8000ca4 *)
mov r7 L0x20017310;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017010; Value = 0xfc20fe5e; PC = 0x8000ca8 *)
mov r4 L0x20017010;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf032@sint32 : and [cf032 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf160@sint32 : and [cf160 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf288@sint32 : and [cf288 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf416@sint32 : and [cf416 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019850; PC = 0x8000d54 *)
mov L0x20019850 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019950; PC = 0x8000d58 *)
mov L0x20019950 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a50; PC = 0x8000d5c *)
mov L0x20019a50 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b50; PC = 0x8000d60 *)
mov L0x20019b50 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c50; PC = 0x8000d64 *)
mov L0x20019c50 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d50; PC = 0x8000d68 *)
mov L0x20019d50 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e50; PC = 0x8000d6c *)
mov L0x20019e50 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f50; PC = 0x8000d70 *)
mov L0x20019f50 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019450; PC = 0x8000d9c *)
mov L0x20019450 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019550; PC = 0x8000da0 *)
mov L0x20019550 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019650; PC = 0x8000da4 *)
mov L0x20019650 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019750; PC = 0x8000da8 *)
mov L0x20019750 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019150; PC = 0x8000dac *)
mov L0x20019150 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019250; PC = 0x8000db0 *)
mov L0x20019250 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019350; PC = 0x8000db4 *)
mov L0x20019350 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019050; PC = 0x8000db8 *)
mov L0x20019050 r8;



(**************** CUT  32, - *****************)

ecut and [
eqmod cf032 f032 2**11, eqmod cf096 f096 2**11, eqmod cf160 f160 2**11,
eqmod cf224 f224 2**11, eqmod cf288 f288 2**11, eqmod cf352 f352 2**11,
eqmod cf416 f416 2**11, eqmod cf480 f480 2**11,
eqmod L0x20019050*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019150*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019250*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019350*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019450*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019550*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019650*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019750*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019850*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019950*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a50*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b50*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c50*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d50*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e50*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f50*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017092; Value = 0xfe5b01b0; PC = 0x8000b7c *)
mov r4 L0x20017092;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017192; Value = 0x016103af; PC = 0x8000b80 *)
mov r5 L0x20017192;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017292; Value = 0xffcdff1d; PC = 0x8000b84 *)
mov r6 L0x20017292;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017392; Value = 0xff2bfc8b; PC = 0x8000b88 *)
mov r7 L0x20017392;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf097@sint32 : and [cf097 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf225@sint32 : and [cf225 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf353@sint32 : and [cf353 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf481@sint32 : and [cf481 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017112; Value = 0xfd72ff18; PC = 0x8000c9c *)
mov r5 L0x20017112;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017212; Value = 0xffd60184; PC = 0x8000ca0 *)
mov r6 L0x20017212;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017312; Value = 0x004d0138; PC = 0x8000ca4 *)
mov r7 L0x20017312;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017012; Value = 0x00fbfc20; PC = 0x8000ca8 *)
mov r4 L0x20017012;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf033@sint32 : and [cf033 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf161@sint32 : and [cf161 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf289@sint32 : and [cf289 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf417@sint32 : and [cf417 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019854; PC = 0x8000d54 *)
mov L0x20019854 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019954; PC = 0x8000d58 *)
mov L0x20019954 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a54; PC = 0x8000d5c *)
mov L0x20019a54 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b54; PC = 0x8000d60 *)
mov L0x20019b54 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c54; PC = 0x8000d64 *)
mov L0x20019c54 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d54; PC = 0x8000d68 *)
mov L0x20019d54 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e54; PC = 0x8000d6c *)
mov L0x20019e54 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f54; PC = 0x8000d70 *)
mov L0x20019f54 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019454; PC = 0x8000d9c *)
mov L0x20019454 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019554; PC = 0x8000da0 *)
mov L0x20019554 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019654; PC = 0x8000da4 *)
mov L0x20019654 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019754; PC = 0x8000da8 *)
mov L0x20019754 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019154; PC = 0x8000dac *)
mov L0x20019154 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019254; PC = 0x8000db0 *)
mov L0x20019254 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019354; PC = 0x8000db4 *)
mov L0x20019354 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019054; PC = 0x8000db8 *)
mov L0x20019054 r8;



(**************** CUT  33, - *****************)

ecut and [
eqmod cf033 f033 2**11, eqmod cf097 f097 2**11, eqmod cf161 f161 2**11,
eqmod cf225 f225 2**11, eqmod cf289 f289 2**11, eqmod cf353 f353 2**11,
eqmod cf417 f417 2**11, eqmod cf481 f481 2**11,
eqmod L0x20019054*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019154*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019254*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019354*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019454*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019554*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019654*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019754*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019854*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019954*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a54*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b54*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c54*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d54*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e54*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f54*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017094; Value = 0x011bfe5b; PC = 0x8000b7c *)
mov r4 L0x20017094;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017194; Value = 0xfc170161; PC = 0x8000b80 *)
mov r5 L0x20017194;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017294; Value = 0x03edffcd; PC = 0x8000b84 *)
mov r6 L0x20017294;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017394; Value = 0xff5aff2b; PC = 0x8000b88 *)
mov r7 L0x20017394;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf098@sint32 : and [cf098 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf226@sint32 : and [cf226 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf354@sint32 : and [cf354 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf482@sint32 : and [cf482 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017114; Value = 0x02ebfd72; PC = 0x8000c9c *)
mov r5 L0x20017114;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017214; Value = 0xfd0affd6; PC = 0x8000ca0 *)
mov r6 L0x20017214;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017314; Value = 0xfc66004d; PC = 0x8000ca4 *)
mov r7 L0x20017314;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017014; Value = 0xfc3800fb; PC = 0x8000ca8 *)
mov r4 L0x20017014;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf034@sint32 : and [cf034 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf162@sint32 : and [cf162 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf290@sint32 : and [cf290 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf418@sint32 : and [cf418 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019858; PC = 0x8000d54 *)
mov L0x20019858 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019958; PC = 0x8000d58 *)
mov L0x20019958 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a58; PC = 0x8000d5c *)
mov L0x20019a58 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b58; PC = 0x8000d60 *)
mov L0x20019b58 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c58; PC = 0x8000d64 *)
mov L0x20019c58 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d58; PC = 0x8000d68 *)
mov L0x20019d58 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e58; PC = 0x8000d6c *)
mov L0x20019e58 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f58; PC = 0x8000d70 *)
mov L0x20019f58 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019458; PC = 0x8000d9c *)
mov L0x20019458 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019558; PC = 0x8000da0 *)
mov L0x20019558 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019658; PC = 0x8000da4 *)
mov L0x20019658 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019758; PC = 0x8000da8 *)
mov L0x20019758 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019158; PC = 0x8000dac *)
mov L0x20019158 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019258; PC = 0x8000db0 *)
mov L0x20019258 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019358; PC = 0x8000db4 *)
mov L0x20019358 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019058; PC = 0x8000db8 *)
mov L0x20019058 r8;



(**************** CUT  34, - *****************)

ecut and [
eqmod cf034 f034 2**11, eqmod cf098 f098 2**11, eqmod cf162 f162 2**11,
eqmod cf226 f226 2**11, eqmod cf290 f290 2**11, eqmod cf354 f354 2**11,
eqmod cf418 f418 2**11, eqmod cf482 f482 2**11,
eqmod L0x20019058*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019158*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019258*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019358*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019458*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019558*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019658*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019758*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019858*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019958*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a58*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b58*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c58*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d58*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e58*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f58*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017096; Value = 0xfda3011b; PC = 0x8000b7c *)
mov r4 L0x20017096;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017196; Value = 0xff3bfc17; PC = 0x8000b80 *)
mov r5 L0x20017196;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017296; Value = 0x01c903ed; PC = 0x8000b84 *)
mov r6 L0x20017296;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017396; Value = 0x032fff5a; PC = 0x8000b88 *)
mov r7 L0x20017396;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf099@sint32 : and [cf099 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf227@sint32 : and [cf227 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf355@sint32 : and [cf355 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf483@sint32 : and [cf483 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017116; Value = 0x020602eb; PC = 0x8000c9c *)
mov r5 L0x20017116;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017216; Value = 0xfd87fd0a; PC = 0x8000ca0 *)
mov r6 L0x20017216;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017316; Value = 0x023afc66; PC = 0x8000ca4 *)
mov r7 L0x20017316;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017016; Value = 0x01a8fc38; PC = 0x8000ca8 *)
mov r4 L0x20017016;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf035@sint32 : and [cf035 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf163@sint32 : and [cf163 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf291@sint32 : and [cf291 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf419@sint32 : and [cf419 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x2001985c; PC = 0x8000d54 *)
mov L0x2001985c r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x2001995c; PC = 0x8000d58 *)
mov L0x2001995c r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a5c; PC = 0x8000d5c *)
mov L0x20019a5c r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b5c; PC = 0x8000d60 *)
mov L0x20019b5c r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c5c; PC = 0x8000d64 *)
mov L0x20019c5c r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d5c; PC = 0x8000d68 *)
mov L0x20019d5c r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e5c; PC = 0x8000d6c *)
mov L0x20019e5c r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f5c; PC = 0x8000d70 *)
mov L0x20019f5c r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x2001945c; PC = 0x8000d9c *)
mov L0x2001945c r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x2001955c; PC = 0x8000da0 *)
mov L0x2001955c r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x2001965c; PC = 0x8000da4 *)
mov L0x2001965c r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x2001975c; PC = 0x8000da8 *)
mov L0x2001975c r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x2001915c; PC = 0x8000dac *)
mov L0x2001915c r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x2001925c; PC = 0x8000db0 *)
mov L0x2001925c r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x2001935c; PC = 0x8000db4 *)
mov L0x2001935c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x2001905c; PC = 0x8000db8 *)
mov L0x2001905c r8;



(**************** CUT  35, - *****************)

ecut and [
eqmod cf035 f035 2**11, eqmod cf099 f099 2**11, eqmod cf163 f163 2**11,
eqmod cf227 f227 2**11, eqmod cf291 f291 2**11, eqmod cf355 f355 2**11,
eqmod cf419 f419 2**11, eqmod cf483 f483 2**11,
eqmod L0x2001905c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -       1 ],
eqmod L0x2001915c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x2001925c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  554923 ],
eqmod L0x2001935c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  489046 ],
eqmod L0x2001945c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  287998 ],
eqmod L0x2001955c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  755971 ],
eqmod L0x2001965c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  719789 ],
eqmod L0x2001975c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  324180 ],
eqmod L0x2001985c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -   29512 ],
eqmod L0x2001995c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a5c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b5c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c5c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d5c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e5c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f5c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x20017098; Value = 0x032afda3; PC = 0x8000b7c *)
mov r4 L0x20017098;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x20017198; Value = 0xffbdff3b; PC = 0x8000b80 *)
mov r5 L0x20017198;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x20017298; Value = 0x03fa01c9; PC = 0x8000b84 *)
mov r6 L0x20017298;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x20017398; Value = 0xfe26032f; PC = 0x8000b88 *)
mov r7 L0x20017398;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf100@sint32 : and [cf100 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf228@sint32 : and [cf228 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf356@sint32 : and [cf356 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf484@sint32 : and [cf484 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017118; Value = 0xfff20206; PC = 0x8000c9c *)
mov r5 L0x20017118;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017218; Value = 0xfe55fd87; PC = 0x8000ca0 *)
mov r6 L0x20017218;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017318; Value = 0x031e023a; PC = 0x8000ca4 *)
mov r7 L0x20017318;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017018; Value = 0xfc4e01a8; PC = 0x8000ca8 *)
mov r4 L0x20017018;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf036@sint32 : and [cf036 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf164@sint32 : and [cf164 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf292@sint32 : and [cf292 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf420@sint32 : and [cf420 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019860; PC = 0x8000d54 *)
mov L0x20019860 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019960; PC = 0x8000d58 *)
mov L0x20019960 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a60; PC = 0x8000d5c *)
mov L0x20019a60 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b60; PC = 0x8000d60 *)
mov L0x20019b60 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c60; PC = 0x8000d64 *)
mov L0x20019c60 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d60; PC = 0x8000d68 *)
mov L0x20019d60 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e60; PC = 0x8000d6c *)
mov L0x20019e60 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f60; PC = 0x8000d70 *)
mov L0x20019f60 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019460; PC = 0x8000d9c *)
mov L0x20019460 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019560; PC = 0x8000da0 *)
mov L0x20019560 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019660; PC = 0x8000da4 *)
mov L0x20019660 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019760; PC = 0x8000da8 *)
mov L0x20019760 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019160; PC = 0x8000dac *)
mov L0x20019160 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019260; PC = 0x8000db0 *)
mov L0x20019260 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019360; PC = 0x8000db4 *)
mov L0x20019360 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019060; PC = 0x8000db8 *)
mov L0x20019060 r8;



(**************** CUT  36, - *****************)

ecut and [
eqmod cf036 f036 2**11, eqmod cf100 f100 2**11, eqmod cf164 f164 2**11,
eqmod cf228 f228 2**11, eqmod cf292 f292 2**11, eqmod cf356 f356 2**11,
eqmod cf420 f420 2**11, eqmod cf484 f484 2**11,
eqmod L0x20019060*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019160*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019260*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019360*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019460*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019560*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019660*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019760*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019860*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019960*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a60*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b60*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c60*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d60*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e60*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f60*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001709a; Value = 0x0282032a; PC = 0x8000b7c *)
mov r4 L0x2001709a;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001719a; Value = 0xfe6affbd; PC = 0x8000b80 *)
mov r5 L0x2001719a;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001729a; Value = 0x03c903fa; PC = 0x8000b84 *)
mov r6 L0x2001729a;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001739a; Value = 0xfdbefe26; PC = 0x8000b88 *)
mov r7 L0x2001739a;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf101@sint32 : and [cf101 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf229@sint32 : and [cf229 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf357@sint32 : and [cf357 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf485@sint32 : and [cf485 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001711a; Value = 0x0391fff2; PC = 0x8000c9c *)
mov r5 L0x2001711a;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001721a; Value = 0xfc7afe55; PC = 0x8000ca0 *)
mov r6 L0x2001721a;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001731a; Value = 0x01b7031e; PC = 0x8000ca4 *)
mov r7 L0x2001731a;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001701a; Value = 0x03a6fc4e; PC = 0x8000ca8 *)
mov r4 L0x2001701a;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf037@sint32 : and [cf037 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf165@sint32 : and [cf165 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf293@sint32 : and [cf293 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf421@sint32 : and [cf421 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019864; PC = 0x8000d54 *)
mov L0x20019864 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019964; PC = 0x8000d58 *)
mov L0x20019964 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a64; PC = 0x8000d5c *)
mov L0x20019a64 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b64; PC = 0x8000d60 *)
mov L0x20019b64 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c64; PC = 0x8000d64 *)
mov L0x20019c64 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d64; PC = 0x8000d68 *)
mov L0x20019d64 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e64; PC = 0x8000d6c *)
mov L0x20019e64 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f64; PC = 0x8000d70 *)
mov L0x20019f64 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019464; PC = 0x8000d9c *)
mov L0x20019464 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019564; PC = 0x8000da0 *)
mov L0x20019564 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019664; PC = 0x8000da4 *)
mov L0x20019664 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019764; PC = 0x8000da8 *)
mov L0x20019764 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019164; PC = 0x8000dac *)
mov L0x20019164 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019264; PC = 0x8000db0 *)
mov L0x20019264 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019364; PC = 0x8000db4 *)
mov L0x20019364 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019064; PC = 0x8000db8 *)
mov L0x20019064 r8;



(**************** CUT  37, - *****************)

ecut and [
eqmod cf037 f037 2**11, eqmod cf101 f101 2**11, eqmod cf165 f165 2**11,
eqmod cf229 f229 2**11, eqmod cf293 f293 2**11, eqmod cf357 f357 2**11,
eqmod cf421 f421 2**11, eqmod cf485 f485 2**11,
eqmod L0x20019064*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019164*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019264*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019364*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019464*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019564*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019664*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019764*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019864*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019964*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a64*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b64*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c64*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d64*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e64*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f64*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001709c; Value = 0x00880282; PC = 0x8000b7c *)
mov r4 L0x2001709c;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001719c; Value = 0xfcf5fe6a; PC = 0x8000b80 *)
mov r5 L0x2001719c;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001729c; Value = 0x021803c9; PC = 0x8000b84 *)
mov r6 L0x2001729c;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001739c; Value = 0xffdefdbe; PC = 0x8000b88 *)
mov r7 L0x2001739c;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf102@sint32 : and [cf102 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf230@sint32 : and [cf230 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf358@sint32 : and [cf358 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf486@sint32 : and [cf486 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001711c; Value = 0x00ee0391; PC = 0x8000c9c *)
mov r5 L0x2001711c;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001721c; Value = 0xffa3fc7a; PC = 0x8000ca0 *)
mov r6 L0x2001721c;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001731c; Value = 0xff5001b7; PC = 0x8000ca4 *)
mov r7 L0x2001731c;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001701c; Value = 0x031403a6; PC = 0x8000ca8 *)
mov r4 L0x2001701c;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf038@sint32 : and [cf038 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf166@sint32 : and [cf166 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf294@sint32 : and [cf294 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf422@sint32 : and [cf422 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019868; PC = 0x8000d54 *)
mov L0x20019868 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019968; PC = 0x8000d58 *)
mov L0x20019968 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a68; PC = 0x8000d5c *)
mov L0x20019a68 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b68; PC = 0x8000d60 *)
mov L0x20019b68 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c68; PC = 0x8000d64 *)
mov L0x20019c68 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d68; PC = 0x8000d68 *)
mov L0x20019d68 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e68; PC = 0x8000d6c *)
mov L0x20019e68 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f68; PC = 0x8000d70 *)
mov L0x20019f68 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019468; PC = 0x8000d9c *)
mov L0x20019468 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019568; PC = 0x8000da0 *)
mov L0x20019568 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019668; PC = 0x8000da4 *)
mov L0x20019668 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019768; PC = 0x8000da8 *)
mov L0x20019768 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019168; PC = 0x8000dac *)
mov L0x20019168 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019268; PC = 0x8000db0 *)
mov L0x20019268 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019368; PC = 0x8000db4 *)
mov L0x20019368 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019068; PC = 0x8000db8 *)
mov L0x20019068 r8;



(**************** CUT  38, - *****************)

ecut and [
eqmod cf038 f038 2**11, eqmod cf102 f102 2**11, eqmod cf166 f166 2**11,
eqmod cf230 f230 2**11, eqmod cf294 f294 2**11, eqmod cf358 f358 2**11,
eqmod cf422 f422 2**11, eqmod cf486 f486 2**11,
eqmod L0x20019068*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019168*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019268*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019368*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019468*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019568*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019668*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019768*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019868*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019968*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a68*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b68*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c68*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d68*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e68*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f68*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x2001709e; Value = 0xff440088; PC = 0x8000b7c *)
mov r4 L0x2001709e;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x2001719e; Value = 0x03d0fcf5; PC = 0x8000b80 *)
mov r5 L0x2001719e;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x2001729e; Value = 0xff570218; PC = 0x8000b84 *)
mov r6 L0x2001729e;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x2001739e; Value = 0xfeecffde; PC = 0x8000b88 *)
mov r7 L0x2001739e;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf103@sint32 : and [cf103 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf231@sint32 : and [cf231 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf359@sint32 : and [cf359 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf487@sint32 : and [cf487 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001711e; Value = 0xfeb800ee; PC = 0x8000c9c *)
mov r5 L0x2001711e;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001721e; Value = 0x0275ffa3; PC = 0x8000ca0 *)
mov r6 L0x2001721e;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001731e; Value = 0x00aaff50; PC = 0x8000ca4 *)
mov r7 L0x2001731e;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001701e; Value = 0x01930314; PC = 0x8000ca8 *)
mov r4 L0x2001701e;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf039@sint32 : and [cf039 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf167@sint32 : and [cf167 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf295@sint32 : and [cf295 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf423@sint32 : and [cf423 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x2001986c; PC = 0x8000d54 *)
mov L0x2001986c r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x2001996c; PC = 0x8000d58 *)
mov L0x2001996c r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a6c; PC = 0x8000d5c *)
mov L0x20019a6c r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b6c; PC = 0x8000d60 *)
mov L0x20019b6c r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c6c; PC = 0x8000d64 *)
mov L0x20019c6c r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d6c; PC = 0x8000d68 *)
mov L0x20019d6c r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e6c; PC = 0x8000d6c *)
mov L0x20019e6c r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f6c; PC = 0x8000d70 *)
mov L0x20019f6c r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x2001946c; PC = 0x8000d9c *)
mov L0x2001946c r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x2001956c; PC = 0x8000da0 *)
mov L0x2001956c r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x2001966c; PC = 0x8000da4 *)
mov L0x2001966c r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x2001976c; PC = 0x8000da8 *)
mov L0x2001976c r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x2001916c; PC = 0x8000dac *)
mov L0x2001916c r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x2001926c; PC = 0x8000db0 *)
mov L0x2001926c r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x2001936c; PC = 0x8000db4 *)
mov L0x2001936c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x2001906c; PC = 0x8000db8 *)
mov L0x2001906c r8;



(**************** CUT  39, - *****************)

ecut and [
eqmod cf039 f039 2**11, eqmod cf103 f103 2**11, eqmod cf167 f167 2**11,
eqmod cf231 f231 2**11, eqmod cf295 f295 2**11, eqmod cf359 f359 2**11,
eqmod cf423 f423 2**11, eqmod cf487 f487 2**11,
eqmod L0x2001906c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -       1 ],
eqmod L0x2001916c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x2001926c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  554923 ],
eqmod L0x2001936c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  489046 ],
eqmod L0x2001946c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  287998 ],
eqmod L0x2001956c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  755971 ],
eqmod L0x2001966c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  719789 ],
eqmod L0x2001976c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  324180 ],
eqmod L0x2001986c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -   29512 ],
eqmod L0x2001996c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a6c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b6c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c6c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d6c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e6c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f6c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170a0; Value = 0x0019ff44; PC = 0x8000b7c *)
mov r4 L0x200170a0;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171a0; Value = 0x014403d0; PC = 0x8000b80 *)
mov r5 L0x200171a0;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172a0; Value = 0xff05ff57; PC = 0x8000b84 *)
mov r6 L0x200172a0;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173a0; Value = 0x02bffeec; PC = 0x8000b88 *)
mov r7 L0x200173a0;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf104@sint32 : and [cf104 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf232@sint32 : and [cf232 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf360@sint32 : and [cf360 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf488@sint32 : and [cf488 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017120; Value = 0xfe6cfeb8; PC = 0x8000c9c *)
mov r5 L0x20017120;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017220; Value = 0xfe350275; PC = 0x8000ca0 *)
mov r6 L0x20017220;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017320; Value = 0x036700aa; PC = 0x8000ca4 *)
mov r7 L0x20017320;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017020; Value = 0x03250193; PC = 0x8000ca8 *)
mov r4 L0x20017020;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf040@sint32 : and [cf040 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf168@sint32 : and [cf168 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf296@sint32 : and [cf296 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf424@sint32 : and [cf424 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019870; PC = 0x8000d54 *)
mov L0x20019870 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019970; PC = 0x8000d58 *)
mov L0x20019970 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a70; PC = 0x8000d5c *)
mov L0x20019a70 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b70; PC = 0x8000d60 *)
mov L0x20019b70 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c70; PC = 0x8000d64 *)
mov L0x20019c70 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d70; PC = 0x8000d68 *)
mov L0x20019d70 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e70; PC = 0x8000d6c *)
mov L0x20019e70 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f70; PC = 0x8000d70 *)
mov L0x20019f70 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019470; PC = 0x8000d9c *)
mov L0x20019470 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019570; PC = 0x8000da0 *)
mov L0x20019570 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019670; PC = 0x8000da4 *)
mov L0x20019670 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019770; PC = 0x8000da8 *)
mov L0x20019770 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019170; PC = 0x8000dac *)
mov L0x20019170 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019270; PC = 0x8000db0 *)
mov L0x20019270 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019370; PC = 0x8000db4 *)
mov L0x20019370 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019070; PC = 0x8000db8 *)
mov L0x20019070 r8;



(**************** CUT  40, - *****************)

ecut and [
eqmod cf040 f040 2**11, eqmod cf104 f104 2**11, eqmod cf168 f168 2**11,
eqmod cf232 f232 2**11, eqmod cf296 f296 2**11, eqmod cf360 f360 2**11,
eqmod cf424 f424 2**11, eqmod cf488 f488 2**11,
eqmod L0x20019070*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019170*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019270*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019370*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019470*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019570*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019670*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019770*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019870*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019970*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a70*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b70*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c70*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d70*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e70*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f70*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170a2; Value = 0xff0e0019; PC = 0x8000b7c *)
mov r4 L0x200170a2;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171a2; Value = 0xff5b0144; PC = 0x8000b80 *)
mov r5 L0x200171a2;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172a2; Value = 0xff67ff05; PC = 0x8000b84 *)
mov r6 L0x200172a2;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173a2; Value = 0xfe3802bf; PC = 0x8000b88 *)
mov r7 L0x200173a2;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf105@sint32 : and [cf105 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf233@sint32 : and [cf233 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf361@sint32 : and [cf361 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf489@sint32 : and [cf489 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017122; Value = 0x02befe6c; PC = 0x8000c9c *)
mov r5 L0x20017122;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017222; Value = 0xfebffe35; PC = 0x8000ca0 *)
mov r6 L0x20017222;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017322; Value = 0x03380367; PC = 0x8000ca4 *)
mov r7 L0x20017322;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017022; Value = 0x00560325; PC = 0x8000ca8 *)
mov r4 L0x20017022;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf041@sint32 : and [cf041 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf169@sint32 : and [cf169 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf297@sint32 : and [cf297 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf425@sint32 : and [cf425 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019874; PC = 0x8000d54 *)
mov L0x20019874 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019974; PC = 0x8000d58 *)
mov L0x20019974 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a74; PC = 0x8000d5c *)
mov L0x20019a74 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b74; PC = 0x8000d60 *)
mov L0x20019b74 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c74; PC = 0x8000d64 *)
mov L0x20019c74 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d74; PC = 0x8000d68 *)
mov L0x20019d74 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e74; PC = 0x8000d6c *)
mov L0x20019e74 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f74; PC = 0x8000d70 *)
mov L0x20019f74 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019474; PC = 0x8000d9c *)
mov L0x20019474 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019574; PC = 0x8000da0 *)
mov L0x20019574 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019674; PC = 0x8000da4 *)
mov L0x20019674 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019774; PC = 0x8000da8 *)
mov L0x20019774 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019174; PC = 0x8000dac *)
mov L0x20019174 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019274; PC = 0x8000db0 *)
mov L0x20019274 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019374; PC = 0x8000db4 *)
mov L0x20019374 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019074; PC = 0x8000db8 *)
mov L0x20019074 r8;



(**************** CUT  41, - *****************)

ecut and [
eqmod cf041 f041 2**11, eqmod cf105 f105 2**11, eqmod cf169 f169 2**11,
eqmod cf233 f233 2**11, eqmod cf297 f297 2**11, eqmod cf361 f361 2**11,
eqmod cf425 f425 2**11, eqmod cf489 f489 2**11,
eqmod L0x20019074*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019174*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019274*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019374*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019474*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019574*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019674*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019774*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019874*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019974*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a74*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b74*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c74*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d74*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e74*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f74*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170a4; Value = 0x0230ff0e; PC = 0x8000b7c *)
mov r4 L0x200170a4;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171a4; Value = 0xfd91ff5b; PC = 0x8000b80 *)
mov r5 L0x200171a4;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172a4; Value = 0x00fdff67; PC = 0x8000b84 *)
mov r6 L0x200172a4;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173a4; Value = 0xfcadfe38; PC = 0x8000b88 *)
mov r7 L0x200173a4;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf106@sint32 : and [cf106 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf234@sint32 : and [cf234 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf362@sint32 : and [cf362 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf490@sint32 : and [cf490 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017124; Value = 0xfdd902be; PC = 0x8000c9c *)
mov r5 L0x20017124;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017224; Value = 0xfe3efebf; PC = 0x8000ca0 *)
mov r6 L0x20017224;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017324; Value = 0x02e00338; PC = 0x8000ca4 *)
mov r7 L0x20017324;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017024; Value = 0x02240056; PC = 0x8000ca8 *)
mov r4 L0x20017024;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf042@sint32 : and [cf042 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf170@sint32 : and [cf170 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf298@sint32 : and [cf298 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf426@sint32 : and [cf426 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019878; PC = 0x8000d54 *)
mov L0x20019878 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019978; PC = 0x8000d58 *)
mov L0x20019978 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a78; PC = 0x8000d5c *)
mov L0x20019a78 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b78; PC = 0x8000d60 *)
mov L0x20019b78 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c78; PC = 0x8000d64 *)
mov L0x20019c78 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d78; PC = 0x8000d68 *)
mov L0x20019d78 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e78; PC = 0x8000d6c *)
mov L0x20019e78 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f78; PC = 0x8000d70 *)
mov L0x20019f78 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019478; PC = 0x8000d9c *)
mov L0x20019478 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019578; PC = 0x8000da0 *)
mov L0x20019578 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019678; PC = 0x8000da4 *)
mov L0x20019678 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019778; PC = 0x8000da8 *)
mov L0x20019778 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019178; PC = 0x8000dac *)
mov L0x20019178 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019278; PC = 0x8000db0 *)
mov L0x20019278 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019378; PC = 0x8000db4 *)
mov L0x20019378 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019078; PC = 0x8000db8 *)
mov L0x20019078 r8;



(**************** CUT  42, - *****************)

ecut and [
eqmod cf042 f042 2**11, eqmod cf106 f106 2**11, eqmod cf170 f170 2**11,
eqmod cf234 f234 2**11, eqmod cf298 f298 2**11, eqmod cf362 f362 2**11,
eqmod cf426 f426 2**11, eqmod cf490 f490 2**11,
eqmod L0x20019078*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019178*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019278*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019378*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019478*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019578*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019678*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019778*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019878*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019978*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a78*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b78*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c78*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d78*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e78*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f78*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170a6; Value = 0xfdf20230; PC = 0x8000b7c *)
mov r4 L0x200170a6;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171a6; Value = 0xfdcdfd91; PC = 0x8000b80 *)
mov r5 L0x200171a6;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172a6; Value = 0xfd8900fd; PC = 0x8000b84 *)
mov r6 L0x200172a6;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173a6; Value = 0x03b0fcad; PC = 0x8000b88 *)
mov r7 L0x200173a6;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf107@sint32 : and [cf107 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf235@sint32 : and [cf235 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf363@sint32 : and [cf363 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf491@sint32 : and [cf491 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017126; Value = 0xff34fdd9; PC = 0x8000c9c *)
mov r5 L0x20017126;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017226; Value = 0xfc71fe3e; PC = 0x8000ca0 *)
mov r6 L0x20017226;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017326; Value = 0xffee02e0; PC = 0x8000ca4 *)
mov r7 L0x20017326;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017026; Value = 0x02440224; PC = 0x8000ca8 *)
mov r4 L0x20017026;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf043@sint32 : and [cf043 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf171@sint32 : and [cf171 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf299@sint32 : and [cf299 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf427@sint32 : and [cf427 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x2001987c; PC = 0x8000d54 *)
mov L0x2001987c r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x2001997c; PC = 0x8000d58 *)
mov L0x2001997c r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a7c; PC = 0x8000d5c *)
mov L0x20019a7c r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b7c; PC = 0x8000d60 *)
mov L0x20019b7c r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c7c; PC = 0x8000d64 *)
mov L0x20019c7c r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d7c; PC = 0x8000d68 *)
mov L0x20019d7c r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e7c; PC = 0x8000d6c *)
mov L0x20019e7c r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f7c; PC = 0x8000d70 *)
mov L0x20019f7c r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x2001947c; PC = 0x8000d9c *)
mov L0x2001947c r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x2001957c; PC = 0x8000da0 *)
mov L0x2001957c r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x2001967c; PC = 0x8000da4 *)
mov L0x2001967c r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x2001977c; PC = 0x8000da8 *)
mov L0x2001977c r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x2001917c; PC = 0x8000dac *)
mov L0x2001917c r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x2001927c; PC = 0x8000db0 *)
mov L0x2001927c r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x2001937c; PC = 0x8000db4 *)
mov L0x2001937c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x2001907c; PC = 0x8000db8 *)
mov L0x2001907c r8;



(**************** CUT  43, - *****************)

ecut and [
eqmod cf043 f043 2**11, eqmod cf107 f107 2**11, eqmod cf171 f171 2**11,
eqmod cf235 f235 2**11, eqmod cf299 f299 2**11, eqmod cf363 f363 2**11,
eqmod cf427 f427 2**11, eqmod cf491 f491 2**11,
eqmod L0x2001907c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -       1 ],
eqmod L0x2001917c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x2001927c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  554923 ],
eqmod L0x2001937c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  489046 ],
eqmod L0x2001947c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  287998 ],
eqmod L0x2001957c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  755971 ],
eqmod L0x2001967c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  719789 ],
eqmod L0x2001977c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  324180 ],
eqmod L0x2001987c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -   29512 ],
eqmod L0x2001997c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a7c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b7c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c7c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d7c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e7c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f7c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170a8; Value = 0x016bfdf2; PC = 0x8000b7c *)
mov r4 L0x200170a8;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171a8; Value = 0x035ffdcd; PC = 0x8000b80 *)
mov r5 L0x200171a8;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172a8; Value = 0x0394fd89; PC = 0x8000b84 *)
mov r6 L0x200172a8;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173a8; Value = 0xfedf03b0; PC = 0x8000b88 *)
mov r7 L0x200173a8;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf108@sint32 : and [cf108 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf236@sint32 : and [cf236 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf364@sint32 : and [cf364 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf492@sint32 : and [cf492 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017128; Value = 0x01f7ff34; PC = 0x8000c9c *)
mov r5 L0x20017128;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017228; Value = 0x026cfc71; PC = 0x8000ca0 *)
mov r6 L0x20017228;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017328; Value = 0x00b6ffee; PC = 0x8000ca4 *)
mov r7 L0x20017328;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017028; Value = 0xfedf0244; PC = 0x8000ca8 *)
mov r4 L0x20017028;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf044@sint32 : and [cf044 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf172@sint32 : and [cf172 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf300@sint32 : and [cf300 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf428@sint32 : and [cf428 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019880; PC = 0x8000d54 *)
mov L0x20019880 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019980; PC = 0x8000d58 *)
mov L0x20019980 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a80; PC = 0x8000d5c *)
mov L0x20019a80 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b80; PC = 0x8000d60 *)
mov L0x20019b80 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c80; PC = 0x8000d64 *)
mov L0x20019c80 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d80; PC = 0x8000d68 *)
mov L0x20019d80 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e80; PC = 0x8000d6c *)
mov L0x20019e80 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f80; PC = 0x8000d70 *)
mov L0x20019f80 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019480; PC = 0x8000d9c *)
mov L0x20019480 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019580; PC = 0x8000da0 *)
mov L0x20019580 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019680; PC = 0x8000da4 *)
mov L0x20019680 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019780; PC = 0x8000da8 *)
mov L0x20019780 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019180; PC = 0x8000dac *)
mov L0x20019180 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019280; PC = 0x8000db0 *)
mov L0x20019280 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019380; PC = 0x8000db4 *)
mov L0x20019380 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019080; PC = 0x8000db8 *)
mov L0x20019080 r8;



(**************** CUT  44, - *****************)

ecut and [
eqmod cf044 f044 2**11, eqmod cf108 f108 2**11, eqmod cf172 f172 2**11,
eqmod cf236 f236 2**11, eqmod cf300 f300 2**11, eqmod cf364 f364 2**11,
eqmod cf428 f428 2**11, eqmod cf492 f492 2**11,
eqmod L0x20019080*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019180*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019280*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019380*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019480*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019580*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019680*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019780*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019880*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019980*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a80*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b80*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c80*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d80*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e80*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f80*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170aa; Value = 0xfea3016b; PC = 0x8000b7c *)
mov r4 L0x200170aa;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171aa; Value = 0x0175035f; PC = 0x8000b80 *)
mov r5 L0x200171aa;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172aa; Value = 0x00ea0394; PC = 0x8000b84 *)
mov r6 L0x200172aa;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173aa; Value = 0xffa3fedf; PC = 0x8000b88 *)
mov r7 L0x200173aa;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf109@sint32 : and [cf109 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf237@sint32 : and [cf237 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf365@sint32 : and [cf365 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf493@sint32 : and [cf493 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001712a; Value = 0xfe3401f7; PC = 0x8000c9c *)
mov r5 L0x2001712a;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001722a; Value = 0xfc84026c; PC = 0x8000ca0 *)
mov r6 L0x2001722a;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001732a; Value = 0x006600b6; PC = 0x8000ca4 *)
mov r7 L0x2001732a;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001702a; Value = 0x0159fedf; PC = 0x8000ca8 *)
mov r4 L0x2001702a;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf045@sint32 : and [cf045 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf173@sint32 : and [cf173 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf301@sint32 : and [cf301 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf429@sint32 : and [cf429 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019884; PC = 0x8000d54 *)
mov L0x20019884 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019984; PC = 0x8000d58 *)
mov L0x20019984 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a84; PC = 0x8000d5c *)
mov L0x20019a84 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b84; PC = 0x8000d60 *)
mov L0x20019b84 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c84; PC = 0x8000d64 *)
mov L0x20019c84 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d84; PC = 0x8000d68 *)
mov L0x20019d84 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e84; PC = 0x8000d6c *)
mov L0x20019e84 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f84; PC = 0x8000d70 *)
mov L0x20019f84 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019484; PC = 0x8000d9c *)
mov L0x20019484 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019584; PC = 0x8000da0 *)
mov L0x20019584 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019684; PC = 0x8000da4 *)
mov L0x20019684 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019784; PC = 0x8000da8 *)
mov L0x20019784 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019184; PC = 0x8000dac *)
mov L0x20019184 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019284; PC = 0x8000db0 *)
mov L0x20019284 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019384; PC = 0x8000db4 *)
mov L0x20019384 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019084; PC = 0x8000db8 *)
mov L0x20019084 r8;



(**************** CUT  45, - *****************)

ecut and [
eqmod cf045 f045 2**11, eqmod cf109 f109 2**11, eqmod cf173 f173 2**11,
eqmod cf237 f237 2**11, eqmod cf301 f301 2**11, eqmod cf365 f365 2**11,
eqmod cf429 f429 2**11, eqmod cf493 f493 2**11,
eqmod L0x20019084*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019184*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019284*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019384*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019484*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019584*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019684*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019784*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019884*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019984*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a84*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b84*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c84*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d84*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e84*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f84*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170ac; Value = 0xfcaefea3; PC = 0x8000b7c *)
mov r4 L0x200170ac;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171ac; Value = 0xff5f0175; PC = 0x8000b80 *)
mov r5 L0x200171ac;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172ac; Value = 0x005300ea; PC = 0x8000b84 *)
mov r6 L0x200172ac;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173ac; Value = 0xfd0dffa3; PC = 0x8000b88 *)
mov r7 L0x200173ac;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf110@sint32 : and [cf110 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf238@sint32 : and [cf238 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf366@sint32 : and [cf366 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf494@sint32 : and [cf494 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001712c; Value = 0x0323fe34; PC = 0x8000c9c *)
mov r5 L0x2001712c;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001722c; Value = 0xfc27fc84; PC = 0x8000ca0 *)
mov r6 L0x2001722c;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001732c; Value = 0x028c0066; PC = 0x8000ca4 *)
mov r7 L0x2001732c;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001702c; Value = 0x018d0159; PC = 0x8000ca8 *)
mov r4 L0x2001702c;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf046@sint32 : and [cf046 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf174@sint32 : and [cf174 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf302@sint32 : and [cf302 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf430@sint32 : and [cf430 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019888; PC = 0x8000d54 *)
mov L0x20019888 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019988; PC = 0x8000d58 *)
mov L0x20019988 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a88; PC = 0x8000d5c *)
mov L0x20019a88 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b88; PC = 0x8000d60 *)
mov L0x20019b88 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c88; PC = 0x8000d64 *)
mov L0x20019c88 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d88; PC = 0x8000d68 *)
mov L0x20019d88 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e88; PC = 0x8000d6c *)
mov L0x20019e88 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f88; PC = 0x8000d70 *)
mov L0x20019f88 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019488; PC = 0x8000d9c *)
mov L0x20019488 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019588; PC = 0x8000da0 *)
mov L0x20019588 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019688; PC = 0x8000da4 *)
mov L0x20019688 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019788; PC = 0x8000da8 *)
mov L0x20019788 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019188; PC = 0x8000dac *)
mov L0x20019188 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019288; PC = 0x8000db0 *)
mov L0x20019288 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019388; PC = 0x8000db4 *)
mov L0x20019388 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019088; PC = 0x8000db8 *)
mov L0x20019088 r8;



(**************** CUT  46, - *****************)

ecut and [
eqmod cf046 f046 2**11, eqmod cf110 f110 2**11, eqmod cf174 f174 2**11,
eqmod cf238 f238 2**11, eqmod cf302 f302 2**11, eqmod cf366 f366 2**11,
eqmod cf430 f430 2**11, eqmod cf494 f494 2**11,
eqmod L0x20019088*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019188*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019288*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019388*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019488*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019588*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019688*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019788*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019888*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019988*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a88*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b88*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c88*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d88*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e88*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f88*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170ae; Value = 0xfc0ffcae; PC = 0x8000b7c *)
mov r4 L0x200170ae;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171ae; Value = 0x0341ff5f; PC = 0x8000b80 *)
mov r5 L0x200171ae;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172ae; Value = 0x03bd0053; PC = 0x8000b84 *)
mov r6 L0x200172ae;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173ae; Value = 0x0029fd0d; PC = 0x8000b88 *)
mov r7 L0x200173ae;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf111@sint32 : and [cf111 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf239@sint32 : and [cf239 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf367@sint32 : and [cf367 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf495@sint32 : and [cf495 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001712e; Value = 0xff060323; PC = 0x8000c9c *)
mov r5 L0x2001712e;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001722e; Value = 0xfe77fc27; PC = 0x8000ca0 *)
mov r6 L0x2001722e;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001732e; Value = 0xfc47028c; PC = 0x8000ca4 *)
mov r7 L0x2001732e;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001702e; Value = 0x0143018d; PC = 0x8000ca8 *)
mov r4 L0x2001702e;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf047@sint32 : and [cf047 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf175@sint32 : and [cf175 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf303@sint32 : and [cf303 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf431@sint32 : and [cf431 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x2001988c; PC = 0x8000d54 *)
mov L0x2001988c r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x2001998c; PC = 0x8000d58 *)
mov L0x2001998c r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a8c; PC = 0x8000d5c *)
mov L0x20019a8c r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b8c; PC = 0x8000d60 *)
mov L0x20019b8c r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c8c; PC = 0x8000d64 *)
mov L0x20019c8c r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d8c; PC = 0x8000d68 *)
mov L0x20019d8c r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e8c; PC = 0x8000d6c *)
mov L0x20019e8c r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f8c; PC = 0x8000d70 *)
mov L0x20019f8c r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x2001948c; PC = 0x8000d9c *)
mov L0x2001948c r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x2001958c; PC = 0x8000da0 *)
mov L0x2001958c r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x2001968c; PC = 0x8000da4 *)
mov L0x2001968c r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x2001978c; PC = 0x8000da8 *)
mov L0x2001978c r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x2001918c; PC = 0x8000dac *)
mov L0x2001918c r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x2001928c; PC = 0x8000db0 *)
mov L0x2001928c r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x2001938c; PC = 0x8000db4 *)
mov L0x2001938c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x2001908c; PC = 0x8000db8 *)
mov L0x2001908c r8;



(**************** CUT  47, - *****************)

ecut and [
eqmod cf047 f047 2**11, eqmod cf111 f111 2**11, eqmod cf175 f175 2**11,
eqmod cf239 f239 2**11, eqmod cf303 f303 2**11, eqmod cf367 f367 2**11,
eqmod cf431 f431 2**11, eqmod cf495 f495 2**11,
eqmod L0x2001908c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -       1 ],
eqmod L0x2001918c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x2001928c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  554923 ],
eqmod L0x2001938c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  489046 ],
eqmod L0x2001948c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  287998 ],
eqmod L0x2001958c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  755971 ],
eqmod L0x2001968c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  719789 ],
eqmod L0x2001978c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  324180 ],
eqmod L0x2001988c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -   29512 ],
eqmod L0x2001998c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a8c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b8c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c8c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d8c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e8c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f8c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170b0; Value = 0xfe84fc0f; PC = 0x8000b7c *)
mov r4 L0x200170b0;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171b0; Value = 0xffb80341; PC = 0x8000b80 *)
mov r5 L0x200171b0;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172b0; Value = 0x02ff03bd; PC = 0x8000b84 *)
mov r6 L0x200172b0;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173b0; Value = 0x01c80029; PC = 0x8000b88 *)
mov r7 L0x200173b0;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf112@sint32 : and [cf112 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf240@sint32 : and [cf240 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf368@sint32 : and [cf368 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf496@sint32 : and [cf496 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017130; Value = 0x017aff06; PC = 0x8000c9c *)
mov r5 L0x20017130;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017230; Value = 0xfdc1fe77; PC = 0x8000ca0 *)
mov r6 L0x20017230;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017330; Value = 0x0383fc47; PC = 0x8000ca4 *)
mov r7 L0x20017330;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017030; Value = 0x037b0143; PC = 0x8000ca8 *)
mov r4 L0x20017030;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf048@sint32 : and [cf048 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf176@sint32 : and [cf176 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf304@sint32 : and [cf304 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf432@sint32 : and [cf432 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019890; PC = 0x8000d54 *)
mov L0x20019890 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019990; PC = 0x8000d58 *)
mov L0x20019990 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a90; PC = 0x8000d5c *)
mov L0x20019a90 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b90; PC = 0x8000d60 *)
mov L0x20019b90 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c90; PC = 0x8000d64 *)
mov L0x20019c90 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d90; PC = 0x8000d68 *)
mov L0x20019d90 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e90; PC = 0x8000d6c *)
mov L0x20019e90 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f90; PC = 0x8000d70 *)
mov L0x20019f90 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019490; PC = 0x8000d9c *)
mov L0x20019490 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019590; PC = 0x8000da0 *)
mov L0x20019590 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019690; PC = 0x8000da4 *)
mov L0x20019690 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019790; PC = 0x8000da8 *)
mov L0x20019790 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019190; PC = 0x8000dac *)
mov L0x20019190 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019290; PC = 0x8000db0 *)
mov L0x20019290 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019390; PC = 0x8000db4 *)
mov L0x20019390 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019090; PC = 0x8000db8 *)
mov L0x20019090 r8;



(**************** CUT  48, - *****************)

ecut and [
eqmod cf048 f048 2**11, eqmod cf112 f112 2**11, eqmod cf176 f176 2**11,
eqmod cf240 f240 2**11, eqmod cf304 f304 2**11, eqmod cf368 f368 2**11,
eqmod cf432 f432 2**11, eqmod cf496 f496 2**11,
eqmod L0x20019090*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019190*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019290*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019390*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019490*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019590*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019690*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019790*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019890*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019990*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a90*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b90*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c90*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d90*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e90*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f90*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  268244 ]
];



(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170b2; Value = 0x01cdfe84; PC = 0x8000b7c *)
mov r4 L0x200170b2;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171b2; Value = 0x01d5ffb8; PC = 0x8000b80 *)
mov r5 L0x200171b2;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172b2; Value = 0x024402ff; PC = 0x8000b84 *)
mov r6 L0x200172b2;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173b2; Value = 0x006f01c8; PC = 0x8000b88 *)
mov r7 L0x200173b2;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf113@sint32 : and [cf113 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf241@sint32 : and [cf241 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf369@sint32 : and [cf369 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf497@sint32 : and [cf497 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017132; Value = 0x0133017a; PC = 0x8000c9c *)
mov r5 L0x20017132;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017232; Value = 0x03d1fdc1; PC = 0x8000ca0 *)
mov r6 L0x20017232;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017332; Value = 0x02070383; PC = 0x8000ca4 *)
mov r7 L0x20017332;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017032; Value = 0xffbe037b; PC = 0x8000ca8 *)
mov r4 L0x20017032;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf049@sint32 : and [cf049 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf177@sint32 : and [cf177 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf305@sint32 : and [cf305 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf433@sint32 : and [cf433 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019894; PC = 0x8000d54 *)
mov L0x20019894 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019994; PC = 0x8000d58 *)
mov L0x20019994 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a94; PC = 0x8000d5c *)
mov L0x20019a94 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b94; PC = 0x8000d60 *)
mov L0x20019b94 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c94; PC = 0x8000d64 *)
mov L0x20019c94 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d94; PC = 0x8000d68 *)
mov L0x20019d94 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e94; PC = 0x8000d6c *)
mov L0x20019e94 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f94; PC = 0x8000d70 *)
mov L0x20019f94 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019494; PC = 0x8000d9c *)
mov L0x20019494 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019594; PC = 0x8000da0 *)
mov L0x20019594 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019694; PC = 0x8000da4 *)
mov L0x20019694 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019794; PC = 0x8000da8 *)
mov L0x20019794 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019194; PC = 0x8000dac *)
mov L0x20019194 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019294; PC = 0x8000db0 *)
mov L0x20019294 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019394; PC = 0x8000db4 *)
mov L0x20019394 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019094; PC = 0x8000db8 *)
mov L0x20019094 r8;



(**************** CUT  49, - *****************)

ecut and [
eqmod cf049 f049 2**11, eqmod cf113 f113 2**11, eqmod cf177 f177 2**11,
eqmod cf241 f241 2**11, eqmod cf305 f305 2**11, eqmod cf369 f369 2**11,
eqmod cf433 f433 2**11, eqmod cf497 f497 2**11,
eqmod L0x20019094*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019194*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019294*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019394*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019494*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019594*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019694*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019794*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019894*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019994*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a94*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b94*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c94*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d94*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e94*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f94*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170b4; Value = 0x034901cd; PC = 0x8000b7c *)
mov r4 L0x200170b4;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171b4; Value = 0x003801d5; PC = 0x8000b80 *)
mov r5 L0x200171b4;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172b4; Value = 0xfddc0244; PC = 0x8000b84 *)
mov r6 L0x200172b4;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173b4; Value = 0xffdf006f; PC = 0x8000b88 *)
mov r7 L0x200173b4;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf114@sint32 : and [cf114 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf242@sint32 : and [cf242 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf370@sint32 : and [cf370 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf498@sint32 : and [cf498 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017134; Value = 0x02700133; PC = 0x8000c9c *)
mov r5 L0x20017134;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017234; Value = 0x01c203d1; PC = 0x8000ca0 *)
mov r6 L0x20017234;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017334; Value = 0xfcc30207; PC = 0x8000ca4 *)
mov r7 L0x20017334;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017034; Value = 0xff90ffbe; PC = 0x8000ca8 *)
mov r4 L0x20017034;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf050@sint32 : and [cf050 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf178@sint32 : and [cf178 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf306@sint32 : and [cf306 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf434@sint32 : and [cf434 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x20019898; PC = 0x8000d54 *)
mov L0x20019898 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x20019998; PC = 0x8000d58 *)
mov L0x20019998 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a98; PC = 0x8000d5c *)
mov L0x20019a98 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b98; PC = 0x8000d60 *)
mov L0x20019b98 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c98; PC = 0x8000d64 *)
mov L0x20019c98 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d98; PC = 0x8000d68 *)
mov L0x20019d98 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e98; PC = 0x8000d6c *)
mov L0x20019e98 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f98; PC = 0x8000d70 *)
mov L0x20019f98 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x20019498; PC = 0x8000d9c *)
mov L0x20019498 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x20019598; PC = 0x8000da0 *)
mov L0x20019598 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x20019698; PC = 0x8000da4 *)
mov L0x20019698 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x20019798; PC = 0x8000da8 *)
mov L0x20019798 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x20019198; PC = 0x8000dac *)
mov L0x20019198 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x20019298; PC = 0x8000db0 *)
mov L0x20019298 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x20019398; PC = 0x8000db4 *)
mov L0x20019398 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x20019098; PC = 0x8000db8 *)
mov L0x20019098 r8;



(**************** CUT  50, - *****************)

ecut and [
eqmod cf050 f050 2**11, eqmod cf114 f114 2**11, eqmod cf178 f178 2**11,
eqmod cf242 f242 2**11, eqmod cf306 f306 2**11, eqmod cf370 f370 2**11,
eqmod cf434 f434 2**11, eqmod cf498 f498 2**11,
eqmod L0x20019098*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019198*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019298*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019398*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019498*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019598*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019698*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019798*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019898*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019998*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a98*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b98*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c98*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d98*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e98*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f98*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170b6; Value = 0x02d60349; PC = 0x8000b7c *)
mov r4 L0x200170b6;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171b6; Value = 0xfc070038; PC = 0x8000b80 *)
mov r5 L0x200171b6;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172b6; Value = 0xfca5fddc; PC = 0x8000b84 *)
mov r6 L0x200172b6;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173b6; Value = 0xfc78ffdf; PC = 0x8000b88 *)
mov r7 L0x200173b6;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf115@sint32 : and [cf115 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf243@sint32 : and [cf243 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf371@sint32 : and [cf371 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf499@sint32 : and [cf499 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017136; Value = 0xfe970270; PC = 0x8000c9c *)
mov r5 L0x20017136;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017236; Value = 0x01e001c2; PC = 0x8000ca0 *)
mov r6 L0x20017236;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017336; Value = 0x0364fcc3; PC = 0x8000ca4 *)
mov r7 L0x20017336;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017036; Value = 0x0316ff90; PC = 0x8000ca8 *)
mov r4 L0x20017036;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf051@sint32 : and [cf051 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf179@sint32 : and [cf179 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf307@sint32 : and [cf307 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf435@sint32 : and [cf435 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x2001989c; PC = 0x8000d54 *)
mov L0x2001989c r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x2001999c; PC = 0x8000d58 *)
mov L0x2001999c r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019a9c; PC = 0x8000d5c *)
mov L0x20019a9c r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019b9c; PC = 0x8000d60 *)
mov L0x20019b9c r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019c9c; PC = 0x8000d64 *)
mov L0x20019c9c r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019d9c; PC = 0x8000d68 *)
mov L0x20019d9c r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019e9c; PC = 0x8000d6c *)
mov L0x20019e9c r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019f9c; PC = 0x8000d70 *)
mov L0x20019f9c r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x2001949c; PC = 0x8000d9c *)
mov L0x2001949c r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x2001959c; PC = 0x8000da0 *)
mov L0x2001959c r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x2001969c; PC = 0x8000da4 *)
mov L0x2001969c r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x2001979c; PC = 0x8000da8 *)
mov L0x2001979c r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x2001919c; PC = 0x8000dac *)
mov L0x2001919c r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x2001929c; PC = 0x8000db0 *)
mov L0x2001929c r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x2001939c; PC = 0x8000db4 *)
mov L0x2001939c r5;
(* str.w	r8, [r0], #4                              #! EA = L0x2001909c; PC = 0x8000db8 *)
mov L0x2001909c r8;



(**************** CUT  51, - *****************)

ecut and [
eqmod cf051 f051 2**11, eqmod cf115 f115 2**11, eqmod cf179 f179 2**11,
eqmod cf243 f243 2**11, eqmod cf307 f307 2**11, eqmod cf371 f371 2**11,
eqmod cf435 f435 2**11, eqmod cf499 f499 2**11,
eqmod L0x2001909c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -       1 ],
eqmod L0x2001919c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x2001929c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  554923 ],
eqmod L0x2001939c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  489046 ],
eqmod L0x2001949c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  287998 ],
eqmod L0x2001959c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  755971 ],
eqmod L0x2001969c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  719789 ],
eqmod L0x2001979c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  324180 ],
eqmod L0x2001989c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -   29512 ],
eqmod L0x2001999c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019a9c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019b9c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019c9c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019d9c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019e9c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019f9c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170b8; Value = 0xfe8302d6; PC = 0x8000b7c *)
mov r4 L0x200170b8;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171b8; Value = 0x01fefc07; PC = 0x8000b80 *)
mov r5 L0x200171b8;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172b8; Value = 0x0120fca5; PC = 0x8000b84 *)
mov r6 L0x200172b8;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173b8; Value = 0x0206fc78; PC = 0x8000b88 *)
mov r7 L0x200173b8;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf116@sint32 : and [cf116 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf244@sint32 : and [cf244 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf372@sint32 : and [cf372 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf500@sint32 : and [cf500 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017138; Value = 0xfcd7fe97; PC = 0x8000c9c *)
mov r5 L0x20017138;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017238; Value = 0xff9601e0; PC = 0x8000ca0 *)
mov r6 L0x20017238;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017338; Value = 0x01c30364; PC = 0x8000ca4 *)
mov r7 L0x20017338;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017038; Value = 0x01890316; PC = 0x8000ca8 *)
mov r4 L0x20017038;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf052@sint32 : and [cf052 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf180@sint32 : and [cf180 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf308@sint32 : and [cf308 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf436@sint32 : and [cf436 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198a0; PC = 0x8000d54 *)
mov L0x200198a0 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199a0; PC = 0x8000d58 *)
mov L0x200199a0 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019aa0; PC = 0x8000d5c *)
mov L0x20019aa0 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019ba0; PC = 0x8000d60 *)
mov L0x20019ba0 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019ca0; PC = 0x8000d64 *)
mov L0x20019ca0 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019da0; PC = 0x8000d68 *)
mov L0x20019da0 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019ea0; PC = 0x8000d6c *)
mov L0x20019ea0 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fa0; PC = 0x8000d70 *)
mov L0x20019fa0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194a0; PC = 0x8000d9c *)
mov L0x200194a0 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195a0; PC = 0x8000da0 *)
mov L0x200195a0 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196a0; PC = 0x8000da4 *)
mov L0x200196a0 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197a0; PC = 0x8000da8 *)
mov L0x200197a0 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191a0; PC = 0x8000dac *)
mov L0x200191a0 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192a0; PC = 0x8000db0 *)
mov L0x200192a0 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193a0; PC = 0x8000db4 *)
mov L0x200193a0 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190a0; PC = 0x8000db8 *)
mov L0x200190a0 r8;



(**************** CUT  52, - *****************)

ecut and [
eqmod cf052 f052 2**11, eqmod cf116 f116 2**11, eqmod cf180 f180 2**11,
eqmod cf244 f244 2**11, eqmod cf308 f308 2**11, eqmod cf372 f372 2**11,
eqmod cf436 f436 2**11, eqmod cf500 f500 2**11,
eqmod L0x200190a0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -       1 ],
eqmod L0x200191a0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200192a0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200193a0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200194a0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200195a0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200196a0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200197a0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200198a0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200199a0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019aa0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019ba0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019ca0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019da0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019ea0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019fa0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170ba; Value = 0x0142fe83; PC = 0x8000b7c *)
mov r4 L0x200170ba;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171ba; Value = 0xffd801fe; PC = 0x8000b80 *)
mov r5 L0x200171ba;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172ba; Value = 0x03d50120; PC = 0x8000b84 *)
mov r6 L0x200172ba;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173ba; Value = 0x01b80206; PC = 0x8000b88 *)
mov r7 L0x200173ba;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf117@sint32 : and [cf117 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf245@sint32 : and [cf245 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf373@sint32 : and [cf373 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf501@sint32 : and [cf501 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001713a; Value = 0x0052fcd7; PC = 0x8000c9c *)
mov r5 L0x2001713a;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001723a; Value = 0xfc65ff96; PC = 0x8000ca0 *)
mov r6 L0x2001723a;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001733a; Value = 0x000c01c3; PC = 0x8000ca4 *)
mov r7 L0x2001733a;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001703a; Value = 0xfc9d0189; PC = 0x8000ca8 *)
mov r4 L0x2001703a;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf053@sint32 : and [cf053 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf181@sint32 : and [cf181 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf309@sint32 : and [cf309 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf437@sint32 : and [cf437 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198a4; PC = 0x8000d54 *)
mov L0x200198a4 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199a4; PC = 0x8000d58 *)
mov L0x200199a4 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019aa4; PC = 0x8000d5c *)
mov L0x20019aa4 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019ba4; PC = 0x8000d60 *)
mov L0x20019ba4 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019ca4; PC = 0x8000d64 *)
mov L0x20019ca4 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019da4; PC = 0x8000d68 *)
mov L0x20019da4 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019ea4; PC = 0x8000d6c *)
mov L0x20019ea4 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fa4; PC = 0x8000d70 *)
mov L0x20019fa4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194a4; PC = 0x8000d9c *)
mov L0x200194a4 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195a4; PC = 0x8000da0 *)
mov L0x200195a4 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196a4; PC = 0x8000da4 *)
mov L0x200196a4 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197a4; PC = 0x8000da8 *)
mov L0x200197a4 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191a4; PC = 0x8000dac *)
mov L0x200191a4 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192a4; PC = 0x8000db0 *)
mov L0x200192a4 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193a4; PC = 0x8000db4 *)
mov L0x200193a4 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190a4; PC = 0x8000db8 *)
mov L0x200190a4 r8;



(**************** CUT  53, - *****************)

ecut and [
eqmod cf053 f053 2**11, eqmod cf117 f117 2**11, eqmod cf181 f181 2**11,
eqmod cf245 f245 2**11, eqmod cf309 f309 2**11, eqmod cf373 f373 2**11,
eqmod cf437 f437 2**11, eqmod cf501 f501 2**11,
eqmod L0x200190a4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -       1 ],
eqmod L0x200191a4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200192a4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200193a4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200194a4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200195a4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200196a4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200197a4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200198a4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200199a4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019aa4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019ba4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019ca4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019da4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019ea4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019fa4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170bc; Value = 0x00170142; PC = 0x8000b7c *)
mov r4 L0x200170bc;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171bc; Value = 0x015affd8; PC = 0x8000b80 *)
mov r5 L0x200171bc;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172bc; Value = 0x01a303d5; PC = 0x8000b84 *)
mov r6 L0x200172bc;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173bc; Value = 0xfd8401b8; PC = 0x8000b88 *)
mov r7 L0x200173bc;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf118@sint32 : and [cf118 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf246@sint32 : and [cf246 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf374@sint32 : and [cf374 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf502@sint32 : and [cf502 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001713c; Value = 0x02120052; PC = 0x8000c9c *)
mov r5 L0x2001713c;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001723c; Value = 0x0220fc65; PC = 0x8000ca0 *)
mov r6 L0x2001723c;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001733c; Value = 0x01ea000c; PC = 0x8000ca4 *)
mov r7 L0x2001733c;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001703c; Value = 0xfc7efc9d; PC = 0x8000ca8 *)
mov r4 L0x2001703c;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf054@sint32 : and [cf054 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf182@sint32 : and [cf182 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf310@sint32 : and [cf310 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf438@sint32 : and [cf438 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198a8; PC = 0x8000d54 *)
mov L0x200198a8 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199a8; PC = 0x8000d58 *)
mov L0x200199a8 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019aa8; PC = 0x8000d5c *)
mov L0x20019aa8 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019ba8; PC = 0x8000d60 *)
mov L0x20019ba8 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019ca8; PC = 0x8000d64 *)
mov L0x20019ca8 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019da8; PC = 0x8000d68 *)
mov L0x20019da8 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019ea8; PC = 0x8000d6c *)
mov L0x20019ea8 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fa8; PC = 0x8000d70 *)
mov L0x20019fa8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194a8; PC = 0x8000d9c *)
mov L0x200194a8 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195a8; PC = 0x8000da0 *)
mov L0x200195a8 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196a8; PC = 0x8000da4 *)
mov L0x200196a8 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197a8; PC = 0x8000da8 *)
mov L0x200197a8 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191a8; PC = 0x8000dac *)
mov L0x200191a8 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192a8; PC = 0x8000db0 *)
mov L0x200192a8 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193a8; PC = 0x8000db4 *)
mov L0x200193a8 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190a8; PC = 0x8000db8 *)
mov L0x200190a8 r8;



(**************** CUT  54, - *****************)

ecut and [
eqmod cf054 f054 2**11, eqmod cf118 f118 2**11, eqmod cf182 f182 2**11,
eqmod cf246 f246 2**11, eqmod cf310 f310 2**11, eqmod cf374 f374 2**11,
eqmod cf438 f438 2**11, eqmod cf502 f502 2**11,
eqmod L0x200190a8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -       1 ],
eqmod L0x200191a8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200192a8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200193a8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200194a8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200195a8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200196a8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200197a8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200198a8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200199a8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019aa8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019ba8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019ca8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019da8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019ea8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019fa8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170be; Value = 0x03e70017; PC = 0x8000b7c *)
mov r4 L0x200170be;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171be; Value = 0x00b1015a; PC = 0x8000b80 *)
mov r5 L0x200171be;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172be; Value = 0xfe4c01a3; PC = 0x8000b84 *)
mov r6 L0x200172be;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173be; Value = 0xff0afd84; PC = 0x8000b88 *)
mov r7 L0x200173be;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf119@sint32 : and [cf119 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf247@sint32 : and [cf247 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf375@sint32 : and [cf375 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf503@sint32 : and [cf503 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001713e; Value = 0xfef50212; PC = 0x8000c9c *)
mov r5 L0x2001713e;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001723e; Value = 0xfd640220; PC = 0x8000ca0 *)
mov r6 L0x2001723e;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001733e; Value = 0x026001ea; PC = 0x8000ca4 *)
mov r7 L0x2001733e;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001703e; Value = 0xff77fc7e; PC = 0x8000ca8 *)
mov r4 L0x2001703e;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf055@sint32 : and [cf055 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf183@sint32 : and [cf183 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf311@sint32 : and [cf311 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf439@sint32 : and [cf439 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198ac; PC = 0x8000d54 *)
mov L0x200198ac r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199ac; PC = 0x8000d58 *)
mov L0x200199ac r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019aac; PC = 0x8000d5c *)
mov L0x20019aac r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019bac; PC = 0x8000d60 *)
mov L0x20019bac r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019cac; PC = 0x8000d64 *)
mov L0x20019cac r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019dac; PC = 0x8000d68 *)
mov L0x20019dac r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019eac; PC = 0x8000d6c *)
mov L0x20019eac r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fac; PC = 0x8000d70 *)
mov L0x20019fac r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194ac; PC = 0x8000d9c *)
mov L0x200194ac r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195ac; PC = 0x8000da0 *)
mov L0x200195ac r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196ac; PC = 0x8000da4 *)
mov L0x200196ac r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197ac; PC = 0x8000da8 *)
mov L0x200197ac r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191ac; PC = 0x8000dac *)
mov L0x200191ac r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192ac; PC = 0x8000db0 *)
mov L0x200192ac r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193ac; PC = 0x8000db4 *)
mov L0x200193ac r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190ac; PC = 0x8000db8 *)
mov L0x200190ac r8;



(**************** CUT  55, - *****************)

ecut and [
eqmod cf055 f055 2**11, eqmod cf119 f119 2**11, eqmod cf183 f183 2**11,
eqmod cf247 f247 2**11, eqmod cf311 f311 2**11, eqmod cf375 f375 2**11,
eqmod cf439 f439 2**11, eqmod cf503 f503 2**11,
eqmod L0x200190ac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -       1 ],
eqmod L0x200191ac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200192ac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200193ac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200194ac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200195ac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200196ac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200197ac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200198ac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200199ac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019aac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019bac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019cac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019dac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019eac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019fac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170c0; Value = 0x03a303e7; PC = 0x8000b7c *)
mov r4 L0x200170c0;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171c0; Value = 0x012f00b1; PC = 0x8000b80 *)
mov r5 L0x200171c0;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172c0; Value = 0x018efe4c; PC = 0x8000b84 *)
mov r6 L0x200172c0;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173c0; Value = 0xfc72ff0a; PC = 0x8000b88 *)
mov r7 L0x200173c0;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf120@sint32 : and [cf120 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf248@sint32 : and [cf248 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf376@sint32 : and [cf376 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf504@sint32 : and [cf504 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017140; Value = 0x007efef5; PC = 0x8000c9c *)
mov r5 L0x20017140;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017240; Value = 0xfc6dfd64; PC = 0x8000ca0 *)
mov r6 L0x20017240;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017340; Value = 0x02480260; PC = 0x8000ca4 *)
mov r7 L0x20017340;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017040; Value = 0x02c6ff77; PC = 0x8000ca8 *)
mov r4 L0x20017040;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf056@sint32 : and [cf056 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf184@sint32 : and [cf184 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf312@sint32 : and [cf312 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf440@sint32 : and [cf440 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198b0; PC = 0x8000d54 *)
mov L0x200198b0 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199b0; PC = 0x8000d58 *)
mov L0x200199b0 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019ab0; PC = 0x8000d5c *)
mov L0x20019ab0 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019bb0; PC = 0x8000d60 *)
mov L0x20019bb0 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019cb0; PC = 0x8000d64 *)
mov L0x20019cb0 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019db0; PC = 0x8000d68 *)
mov L0x20019db0 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019eb0; PC = 0x8000d6c *)
mov L0x20019eb0 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fb0; PC = 0x8000d70 *)
mov L0x20019fb0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194b0; PC = 0x8000d9c *)
mov L0x200194b0 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195b0; PC = 0x8000da0 *)
mov L0x200195b0 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196b0; PC = 0x8000da4 *)
mov L0x200196b0 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197b0; PC = 0x8000da8 *)
mov L0x200197b0 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191b0; PC = 0x8000dac *)
mov L0x200191b0 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192b0; PC = 0x8000db0 *)
mov L0x200192b0 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193b0; PC = 0x8000db4 *)
mov L0x200193b0 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190b0; PC = 0x8000db8 *)
mov L0x200190b0 r8;



(**************** CUT  56, - *****************)

ecut and [
eqmod cf056 f056 2**11, eqmod cf120 f120 2**11, eqmod cf184 f184 2**11,
eqmod cf248 f248 2**11, eqmod cf312 f312 2**11, eqmod cf376 f376 2**11,
eqmod cf440 f440 2**11, eqmod cf504 f504 2**11,
eqmod L0x200190b0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -       1 ],
eqmod L0x200191b0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200192b0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200193b0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200194b0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200195b0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200196b0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200197b0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200198b0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200199b0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019ab0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019bb0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019cb0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019db0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019eb0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019fb0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170c2; Value = 0xfc3903a3; PC = 0x8000b7c *)
mov r4 L0x200170c2;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171c2; Value = 0xfc46012f; PC = 0x8000b80 *)
mov r5 L0x200171c2;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172c2; Value = 0xff94018e; PC = 0x8000b84 *)
mov r6 L0x200172c2;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173c2; Value = 0x01a2fc72; PC = 0x8000b88 *)
mov r7 L0x200173c2;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf121@sint32 : and [cf121 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf249@sint32 : and [cf249 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf377@sint32 : and [cf377 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf505@sint32 : and [cf505 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017142; Value = 0xfed2007e; PC = 0x8000c9c *)
mov r5 L0x20017142;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017242; Value = 0xff74fc6d; PC = 0x8000ca0 *)
mov r6 L0x20017242;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017342; Value = 0xfeee0248; PC = 0x8000ca4 *)
mov r7 L0x20017342;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017042; Value = 0xff2f02c6; PC = 0x8000ca8 *)
mov r4 L0x20017042;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf057@sint32 : and [cf057 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf185@sint32 : and [cf185 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf313@sint32 : and [cf313 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf441@sint32 : and [cf441 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198b4; PC = 0x8000d54 *)
mov L0x200198b4 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199b4; PC = 0x8000d58 *)
mov L0x200199b4 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019ab4; PC = 0x8000d5c *)
mov L0x20019ab4 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019bb4; PC = 0x8000d60 *)
mov L0x20019bb4 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019cb4; PC = 0x8000d64 *)
mov L0x20019cb4 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019db4; PC = 0x8000d68 *)
mov L0x20019db4 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019eb4; PC = 0x8000d6c *)
mov L0x20019eb4 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fb4; PC = 0x8000d70 *)
mov L0x20019fb4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194b4; PC = 0x8000d9c *)
mov L0x200194b4 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195b4; PC = 0x8000da0 *)
mov L0x200195b4 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196b4; PC = 0x8000da4 *)
mov L0x200196b4 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197b4; PC = 0x8000da8 *)
mov L0x200197b4 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191b4; PC = 0x8000dac *)
mov L0x200191b4 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192b4; PC = 0x8000db0 *)
mov L0x200192b4 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193b4; PC = 0x8000db4 *)
mov L0x200193b4 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190b4; PC = 0x8000db8 *)
mov L0x200190b4 r8;



(**************** CUT  57, - *****************)

ecut and [
eqmod cf057 f057 2**11, eqmod cf121 f121 2**11, eqmod cf185 f185 2**11,
eqmod cf249 f249 2**11, eqmod cf313 f313 2**11, eqmod cf377 f377 2**11,
eqmod cf441 f441 2**11, eqmod cf505 f505 2**11,
eqmod L0x200190b4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -       1 ],
eqmod L0x200191b4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200192b4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200193b4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200194b4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200195b4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200196b4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200197b4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200198b4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200199b4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019ab4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019bb4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019cb4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019db4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019eb4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019fb4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170c4; Value = 0x0093fc39; PC = 0x8000b7c *)
mov r4 L0x200170c4;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171c4; Value = 0x01bcfc46; PC = 0x8000b80 *)
mov r5 L0x200171c4;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172c4; Value = 0xfd93ff94; PC = 0x8000b84 *)
mov r6 L0x200172c4;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173c4; Value = 0xfc2a01a2; PC = 0x8000b88 *)
mov r7 L0x200173c4;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf122@sint32 : and [cf122 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf250@sint32 : and [cf250 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf378@sint32 : and [cf378 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf506@sint32 : and [cf506 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017144; Value = 0x0394fed2; PC = 0x8000c9c *)
mov r5 L0x20017144;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017244; Value = 0xff07ff74; PC = 0x8000ca0 *)
mov r6 L0x20017244;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017344; Value = 0xfd66feee; PC = 0x8000ca4 *)
mov r7 L0x20017344;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017044; Value = 0x0326ff2f; PC = 0x8000ca8 *)
mov r4 L0x20017044;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf058@sint32 : and [cf058 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf186@sint32 : and [cf186 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf314@sint32 : and [cf314 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf442@sint32 : and [cf442 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198b8; PC = 0x8000d54 *)
mov L0x200198b8 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199b8; PC = 0x8000d58 *)
mov L0x200199b8 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019ab8; PC = 0x8000d5c *)
mov L0x20019ab8 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019bb8; PC = 0x8000d60 *)
mov L0x20019bb8 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019cb8; PC = 0x8000d64 *)
mov L0x20019cb8 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019db8; PC = 0x8000d68 *)
mov L0x20019db8 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019eb8; PC = 0x8000d6c *)
mov L0x20019eb8 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fb8; PC = 0x8000d70 *)
mov L0x20019fb8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194b8; PC = 0x8000d9c *)
mov L0x200194b8 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195b8; PC = 0x8000da0 *)
mov L0x200195b8 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196b8; PC = 0x8000da4 *)
mov L0x200196b8 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197b8; PC = 0x8000da8 *)
mov L0x200197b8 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191b8; PC = 0x8000dac *)
mov L0x200191b8 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192b8; PC = 0x8000db0 *)
mov L0x200192b8 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193b8; PC = 0x8000db4 *)
mov L0x200193b8 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190b8; PC = 0x8000db8 *)
mov L0x200190b8 r8;



(**************** CUT  58, - *****************)

ecut and [
eqmod cf058 f058 2**11, eqmod cf122 f122 2**11, eqmod cf186 f186 2**11,
eqmod cf250 f250 2**11, eqmod cf314 f314 2**11, eqmod cf378 f378 2**11,
eqmod cf442 f442 2**11, eqmod cf506 f506 2**11,
eqmod L0x200190b8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -       1 ],
eqmod L0x200191b8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200192b8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200193b8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200194b8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200195b8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200196b8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200197b8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200198b8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200199b8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019ab8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019bb8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019cb8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019db8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019eb8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019fb8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170c6; Value = 0xfc730093; PC = 0x8000b7c *)
mov r4 L0x200170c6;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171c6; Value = 0x008e01bc; PC = 0x8000b80 *)
mov r5 L0x200171c6;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172c6; Value = 0x01a1fd93; PC = 0x8000b84 *)
mov r6 L0x200172c6;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173c6; Value = 0xfc4cfc2a; PC = 0x8000b88 *)
mov r7 L0x200173c6;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf123@sint32 : and [cf123 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf251@sint32 : and [cf251 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf379@sint32 : and [cf379 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf507@sint32 : and [cf507 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017146; Value = 0xfe950394; PC = 0x8000c9c *)
mov r5 L0x20017146;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017246; Value = 0x000bff07; PC = 0x8000ca0 *)
mov r6 L0x20017246;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017346; Value = 0xfd83fd66; PC = 0x8000ca4 *)
mov r7 L0x20017346;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017046; Value = 0xfe980326; PC = 0x8000ca8 *)
mov r4 L0x20017046;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf059@sint32 : and [cf059 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf187@sint32 : and [cf187 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf315@sint32 : and [cf315 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf443@sint32 : and [cf443 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198bc; PC = 0x8000d54 *)
mov L0x200198bc r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199bc; PC = 0x8000d58 *)
mov L0x200199bc r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019abc; PC = 0x8000d5c *)
mov L0x20019abc r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019bbc; PC = 0x8000d60 *)
mov L0x20019bbc r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019cbc; PC = 0x8000d64 *)
mov L0x20019cbc r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019dbc; PC = 0x8000d68 *)
mov L0x20019dbc r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019ebc; PC = 0x8000d6c *)
mov L0x20019ebc r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fbc; PC = 0x8000d70 *)
mov L0x20019fbc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194bc; PC = 0x8000d9c *)
mov L0x200194bc r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195bc; PC = 0x8000da0 *)
mov L0x200195bc r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196bc; PC = 0x8000da4 *)
mov L0x200196bc r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197bc; PC = 0x8000da8 *)
mov L0x200197bc r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191bc; PC = 0x8000dac *)
mov L0x200191bc r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192bc; PC = 0x8000db0 *)
mov L0x200192bc r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193bc; PC = 0x8000db4 *)
mov L0x200193bc r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190bc; PC = 0x8000db8 *)
mov L0x200190bc r8;



(**************** CUT  59, - *****************)

ecut and [
eqmod cf059 f059 2**11, eqmod cf123 f123 2**11, eqmod cf187 f187 2**11,
eqmod cf251 f251 2**11, eqmod cf315 f315 2**11, eqmod cf379 f379 2**11,
eqmod cf443 f443 2**11, eqmod cf507 f507 2**11,
eqmod L0x200190bc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -       1 ],
eqmod L0x200191bc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200192bc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200193bc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200194bc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200195bc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200196bc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200197bc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200198bc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200199bc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019abc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019bbc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019cbc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019dbc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019ebc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019fbc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* vmov	lr, s0                                     #! PC = 0x8000b78 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170c8; Value = 0xffc6fc73; PC = 0x8000b7c *)
mov r4 L0x200170c8;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171c8; Value = 0xfc35008e; PC = 0x8000b80 *)
mov r5 L0x200171c8;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172c8; Value = 0x012401a1; PC = 0x8000b84 *)
mov r6 L0x200172c8;
(* ldrsh.w	r7, [lr, #896]	; 0x380                  #! EA = L0x200173c8; Value = 0x9341fc4c; PC = 0x8000b88 *)
mov r7 L0x200173c8;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000b8c *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf124@sint32 : and [cf124 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000b90 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf252@sint32 : and [cf252 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000b94 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf380@sint32 : and [cf380 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000b98 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf508@sint32 : and [cf508 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000b9c *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000ba0 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000ba4 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ba8 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000bac *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000bb0 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000bb4 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000bb8 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000bbc *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000bc0 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000bc4 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000bc8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000bcc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000bd0 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000bd4 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000bd8 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000bda *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000bdc *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000be0 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000be4 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000be8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000bec *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000bf0 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000bf2 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000bf4 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000bf6 *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000bf8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000bfc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000c00 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000c04 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000c08 *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000c0c *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c10 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000c14 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000c18 *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000c1c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c20 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000c24 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000c28 *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000c2c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c30 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000c34 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000c38 *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000c3c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000c44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000c48 *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000c4c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c50 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000c54 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000c58 *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000c5c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000c64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000c68 *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000c6c *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c70 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000c74 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000c78 *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000c7c *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000c80 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000c84 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000c88 *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000c8c *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000c90 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000c94 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000c98 *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x20017148; Value = 0xff66fe95; PC = 0x8000c9c *)
mov r5 L0x20017148;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x20017248; Value = 0xfe5d000b; PC = 0x8000ca0 *)
mov r6 L0x20017248;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x20017348; Value = 0x00abfd83; PC = 0x8000ca4 *)
mov r7 L0x20017348;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x20017048; Value = 0x0288fe98; PC = 0x8000ca8 *)
mov r4 L0x20017048;
(* vmov	s0, lr                                     #! PC = 0x8000cac *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000cb0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf060@sint32 : and [cf060 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000cb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf188@sint32 : and [cf188 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000cb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf316@sint32 : and [cf316 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000cbc *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf444@sint32 : and [cf444 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000cc0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000cc4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000cc8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000ccc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000cd0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000cd4 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000cd8 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000cdc *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000ce0 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000ce4 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000ce8 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000cec *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000cf0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000cf4 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000cf8 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000cfc *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000cfe *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000d00 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000d04 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000d08 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000d0c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000d10 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000d14 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000d16 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000d18 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000d1a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000d1c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000d20 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000d24 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000d28 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000d2c *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000d30 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000d34 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000d38 *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000d3c *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d3e *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d40 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d42 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d44 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d48 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d4c *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d50 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198c0; PC = 0x8000d54 *)
mov L0x200198c0 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199c0; PC = 0x8000d58 *)
mov L0x200199c0 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019ac0; PC = 0x8000d5c *)
mov L0x20019ac0 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019bc0; PC = 0x8000d60 *)
mov L0x20019bc0 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019cc0; PC = 0x8000d64 *)
mov L0x20019cc0 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019dc0; PC = 0x8000d68 *)
mov L0x20019dc0 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019ec0; PC = 0x8000d6c *)
mov L0x20019ec0 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fc0; PC = 0x8000d70 *)
mov L0x20019fc0 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000d74 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000d78 *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000d7c *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000d80 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000d84 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000d86 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000d88 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000d8a *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000d8c *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000d90 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000d94 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000d98 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194c0; PC = 0x8000d9c *)
mov L0x200194c0 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195c0; PC = 0x8000da0 *)
mov L0x200195c0 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196c0; PC = 0x8000da4 *)
mov L0x200196c0 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197c0; PC = 0x8000da8 *)
mov L0x200197c0 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191c0; PC = 0x8000dac *)
mov L0x200191c0 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192c0; PC = 0x8000db0 *)
mov L0x200192c0 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193c0; PC = 0x8000db4 *)
mov L0x200193c0 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190c0; PC = 0x8000db8 *)
mov L0x200190c0 r8;



(**************** CUT  60, - *****************)

ecut and [
eqmod cf060 f060 2**11, eqmod cf124 f124 2**11, eqmod cf188 f188 2**11,
eqmod cf252 f252 2**11, eqmod cf316 f316 2**11, eqmod cf380 f380 2**11,
eqmod cf444 f444 2**11, eqmod cf508 f508 2**11,
eqmod L0x200190c0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -       1 ],
eqmod L0x200191c0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200192c0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200193c0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200194c0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200195c0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200196c0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200197c0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200198c0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200199c0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019ac0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019bc0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019cc0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019dc0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019ec0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019fc0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8000dbc *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8000dc0 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000b78 <_0_1_2_3>                     #! PC = 0x8000dc4 *)
#bne.w	0x8000b78 <_0_1_2_3>                     #! 0x8000dc4 = 0x8000dc4;
(* add.w	r12, r0, #12                              #! PC = 0x8000dc8 *)
adds dontcare r12 r0 12@uint32;
(* vmov	s2, r12                                    #! PC = 0x8000dcc *)
mov s2 r12;
(* vmov	lr, s0                                     #! PC = 0x8000dd0 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170ca; Value = 0x02e6ffc6; PC = 0x8000dd4 *)
mov r4 L0x200170ca;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171ca; Value = 0xfd2afc35; PC = 0x8000dd8 *)
mov r5 L0x200171ca;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172ca; Value = 0xfc740124; PC = 0x8000ddc *)
mov r6 L0x200172ca;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000de0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf125@sint32 : and [cf125 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000de4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf253@sint32 : and [cf253 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000de8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf381@sint32 : and [cf381 = r6] && true;
(* movw	r7, #0                                     #! PC = 0x8000dec *)
mov r7 0@sint32;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000df0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000df4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000df8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000dfc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000e00 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000e04 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000e08 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000e0c *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000e10 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000e14 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000e18 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e1c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000e20 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000e24 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000e28 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000e2c *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000e2e *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000e30 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000e34 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000e38 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e3c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000e40 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000e44 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000e46 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000e48 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000e4a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000e4c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000e50 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000e54 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000e58 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000e5c *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000e60 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e64 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000e68 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000e6c *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000e70 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e74 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000e78 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000e7c *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000e80 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e84 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000e88 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000e8c *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000e90 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e94 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000e98 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000e9c *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000ea0 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000ea4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000ea8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000eac *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000eb0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000eb4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000eb8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000ebc *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000ec0 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000ec4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000ec8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000ecc *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000ed0 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000ed4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000ed8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000edc *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000ee0 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000ee4 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000ee8 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000eec *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001714a; Value = 0x0206ff66; PC = 0x8000ef0 *)
mov r5 L0x2001714a;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001724a; Value = 0xff87fe5d; PC = 0x8000ef4 *)
mov r6 L0x2001724a;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001734a; Value = 0x022700ab; PC = 0x8000ef8 *)
mov r7 L0x2001734a;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001704a; Value = 0x01f50288; PC = 0x8000efc *)
mov r4 L0x2001704a;
(* vmov	s0, lr                                     #! PC = 0x8000f00 *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000f04 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf061@sint32 : and [cf061 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000f08 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf189@sint32 : and [cf189 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000f0c *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf317@sint32 : and [cf317 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000f10 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf445@sint32 : and [cf445 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000f14 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000f18 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000f1c *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000f20 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000f24 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000f28 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000f2c *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000f30 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000f34 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000f38 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000f3c *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000f40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000f44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000f48 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000f4c *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000f50 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000f52 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000f54 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000f58 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000f5c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000f60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000f64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000f68 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000f6a *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000f6c *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000f6e *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000f70 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000f74 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000f78 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000f7c *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000f80 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000f84 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000f88 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000f8c *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000f90 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000f92 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000f94 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000f96 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000f98 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000f9c *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000fa0 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000fa4 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198c4; PC = 0x8000fa8 *)
mov L0x200198c4 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199c4; PC = 0x8000fac *)
mov L0x200199c4 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019ac4; PC = 0x8000fb0 *)
mov L0x20019ac4 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019bc4; PC = 0x8000fb4 *)
mov L0x20019bc4 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019cc4; PC = 0x8000fb8 *)
mov L0x20019cc4 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019dc4; PC = 0x8000fbc *)
mov L0x20019dc4 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019ec4; PC = 0x8000fc0 *)
mov L0x20019ec4 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fc4; PC = 0x8000fc4 *)
mov L0x20019fc4 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000fc8 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000fcc *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000fd0 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000fd4 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000fd8 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000fda *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000fdc *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000fde *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000fe0 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000fe4 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000fe8 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000fec *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194c4; PC = 0x8000ff0 *)
mov L0x200194c4 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195c4; PC = 0x8000ff4 *)
mov L0x200195c4 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196c4; PC = 0x8000ff8 *)
mov L0x200196c4 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197c4; PC = 0x8000ffc *)
mov L0x200197c4 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191c4; PC = 0x8001000 *)
mov L0x200191c4 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192c4; PC = 0x8001004 *)
mov L0x200192c4 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193c4; PC = 0x8001008 *)
mov L0x200193c4 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190c4; PC = 0x800100c *)
mov L0x200190c4 r8;



(**************** CUT  61, - *****************)

ecut and [
eqmod cf061 f061 2**11, eqmod cf125 f125 2**11, eqmod cf189 f189 2**11,
eqmod cf253 f253 2**11, eqmod cf317 f317 2**11, eqmod cf381 f381 2**11,
eqmod cf445 f445 2**11,
eqmod L0x200190c4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -       1 ],
eqmod L0x200191c4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200192c4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200193c4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200194c4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200195c4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200196c4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200197c4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200198c4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200199c4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019ac4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019bc4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019cc4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019dc4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019ec4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019fc4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8001010 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8001014 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000dd0 <_0_1_2_3_last>                #! PC = 0x8001018 *)
#bne.w	0x8000dd0 <_0_1_2_3_last>                #! 0x8001018 = 0x8001018;
(* vmov	lr, s0                                     #! PC = 0x8000dd0 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170cc; Value = 0x02f102e6; PC = 0x8000dd4 *)
mov r4 L0x200170cc;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171cc; Value = 0x0338fd2a; PC = 0x8000dd8 *)
mov r5 L0x200171cc;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172cc; Value = 0x00cdfc74; PC = 0x8000ddc *)
mov r6 L0x200172cc;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000de0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf126@sint32 : and [cf126 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000de4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf254@sint32 : and [cf254 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000de8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf382@sint32 : and [cf382 = r6] && true;
(* movw	r7, #0                                     #! PC = 0x8000dec *)
mov r7 0@sint32;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000df0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000df4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000df8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000dfc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000e00 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000e04 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000e08 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000e0c *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000e10 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000e14 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000e18 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e1c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000e20 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000e24 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000e28 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000e2c *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000e2e *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000e30 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000e34 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000e38 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e3c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000e40 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000e44 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000e46 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000e48 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000e4a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000e4c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000e50 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000e54 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000e58 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000e5c *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000e60 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e64 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000e68 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000e6c *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000e70 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e74 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000e78 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000e7c *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000e80 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e84 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000e88 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000e8c *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000e90 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e94 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000e98 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000e9c *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000ea0 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000ea4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000ea8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000eac *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000eb0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000eb4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000eb8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000ebc *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000ec0 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000ec4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000ec8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000ecc *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000ed0 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000ed4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000ed8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000edc *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000ee0 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000ee4 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000ee8 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000eec *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001714c; Value = 0xfd230206; PC = 0x8000ef0 *)
mov r5 L0x2001714c;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001724c; Value = 0xfd60ff87; PC = 0x8000ef4 *)
mov r6 L0x2001724c;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001734c; Value = 0x03730227; PC = 0x8000ef8 *)
mov r7 L0x2001734c;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001704c; Value = 0xfdb401f5; PC = 0x8000efc *)
mov r4 L0x2001704c;
(* vmov	s0, lr                                     #! PC = 0x8000f00 *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000f04 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf062@sint32 : and [cf062 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000f08 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf190@sint32 : and [cf190 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000f0c *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf318@sint32 : and [cf318 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000f10 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf446@sint32 : and [cf446 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000f14 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000f18 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000f1c *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000f20 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000f24 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000f28 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000f2c *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000f30 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000f34 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000f38 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000f3c *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000f40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000f44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000f48 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000f4c *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000f50 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000f52 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000f54 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000f58 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000f5c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000f60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000f64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000f68 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000f6a *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000f6c *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000f6e *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000f70 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000f74 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000f78 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000f7c *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000f80 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000f84 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000f88 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000f8c *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000f90 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000f92 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000f94 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000f96 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000f98 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000f9c *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000fa0 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000fa4 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198c8; PC = 0x8000fa8 *)
mov L0x200198c8 r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199c8; PC = 0x8000fac *)
mov L0x200199c8 r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019ac8; PC = 0x8000fb0 *)
mov L0x20019ac8 r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019bc8; PC = 0x8000fb4 *)
mov L0x20019bc8 r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019cc8; PC = 0x8000fb8 *)
mov L0x20019cc8 r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019dc8; PC = 0x8000fbc *)
mov L0x20019dc8 r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019ec8; PC = 0x8000fc0 *)
mov L0x20019ec8 r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fc8; PC = 0x8000fc4 *)
mov L0x20019fc8 r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000fc8 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000fcc *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000fd0 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000fd4 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000fd8 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000fda *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000fdc *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000fde *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000fe0 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000fe4 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000fe8 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000fec *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194c8; PC = 0x8000ff0 *)
mov L0x200194c8 r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195c8; PC = 0x8000ff4 *)
mov L0x200195c8 r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196c8; PC = 0x8000ff8 *)
mov L0x200196c8 r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197c8; PC = 0x8000ffc *)
mov L0x200197c8 r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191c8; PC = 0x8001000 *)
mov L0x200191c8 r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192c8; PC = 0x8001004 *)
mov L0x200192c8 r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193c8; PC = 0x8001008 *)
mov L0x200193c8 r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190c8; PC = 0x800100c *)
mov L0x200190c8 r8;



(**************** CUT  62, - *****************)

ecut and [
eqmod cf062 f062 2**11, eqmod cf126 f126 2**11, eqmod cf190 f190 2**11,
eqmod cf254 f254 2**11, eqmod cf318 f318 2**11, eqmod cf382 f382 2**11,
eqmod cf446 f446 2**11,
eqmod L0x200190c8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -       1 ],
eqmod L0x200191c8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200192c8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200193c8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200194c8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200195c8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200196c8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200197c8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200198c8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200199c8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019ac8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019bc8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019cc8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019dc8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019ec8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019fc8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8001010 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8001014 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000dd0 <_0_1_2_3_last>                #! PC = 0x8001018 *)
#bne.w	0x8000dd0 <_0_1_2_3_last>                #! 0x8001018 = 0x8001018;
(* vmov	lr, s0                                     #! PC = 0x8000dd0 *)
mov lr s0;
(* ldrsh.w	r4, [lr, #128]	; 0x80                   #! EA = L0x200170ce; Value = 0x008a02f1; PC = 0x8000dd4 *)
mov r4 L0x200170ce;
(* ldrsh.w	r5, [lr, #384]	; 0x180                  #! EA = L0x200171ce; Value = 0x00cc0338; PC = 0x8000dd8 *)
mov r5 L0x200171ce;
(* ldrsh.w	r6, [lr, #640]	; 0x280                  #! EA = L0x200172ce; Value = 0xff1a00cd; PC = 0x8000ddc *)
mov r6 L0x200172ce;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000de0 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf127@sint32 : and [cf127 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000de4 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf255@sint32 : and [cf255 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000de8 *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf383@sint32 : and [cf383 = r6] && true;
(* movw	r7, #0                                     #! PC = 0x8000dec *)
mov r7 0@sint32;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000df0 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000df4 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000df8 *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000dfc *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000e00 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000e04 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000e08 *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000e0c *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000e10 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000e14 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000e18 *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e1c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000e20 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000e24 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000e28 *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000e2c *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000e2e *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000e30 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000e34 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000e38 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e3c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000e40 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000e44 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000e46 *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000e48 *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000e4a *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000e4c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000e50 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000e54 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000e58 *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	r1, s11                                    #! PC = 0x8000e5c *)
mov r1 s11;
(* smull	r12, r4, r4, r1                           #! PC = 0x8000e60 *)
smull r4 r12 r4 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e64 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r4, lr, r3                           #! PC = 0x8000e68 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r4 r4 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x8000e6c *)
mov r1 s12;
(* smull	r12, r5, r5, r1                           #! PC = 0x8000e70 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e74 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8000e78 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s13                                    #! PC = 0x8000e7c *)
mov r1 s13;
(* smull	r12, r6, r6, r1                           #! PC = 0x8000e80 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e84 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8000e88 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8000e8c *)
mov r1 s14;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000e90 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000e94 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000e98 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8000e9c *)
mov r1 s15;
(* smull	r12, r8, r8, r1                           #! PC = 0x8000ea0 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000ea4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8000ea8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8000eac *)
mov r1 s16;
(* smull	r12, r9, r9, r1                           #! PC = 0x8000eb0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000eb4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8000eb8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8000ebc *)
mov r1 s17;
(* smull	r12, r10, r10, r1                         #! PC = 0x8000ec0 *)
smull r10 r12 r10 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000ec4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000ec8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8000ecc *)
mov r1 s18;
(* smull	r12, r11, r11, r1                         #! PC = 0x8000ed0 *)
smull r11 r12 r11 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000ed4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r11, lr, r3                          #! PC = 0x8000ed8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	s20, s21, r4, r5                           #! PC = 0x8000edc *)
mov s20 r4;
mov s21 r5;
(* vmov	s22, s23, r6, r7                           #! PC = 0x8000ee0 *)
mov s22 r6;
mov s23 r7;
(* vmov	s24, s25, r8, r9                           #! PC = 0x8000ee4 *)
mov s24 r8;
mov s25 r9;
(* vmov	s26, s27, r10, r11                         #! PC = 0x8000ee8 *)
mov s26 r10;
mov s27 r11;
(* vmov	lr, s0                                     #! PC = 0x8000eec *)
mov lr s0;
(* ldrsh.w	r5, [lr, #256]	; 0x100                  #! EA = L0x2001714e; Value = 0x0159fd23; PC = 0x8000ef0 *)
mov r5 L0x2001714e;
(* ldrsh.w	r6, [lr, #512]	; 0x200                  #! EA = L0x2001724e; Value = 0xfddefd60; PC = 0x8000ef4 *)
mov r6 L0x2001724e;
(* ldrsh.w	r7, [lr, #768]	; 0x300                  #! EA = L0x2001734e; Value = 0x035c0373; PC = 0x8000ef8 *)
mov r7 L0x2001734e;
(* ldrsh.w	r4, [lr], #2                            #! EA = L0x2001704e; Value = 0xfe30fdb4; PC = 0x8000efc *)
mov r4 L0x2001704e;
(* vmov	s0, lr                                     #! PC = 0x8000f00 *)
mov s0 lr;
(* sbfx	r4, r4, #0, #11                            #! PC = 0x8000f04 *)
mov r4_o r4;
split dontcare sbfxlow r4 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
assert eqmod r4 r4_o (2**11) && and [(-(2**10))@32 <=s r4, r4 <s (2**10)@32];
ghost cf063@sint32 : and [cf063 = r4] && true;
(* sbfx	r5, r5, #0, #11                            #! PC = 0x8000f08 *)
mov r5_o r5;
split dontcare sbfxlow r5 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
assert eqmod r5 r5_o (2**11) && and [(-(2**10))@32 <=s r5, r5 <s (2**10)@32];
ghost cf191@sint32 : and [cf191 = r5] && true;
(* sbfx	r6, r6, #0, #11                            #! PC = 0x8000f0c *)
mov r6_o r6;
split dontcare sbfxlow r6 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
assert eqmod r6 r6_o (2**11) && and [(-(2**10))@32 <=s r6, r6 <s (2**10)@32];
ghost cf319@sint32 : and [cf319 = r6] && true;
(* sbfx	r7, r7, #0, #11                            #! PC = 0x8000f10 *)
mov r7_o r7;
split dontcare sbfxlow r7 11;
split sbfxmsb dontcare sbfxlow 10;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff800@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
assert eqmod r7 r7_o (2**11) && and [(-(2**10))@32 <=s r7, r7 <s (2**10)@32];
ghost cf447@sint32 : and [cf447 = r7] && true;
(* vmov	r12, lr, s9, s10                           #! PC = 0x8000f14 *)
mov r12 s9;
mov lr s10;
(* smull	r8, r9, r5, r12                           #! PC = 0x8000f18 *)
smull r9 r8 r5 r12;
(* smlal	r8, r9, r7, lr                            #! PC = 0x8000f1c *)
smull tmpml_h tmpml_l r7 lr;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
(* mul.w	r1, r8, r2                                #! PC = 0x8000f20 *)
mull dontcare r1 r8 r2;
cast r1@sint32 r1;
(* smlal	r8, r9, r1, r3                            #! PC = 0x8000f24 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r8 r8 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* smull	r10, r11, r5, lr                          #! PC = 0x8000f28 *)
smull r11 r10 r5 lr;
(* smlal	r10, r11, r7, r12                         #! PC = 0x8000f2c *)
smull tmpml_h tmpml_l r7 r12;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
(* mul.w	r1, r10, r2                               #! PC = 0x8000f30 *)
mull dontcare r1 r10 r2;
cast r1@sint32 r1;
(* smlal	r10, r11, r1, r3                          #! PC = 0x8000f34 *)
smull tmpml_h tmpml_l r1 r3;
adds carry r10 r10 tmpml_l;
adc r11 r11 tmpml_h carry;
assert eqmod r10 0 (2**32) && true;
assume r10 = 0 && true;
(* vmov	r1, s6                                     #! PC = 0x8000f38 *)
mov r1 s6;
(* smull	r12, r10, r6, r1                          #! PC = 0x8000f3c *)
smull r10 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000f40 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r10, lr, r3                          #! PC = 0x8000f44 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r10 r10 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add.w	r8, r4, r10                               #! PC = 0x8000f48 *)
add r8 r4 r10;
(* sub.w	r10, r8, r10, lsl #1                      #! PC = 0x8000f4c *)
shl tmpx2 r10 1;
sub r10 r8 tmpx2;
(* add	r4, r6                                      #! PC = 0x8000f50 *)
add r4 r4 r6;
(* add	r5, r7                                      #! PC = 0x8000f52 *)
add r5 r5 r7;
(* sub.w	r6, r4, r6, lsl #1                        #! PC = 0x8000f54 *)
shl tmpx2 r6 1;
sub r6 r4 tmpx2;
(* sub.w	r7, r5, r7, lsl #1                        #! PC = 0x8000f58 *)
shl tmpx2 r7 1;
sub r7 r5 tmpx2;
(* smull	r12, r7, r7, r1                           #! PC = 0x8000f5c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8000f60 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8000f64 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r5                                      #! PC = 0x8000f68 *)
add r4 r4 r5;
(* add	r6, r7                                      #! PC = 0x8000f6a *)
add r6 r6 r7;
(* add	r8, r9                                      #! PC = 0x8000f6c *)
add r8 r8 r9;
(* add	r10, r11                                    #! PC = 0x8000f6e *)
add r10 r10 r11;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8000f70 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8000f74 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8000f78 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* sub.w	r11, r10, r11, lsl #1                     #! PC = 0x8000f7c *)
shl tmpx2 r11 1;
sub r11 r10 tmpx2;
(* vmov	s28, s29, r4, r5                           #! PC = 0x8000f80 *)
mov s28 r4;
mov s29 r5;
(* vmov	s30, s31, r6, r7                           #! PC = 0x8000f84 *)
mov s30 r6;
mov s31 r7;
(* vmov	r4, r5, s24, s25                           #! PC = 0x8000f88 *)
mov r4 s24;
mov r5 s25;
(* vmov	r6, r7, s26, s27                           #! PC = 0x8000f8c *)
mov r6 s26;
mov r7 s27;
(* add	r8, r4                                      #! PC = 0x8000f90 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000f92 *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000f94 *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000f96 *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000f98 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000f9c *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000fa0 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000fa4 *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r8, [r0, #2048]	; 0x800                   #! EA = L0x200198cc; PC = 0x8000fa8 *)
mov L0x200198cc r8;
(* str.w	r4, [r0, #2304]	; 0x900                   #! EA = L0x200199cc; PC = 0x8000fac *)
mov L0x200199cc r4;
(* str.w	r9, [r0, #2560]	; 0xa00                   #! EA = L0x20019acc; PC = 0x8000fb0 *)
mov L0x20019acc r9;
(* str.w	r5, [r0, #2816]	; 0xb00                   #! EA = L0x20019bcc; PC = 0x8000fb4 *)
mov L0x20019bcc r5;
(* str.w	r10, [r0, #3072]	; 0xc00                  #! EA = L0x20019ccc; PC = 0x8000fb8 *)
mov L0x20019ccc r10;
(* str.w	r6, [r0, #3328]	; 0xd00                   #! EA = L0x20019dcc; PC = 0x8000fbc *)
mov L0x20019dcc r6;
(* str.w	r11, [r0, #3584]	; 0xe00                  #! EA = L0x20019ecc; PC = 0x8000fc0 *)
mov L0x20019ecc r11;
(* str.w	r7, [r0, #3840]	; 0xf00                   #! EA = L0x20019fcc; PC = 0x8000fc4 *)
mov L0x20019fcc r7;
(* vmov	r4, r5, s20, s21                           #! PC = 0x8000fc8 *)
mov r4 s20;
mov r5 s21;
(* vmov	r6, r7, s22, s23                           #! PC = 0x8000fcc *)
mov r6 s22;
mov r7 s23;
(* vmov	r8, r9, s28, s29                           #! PC = 0x8000fd0 *)
mov r8 s28;
mov r9 s29;
(* vmov	r10, r11, s30, s31                         #! PC = 0x8000fd4 *)
mov r10 s30;
mov r11 s31;
(* add	r8, r4                                      #! PC = 0x8000fd8 *)
add r8 r8 r4;
(* add	r9, r5                                      #! PC = 0x8000fda *)
add r9 r9 r5;
(* add	r10, r6                                     #! PC = 0x8000fdc *)
add r10 r10 r6;
(* add	r11, r7                                     #! PC = 0x8000fde *)
add r11 r11 r7;
(* sub.w	r4, r8, r4, lsl #1                        #! PC = 0x8000fe0 *)
shl tmpx2 r4 1;
sub r4 r8 tmpx2;
(* sub.w	r5, r9, r5, lsl #1                        #! PC = 0x8000fe4 *)
shl tmpx2 r5 1;
sub r5 r9 tmpx2;
(* sub.w	r6, r10, r6, lsl #1                       #! PC = 0x8000fe8 *)
shl tmpx2 r6 1;
sub r6 r10 tmpx2;
(* sub.w	r7, r11, r7, lsl #1                       #! PC = 0x8000fec *)
shl tmpx2 r7 1;
sub r7 r11 tmpx2;
(* str.w	r10, [r0, #1024]	; 0x400                  #! EA = L0x200194cc; PC = 0x8000ff0 *)
mov L0x200194cc r10;
(* str.w	r6, [r0, #1280]	; 0x500                   #! EA = L0x200195cc; PC = 0x8000ff4 *)
mov L0x200195cc r6;
(* str.w	r11, [r0, #1536]	; 0x600                  #! EA = L0x200196cc; PC = 0x8000ff8 *)
mov L0x200196cc r11;
(* str.w	r7, [r0, #1792]	; 0x700                   #! EA = L0x200197cc; PC = 0x8000ffc *)
mov L0x200197cc r7;
(* str.w	r4, [r0, #256]	; 0x100                    #! EA = L0x200191cc; PC = 0x8001000 *)
mov L0x200191cc r4;
(* str.w	r9, [r0, #512]	; 0x200                    #! EA = L0x200192cc; PC = 0x8001004 *)
mov L0x200192cc r9;
(* str.w	r5, [r0, #768]	; 0x300                    #! EA = L0x200193cc; PC = 0x8001008 *)
mov L0x200193cc r5;
(* str.w	r8, [r0], #4                              #! EA = L0x200190cc; PC = 0x800100c *)
mov L0x200190cc r8;



(**************** CUT  63, - *****************)

ecut and [
eqmod cf063 f063 2**11, eqmod cf127 f127 2**11, eqmod cf191 f191 2**11,
eqmod cf255 f255 2**11, eqmod cf319 f319 2**11, eqmod cf383 f383 2**11,
eqmod cf447 f447 2**11,
eqmod L0x200190cc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -       1 ],
eqmod L0x200191cc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200192cc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200193cc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200194cc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200195cc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200196cc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200197cc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200198cc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200199cc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019acc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019bcc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019ccc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019dcc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019ecc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019fcc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  268244 ]
];




(* vmov	r12, s2                                    #! PC = 0x8001010 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x8001014 *)
(* cmp.w r0, r12 *)
nop;
(* #bne.w	0x8000dd0 <_0_1_2_3_last>                #! PC = 0x8001018 *)
#bne.w	0x8000dd0 <_0_1_2_3_last>                #! 0x8001018 = 0x8001018;
(* #vpop	{s16-s31}                                  #! PC = 0x800101c *)
#vpop	{%%s16-%%s31}                                  #! 0x800101c = 0x800101c;
(* #ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, r10, r11, r12, pc}#! EA = L0x20016f88; Value = 0x20016fd0; PC = 0x8001020 *)
#ldmia.w	sp!, {%%r4, %%r5, %%r6, %%r7, %%r8, %%r9, %%r10, %%r11, %%r12, pc}#! L0x20016f88 = L0x20016f88; 0x20016fd0 = 0x20016fd0; 0x8001020 = 0x8001020;
(* #add.w	r8, r8, #32                               #! PC = 0x800031c *)
#add.w	%%r8, %%r8, #32                               #! 0x800031c = 0x800031c;

(**************** center input poly ****************)

ghost cinp_poly@bit, 
cinp_poly_0@bit, cinp_poly_1@bit, cinp_poly_2@bit, cinp_poly_3@bit,
cinp_poly_4@bit, cinp_poly_5@bit, cinp_poly_6@bit, cinp_poly_7@bit :
and [
cinp_poly_0**2 = 
cf000*(x**0)+cf001*(x**1)+cf002*(x**2)+cf003*(x**3)+cf004*(x**4)+
cf005*(x**5)+cf006*(x**6)+cf007*(x**7)+cf008*(x**8)+cf009*(x**9)+
cf010*(x**10)+cf011*(x**11)+cf012*(x**12)+cf013*(x**13)+cf014*(x**14)+
cf015*(x**15)+cf016*(x**16)+cf017*(x**17)+cf018*(x**18)+cf019*(x**19)+
cf020*(x**20)+cf021*(x**21)+cf022*(x**22)+cf023*(x**23)+cf024*(x**24)+
cf025*(x**25)+cf026*(x**26)+cf027*(x**27)+cf028*(x**28)+cf029*(x**29)+
cf030*(x**30)+cf031*(x**31)+cf032*(x**32)+cf033*(x**33)+cf034*(x**34)+
cf035*(x**35)+cf036*(x**36)+cf037*(x**37)+cf038*(x**38)+cf039*(x**39)+
cf040*(x**40)+cf041*(x**41)+cf042*(x**42)+cf043*(x**43)+cf044*(x**44)+
cf045*(x**45)+cf046*(x**46)+cf047*(x**47)+cf048*(x**48)+cf049*(x**49)+
cf050*(x**50)+cf051*(x**51)+cf052*(x**52)+cf053*(x**53)+cf054*(x**54)+
cf055*(x**55)+cf056*(x**56)+cf057*(x**57)+cf058*(x**58)+cf059*(x**59)+
cf060*(x**60)+cf061*(x**61)+cf062*(x**62)+cf063*(x**63),
cinp_poly_1**2 = 
cf064*(x**0)+cf065*(x**1)+cf066*(x**2)+cf067*(x**3)+cf068*(x**4)+
cf069*(x**5)+cf070*(x**6)+cf071*(x**7)+cf072*(x**8)+cf073*(x**9)+
cf074*(x**10)+cf075*(x**11)+cf076*(x**12)+cf077*(x**13)+cf078*(x**14)+
cf079*(x**15)+cf080*(x**16)+cf081*(x**17)+cf082*(x**18)+cf083*(x**19)+
cf084*(x**20)+cf085*(x**21)+cf086*(x**22)+cf087*(x**23)+cf088*(x**24)+
cf089*(x**25)+cf090*(x**26)+cf091*(x**27)+cf092*(x**28)+cf093*(x**29)+
cf094*(x**30)+cf095*(x**31)+cf096*(x**32)+cf097*(x**33)+cf098*(x**34)+
cf099*(x**35)+cf100*(x**36)+cf101*(x**37)+cf102*(x**38)+cf103*(x**39)+
cf104*(x**40)+cf105*(x**41)+cf106*(x**42)+cf107*(x**43)+cf108*(x**44)+
cf109*(x**45)+cf110*(x**46)+cf111*(x**47)+cf112*(x**48)+cf113*(x**49)+
cf114*(x**50)+cf115*(x**51)+cf116*(x**52)+cf117*(x**53)+cf118*(x**54)+
cf119*(x**55)+cf120*(x**56)+cf121*(x**57)+cf122*(x**58)+cf123*(x**59)+
cf124*(x**60)+cf125*(x**61)+cf126*(x**62)+cf127*(x**63),
cinp_poly_2**2 = 
cf128*(x**0)+cf129*(x**1)+cf130*(x**2)+cf131*(x**3)+cf132*(x**4)+
cf133*(x**5)+cf134*(x**6)+cf135*(x**7)+cf136*(x**8)+cf137*(x**9)+
cf138*(x**10)+cf139*(x**11)+cf140*(x**12)+cf141*(x**13)+cf142*(x**14)+
cf143*(x**15)+cf144*(x**16)+cf145*(x**17)+cf146*(x**18)+cf147*(x**19)+
cf148*(x**20)+cf149*(x**21)+cf150*(x**22)+cf151*(x**23)+cf152*(x**24)+
cf153*(x**25)+cf154*(x**26)+cf155*(x**27)+cf156*(x**28)+cf157*(x**29)+
cf158*(x**30)+cf159*(x**31)+cf160*(x**32)+cf161*(x**33)+cf162*(x**34)+
cf163*(x**35)+cf164*(x**36)+cf165*(x**37)+cf166*(x**38)+cf167*(x**39)+
cf168*(x**40)+cf169*(x**41)+cf170*(x**42)+cf171*(x**43)+cf172*(x**44)+
cf173*(x**45)+cf174*(x**46)+cf175*(x**47)+cf176*(x**48)+cf177*(x**49)+
cf178*(x**50)+cf179*(x**51)+cf180*(x**52)+cf181*(x**53)+cf182*(x**54)+
cf183*(x**55)+cf184*(x**56)+cf185*(x**57)+cf186*(x**58)+cf187*(x**59)+
cf188*(x**60)+cf189*(x**61)+cf190*(x**62)+cf191*(x**63),
cinp_poly_3**2 = 
cf192*(x**0)+cf193*(x**1)+cf194*(x**2)+cf195*(x**3)+cf196*(x**4)+
cf197*(x**5)+cf198*(x**6)+cf199*(x**7)+cf200*(x**8)+cf201*(x**9)+
cf202*(x**10)+cf203*(x**11)+cf204*(x**12)+cf205*(x**13)+cf206*(x**14)+
cf207*(x**15)+cf208*(x**16)+cf209*(x**17)+cf210*(x**18)+cf211*(x**19)+
cf212*(x**20)+cf213*(x**21)+cf214*(x**22)+cf215*(x**23)+cf216*(x**24)+
cf217*(x**25)+cf218*(x**26)+cf219*(x**27)+cf220*(x**28)+cf221*(x**29)+
cf222*(x**30)+cf223*(x**31)+cf224*(x**32)+cf225*(x**33)+cf226*(x**34)+
cf227*(x**35)+cf228*(x**36)+cf229*(x**37)+cf230*(x**38)+cf231*(x**39)+
cf232*(x**40)+cf233*(x**41)+cf234*(x**42)+cf235*(x**43)+cf236*(x**44)+
cf237*(x**45)+cf238*(x**46)+cf239*(x**47)+cf240*(x**48)+cf241*(x**49)+
cf242*(x**50)+cf243*(x**51)+cf244*(x**52)+cf245*(x**53)+cf246*(x**54)+
cf247*(x**55)+cf248*(x**56)+cf249*(x**57)+cf250*(x**58)+cf251*(x**59)+
cf252*(x**60)+cf253*(x**61)+cf254*(x**62)+cf255*(x**63),
cinp_poly_4**2 = 
cf256*(x**0)+cf257*(x**1)+cf258*(x**2)+cf259*(x**3)+cf260*(x**4)+
cf261*(x**5)+cf262*(x**6)+cf263*(x**7)+cf264*(x**8)+cf265*(x**9)+
cf266*(x**10)+cf267*(x**11)+cf268*(x**12)+cf269*(x**13)+cf270*(x**14)+
cf271*(x**15)+cf272*(x**16)+cf273*(x**17)+cf274*(x**18)+cf275*(x**19)+
cf276*(x**20)+cf277*(x**21)+cf278*(x**22)+cf279*(x**23)+cf280*(x**24)+
cf281*(x**25)+cf282*(x**26)+cf283*(x**27)+cf284*(x**28)+cf285*(x**29)+
cf286*(x**30)+cf287*(x**31)+cf288*(x**32)+cf289*(x**33)+cf290*(x**34)+
cf291*(x**35)+cf292*(x**36)+cf293*(x**37)+cf294*(x**38)+cf295*(x**39)+
cf296*(x**40)+cf297*(x**41)+cf298*(x**42)+cf299*(x**43)+cf300*(x**44)+
cf301*(x**45)+cf302*(x**46)+cf303*(x**47)+cf304*(x**48)+cf305*(x**49)+
cf306*(x**50)+cf307*(x**51)+cf308*(x**52)+cf309*(x**53)+cf310*(x**54)+
cf311*(x**55)+cf312*(x**56)+cf313*(x**57)+cf314*(x**58)+cf315*(x**59)+
cf316*(x**60)+cf317*(x**61)+cf318*(x**62)+cf319*(x**63),
cinp_poly_5**2 = 
cf320*(x**0)+cf321*(x**1)+cf322*(x**2)+cf323*(x**3)+cf324*(x**4)+
cf325*(x**5)+cf326*(x**6)+cf327*(x**7)+cf328*(x**8)+cf329*(x**9)+
cf330*(x**10)+cf331*(x**11)+cf332*(x**12)+cf333*(x**13)+cf334*(x**14)+
cf335*(x**15)+cf336*(x**16)+cf337*(x**17)+cf338*(x**18)+cf339*(x**19)+
cf340*(x**20)+cf341*(x**21)+cf342*(x**22)+cf343*(x**23)+cf344*(x**24)+
cf345*(x**25)+cf346*(x**26)+cf347*(x**27)+cf348*(x**28)+cf349*(x**29)+
cf350*(x**30)+cf351*(x**31)+cf352*(x**32)+cf353*(x**33)+cf354*(x**34)+
cf355*(x**35)+cf356*(x**36)+cf357*(x**37)+cf358*(x**38)+cf359*(x**39)+
cf360*(x**40)+cf361*(x**41)+cf362*(x**42)+cf363*(x**43)+cf364*(x**44)+
cf365*(x**45)+cf366*(x**46)+cf367*(x**47)+cf368*(x**48)+cf369*(x**49)+
cf370*(x**50)+cf371*(x**51)+cf372*(x**52)+cf373*(x**53)+cf374*(x**54)+
cf375*(x**55)+cf376*(x**56)+cf377*(x**57)+cf378*(x**58)+cf379*(x**59)+
cf380*(x**60)+cf381*(x**61)+cf382*(x**62)+cf383*(x**63),
cinp_poly_6**2 = 
cf384*(x**0)+cf385*(x**1)+cf386*(x**2)+cf387*(x**3)+cf388*(x**4)+
cf389*(x**5)+cf390*(x**6)+cf391*(x**7)+cf392*(x**8)+cf393*(x**9)+
cf394*(x**10)+cf395*(x**11)+cf396*(x**12)+cf397*(x**13)+cf398*(x**14)+
cf399*(x**15)+cf400*(x**16)+cf401*(x**17)+cf402*(x**18)+cf403*(x**19)+
cf404*(x**20)+cf405*(x**21)+cf406*(x**22)+cf407*(x**23)+cf408*(x**24)+
cf409*(x**25)+cf410*(x**26)+cf411*(x**27)+cf412*(x**28)+cf413*(x**29)+
cf414*(x**30)+cf415*(x**31)+cf416*(x**32)+cf417*(x**33)+cf418*(x**34)+
cf419*(x**35)+cf420*(x**36)+cf421*(x**37)+cf422*(x**38)+cf423*(x**39)+
cf424*(x**40)+cf425*(x**41)+cf426*(x**42)+cf427*(x**43)+cf428*(x**44)+
cf429*(x**45)+cf430*(x**46)+cf431*(x**47)+cf432*(x**48)+cf433*(x**49)+
cf434*(x**50)+cf435*(x**51)+cf436*(x**52)+cf437*(x**53)+cf438*(x**54)+
cf439*(x**55)+cf440*(x**56)+cf441*(x**57)+cf442*(x**58)+cf443*(x**59)+
cf444*(x**60)+cf445*(x**61)+cf446*(x**62)+cf447*(x**63),
cinp_poly_7**2 = 
cf448*(x**0)+cf449*(x**1)+cf450*(x**2)+cf451*(x**3)+cf452*(x**4)+
cf453*(x**5)+cf454*(x**6)+cf455*(x**7)+cf456*(x**8)+cf457*(x**9)+
cf458*(x**10)+cf459*(x**11)+cf460*(x**12)+cf461*(x**13)+cf462*(x**14)+
cf463*(x**15)+cf464*(x**16)+cf465*(x**17)+cf466*(x**18)+cf467*(x**19)+
cf468*(x**20)+cf469*(x**21)+cf470*(x**22)+cf471*(x**23)+cf472*(x**24)+
cf473*(x**25)+cf474*(x**26)+cf475*(x**27)+cf476*(x**28)+cf477*(x**29)+
cf478*(x**30)+cf479*(x**31)+cf480*(x**32)+cf481*(x**33)+cf482*(x**34)+
cf483*(x**35)+cf484*(x**36)+cf485*(x**37)+cf486*(x**38)+cf487*(x**39)+
cf488*(x**40)+cf489*(x**41)+cf490*(x**42)+cf491*(x**43)+cf492*(x**44)+
cf493*(x**45)+cf494*(x**46)+cf495*(x**47)+cf496*(x**48)+cf497*(x**49)+
cf498*(x**50)+cf499*(x**51)+cf500*(x**52)+cf501*(x**53)+cf502*(x**54)+
cf503*(x**55)+cf504*(x**56)+cf505*(x**57)+cf506*(x**58)+cf507*(x**59)+
cf508*(x**60),
cinp_poly**2 = 
(cinp_poly_0**2)*(x**0)+(cinp_poly_1**2)*(x**64)+(cinp_poly_2**2)*(x**128)+
(cinp_poly_3**2)*(x**192)+(cinp_poly_4**2)*(x**256)+(cinp_poly_5**2)*(x**320)+
(cinp_poly_6**2)*(x**384)+(cinp_poly_7**2)*(x**448)
] && true;



(**************** summary ****************)


(**************** CUT  64, - *****************)

ecut and [
eqmod cf000 f000 2**11, eqmod cf001 f001 2**11, eqmod cf002 f002 2**11,
eqmod cf003 f003 2**11, eqmod cf004 f004 2**11, eqmod cf005 f005 2**11,
eqmod cf006 f006 2**11, eqmod cf007 f007 2**11, eqmod cf008 f008 2**11,
eqmod cf009 f009 2**11, eqmod cf010 f010 2**11, eqmod cf011 f011 2**11,
eqmod cf012 f012 2**11, eqmod cf013 f013 2**11, eqmod cf014 f014 2**11,
eqmod cf015 f015 2**11, eqmod cf016 f016 2**11, eqmod cf017 f017 2**11,
eqmod cf018 f018 2**11, eqmod cf019 f019 2**11, eqmod cf020 f020 2**11,
eqmod cf021 f021 2**11, eqmod cf022 f022 2**11, eqmod cf023 f023 2**11,
eqmod cf024 f024 2**11, eqmod cf025 f025 2**11, eqmod cf026 f026 2**11,
eqmod cf027 f027 2**11, eqmod cf028 f028 2**11, eqmod cf029 f029 2**11,
eqmod cf030 f030 2**11, eqmod cf031 f031 2**11, eqmod cf032 f032 2**11,
eqmod cf033 f033 2**11, eqmod cf034 f034 2**11, eqmod cf035 f035 2**11,
eqmod cf036 f036 2**11, eqmod cf037 f037 2**11, eqmod cf038 f038 2**11,
eqmod cf039 f039 2**11, eqmod cf040 f040 2**11, eqmod cf041 f041 2**11,
eqmod cf042 f042 2**11, eqmod cf043 f043 2**11, eqmod cf044 f044 2**11,
eqmod cf045 f045 2**11, eqmod cf046 f046 2**11, eqmod cf047 f047 2**11,
eqmod cf048 f048 2**11, eqmod cf049 f049 2**11, eqmod cf050 f050 2**11,
eqmod cf051 f051 2**11, eqmod cf052 f052 2**11, eqmod cf053 f053 2**11,
eqmod cf054 f054 2**11, eqmod cf055 f055 2**11, eqmod cf056 f056 2**11,
eqmod cf057 f057 2**11, eqmod cf058 f058 2**11, eqmod cf059 f059 2**11,
eqmod cf060 f060 2**11, eqmod cf061 f061 2**11, eqmod cf062 f062 2**11,
eqmod cf063 f063 2**11, eqmod cf064 f064 2**11, eqmod cf065 f065 2**11,
eqmod cf066 f066 2**11, eqmod cf067 f067 2**11, eqmod cf068 f068 2**11,
eqmod cf069 f069 2**11, eqmod cf070 f070 2**11, eqmod cf071 f071 2**11,
eqmod cf072 f072 2**11, eqmod cf073 f073 2**11, eqmod cf074 f074 2**11,
eqmod cf075 f075 2**11, eqmod cf076 f076 2**11, eqmod cf077 f077 2**11,
eqmod cf078 f078 2**11, eqmod cf079 f079 2**11, eqmod cf080 f080 2**11,
eqmod cf081 f081 2**11, eqmod cf082 f082 2**11, eqmod cf083 f083 2**11,
eqmod cf084 f084 2**11, eqmod cf085 f085 2**11, eqmod cf086 f086 2**11,
eqmod cf087 f087 2**11, eqmod cf088 f088 2**11, eqmod cf089 f089 2**11,
eqmod cf090 f090 2**11, eqmod cf091 f091 2**11, eqmod cf092 f092 2**11,
eqmod cf093 f093 2**11, eqmod cf094 f094 2**11, eqmod cf095 f095 2**11,
eqmod cf096 f096 2**11, eqmod cf097 f097 2**11, eqmod cf098 f098 2**11,
eqmod cf099 f099 2**11, eqmod cf100 f100 2**11, eqmod cf101 f101 2**11,
eqmod cf102 f102 2**11, eqmod cf103 f103 2**11, eqmod cf104 f104 2**11,
eqmod cf105 f105 2**11, eqmod cf106 f106 2**11, eqmod cf107 f107 2**11,
eqmod cf108 f108 2**11, eqmod cf109 f109 2**11, eqmod cf110 f110 2**11,
eqmod cf111 f111 2**11, eqmod cf112 f112 2**11, eqmod cf113 f113 2**11,
eqmod cf114 f114 2**11, eqmod cf115 f115 2**11, eqmod cf116 f116 2**11,
eqmod cf117 f117 2**11, eqmod cf118 f118 2**11, eqmod cf119 f119 2**11,
eqmod cf120 f120 2**11, eqmod cf121 f121 2**11, eqmod cf122 f122 2**11,
eqmod cf123 f123 2**11, eqmod cf124 f124 2**11, eqmod cf125 f125 2**11,
eqmod cf126 f126 2**11, eqmod cf127 f127 2**11, eqmod cf128 f128 2**11,
eqmod cf129 f129 2**11, eqmod cf130 f130 2**11, eqmod cf131 f131 2**11,
eqmod cf132 f132 2**11, eqmod cf133 f133 2**11, eqmod cf134 f134 2**11,
eqmod cf135 f135 2**11, eqmod cf136 f136 2**11, eqmod cf137 f137 2**11,
eqmod cf138 f138 2**11, eqmod cf139 f139 2**11, eqmod cf140 f140 2**11,
eqmod cf141 f141 2**11, eqmod cf142 f142 2**11, eqmod cf143 f143 2**11,
eqmod cf144 f144 2**11, eqmod cf145 f145 2**11, eqmod cf146 f146 2**11,
eqmod cf147 f147 2**11, eqmod cf148 f148 2**11, eqmod cf149 f149 2**11,
eqmod cf150 f150 2**11, eqmod cf151 f151 2**11, eqmod cf152 f152 2**11,
eqmod cf153 f153 2**11, eqmod cf154 f154 2**11, eqmod cf155 f155 2**11,
eqmod cf156 f156 2**11, eqmod cf157 f157 2**11, eqmod cf158 f158 2**11,
eqmod cf159 f159 2**11, eqmod cf160 f160 2**11, eqmod cf161 f161 2**11,
eqmod cf162 f162 2**11, eqmod cf163 f163 2**11, eqmod cf164 f164 2**11,
eqmod cf165 f165 2**11, eqmod cf166 f166 2**11, eqmod cf167 f167 2**11,
eqmod cf168 f168 2**11, eqmod cf169 f169 2**11, eqmod cf170 f170 2**11,
eqmod cf171 f171 2**11, eqmod cf172 f172 2**11, eqmod cf173 f173 2**11,
eqmod cf174 f174 2**11, eqmod cf175 f175 2**11, eqmod cf176 f176 2**11,
eqmod cf177 f177 2**11, eqmod cf178 f178 2**11, eqmod cf179 f179 2**11,
eqmod cf180 f180 2**11, eqmod cf181 f181 2**11, eqmod cf182 f182 2**11,
eqmod cf183 f183 2**11, eqmod cf184 f184 2**11, eqmod cf185 f185 2**11,
eqmod cf186 f186 2**11, eqmod cf187 f187 2**11, eqmod cf188 f188 2**11,
eqmod cf189 f189 2**11, eqmod cf190 f190 2**11, eqmod cf191 f191 2**11,
eqmod cf192 f192 2**11, eqmod cf193 f193 2**11, eqmod cf194 f194 2**11,
eqmod cf195 f195 2**11, eqmod cf196 f196 2**11, eqmod cf197 f197 2**11,
eqmod cf198 f198 2**11, eqmod cf199 f199 2**11, eqmod cf200 f200 2**11,
eqmod cf201 f201 2**11, eqmod cf202 f202 2**11, eqmod cf203 f203 2**11,
eqmod cf204 f204 2**11, eqmod cf205 f205 2**11, eqmod cf206 f206 2**11,
eqmod cf207 f207 2**11, eqmod cf208 f208 2**11, eqmod cf209 f209 2**11,
eqmod cf210 f210 2**11, eqmod cf211 f211 2**11, eqmod cf212 f212 2**11,
eqmod cf213 f213 2**11, eqmod cf214 f214 2**11, eqmod cf215 f215 2**11,
eqmod cf216 f216 2**11, eqmod cf217 f217 2**11, eqmod cf218 f218 2**11,
eqmod cf219 f219 2**11, eqmod cf220 f220 2**11, eqmod cf221 f221 2**11,
eqmod cf222 f222 2**11, eqmod cf223 f223 2**11, eqmod cf224 f224 2**11,
eqmod cf225 f225 2**11, eqmod cf226 f226 2**11, eqmod cf227 f227 2**11,
eqmod cf228 f228 2**11, eqmod cf229 f229 2**11, eqmod cf230 f230 2**11,
eqmod cf231 f231 2**11, eqmod cf232 f232 2**11, eqmod cf233 f233 2**11,
eqmod cf234 f234 2**11, eqmod cf235 f235 2**11, eqmod cf236 f236 2**11,
eqmod cf237 f237 2**11, eqmod cf238 f238 2**11, eqmod cf239 f239 2**11,
eqmod cf240 f240 2**11, eqmod cf241 f241 2**11, eqmod cf242 f242 2**11,
eqmod cf243 f243 2**11, eqmod cf244 f244 2**11, eqmod cf245 f245 2**11,
eqmod cf246 f246 2**11, eqmod cf247 f247 2**11, eqmod cf248 f248 2**11,
eqmod cf249 f249 2**11, eqmod cf250 f250 2**11, eqmod cf251 f251 2**11,
eqmod cf252 f252 2**11, eqmod cf253 f253 2**11, eqmod cf254 f254 2**11,
eqmod cf255 f255 2**11, eqmod cf256 f256 2**11, eqmod cf257 f257 2**11,
eqmod cf258 f258 2**11, eqmod cf259 f259 2**11, eqmod cf260 f260 2**11,
eqmod cf261 f261 2**11, eqmod cf262 f262 2**11, eqmod cf263 f263 2**11,
eqmod cf264 f264 2**11, eqmod cf265 f265 2**11, eqmod cf266 f266 2**11,
eqmod cf267 f267 2**11, eqmod cf268 f268 2**11, eqmod cf269 f269 2**11,
eqmod cf270 f270 2**11, eqmod cf271 f271 2**11, eqmod cf272 f272 2**11,
eqmod cf273 f273 2**11, eqmod cf274 f274 2**11, eqmod cf275 f275 2**11,
eqmod cf276 f276 2**11, eqmod cf277 f277 2**11, eqmod cf278 f278 2**11,
eqmod cf279 f279 2**11, eqmod cf280 f280 2**11, eqmod cf281 f281 2**11,
eqmod cf282 f282 2**11, eqmod cf283 f283 2**11, eqmod cf284 f284 2**11,
eqmod cf285 f285 2**11, eqmod cf286 f286 2**11, eqmod cf287 f287 2**11,
eqmod cf288 f288 2**11, eqmod cf289 f289 2**11, eqmod cf290 f290 2**11,
eqmod cf291 f291 2**11, eqmod cf292 f292 2**11, eqmod cf293 f293 2**11,
eqmod cf294 f294 2**11, eqmod cf295 f295 2**11, eqmod cf296 f296 2**11,
eqmod cf297 f297 2**11, eqmod cf298 f298 2**11, eqmod cf299 f299 2**11,
eqmod cf300 f300 2**11, eqmod cf301 f301 2**11, eqmod cf302 f302 2**11,
eqmod cf303 f303 2**11, eqmod cf304 f304 2**11, eqmod cf305 f305 2**11,
eqmod cf306 f306 2**11, eqmod cf307 f307 2**11, eqmod cf308 f308 2**11,
eqmod cf309 f309 2**11, eqmod cf310 f310 2**11, eqmod cf311 f311 2**11,
eqmod cf312 f312 2**11, eqmod cf313 f313 2**11, eqmod cf314 f314 2**11,
eqmod cf315 f315 2**11, eqmod cf316 f316 2**11, eqmod cf317 f317 2**11,
eqmod cf318 f318 2**11, eqmod cf319 f319 2**11, eqmod cf320 f320 2**11,
eqmod cf321 f321 2**11, eqmod cf322 f322 2**11, eqmod cf323 f323 2**11,
eqmod cf324 f324 2**11, eqmod cf325 f325 2**11, eqmod cf326 f326 2**11,
eqmod cf327 f327 2**11, eqmod cf328 f328 2**11, eqmod cf329 f329 2**11,
eqmod cf330 f330 2**11, eqmod cf331 f331 2**11, eqmod cf332 f332 2**11,
eqmod cf333 f333 2**11, eqmod cf334 f334 2**11, eqmod cf335 f335 2**11,
eqmod cf336 f336 2**11, eqmod cf337 f337 2**11, eqmod cf338 f338 2**11,
eqmod cf339 f339 2**11, eqmod cf340 f340 2**11, eqmod cf341 f341 2**11,
eqmod cf342 f342 2**11, eqmod cf343 f343 2**11, eqmod cf344 f344 2**11,
eqmod cf345 f345 2**11, eqmod cf346 f346 2**11, eqmod cf347 f347 2**11,
eqmod cf348 f348 2**11, eqmod cf349 f349 2**11, eqmod cf350 f350 2**11,
eqmod cf351 f351 2**11, eqmod cf352 f352 2**11, eqmod cf353 f353 2**11,
eqmod cf354 f354 2**11, eqmod cf355 f355 2**11, eqmod cf356 f356 2**11,
eqmod cf357 f357 2**11, eqmod cf358 f358 2**11, eqmod cf359 f359 2**11,
eqmod cf360 f360 2**11, eqmod cf361 f361 2**11, eqmod cf362 f362 2**11,
eqmod cf363 f363 2**11, eqmod cf364 f364 2**11, eqmod cf365 f365 2**11,
eqmod cf366 f366 2**11, eqmod cf367 f367 2**11, eqmod cf368 f368 2**11,
eqmod cf369 f369 2**11, eqmod cf370 f370 2**11, eqmod cf371 f371 2**11,
eqmod cf372 f372 2**11, eqmod cf373 f373 2**11, eqmod cf374 f374 2**11,
eqmod cf375 f375 2**11, eqmod cf376 f376 2**11, eqmod cf377 f377 2**11,
eqmod cf378 f378 2**11, eqmod cf379 f379 2**11, eqmod cf380 f380 2**11,
eqmod cf381 f381 2**11, eqmod cf382 f382 2**11, eqmod cf383 f383 2**11,
eqmod cf384 f384 2**11, eqmod cf385 f385 2**11, eqmod cf386 f386 2**11,
eqmod cf387 f387 2**11, eqmod cf388 f388 2**11, eqmod cf389 f389 2**11,
eqmod cf390 f390 2**11, eqmod cf391 f391 2**11, eqmod cf392 f392 2**11,
eqmod cf393 f393 2**11, eqmod cf394 f394 2**11, eqmod cf395 f395 2**11,
eqmod cf396 f396 2**11, eqmod cf397 f397 2**11, eqmod cf398 f398 2**11,
eqmod cf399 f399 2**11, eqmod cf400 f400 2**11, eqmod cf401 f401 2**11,
eqmod cf402 f402 2**11, eqmod cf403 f403 2**11, eqmod cf404 f404 2**11,
eqmod cf405 f405 2**11, eqmod cf406 f406 2**11, eqmod cf407 f407 2**11,
eqmod cf408 f408 2**11, eqmod cf409 f409 2**11, eqmod cf410 f410 2**11,
eqmod cf411 f411 2**11, eqmod cf412 f412 2**11, eqmod cf413 f413 2**11,
eqmod cf414 f414 2**11, eqmod cf415 f415 2**11, eqmod cf416 f416 2**11,
eqmod cf417 f417 2**11, eqmod cf418 f418 2**11, eqmod cf419 f419 2**11,
eqmod cf420 f420 2**11, eqmod cf421 f421 2**11, eqmod cf422 f422 2**11,
eqmod cf423 f423 2**11, eqmod cf424 f424 2**11, eqmod cf425 f425 2**11,
eqmod cf426 f426 2**11, eqmod cf427 f427 2**11, eqmod cf428 f428 2**11,
eqmod cf429 f429 2**11, eqmod cf430 f430 2**11, eqmod cf431 f431 2**11,
eqmod cf432 f432 2**11, eqmod cf433 f433 2**11, eqmod cf434 f434 2**11,
eqmod cf435 f435 2**11, eqmod cf436 f436 2**11, eqmod cf437 f437 2**11,
eqmod cf438 f438 2**11, eqmod cf439 f439 2**11, eqmod cf440 f440 2**11,
eqmod cf441 f441 2**11, eqmod cf442 f442 2**11, eqmod cf443 f443 2**11,
eqmod cf444 f444 2**11, eqmod cf445 f445 2**11, eqmod cf446 f446 2**11,
eqmod cf447 f447 2**11, eqmod cf448 f448 2**11, eqmod cf449 f449 2**11,
eqmod cf450 f450 2**11, eqmod cf451 f451 2**11, eqmod cf452 f452 2**11,
eqmod cf453 f453 2**11, eqmod cf454 f454 2**11, eqmod cf455 f455 2**11,
eqmod cf456 f456 2**11, eqmod cf457 f457 2**11, eqmod cf458 f458 2**11,
eqmod cf459 f459 2**11, eqmod cf460 f460 2**11, eqmod cf461 f461 2**11,
eqmod cf462 f462 2**11, eqmod cf463 f463 2**11, eqmod cf464 f464 2**11,
eqmod cf465 f465 2**11, eqmod cf466 f466 2**11, eqmod cf467 f467 2**11,
eqmod cf468 f468 2**11, eqmod cf469 f469 2**11, eqmod cf470 f470 2**11,
eqmod cf471 f471 2**11, eqmod cf472 f472 2**11, eqmod cf473 f473 2**11,
eqmod cf474 f474 2**11, eqmod cf475 f475 2**11, eqmod cf476 f476 2**11,
eqmod cf477 f477 2**11, eqmod cf478 f478 2**11, eqmod cf479 f479 2**11,
eqmod cf480 f480 2**11, eqmod cf481 f481 2**11, eqmod cf482 f482 2**11,
eqmod cf483 f483 2**11, eqmod cf484 f484 2**11, eqmod cf485 f485 2**11,
eqmod cf486 f486 2**11, eqmod cf487 f487 2**11, eqmod cf488 f488 2**11,
eqmod cf489 f489 2**11, eqmod cf490 f490 2**11, eqmod cf491 f491 2**11,
eqmod cf492 f492 2**11, eqmod cf493 f493 2**11, eqmod cf494 f494 2**11,
eqmod cf495 f495 2**11, eqmod cf496 f496 2**11, eqmod cf497 f497 2**11,
eqmod cf498 f498 2**11, eqmod cf499 f499 2**11, eqmod cf500 f500 2**11,
eqmod cf501 f501 2**11, eqmod cf502 f502 2**11, eqmod cf503 f503 2**11,
eqmod cf504 f504 2**11, eqmod cf505 f505 2**11, eqmod cf506 f506 2**11,
eqmod cf507 f507 2**11, eqmod cf508 f508 2**11
] prove with [ cuts [  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,
                      13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,
                      26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
                      39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
                      52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63 ] ];

(**************** CUT  65, - *****************)

ecut eqmod inp_poly**2 cinp_poly**2 2**11 prove with [ all ghosts ];

(**************** CUT  66, - *****************)

ecut and [
eqmod L0x20018fd0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -       1 ],
eqmod L0x20018fd4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -       1 ],
eqmod L0x20018fd8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -       1 ],
eqmod L0x20018fdc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -       1 ],
eqmod L0x20018fe0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -       1 ],
eqmod L0x20018fe4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -       1 ],
eqmod L0x20018fe8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -       1 ],
eqmod L0x20018fec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -       1 ],
eqmod L0x20018ff0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -       1 ],
eqmod L0x20018ff4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -       1 ],
eqmod L0x20018ff8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -       1 ],
eqmod L0x20018ffc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019000*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019004*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019008*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -       1 ],
eqmod L0x2001900c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019010*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019014*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019018*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -       1 ],
eqmod L0x2001901c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019020*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019024*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019028*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -       1 ],
eqmod L0x2001902c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019030*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019034*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019038*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -       1 ],
eqmod L0x2001903c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019040*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019044*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019048*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -       1 ],
eqmod L0x2001904c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019050*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019054*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019058*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -       1 ],
eqmod L0x2001905c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019060*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019064*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019068*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -       1 ],
eqmod L0x2001906c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019070*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019074*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019078*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -       1 ],
eqmod L0x2001907c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019080*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019084*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019088*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -       1 ],
eqmod L0x2001908c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019090*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019094*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -       1 ],
eqmod L0x20019098*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -       1 ],
eqmod L0x2001909c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190a0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190a4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190a8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190ac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190b0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190b4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190b8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190bc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190c0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190c4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190c8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -       1 ],
eqmod L0x200190cc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -       1 ]
] prove with [ cuts [  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,
                      13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,
                      26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
                      39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
                      52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63 ] ];

(**************** CUT  67, - *****************)

ecut eqmod (cinp_poly**2)
    (L0x20018fd0*(x** 0)+L0x20018fd4*(x** 1)+L0x20018fd8*(x** 2)+
     L0x20018fdc*(x** 3)+L0x20018fe0*(x** 4)+L0x20018fe4*(x** 5)+
     L0x20018fe8*(x** 6)+L0x20018fec*(x** 7)+L0x20018ff0*(x** 8)+
     L0x20018ff4*(x** 9)+L0x20018ff8*(x**10)+L0x20018ffc*(x**11)+
     L0x20019000*(x**12)+L0x20019004*(x**13)+L0x20019008*(x**14)+
     L0x2001900c*(x**15)+L0x20019010*(x**16)+L0x20019014*(x**17)+
     L0x20019018*(x**18)+L0x2001901c*(x**19)+L0x20019020*(x**20)+
     L0x20019024*(x**21)+L0x20019028*(x**22)+L0x2001902c*(x**23)+
     L0x20019030*(x**24)+L0x20019034*(x**25)+L0x20019038*(x**26)+
     L0x2001903c*(x**27)+L0x20019040*(x**28)+L0x20019044*(x**29)+
     L0x20019048*(x**30)+L0x2001904c*(x**31)+L0x20019050*(x**32)+
     L0x20019054*(x**33)+L0x20019058*(x**34)+L0x2001905c*(x**35)+
     L0x20019060*(x**36)+L0x20019064*(x**37)+L0x20019068*(x**38)+
     L0x2001906c*(x**39)+L0x20019070*(x**40)+L0x20019074*(x**41)+
     L0x20019078*(x**42)+L0x2001907c*(x**43)+L0x20019080*(x**44)+
     L0x20019084*(x**45)+L0x20019088*(x**46)+L0x2001908c*(x**47)+
     L0x20019090*(x**48)+L0x20019094*(x**49)+L0x20019098*(x**50)+
     L0x2001909c*(x**51)+L0x200190a0*(x**52)+L0x200190a4*(x**53)+
     L0x200190a8*(x**54)+L0x200190ac*(x**55)+L0x200190b0*(x**56)+
     L0x200190b4*(x**57)+L0x200190b8*(x**58)+L0x200190bc*(x**59)+
     L0x200190c0*(x**60)+L0x200190c4*(x**61)+L0x200190c8*(x**62)+
     L0x200190cc*(x**63))
    [ 1043969, x**64 -       1 ] prove with [ all ghosts ];

(**************** CUT  68, - *****************)

ecut and [
eqmod L0x200190d0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200190d4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200190d8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200190dc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200190e0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200190e4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200190e8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200190ec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200190f0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200190f4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200190f8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200190fc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019100*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019104*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019108*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x2001910c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019110*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019114*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019118*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x2001911c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019120*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019124*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019128*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x2001912c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019130*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019134*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019138*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x2001913c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019140*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019144*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019148*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x2001914c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019150*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019154*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019158*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x2001915c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019160*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019164*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019168*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x2001916c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019170*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019174*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019178*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x2001917c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019180*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019184*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019188*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x2001918c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019190*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019194*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x20019198*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x2001919c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191a0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191a4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191a8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191ac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191b0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191b4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191b8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191bc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191c0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191c4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191c8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 - 1043968 ],
eqmod L0x200191cc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 - 1043968 ]
] prove with [ cuts [  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,
                      13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,
                      26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
                      39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
                      52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63 ] ];

(**************** CUT  69, - *****************)

ecut eqmod (cinp_poly**2)
    (L0x200190d0*(x** 0)+L0x200190d4*(x** 1)+L0x200190d8*(x** 2)+
     L0x200190dc*(x** 3)+L0x200190e0*(x** 4)+L0x200190e4*(x** 5)+
     L0x200190e8*(x** 6)+L0x200190ec*(x** 7)+L0x200190f0*(x** 8)+
     L0x200190f4*(x** 9)+L0x200190f8*(x**10)+L0x200190fc*(x**11)+
     L0x20019100*(x**12)+L0x20019104*(x**13)+L0x20019108*(x**14)+
     L0x2001910c*(x**15)+L0x20019110*(x**16)+L0x20019114*(x**17)+
     L0x20019118*(x**18)+L0x2001911c*(x**19)+L0x20019120*(x**20)+
     L0x20019124*(x**21)+L0x20019128*(x**22)+L0x2001912c*(x**23)+
     L0x20019130*(x**24)+L0x20019134*(x**25)+L0x20019138*(x**26)+
     L0x2001913c*(x**27)+L0x20019140*(x**28)+L0x20019144*(x**29)+
     L0x20019148*(x**30)+L0x2001914c*(x**31)+L0x20019150*(x**32)+
     L0x20019154*(x**33)+L0x20019158*(x**34)+L0x2001915c*(x**35)+
     L0x20019160*(x**36)+L0x20019164*(x**37)+L0x20019168*(x**38)+
     L0x2001916c*(x**39)+L0x20019170*(x**40)+L0x20019174*(x**41)+
     L0x20019178*(x**42)+L0x2001917c*(x**43)+L0x20019180*(x**44)+
     L0x20019184*(x**45)+L0x20019188*(x**46)+L0x2001918c*(x**47)+
     L0x20019190*(x**48)+L0x20019194*(x**49)+L0x20019198*(x**50)+
     L0x2001919c*(x**51)+L0x200191a0*(x**52)+L0x200191a4*(x**53)+
     L0x200191a8*(x**54)+L0x200191ac*(x**55)+L0x200191b0*(x**56)+
     L0x200191b4*(x**57)+L0x200191b8*(x**58)+L0x200191bc*(x**59)+
     L0x200191c0*(x**60)+L0x200191c4*(x**61)+L0x200191c8*(x**62)+
     L0x200191cc*(x**63))
    [ 1043969, x**64 - 1043968 ] prove with [ all ghosts ];

(**************** CUT  70, - *****************)

ecut and [
eqmod L0x200191d0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200191d4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200191d8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200191dc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200191e0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200191e4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200191e8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200191ec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200191f0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200191f4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200191f8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200191fc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019200*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019204*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019208*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  554923 ],
eqmod L0x2001920c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019210*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019214*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019218*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  554923 ],
eqmod L0x2001921c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019220*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019224*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019228*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  554923 ],
eqmod L0x2001922c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019230*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019234*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019238*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  554923 ],
eqmod L0x2001923c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019240*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019244*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019248*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  554923 ],
eqmod L0x2001924c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019250*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019254*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019258*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  554923 ],
eqmod L0x2001925c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019260*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019264*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019268*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  554923 ],
eqmod L0x2001926c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019270*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019274*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019278*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  554923 ],
eqmod L0x2001927c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019280*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019284*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019288*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  554923 ],
eqmod L0x2001928c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019290*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019294*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  554923 ],
eqmod L0x20019298*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  554923 ],
eqmod L0x2001929c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192a0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192a4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192a8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192ac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192b0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192b4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192b8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192bc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192c0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192c4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192c8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  554923 ],
eqmod L0x200192cc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  554923 ]
] prove with [ cuts [  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,
                      13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,
                      26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
                      39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
                      52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63 ] ];

(**************** CUT  71, - *****************)

ecut eqmod (cinp_poly**2)
    (L0x200191d0*(x** 0)+L0x200191d4*(x** 1)+L0x200191d8*(x** 2)+
     L0x200191dc*(x** 3)+L0x200191e0*(x** 4)+L0x200191e4*(x** 5)+
     L0x200191e8*(x** 6)+L0x200191ec*(x** 7)+L0x200191f0*(x** 8)+
     L0x200191f4*(x** 9)+L0x200191f8*(x**10)+L0x200191fc*(x**11)+
     L0x20019200*(x**12)+L0x20019204*(x**13)+L0x20019208*(x**14)+
     L0x2001920c*(x**15)+L0x20019210*(x**16)+L0x20019214*(x**17)+
     L0x20019218*(x**18)+L0x2001921c*(x**19)+L0x20019220*(x**20)+
     L0x20019224*(x**21)+L0x20019228*(x**22)+L0x2001922c*(x**23)+
     L0x20019230*(x**24)+L0x20019234*(x**25)+L0x20019238*(x**26)+
     L0x2001923c*(x**27)+L0x20019240*(x**28)+L0x20019244*(x**29)+
     L0x20019248*(x**30)+L0x2001924c*(x**31)+L0x20019250*(x**32)+
     L0x20019254*(x**33)+L0x20019258*(x**34)+L0x2001925c*(x**35)+
     L0x20019260*(x**36)+L0x20019264*(x**37)+L0x20019268*(x**38)+
     L0x2001926c*(x**39)+L0x20019270*(x**40)+L0x20019274*(x**41)+
     L0x20019278*(x**42)+L0x2001927c*(x**43)+L0x20019280*(x**44)+
     L0x20019284*(x**45)+L0x20019288*(x**46)+L0x2001928c*(x**47)+
     L0x20019290*(x**48)+L0x20019294*(x**49)+L0x20019298*(x**50)+
     L0x2001929c*(x**51)+L0x200192a0*(x**52)+L0x200192a4*(x**53)+
     L0x200192a8*(x**54)+L0x200192ac*(x**55)+L0x200192b0*(x**56)+
     L0x200192b4*(x**57)+L0x200192b8*(x**58)+L0x200192bc*(x**59)+
     L0x200192c0*(x**60)+L0x200192c4*(x**61)+L0x200192c8*(x**62)+
     L0x200192cc*(x**63))
    [ 1043969, x**64 -  554923 ] prove with [ all ghosts ];

(**************** CUT  72, - *****************)

ecut and [
eqmod L0x200192d0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200192d4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200192d8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200192dc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200192e0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200192e4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200192e8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200192ec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200192f0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200192f4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200192f8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200192fc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019300*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019304*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019308*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  489046 ],
eqmod L0x2001930c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019310*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019314*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019318*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  489046 ],
eqmod L0x2001931c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019320*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019324*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019328*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  489046 ],
eqmod L0x2001932c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019330*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019334*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019338*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  489046 ],
eqmod L0x2001933c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019340*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019344*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019348*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  489046 ],
eqmod L0x2001934c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019350*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019354*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019358*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  489046 ],
eqmod L0x2001935c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019360*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019364*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019368*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  489046 ],
eqmod L0x2001936c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019370*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019374*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019378*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  489046 ],
eqmod L0x2001937c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019380*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019384*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019388*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  489046 ],
eqmod L0x2001938c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019390*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019394*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  489046 ],
eqmod L0x20019398*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  489046 ],
eqmod L0x2001939c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193a0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193a4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193a8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193ac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193b0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193b4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193b8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193bc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193c0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193c4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193c8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  489046 ],
eqmod L0x200193cc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  489046 ]
] prove with [ cuts [  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,
                      13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,
                      26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
                      39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
                      52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63 ] ];

(**************** CUT  73, - *****************)

ecut eqmod (cinp_poly**2)
    (L0x200192d0*(x** 0)+L0x200192d4*(x** 1)+L0x200192d8*(x** 2)+
     L0x200192dc*(x** 3)+L0x200192e0*(x** 4)+L0x200192e4*(x** 5)+
     L0x200192e8*(x** 6)+L0x200192ec*(x** 7)+L0x200192f0*(x** 8)+
     L0x200192f4*(x** 9)+L0x200192f8*(x**10)+L0x200192fc*(x**11)+
     L0x20019300*(x**12)+L0x20019304*(x**13)+L0x20019308*(x**14)+
     L0x2001930c*(x**15)+L0x20019310*(x**16)+L0x20019314*(x**17)+
     L0x20019318*(x**18)+L0x2001931c*(x**19)+L0x20019320*(x**20)+
     L0x20019324*(x**21)+L0x20019328*(x**22)+L0x2001932c*(x**23)+
     L0x20019330*(x**24)+L0x20019334*(x**25)+L0x20019338*(x**26)+
     L0x2001933c*(x**27)+L0x20019340*(x**28)+L0x20019344*(x**29)+
     L0x20019348*(x**30)+L0x2001934c*(x**31)+L0x20019350*(x**32)+
     L0x20019354*(x**33)+L0x20019358*(x**34)+L0x2001935c*(x**35)+
     L0x20019360*(x**36)+L0x20019364*(x**37)+L0x20019368*(x**38)+
     L0x2001936c*(x**39)+L0x20019370*(x**40)+L0x20019374*(x**41)+
     L0x20019378*(x**42)+L0x2001937c*(x**43)+L0x20019380*(x**44)+
     L0x20019384*(x**45)+L0x20019388*(x**46)+L0x2001938c*(x**47)+
     L0x20019390*(x**48)+L0x20019394*(x**49)+L0x20019398*(x**50)+
     L0x2001939c*(x**51)+L0x200193a0*(x**52)+L0x200193a4*(x**53)+
     L0x200193a8*(x**54)+L0x200193ac*(x**55)+L0x200193b0*(x**56)+
     L0x200193b4*(x**57)+L0x200193b8*(x**58)+L0x200193bc*(x**59)+
     L0x200193c0*(x**60)+L0x200193c4*(x**61)+L0x200193c8*(x**62)+
     L0x200193cc*(x**63))
    [ 1043969, x**64 -  489046 ] prove with [ all ghosts ];

(**************** CUT  74, - *****************)

ecut and [
eqmod L0x200193d0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200193d4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200193d8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200193dc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200193e0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200193e4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200193e8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200193ec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200193f0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200193f4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200193f8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200193fc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019400*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019404*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019408*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  287998 ],
eqmod L0x2001940c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019410*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019414*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019418*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  287998 ],
eqmod L0x2001941c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019420*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019424*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019428*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  287998 ],
eqmod L0x2001942c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019430*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019434*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019438*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  287998 ],
eqmod L0x2001943c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019440*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019444*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019448*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  287998 ],
eqmod L0x2001944c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019450*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019454*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019458*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  287998 ],
eqmod L0x2001945c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019460*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019464*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019468*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  287998 ],
eqmod L0x2001946c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019470*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019474*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019478*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  287998 ],
eqmod L0x2001947c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019480*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019484*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019488*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  287998 ],
eqmod L0x2001948c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019490*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019494*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  287998 ],
eqmod L0x20019498*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  287998 ],
eqmod L0x2001949c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194a0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194a4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194a8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194ac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194b0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194b4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194b8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194bc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194c0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194c4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194c8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  287998 ],
eqmod L0x200194cc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  287998 ]
] prove with [ cuts [  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,
                      13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,
                      26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
                      39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
                      52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63 ] ];

(**************** CUT  75, - *****************)

ecut eqmod (cinp_poly**2)
    (L0x200193d0*(x** 0)+L0x200193d4*(x** 1)+L0x200193d8*(x** 2)+
     L0x200193dc*(x** 3)+L0x200193e0*(x** 4)+L0x200193e4*(x** 5)+
     L0x200193e8*(x** 6)+L0x200193ec*(x** 7)+L0x200193f0*(x** 8)+
     L0x200193f4*(x** 9)+L0x200193f8*(x**10)+L0x200193fc*(x**11)+
     L0x20019400*(x**12)+L0x20019404*(x**13)+L0x20019408*(x**14)+
     L0x2001940c*(x**15)+L0x20019410*(x**16)+L0x20019414*(x**17)+
     L0x20019418*(x**18)+L0x2001941c*(x**19)+L0x20019420*(x**20)+
     L0x20019424*(x**21)+L0x20019428*(x**22)+L0x2001942c*(x**23)+
     L0x20019430*(x**24)+L0x20019434*(x**25)+L0x20019438*(x**26)+
     L0x2001943c*(x**27)+L0x20019440*(x**28)+L0x20019444*(x**29)+
     L0x20019448*(x**30)+L0x2001944c*(x**31)+L0x20019450*(x**32)+
     L0x20019454*(x**33)+L0x20019458*(x**34)+L0x2001945c*(x**35)+
     L0x20019460*(x**36)+L0x20019464*(x**37)+L0x20019468*(x**38)+
     L0x2001946c*(x**39)+L0x20019470*(x**40)+L0x20019474*(x**41)+
     L0x20019478*(x**42)+L0x2001947c*(x**43)+L0x20019480*(x**44)+
     L0x20019484*(x**45)+L0x20019488*(x**46)+L0x2001948c*(x**47)+
     L0x20019490*(x**48)+L0x20019494*(x**49)+L0x20019498*(x**50)+
     L0x2001949c*(x**51)+L0x200194a0*(x**52)+L0x200194a4*(x**53)+
     L0x200194a8*(x**54)+L0x200194ac*(x**55)+L0x200194b0*(x**56)+
     L0x200194b4*(x**57)+L0x200194b8*(x**58)+L0x200194bc*(x**59)+
     L0x200194c0*(x**60)+L0x200194c4*(x**61)+L0x200194c8*(x**62)+
     L0x200194cc*(x**63))
    [ 1043969, x**64 -  287998 ] prove with [ all ghosts ];

(**************** CUT  76, - *****************)

ecut and [
eqmod L0x200194d0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200194d4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200194d8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200194dc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200194e0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200194e4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200194e8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200194ec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200194f0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200194f4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200194f8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200194fc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019500*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019504*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019508*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  755971 ],
eqmod L0x2001950c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019510*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019514*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019518*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  755971 ],
eqmod L0x2001951c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019520*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019524*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019528*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  755971 ],
eqmod L0x2001952c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019530*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019534*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019538*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  755971 ],
eqmod L0x2001953c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019540*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019544*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019548*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  755971 ],
eqmod L0x2001954c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019550*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019554*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019558*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  755971 ],
eqmod L0x2001955c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019560*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019564*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019568*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  755971 ],
eqmod L0x2001956c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019570*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019574*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019578*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  755971 ],
eqmod L0x2001957c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019580*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019584*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019588*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  755971 ],
eqmod L0x2001958c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019590*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019594*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  755971 ],
eqmod L0x20019598*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  755971 ],
eqmod L0x2001959c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195a0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195a4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195a8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195ac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195b0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195b4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195b8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195bc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195c0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195c4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195c8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  755971 ],
eqmod L0x200195cc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  755971 ]
] prove with [ cuts [  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,
                      13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,
                      26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
                      39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
                      52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63 ] ];

(**************** CUT  77, - *****************)

ecut eqmod (cinp_poly**2)
    (L0x200194d0*(x** 0)+L0x200194d4*(x** 1)+L0x200194d8*(x** 2)+
     L0x200194dc*(x** 3)+L0x200194e0*(x** 4)+L0x200194e4*(x** 5)+
     L0x200194e8*(x** 6)+L0x200194ec*(x** 7)+L0x200194f0*(x** 8)+
     L0x200194f4*(x** 9)+L0x200194f8*(x**10)+L0x200194fc*(x**11)+
     L0x20019500*(x**12)+L0x20019504*(x**13)+L0x20019508*(x**14)+
     L0x2001950c*(x**15)+L0x20019510*(x**16)+L0x20019514*(x**17)+
     L0x20019518*(x**18)+L0x2001951c*(x**19)+L0x20019520*(x**20)+
     L0x20019524*(x**21)+L0x20019528*(x**22)+L0x2001952c*(x**23)+
     L0x20019530*(x**24)+L0x20019534*(x**25)+L0x20019538*(x**26)+
     L0x2001953c*(x**27)+L0x20019540*(x**28)+L0x20019544*(x**29)+
     L0x20019548*(x**30)+L0x2001954c*(x**31)+L0x20019550*(x**32)+
     L0x20019554*(x**33)+L0x20019558*(x**34)+L0x2001955c*(x**35)+
     L0x20019560*(x**36)+L0x20019564*(x**37)+L0x20019568*(x**38)+
     L0x2001956c*(x**39)+L0x20019570*(x**40)+L0x20019574*(x**41)+
     L0x20019578*(x**42)+L0x2001957c*(x**43)+L0x20019580*(x**44)+
     L0x20019584*(x**45)+L0x20019588*(x**46)+L0x2001958c*(x**47)+
     L0x20019590*(x**48)+L0x20019594*(x**49)+L0x20019598*(x**50)+
     L0x2001959c*(x**51)+L0x200195a0*(x**52)+L0x200195a4*(x**53)+
     L0x200195a8*(x**54)+L0x200195ac*(x**55)+L0x200195b0*(x**56)+
     L0x200195b4*(x**57)+L0x200195b8*(x**58)+L0x200195bc*(x**59)+
     L0x200195c0*(x**60)+L0x200195c4*(x**61)+L0x200195c8*(x**62)+
     L0x200195cc*(x**63))
    [ 1043969, x**64 -  755971 ] prove with [ all ghosts ];

(**************** CUT  78, - *****************)

ecut and [
eqmod L0x200195d0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200195d4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200195d8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200195dc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200195e0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200195e4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200195e8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200195ec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200195f0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200195f4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200195f8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200195fc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019600*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019604*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019608*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  719789 ],
eqmod L0x2001960c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019610*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019614*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019618*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  719789 ],
eqmod L0x2001961c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019620*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019624*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019628*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  719789 ],
eqmod L0x2001962c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019630*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019634*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019638*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  719789 ],
eqmod L0x2001963c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019640*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019644*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019648*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  719789 ],
eqmod L0x2001964c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019650*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019654*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019658*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  719789 ],
eqmod L0x2001965c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019660*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019664*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019668*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  719789 ],
eqmod L0x2001966c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019670*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019674*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019678*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  719789 ],
eqmod L0x2001967c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019680*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019684*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019688*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  719789 ],
eqmod L0x2001968c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019690*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019694*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  719789 ],
eqmod L0x20019698*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  719789 ],
eqmod L0x2001969c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196a0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196a4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196a8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196ac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196b0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196b4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196b8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196bc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196c0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196c4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196c8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  719789 ],
eqmod L0x200196cc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  719789 ]
] prove with [ cuts [  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,
                      13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,
                      26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
                      39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
                      52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63 ] ];

(**************** CUT  79, - *****************)

ecut eqmod (cinp_poly**2)
    (L0x200195d0*(x** 0)+L0x200195d4*(x** 1)+L0x200195d8*(x** 2)+
     L0x200195dc*(x** 3)+L0x200195e0*(x** 4)+L0x200195e4*(x** 5)+
     L0x200195e8*(x** 6)+L0x200195ec*(x** 7)+L0x200195f0*(x** 8)+
     L0x200195f4*(x** 9)+L0x200195f8*(x**10)+L0x200195fc*(x**11)+
     L0x20019600*(x**12)+L0x20019604*(x**13)+L0x20019608*(x**14)+
     L0x2001960c*(x**15)+L0x20019610*(x**16)+L0x20019614*(x**17)+
     L0x20019618*(x**18)+L0x2001961c*(x**19)+L0x20019620*(x**20)+
     L0x20019624*(x**21)+L0x20019628*(x**22)+L0x2001962c*(x**23)+
     L0x20019630*(x**24)+L0x20019634*(x**25)+L0x20019638*(x**26)+
     L0x2001963c*(x**27)+L0x20019640*(x**28)+L0x20019644*(x**29)+
     L0x20019648*(x**30)+L0x2001964c*(x**31)+L0x20019650*(x**32)+
     L0x20019654*(x**33)+L0x20019658*(x**34)+L0x2001965c*(x**35)+
     L0x20019660*(x**36)+L0x20019664*(x**37)+L0x20019668*(x**38)+
     L0x2001966c*(x**39)+L0x20019670*(x**40)+L0x20019674*(x**41)+
     L0x20019678*(x**42)+L0x2001967c*(x**43)+L0x20019680*(x**44)+
     L0x20019684*(x**45)+L0x20019688*(x**46)+L0x2001968c*(x**47)+
     L0x20019690*(x**48)+L0x20019694*(x**49)+L0x20019698*(x**50)+
     L0x2001969c*(x**51)+L0x200196a0*(x**52)+L0x200196a4*(x**53)+
     L0x200196a8*(x**54)+L0x200196ac*(x**55)+L0x200196b0*(x**56)+
     L0x200196b4*(x**57)+L0x200196b8*(x**58)+L0x200196bc*(x**59)+
     L0x200196c0*(x**60)+L0x200196c4*(x**61)+L0x200196c8*(x**62)+
     L0x200196cc*(x**63))
    [ 1043969, x**64 -  719789 ] prove with [ all ghosts ];

(**************** CUT  80, - *****************)

ecut and [
eqmod L0x200196d0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200196d4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200196d8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200196dc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200196e0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200196e4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200196e8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200196ec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200196f0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200196f4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200196f8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200196fc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019700*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019704*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019708*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  324180 ],
eqmod L0x2001970c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019710*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019714*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019718*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  324180 ],
eqmod L0x2001971c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019720*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019724*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019728*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  324180 ],
eqmod L0x2001972c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019730*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019734*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019738*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  324180 ],
eqmod L0x2001973c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019740*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019744*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019748*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  324180 ],
eqmod L0x2001974c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019750*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019754*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019758*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  324180 ],
eqmod L0x2001975c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019760*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019764*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019768*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  324180 ],
eqmod L0x2001976c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019770*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019774*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019778*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  324180 ],
eqmod L0x2001977c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019780*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019784*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019788*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  324180 ],
eqmod L0x2001978c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019790*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019794*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  324180 ],
eqmod L0x20019798*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  324180 ],
eqmod L0x2001979c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197a0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197a4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197a8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197ac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197b0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197b4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197b8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197bc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197c0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197c4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197c8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  324180 ],
eqmod L0x200197cc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  324180 ]
] prove with [ cuts [  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,
                      13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,
                      26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
                      39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
                      52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63 ] ];

(**************** CUT  81, - *****************)

ecut eqmod (cinp_poly**2)
    (L0x200196d0*(x** 0)+L0x200196d4*(x** 1)+L0x200196d8*(x** 2)+
     L0x200196dc*(x** 3)+L0x200196e0*(x** 4)+L0x200196e4*(x** 5)+
     L0x200196e8*(x** 6)+L0x200196ec*(x** 7)+L0x200196f0*(x** 8)+
     L0x200196f4*(x** 9)+L0x200196f8*(x**10)+L0x200196fc*(x**11)+
     L0x20019700*(x**12)+L0x20019704*(x**13)+L0x20019708*(x**14)+
     L0x2001970c*(x**15)+L0x20019710*(x**16)+L0x20019714*(x**17)+
     L0x20019718*(x**18)+L0x2001971c*(x**19)+L0x20019720*(x**20)+
     L0x20019724*(x**21)+L0x20019728*(x**22)+L0x2001972c*(x**23)+
     L0x20019730*(x**24)+L0x20019734*(x**25)+L0x20019738*(x**26)+
     L0x2001973c*(x**27)+L0x20019740*(x**28)+L0x20019744*(x**29)+
     L0x20019748*(x**30)+L0x2001974c*(x**31)+L0x20019750*(x**32)+
     L0x20019754*(x**33)+L0x20019758*(x**34)+L0x2001975c*(x**35)+
     L0x20019760*(x**36)+L0x20019764*(x**37)+L0x20019768*(x**38)+
     L0x2001976c*(x**39)+L0x20019770*(x**40)+L0x20019774*(x**41)+
     L0x20019778*(x**42)+L0x2001977c*(x**43)+L0x20019780*(x**44)+
     L0x20019784*(x**45)+L0x20019788*(x**46)+L0x2001978c*(x**47)+
     L0x20019790*(x**48)+L0x20019794*(x**49)+L0x20019798*(x**50)+
     L0x2001979c*(x**51)+L0x200197a0*(x**52)+L0x200197a4*(x**53)+
     L0x200197a8*(x**54)+L0x200197ac*(x**55)+L0x200197b0*(x**56)+
     L0x200197b4*(x**57)+L0x200197b8*(x**58)+L0x200197bc*(x**59)+
     L0x200197c0*(x**60)+L0x200197c4*(x**61)+L0x200197c8*(x**62)+
     L0x200197cc*(x**63))
    [ 1043969, x**64 -  324180 ] prove with [ all ghosts ];

(**************** CUT  82, - *****************)

ecut and [
eqmod L0x200197d0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200197d4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200197d8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200197dc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200197e0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200197e4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200197e8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200197ec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200197f0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200197f4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200197f8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200197fc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019800*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019804*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019808*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -   29512 ],
eqmod L0x2001980c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019810*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019814*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019818*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -   29512 ],
eqmod L0x2001981c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019820*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019824*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019828*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -   29512 ],
eqmod L0x2001982c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019830*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019834*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019838*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -   29512 ],
eqmod L0x2001983c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019840*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019844*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019848*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -   29512 ],
eqmod L0x2001984c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019850*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019854*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019858*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -   29512 ],
eqmod L0x2001985c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019860*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019864*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019868*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -   29512 ],
eqmod L0x2001986c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019870*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019874*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019878*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -   29512 ],
eqmod L0x2001987c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019880*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019884*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019888*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -   29512 ],
eqmod L0x2001988c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019890*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019894*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -   29512 ],
eqmod L0x20019898*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -   29512 ],
eqmod L0x2001989c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198a0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198a4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198a8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198ac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198b0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198b4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198b8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198bc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198c0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198c4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198c8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -   29512 ],
eqmod L0x200198cc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -   29512 ]
] prove with [ cuts [  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,
                      13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,
                      26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
                      39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
                      52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63 ] ];

(**************** CUT  83, - *****************)

ecut eqmod (cinp_poly**2)
    (L0x200197d0*(x** 0)+L0x200197d4*(x** 1)+L0x200197d8*(x** 2)+
     L0x200197dc*(x** 3)+L0x200197e0*(x** 4)+L0x200197e4*(x** 5)+
     L0x200197e8*(x** 6)+L0x200197ec*(x** 7)+L0x200197f0*(x** 8)+
     L0x200197f4*(x** 9)+L0x200197f8*(x**10)+L0x200197fc*(x**11)+
     L0x20019800*(x**12)+L0x20019804*(x**13)+L0x20019808*(x**14)+
     L0x2001980c*(x**15)+L0x20019810*(x**16)+L0x20019814*(x**17)+
     L0x20019818*(x**18)+L0x2001981c*(x**19)+L0x20019820*(x**20)+
     L0x20019824*(x**21)+L0x20019828*(x**22)+L0x2001982c*(x**23)+
     L0x20019830*(x**24)+L0x20019834*(x**25)+L0x20019838*(x**26)+
     L0x2001983c*(x**27)+L0x20019840*(x**28)+L0x20019844*(x**29)+
     L0x20019848*(x**30)+L0x2001984c*(x**31)+L0x20019850*(x**32)+
     L0x20019854*(x**33)+L0x20019858*(x**34)+L0x2001985c*(x**35)+
     L0x20019860*(x**36)+L0x20019864*(x**37)+L0x20019868*(x**38)+
     L0x2001986c*(x**39)+L0x20019870*(x**40)+L0x20019874*(x**41)+
     L0x20019878*(x**42)+L0x2001987c*(x**43)+L0x20019880*(x**44)+
     L0x20019884*(x**45)+L0x20019888*(x**46)+L0x2001988c*(x**47)+
     L0x20019890*(x**48)+L0x20019894*(x**49)+L0x20019898*(x**50)+
     L0x2001989c*(x**51)+L0x200198a0*(x**52)+L0x200198a4*(x**53)+
     L0x200198a8*(x**54)+L0x200198ac*(x**55)+L0x200198b0*(x**56)+
     L0x200198b4*(x**57)+L0x200198b8*(x**58)+L0x200198bc*(x**59)+
     L0x200198c0*(x**60)+L0x200198c4*(x**61)+L0x200198c8*(x**62)+
     L0x200198cc*(x**63))
    [ 1043969, x**64 -   29512 ] prove with [ all ghosts ];

(**************** CUT  84, - *****************)

ecut and [
eqmod L0x200198d0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200198d4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200198d8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200198dc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200198e0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200198e4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200198e8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200198ec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200198f0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200198f4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200198f8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200198fc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019900*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019904*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019908*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x2001990c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019910*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019914*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019918*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x2001991c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019920*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019924*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019928*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x2001992c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019930*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019934*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019938*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x2001993c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019940*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019944*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019948*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x2001994c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019950*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019954*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019958*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x2001995c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019960*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019964*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019968*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x2001996c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019970*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019974*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019978*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x2001997c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019980*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019984*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019988*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x2001998c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019990*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019994*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x20019998*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x2001999c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199a0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199a4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199a8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199ac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199b0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199b4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199b8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199bc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199c0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199c4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199c8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 - 1014457 ],
eqmod L0x200199cc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 - 1014457 ]
] prove with [ cuts [  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,
                      13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,
                      26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
                      39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
                      52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63 ] ];

(**************** CUT  85, - *****************)

ecut eqmod (cinp_poly**2)
    (L0x200198d0*(x** 0)+L0x200198d4*(x** 1)+L0x200198d8*(x** 2)+
     L0x200198dc*(x** 3)+L0x200198e0*(x** 4)+L0x200198e4*(x** 5)+
     L0x200198e8*(x** 6)+L0x200198ec*(x** 7)+L0x200198f0*(x** 8)+
     L0x200198f4*(x** 9)+L0x200198f8*(x**10)+L0x200198fc*(x**11)+
     L0x20019900*(x**12)+L0x20019904*(x**13)+L0x20019908*(x**14)+
     L0x2001990c*(x**15)+L0x20019910*(x**16)+L0x20019914*(x**17)+
     L0x20019918*(x**18)+L0x2001991c*(x**19)+L0x20019920*(x**20)+
     L0x20019924*(x**21)+L0x20019928*(x**22)+L0x2001992c*(x**23)+
     L0x20019930*(x**24)+L0x20019934*(x**25)+L0x20019938*(x**26)+
     L0x2001993c*(x**27)+L0x20019940*(x**28)+L0x20019944*(x**29)+
     L0x20019948*(x**30)+L0x2001994c*(x**31)+L0x20019950*(x**32)+
     L0x20019954*(x**33)+L0x20019958*(x**34)+L0x2001995c*(x**35)+
     L0x20019960*(x**36)+L0x20019964*(x**37)+L0x20019968*(x**38)+
     L0x2001996c*(x**39)+L0x20019970*(x**40)+L0x20019974*(x**41)+
     L0x20019978*(x**42)+L0x2001997c*(x**43)+L0x20019980*(x**44)+
     L0x20019984*(x**45)+L0x20019988*(x**46)+L0x2001998c*(x**47)+
     L0x20019990*(x**48)+L0x20019994*(x**49)+L0x20019998*(x**50)+
     L0x2001999c*(x**51)+L0x200199a0*(x**52)+L0x200199a4*(x**53)+
     L0x200199a8*(x**54)+L0x200199ac*(x**55)+L0x200199b0*(x**56)+
     L0x200199b4*(x**57)+L0x200199b8*(x**58)+L0x200199bc*(x**59)+
     L0x200199c0*(x**60)+L0x200199c4*(x**61)+L0x200199c8*(x**62)+
     L0x200199cc*(x**63))
    [ 1043969, x**64 - 1014457 ] prove with [ all ghosts ];

(**************** CUT  86, - *****************)

ecut and [
eqmod L0x200199d0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  145873 ],
eqmod L0x200199d4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  145873 ],
eqmod L0x200199d8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  145873 ],
eqmod L0x200199dc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  145873 ],
eqmod L0x200199e0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  145873 ],
eqmod L0x200199e4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  145873 ],
eqmod L0x200199e8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  145873 ],
eqmod L0x200199ec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  145873 ],
eqmod L0x200199f0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  145873 ],
eqmod L0x200199f4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  145873 ],
eqmod L0x200199f8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  145873 ],
eqmod L0x200199fc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a00*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a04*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a08*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a0c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a10*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a14*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a18*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a1c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a20*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a24*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a28*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a2c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a30*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a34*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a38*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a3c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a40*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a44*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a48*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a4c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a50*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a54*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a58*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a5c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a60*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a64*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a68*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a6c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a70*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a74*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a78*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a7c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a80*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a84*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a88*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a8c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a90*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a94*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a98*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019a9c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019aa0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019aa4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019aa8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019aac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019ab0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019ab4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019ab8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019abc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019ac0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019ac4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019ac8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  145873 ],
eqmod L0x20019acc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  145873 ]
] prove with [ cuts [  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,
                      13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,
                      26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
                      39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
                      52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63 ] ];

(**************** CUT  87, - *****************)

ecut eqmod (cinp_poly**2)
    (L0x200199d0*(x** 0)+L0x200199d4*(x** 1)+L0x200199d8*(x** 2)+
     L0x200199dc*(x** 3)+L0x200199e0*(x** 4)+L0x200199e4*(x** 5)+
     L0x200199e8*(x** 6)+L0x200199ec*(x** 7)+L0x200199f0*(x** 8)+
     L0x200199f4*(x** 9)+L0x200199f8*(x**10)+L0x200199fc*(x**11)+
     L0x20019a00*(x**12)+L0x20019a04*(x**13)+L0x20019a08*(x**14)+
     L0x20019a0c*(x**15)+L0x20019a10*(x**16)+L0x20019a14*(x**17)+
     L0x20019a18*(x**18)+L0x20019a1c*(x**19)+L0x20019a20*(x**20)+
     L0x20019a24*(x**21)+L0x20019a28*(x**22)+L0x20019a2c*(x**23)+
     L0x20019a30*(x**24)+L0x20019a34*(x**25)+L0x20019a38*(x**26)+
     L0x20019a3c*(x**27)+L0x20019a40*(x**28)+L0x20019a44*(x**29)+
     L0x20019a48*(x**30)+L0x20019a4c*(x**31)+L0x20019a50*(x**32)+
     L0x20019a54*(x**33)+L0x20019a58*(x**34)+L0x20019a5c*(x**35)+
     L0x20019a60*(x**36)+L0x20019a64*(x**37)+L0x20019a68*(x**38)+
     L0x20019a6c*(x**39)+L0x20019a70*(x**40)+L0x20019a74*(x**41)+
     L0x20019a78*(x**42)+L0x20019a7c*(x**43)+L0x20019a80*(x**44)+
     L0x20019a84*(x**45)+L0x20019a88*(x**46)+L0x20019a8c*(x**47)+
     L0x20019a90*(x**48)+L0x20019a94*(x**49)+L0x20019a98*(x**50)+
     L0x20019a9c*(x**51)+L0x20019aa0*(x**52)+L0x20019aa4*(x**53)+
     L0x20019aa8*(x**54)+L0x20019aac*(x**55)+L0x20019ab0*(x**56)+
     L0x20019ab4*(x**57)+L0x20019ab8*(x**58)+L0x20019abc*(x**59)+
     L0x20019ac0*(x**60)+L0x20019ac4*(x**61)+L0x20019ac8*(x**62)+
     L0x20019acc*(x**63))
    [ 1043969, x**64 -  145873 ] prove with [ all ghosts ];

(**************** CUT  88, - *****************)

ecut and [
eqmod L0x20019ad0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019ad4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019ad8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019adc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019ae0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019ae4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019ae8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019aec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019af0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019af4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019af8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019afc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b00*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b04*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b08*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b0c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b10*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b14*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b18*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b1c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b20*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b24*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b28*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b2c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b30*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b34*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b38*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b3c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b40*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b44*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b48*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b4c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b50*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b54*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b58*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b5c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b60*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b64*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b68*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b6c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b70*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b74*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b78*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b7c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b80*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b84*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b88*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b8c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b90*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b94*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b98*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019b9c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019ba0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019ba4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019ba8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019bac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019bb0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019bb4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019bb8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019bbc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019bc0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019bc4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019bc8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  898096 ],
eqmod L0x20019bcc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  898096 ]
] prove with [ cuts [  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,
                      13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,
                      26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
                      39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
                      52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63 ] ];

(**************** CUT  89, - *****************)

ecut eqmod (cinp_poly**2)
    (L0x20019ad0*(x** 0)+L0x20019ad4*(x** 1)+L0x20019ad8*(x** 2)+
     L0x20019adc*(x** 3)+L0x20019ae0*(x** 4)+L0x20019ae4*(x** 5)+
     L0x20019ae8*(x** 6)+L0x20019aec*(x** 7)+L0x20019af0*(x** 8)+
     L0x20019af4*(x** 9)+L0x20019af8*(x**10)+L0x20019afc*(x**11)+
     L0x20019b00*(x**12)+L0x20019b04*(x**13)+L0x20019b08*(x**14)+
     L0x20019b0c*(x**15)+L0x20019b10*(x**16)+L0x20019b14*(x**17)+
     L0x20019b18*(x**18)+L0x20019b1c*(x**19)+L0x20019b20*(x**20)+
     L0x20019b24*(x**21)+L0x20019b28*(x**22)+L0x20019b2c*(x**23)+
     L0x20019b30*(x**24)+L0x20019b34*(x**25)+L0x20019b38*(x**26)+
     L0x20019b3c*(x**27)+L0x20019b40*(x**28)+L0x20019b44*(x**29)+
     L0x20019b48*(x**30)+L0x20019b4c*(x**31)+L0x20019b50*(x**32)+
     L0x20019b54*(x**33)+L0x20019b58*(x**34)+L0x20019b5c*(x**35)+
     L0x20019b60*(x**36)+L0x20019b64*(x**37)+L0x20019b68*(x**38)+
     L0x20019b6c*(x**39)+L0x20019b70*(x**40)+L0x20019b74*(x**41)+
     L0x20019b78*(x**42)+L0x20019b7c*(x**43)+L0x20019b80*(x**44)+
     L0x20019b84*(x**45)+L0x20019b88*(x**46)+L0x20019b8c*(x**47)+
     L0x20019b90*(x**48)+L0x20019b94*(x**49)+L0x20019b98*(x**50)+
     L0x20019b9c*(x**51)+L0x20019ba0*(x**52)+L0x20019ba4*(x**53)+
     L0x20019ba8*(x**54)+L0x20019bac*(x**55)+L0x20019bb0*(x**56)+
     L0x20019bb4*(x**57)+L0x20019bb8*(x**58)+L0x20019bbc*(x**59)+
     L0x20019bc0*(x**60)+L0x20019bc4*(x**61)+L0x20019bc8*(x**62)+
     L0x20019bcc*(x**63))
    [ 1043969, x**64 -  898096 ] prove with [ all ghosts ];

(**************** CUT  90, - *****************)

ecut and [
eqmod L0x20019bd0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019bd4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019bd8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019bdc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019be0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019be4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019be8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019bec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019bf0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019bf4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019bf8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019bfc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c00*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c04*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c08*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c0c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c10*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c14*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c18*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c1c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c20*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c24*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c28*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c2c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c30*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c34*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c38*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c3c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c40*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c44*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c48*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c4c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c50*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c54*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c58*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c5c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c60*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c64*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c68*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c6c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c70*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c74*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c78*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c7c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c80*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c84*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c88*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c8c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c90*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c94*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c98*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019c9c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019ca0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019ca4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019ca8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019cac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019cb0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019cb4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019cb8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019cbc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019cc0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019cc4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019cc8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  445347 ],
eqmod L0x20019ccc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  445347 ]
] prove with [ cuts [  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,
                      13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,
                      26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
                      39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
                      52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63 ] ];

(**************** CUT  91, - *****************)

ecut eqmod (cinp_poly**2)
    (L0x20019bd0*(x** 0)+L0x20019bd4*(x** 1)+L0x20019bd8*(x** 2)+
     L0x20019bdc*(x** 3)+L0x20019be0*(x** 4)+L0x20019be4*(x** 5)+
     L0x20019be8*(x** 6)+L0x20019bec*(x** 7)+L0x20019bf0*(x** 8)+
     L0x20019bf4*(x** 9)+L0x20019bf8*(x**10)+L0x20019bfc*(x**11)+
     L0x20019c00*(x**12)+L0x20019c04*(x**13)+L0x20019c08*(x**14)+
     L0x20019c0c*(x**15)+L0x20019c10*(x**16)+L0x20019c14*(x**17)+
     L0x20019c18*(x**18)+L0x20019c1c*(x**19)+L0x20019c20*(x**20)+
     L0x20019c24*(x**21)+L0x20019c28*(x**22)+L0x20019c2c*(x**23)+
     L0x20019c30*(x**24)+L0x20019c34*(x**25)+L0x20019c38*(x**26)+
     L0x20019c3c*(x**27)+L0x20019c40*(x**28)+L0x20019c44*(x**29)+
     L0x20019c48*(x**30)+L0x20019c4c*(x**31)+L0x20019c50*(x**32)+
     L0x20019c54*(x**33)+L0x20019c58*(x**34)+L0x20019c5c*(x**35)+
     L0x20019c60*(x**36)+L0x20019c64*(x**37)+L0x20019c68*(x**38)+
     L0x20019c6c*(x**39)+L0x20019c70*(x**40)+L0x20019c74*(x**41)+
     L0x20019c78*(x**42)+L0x20019c7c*(x**43)+L0x20019c80*(x**44)+
     L0x20019c84*(x**45)+L0x20019c88*(x**46)+L0x20019c8c*(x**47)+
     L0x20019c90*(x**48)+L0x20019c94*(x**49)+L0x20019c98*(x**50)+
     L0x20019c9c*(x**51)+L0x20019ca0*(x**52)+L0x20019ca4*(x**53)+
     L0x20019ca8*(x**54)+L0x20019cac*(x**55)+L0x20019cb0*(x**56)+
     L0x20019cb4*(x**57)+L0x20019cb8*(x**58)+L0x20019cbc*(x**59)+
     L0x20019cc0*(x**60)+L0x20019cc4*(x**61)+L0x20019cc8*(x**62)+
     L0x20019ccc*(x**63))
    [ 1043969, x**64 -  445347 ] prove with [ all ghosts ];

(**************** CUT  92, - *****************)

ecut and [
eqmod L0x20019cd0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019cd4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019cd8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019cdc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019ce0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019ce4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019ce8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019cec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019cf0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019cf4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019cf8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019cfc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d00*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d04*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d08*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d0c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d10*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d14*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d18*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d1c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d20*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d24*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d28*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d2c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d30*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d34*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d38*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d3c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d40*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d44*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d48*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d4c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d50*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d54*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d58*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d5c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d60*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d64*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d68*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d6c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d70*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d74*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d78*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d7c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d80*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d84*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d88*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d8c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d90*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d94*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d98*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019d9c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019da0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019da4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019da8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019dac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019db0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019db4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019db8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019dbc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019dc0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019dc4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019dc8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  598622 ],
eqmod L0x20019dcc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  598622 ]
] prove with [ cuts [  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,
                      13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,
                      26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
                      39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
                      52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63 ] ];

(**************** CUT  93, - *****************)

ecut eqmod (cinp_poly**2)
    (L0x20019cd0*(x** 0)+L0x20019cd4*(x** 1)+L0x20019cd8*(x** 2)+
     L0x20019cdc*(x** 3)+L0x20019ce0*(x** 4)+L0x20019ce4*(x** 5)+
     L0x20019ce8*(x** 6)+L0x20019cec*(x** 7)+L0x20019cf0*(x** 8)+
     L0x20019cf4*(x** 9)+L0x20019cf8*(x**10)+L0x20019cfc*(x**11)+
     L0x20019d00*(x**12)+L0x20019d04*(x**13)+L0x20019d08*(x**14)+
     L0x20019d0c*(x**15)+L0x20019d10*(x**16)+L0x20019d14*(x**17)+
     L0x20019d18*(x**18)+L0x20019d1c*(x**19)+L0x20019d20*(x**20)+
     L0x20019d24*(x**21)+L0x20019d28*(x**22)+L0x20019d2c*(x**23)+
     L0x20019d30*(x**24)+L0x20019d34*(x**25)+L0x20019d38*(x**26)+
     L0x20019d3c*(x**27)+L0x20019d40*(x**28)+L0x20019d44*(x**29)+
     L0x20019d48*(x**30)+L0x20019d4c*(x**31)+L0x20019d50*(x**32)+
     L0x20019d54*(x**33)+L0x20019d58*(x**34)+L0x20019d5c*(x**35)+
     L0x20019d60*(x**36)+L0x20019d64*(x**37)+L0x20019d68*(x**38)+
     L0x20019d6c*(x**39)+L0x20019d70*(x**40)+L0x20019d74*(x**41)+
     L0x20019d78*(x**42)+L0x20019d7c*(x**43)+L0x20019d80*(x**44)+
     L0x20019d84*(x**45)+L0x20019d88*(x**46)+L0x20019d8c*(x**47)+
     L0x20019d90*(x**48)+L0x20019d94*(x**49)+L0x20019d98*(x**50)+
     L0x20019d9c*(x**51)+L0x20019da0*(x**52)+L0x20019da4*(x**53)+
     L0x20019da8*(x**54)+L0x20019dac*(x**55)+L0x20019db0*(x**56)+
     L0x20019db4*(x**57)+L0x20019db8*(x**58)+L0x20019dbc*(x**59)+
     L0x20019dc0*(x**60)+L0x20019dc4*(x**61)+L0x20019dc8*(x**62)+
     L0x20019dcc*(x**63))
    [ 1043969, x**64 -  598622 ] prove with [ all ghosts ];

(**************** CUT  94, - *****************)

ecut and [
eqmod L0x20019dd0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019dd4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019dd8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019ddc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019de0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019de4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019de8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019dec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019df0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019df4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019df8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019dfc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e00*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e04*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e08*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e0c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e10*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e14*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e18*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e1c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e20*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e24*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e28*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e2c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e30*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e34*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e38*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e3c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e40*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e44*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e48*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e4c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e50*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e54*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e58*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e5c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e60*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e64*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e68*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e6c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e70*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e74*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e78*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e7c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e80*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e84*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e88*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e8c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e90*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e94*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e98*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019e9c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019ea0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019ea4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019ea8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019eac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019eb0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019eb4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019eb8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019ebc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019ec0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019ec4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019ec8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  775725 ],
eqmod L0x20019ecc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  775725 ]
] prove with [ cuts [  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,
                      13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,
                      26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
                      39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
                      52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63 ] ];

(**************** CUT  95, - *****************)

ecut eqmod (cinp_poly**2)
    (L0x20019dd0*(x** 0)+L0x20019dd4*(x** 1)+L0x20019dd8*(x** 2)+
     L0x20019ddc*(x** 3)+L0x20019de0*(x** 4)+L0x20019de4*(x** 5)+
     L0x20019de8*(x** 6)+L0x20019dec*(x** 7)+L0x20019df0*(x** 8)+
     L0x20019df4*(x** 9)+L0x20019df8*(x**10)+L0x20019dfc*(x**11)+
     L0x20019e00*(x**12)+L0x20019e04*(x**13)+L0x20019e08*(x**14)+
     L0x20019e0c*(x**15)+L0x20019e10*(x**16)+L0x20019e14*(x**17)+
     L0x20019e18*(x**18)+L0x20019e1c*(x**19)+L0x20019e20*(x**20)+
     L0x20019e24*(x**21)+L0x20019e28*(x**22)+L0x20019e2c*(x**23)+
     L0x20019e30*(x**24)+L0x20019e34*(x**25)+L0x20019e38*(x**26)+
     L0x20019e3c*(x**27)+L0x20019e40*(x**28)+L0x20019e44*(x**29)+
     L0x20019e48*(x**30)+L0x20019e4c*(x**31)+L0x20019e50*(x**32)+
     L0x20019e54*(x**33)+L0x20019e58*(x**34)+L0x20019e5c*(x**35)+
     L0x20019e60*(x**36)+L0x20019e64*(x**37)+L0x20019e68*(x**38)+
     L0x20019e6c*(x**39)+L0x20019e70*(x**40)+L0x20019e74*(x**41)+
     L0x20019e78*(x**42)+L0x20019e7c*(x**43)+L0x20019e80*(x**44)+
     L0x20019e84*(x**45)+L0x20019e88*(x**46)+L0x20019e8c*(x**47)+
     L0x20019e90*(x**48)+L0x20019e94*(x**49)+L0x20019e98*(x**50)+
     L0x20019e9c*(x**51)+L0x20019ea0*(x**52)+L0x20019ea4*(x**53)+
     L0x20019ea8*(x**54)+L0x20019eac*(x**55)+L0x20019eb0*(x**56)+
     L0x20019eb4*(x**57)+L0x20019eb8*(x**58)+L0x20019ebc*(x**59)+
     L0x20019ec0*(x**60)+L0x20019ec4*(x**61)+L0x20019ec8*(x**62)+
     L0x20019ecc*(x**63))
    [ 1043969, x**64 -  775725 ] prove with [ all ghosts ];

(**************** CUT  96, - *****************)

ecut and [
eqmod L0x20019ed0*x**  0
      (cf000*x**  0+cf064*x** 64+cf128*x**128+cf192*x**192+
       cf256*x**256+cf320*x**320+cf384*x**384+cf448*x**448)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019ed4*x**  1
      (cf001*x**  1+cf065*x** 65+cf129*x**129+cf193*x**193+
       cf257*x**257+cf321*x**321+cf385*x**385+cf449*x**449)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019ed8*x**  2
      (cf002*x**  2+cf066*x** 66+cf130*x**130+cf194*x**194+
       cf258*x**258+cf322*x**322+cf386*x**386+cf450*x**450)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019edc*x**  3
      (cf003*x**  3+cf067*x** 67+cf131*x**131+cf195*x**195+
       cf259*x**259+cf323*x**323+cf387*x**387+cf451*x**451)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019ee0*x**  4
      (cf004*x**  4+cf068*x** 68+cf132*x**132+cf196*x**196+
       cf260*x**260+cf324*x**324+cf388*x**388+cf452*x**452)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019ee4*x**  5
      (cf005*x**  5+cf069*x** 69+cf133*x**133+cf197*x**197+
       cf261*x**261+cf325*x**325+cf389*x**389+cf453*x**453)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019ee8*x**  6
      (cf006*x**  6+cf070*x** 70+cf134*x**134+cf198*x**198+
       cf262*x**262+cf326*x**326+cf390*x**390+cf454*x**454)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019eec*x**  7
      (cf007*x**  7+cf071*x** 71+cf135*x**135+cf199*x**199+
       cf263*x**263+cf327*x**327+cf391*x**391+cf455*x**455)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019ef0*x**  8
      (cf008*x**  8+cf072*x** 72+cf136*x**136+cf200*x**200+
       cf264*x**264+cf328*x**328+cf392*x**392+cf456*x**456)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019ef4*x**  9
      (cf009*x**  9+cf073*x** 73+cf137*x**137+cf201*x**201+
       cf265*x**265+cf329*x**329+cf393*x**393+cf457*x**457)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019ef8*x** 10
      (cf010*x** 10+cf074*x** 74+cf138*x**138+cf202*x**202+
       cf266*x**266+cf330*x**330+cf394*x**394+cf458*x**458)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019efc*x** 11
      (cf011*x** 11+cf075*x** 75+cf139*x**139+cf203*x**203+
       cf267*x**267+cf331*x**331+cf395*x**395+cf459*x**459)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f00*x** 12
      (cf012*x** 12+cf076*x** 76+cf140*x**140+cf204*x**204+
       cf268*x**268+cf332*x**332+cf396*x**396+cf460*x**460)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f04*x** 13
      (cf013*x** 13+cf077*x** 77+cf141*x**141+cf205*x**205+
       cf269*x**269+cf333*x**333+cf397*x**397+cf461*x**461)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f08*x** 14
      (cf014*x** 14+cf078*x** 78+cf142*x**142+cf206*x**206+
       cf270*x**270+cf334*x**334+cf398*x**398+cf462*x**462)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f0c*x** 15
      (cf015*x** 15+cf079*x** 79+cf143*x**143+cf207*x**207+
       cf271*x**271+cf335*x**335+cf399*x**399+cf463*x**463)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f10*x** 16
      (cf016*x** 16+cf080*x** 80+cf144*x**144+cf208*x**208+
       cf272*x**272+cf336*x**336+cf400*x**400+cf464*x**464)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f14*x** 17
      (cf017*x** 17+cf081*x** 81+cf145*x**145+cf209*x**209+
       cf273*x**273+cf337*x**337+cf401*x**401+cf465*x**465)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f18*x** 18
      (cf018*x** 18+cf082*x** 82+cf146*x**146+cf210*x**210+
       cf274*x**274+cf338*x**338+cf402*x**402+cf466*x**466)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f1c*x** 19
      (cf019*x** 19+cf083*x** 83+cf147*x**147+cf211*x**211+
       cf275*x**275+cf339*x**339+cf403*x**403+cf467*x**467)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f20*x** 20
      (cf020*x** 20+cf084*x** 84+cf148*x**148+cf212*x**212+
       cf276*x**276+cf340*x**340+cf404*x**404+cf468*x**468)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f24*x** 21
      (cf021*x** 21+cf085*x** 85+cf149*x**149+cf213*x**213+
       cf277*x**277+cf341*x**341+cf405*x**405+cf469*x**469)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f28*x** 22
      (cf022*x** 22+cf086*x** 86+cf150*x**150+cf214*x**214+
       cf278*x**278+cf342*x**342+cf406*x**406+cf470*x**470)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f2c*x** 23
      (cf023*x** 23+cf087*x** 87+cf151*x**151+cf215*x**215+
       cf279*x**279+cf343*x**343+cf407*x**407+cf471*x**471)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f30*x** 24
      (cf024*x** 24+cf088*x** 88+cf152*x**152+cf216*x**216+
       cf280*x**280+cf344*x**344+cf408*x**408+cf472*x**472)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f34*x** 25
      (cf025*x** 25+cf089*x** 89+cf153*x**153+cf217*x**217+
       cf281*x**281+cf345*x**345+cf409*x**409+cf473*x**473)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f38*x** 26
      (cf026*x** 26+cf090*x** 90+cf154*x**154+cf218*x**218+
       cf282*x**282+cf346*x**346+cf410*x**410+cf474*x**474)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f3c*x** 27
      (cf027*x** 27+cf091*x** 91+cf155*x**155+cf219*x**219+
       cf283*x**283+cf347*x**347+cf411*x**411+cf475*x**475)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f40*x** 28
      (cf028*x** 28+cf092*x** 92+cf156*x**156+cf220*x**220+
       cf284*x**284+cf348*x**348+cf412*x**412+cf476*x**476)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f44*x** 29
      (cf029*x** 29+cf093*x** 93+cf157*x**157+cf221*x**221+
       cf285*x**285+cf349*x**349+cf413*x**413+cf477*x**477)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f48*x** 30
      (cf030*x** 30+cf094*x** 94+cf158*x**158+cf222*x**222+
       cf286*x**286+cf350*x**350+cf414*x**414+cf478*x**478)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f4c*x** 31
      (cf031*x** 31+cf095*x** 95+cf159*x**159+cf223*x**223+
       cf287*x**287+cf351*x**351+cf415*x**415+cf479*x**479)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f50*x** 32
      (cf032*x** 32+cf096*x** 96+cf160*x**160+cf224*x**224+
       cf288*x**288+cf352*x**352+cf416*x**416+cf480*x**480)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f54*x** 33
      (cf033*x** 33+cf097*x** 97+cf161*x**161+cf225*x**225+
       cf289*x**289+cf353*x**353+cf417*x**417+cf481*x**481)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f58*x** 34
      (cf034*x** 34+cf098*x** 98+cf162*x**162+cf226*x**226+
       cf290*x**290+cf354*x**354+cf418*x**418+cf482*x**482)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f5c*x** 35
      (cf035*x** 35+cf099*x** 99+cf163*x**163+cf227*x**227+
       cf291*x**291+cf355*x**355+cf419*x**419+cf483*x**483)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f60*x** 36
      (cf036*x** 36+cf100*x**100+cf164*x**164+cf228*x**228+
       cf292*x**292+cf356*x**356+cf420*x**420+cf484*x**484)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f64*x** 37
      (cf037*x** 37+cf101*x**101+cf165*x**165+cf229*x**229+
       cf293*x**293+cf357*x**357+cf421*x**421+cf485*x**485)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f68*x** 38
      (cf038*x** 38+cf102*x**102+cf166*x**166+cf230*x**230+
       cf294*x**294+cf358*x**358+cf422*x**422+cf486*x**486)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f6c*x** 39
      (cf039*x** 39+cf103*x**103+cf167*x**167+cf231*x**231+
       cf295*x**295+cf359*x**359+cf423*x**423+cf487*x**487)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f70*x** 40
      (cf040*x** 40+cf104*x**104+cf168*x**168+cf232*x**232+
       cf296*x**296+cf360*x**360+cf424*x**424+cf488*x**488)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f74*x** 41
      (cf041*x** 41+cf105*x**105+cf169*x**169+cf233*x**233+
       cf297*x**297+cf361*x**361+cf425*x**425+cf489*x**489)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f78*x** 42
      (cf042*x** 42+cf106*x**106+cf170*x**170+cf234*x**234+
       cf298*x**298+cf362*x**362+cf426*x**426+cf490*x**490)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f7c*x** 43
      (cf043*x** 43+cf107*x**107+cf171*x**171+cf235*x**235+
       cf299*x**299+cf363*x**363+cf427*x**427+cf491*x**491)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f80*x** 44
      (cf044*x** 44+cf108*x**108+cf172*x**172+cf236*x**236+
       cf300*x**300+cf364*x**364+cf428*x**428+cf492*x**492)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f84*x** 45
      (cf045*x** 45+cf109*x**109+cf173*x**173+cf237*x**237+
       cf301*x**301+cf365*x**365+cf429*x**429+cf493*x**493)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f88*x** 46
      (cf046*x** 46+cf110*x**110+cf174*x**174+cf238*x**238+
       cf302*x**302+cf366*x**366+cf430*x**430+cf494*x**494)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f8c*x** 47
      (cf047*x** 47+cf111*x**111+cf175*x**175+cf239*x**239+
       cf303*x**303+cf367*x**367+cf431*x**431+cf495*x**495)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f90*x** 48
      (cf048*x** 48+cf112*x**112+cf176*x**176+cf240*x**240+
       cf304*x**304+cf368*x**368+cf432*x**432+cf496*x**496)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f94*x** 49
      (cf049*x** 49+cf113*x**113+cf177*x**177+cf241*x**241+
       cf305*x**305+cf369*x**369+cf433*x**433+cf497*x**497)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f98*x** 50
      (cf050*x** 50+cf114*x**114+cf178*x**178+cf242*x**242+
       cf306*x**306+cf370*x**370+cf434*x**434+cf498*x**498)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019f9c*x** 51
      (cf051*x** 51+cf115*x**115+cf179*x**179+cf243*x**243+
       cf307*x**307+cf371*x**371+cf435*x**435+cf499*x**499)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019fa0*x** 52
      (cf052*x** 52+cf116*x**116+cf180*x**180+cf244*x**244+
       cf308*x**308+cf372*x**372+cf436*x**436+cf500*x**500)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019fa4*x** 53
      (cf053*x** 53+cf117*x**117+cf181*x**181+cf245*x**245+
       cf309*x**309+cf373*x**373+cf437*x**437+cf501*x**501)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019fa8*x** 54
      (cf054*x** 54+cf118*x**118+cf182*x**182+cf246*x**246+
       cf310*x**310+cf374*x**374+cf438*x**438+cf502*x**502)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019fac*x** 55
      (cf055*x** 55+cf119*x**119+cf183*x**183+cf247*x**247+
       cf311*x**311+cf375*x**375+cf439*x**439+cf503*x**503)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019fb0*x** 56
      (cf056*x** 56+cf120*x**120+cf184*x**184+cf248*x**248+
       cf312*x**312+cf376*x**376+cf440*x**440+cf504*x**504)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019fb4*x** 57
      (cf057*x** 57+cf121*x**121+cf185*x**185+cf249*x**249+
       cf313*x**313+cf377*x**377+cf441*x**441+cf505*x**505)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019fb8*x** 58
      (cf058*x** 58+cf122*x**122+cf186*x**186+cf250*x**250+
       cf314*x**314+cf378*x**378+cf442*x**442+cf506*x**506)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019fbc*x** 59
      (cf059*x** 59+cf123*x**123+cf187*x**187+cf251*x**251+
       cf315*x**315+cf379*x**379+cf443*x**443+cf507*x**507)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019fc0*x** 60
      (cf060*x** 60+cf124*x**124+cf188*x**188+cf252*x**252+
       cf316*x**316+cf380*x**380+cf444*x**444+cf508*x**508)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019fc4*x** 61
      (cf061*x** 61+cf125*x**125+cf189*x**189+cf253*x**253+
       cf317*x**317+cf381*x**381+cf445*x**445)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019fc8*x** 62
      (cf062*x** 62+cf126*x**126+cf190*x**190+cf254*x**254+
       cf318*x**318+cf382*x**382+cf446*x**446)
      [ 1043969, x**64 -  268244 ],
eqmod L0x20019fcc*x** 63
      (cf063*x** 63+cf127*x**127+cf191*x**191+cf255*x**255+
       cf319*x**319+cf383*x**383+cf447*x**447)
      [ 1043969, x**64 -  268244 ]
] prove with [ cuts [  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,
                      13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,
                      26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
                      39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
                      52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63 ] ];

(**************** CUT  97, - *****************)

ecut eqmod (cinp_poly**2)
    (L0x20019ed0*(x** 0)+L0x20019ed4*(x** 1)+L0x20019ed8*(x** 2)+
     L0x20019edc*(x** 3)+L0x20019ee0*(x** 4)+L0x20019ee4*(x** 5)+
     L0x20019ee8*(x** 6)+L0x20019eec*(x** 7)+L0x20019ef0*(x** 8)+
     L0x20019ef4*(x** 9)+L0x20019ef8*(x**10)+L0x20019efc*(x**11)+
     L0x20019f00*(x**12)+L0x20019f04*(x**13)+L0x20019f08*(x**14)+
     L0x20019f0c*(x**15)+L0x20019f10*(x**16)+L0x20019f14*(x**17)+
     L0x20019f18*(x**18)+L0x20019f1c*(x**19)+L0x20019f20*(x**20)+
     L0x20019f24*(x**21)+L0x20019f28*(x**22)+L0x20019f2c*(x**23)+
     L0x20019f30*(x**24)+L0x20019f34*(x**25)+L0x20019f38*(x**26)+
     L0x20019f3c*(x**27)+L0x20019f40*(x**28)+L0x20019f44*(x**29)+
     L0x20019f48*(x**30)+L0x20019f4c*(x**31)+L0x20019f50*(x**32)+
     L0x20019f54*(x**33)+L0x20019f58*(x**34)+L0x20019f5c*(x**35)+
     L0x20019f60*(x**36)+L0x20019f64*(x**37)+L0x20019f68*(x**38)+
     L0x20019f6c*(x**39)+L0x20019f70*(x**40)+L0x20019f74*(x**41)+
     L0x20019f78*(x**42)+L0x20019f7c*(x**43)+L0x20019f80*(x**44)+
     L0x20019f84*(x**45)+L0x20019f88*(x**46)+L0x20019f8c*(x**47)+
     L0x20019f90*(x**48)+L0x20019f94*(x**49)+L0x20019f98*(x**50)+
     L0x20019f9c*(x**51)+L0x20019fa0*(x**52)+L0x20019fa4*(x**53)+
     L0x20019fa8*(x**54)+L0x20019fac*(x**55)+L0x20019fb0*(x**56)+
     L0x20019fb4*(x**57)+L0x20019fb8*(x**58)+L0x20019fbc*(x**59)+
     L0x20019fc0*(x**60)+L0x20019fc4*(x**61)+L0x20019fc8*(x**62)+
     L0x20019fcc*(x**63))
    [ 1043969, x**64 -  268244 ] prove with [ all ghosts ];

(**************** CUT  98, - *****************)

ecut and [
eqmod inp_poly**2 cinp_poly**2 2**11,
eqmod (cinp_poly**2)
    (L0x20018fd0*(x** 0)+L0x20018fd4*(x** 1)+L0x20018fd8*(x** 2)+
     L0x20018fdc*(x** 3)+L0x20018fe0*(x** 4)+L0x20018fe4*(x** 5)+
     L0x20018fe8*(x** 6)+L0x20018fec*(x** 7)+L0x20018ff0*(x** 8)+
     L0x20018ff4*(x** 9)+L0x20018ff8*(x**10)+L0x20018ffc*(x**11)+
     L0x20019000*(x**12)+L0x20019004*(x**13)+L0x20019008*(x**14)+
     L0x2001900c*(x**15)+L0x20019010*(x**16)+L0x20019014*(x**17)+
     L0x20019018*(x**18)+L0x2001901c*(x**19)+L0x20019020*(x**20)+
     L0x20019024*(x**21)+L0x20019028*(x**22)+L0x2001902c*(x**23)+
     L0x20019030*(x**24)+L0x20019034*(x**25)+L0x20019038*(x**26)+
     L0x2001903c*(x**27)+L0x20019040*(x**28)+L0x20019044*(x**29)+
     L0x20019048*(x**30)+L0x2001904c*(x**31)+L0x20019050*(x**32)+
     L0x20019054*(x**33)+L0x20019058*(x**34)+L0x2001905c*(x**35)+
     L0x20019060*(x**36)+L0x20019064*(x**37)+L0x20019068*(x**38)+
     L0x2001906c*(x**39)+L0x20019070*(x**40)+L0x20019074*(x**41)+
     L0x20019078*(x**42)+L0x2001907c*(x**43)+L0x20019080*(x**44)+
     L0x20019084*(x**45)+L0x20019088*(x**46)+L0x2001908c*(x**47)+
     L0x20019090*(x**48)+L0x20019094*(x**49)+L0x20019098*(x**50)+
     L0x2001909c*(x**51)+L0x200190a0*(x**52)+L0x200190a4*(x**53)+
     L0x200190a8*(x**54)+L0x200190ac*(x**55)+L0x200190b0*(x**56)+
     L0x200190b4*(x**57)+L0x200190b8*(x**58)+L0x200190bc*(x**59)+
     L0x200190c0*(x**60)+L0x200190c4*(x**61)+L0x200190c8*(x**62)+
     L0x200190cc*(x**63))
    [ 1043969, x**64 -       1 ],
eqmod (cinp_poly**2)
    (L0x200190d0*(x** 0)+L0x200190d4*(x** 1)+L0x200190d8*(x** 2)+
     L0x200190dc*(x** 3)+L0x200190e0*(x** 4)+L0x200190e4*(x** 5)+
     L0x200190e8*(x** 6)+L0x200190ec*(x** 7)+L0x200190f0*(x** 8)+
     L0x200190f4*(x** 9)+L0x200190f8*(x**10)+L0x200190fc*(x**11)+
     L0x20019100*(x**12)+L0x20019104*(x**13)+L0x20019108*(x**14)+
     L0x2001910c*(x**15)+L0x20019110*(x**16)+L0x20019114*(x**17)+
     L0x20019118*(x**18)+L0x2001911c*(x**19)+L0x20019120*(x**20)+
     L0x20019124*(x**21)+L0x20019128*(x**22)+L0x2001912c*(x**23)+
     L0x20019130*(x**24)+L0x20019134*(x**25)+L0x20019138*(x**26)+
     L0x2001913c*(x**27)+L0x20019140*(x**28)+L0x20019144*(x**29)+
     L0x20019148*(x**30)+L0x2001914c*(x**31)+L0x20019150*(x**32)+
     L0x20019154*(x**33)+L0x20019158*(x**34)+L0x2001915c*(x**35)+
     L0x20019160*(x**36)+L0x20019164*(x**37)+L0x20019168*(x**38)+
     L0x2001916c*(x**39)+L0x20019170*(x**40)+L0x20019174*(x**41)+
     L0x20019178*(x**42)+L0x2001917c*(x**43)+L0x20019180*(x**44)+
     L0x20019184*(x**45)+L0x20019188*(x**46)+L0x2001918c*(x**47)+
     L0x20019190*(x**48)+L0x20019194*(x**49)+L0x20019198*(x**50)+
     L0x2001919c*(x**51)+L0x200191a0*(x**52)+L0x200191a4*(x**53)+
     L0x200191a8*(x**54)+L0x200191ac*(x**55)+L0x200191b0*(x**56)+
     L0x200191b4*(x**57)+L0x200191b8*(x**58)+L0x200191bc*(x**59)+
     L0x200191c0*(x**60)+L0x200191c4*(x**61)+L0x200191c8*(x**62)+
     L0x200191cc*(x**63))
    [ 1043969, x**64 - 1043968 ],
eqmod (cinp_poly**2)
    (L0x200191d0*(x** 0)+L0x200191d4*(x** 1)+L0x200191d8*(x** 2)+
     L0x200191dc*(x** 3)+L0x200191e0*(x** 4)+L0x200191e4*(x** 5)+
     L0x200191e8*(x** 6)+L0x200191ec*(x** 7)+L0x200191f0*(x** 8)+
     L0x200191f4*(x** 9)+L0x200191f8*(x**10)+L0x200191fc*(x**11)+
     L0x20019200*(x**12)+L0x20019204*(x**13)+L0x20019208*(x**14)+
     L0x2001920c*(x**15)+L0x20019210*(x**16)+L0x20019214*(x**17)+
     L0x20019218*(x**18)+L0x2001921c*(x**19)+L0x20019220*(x**20)+
     L0x20019224*(x**21)+L0x20019228*(x**22)+L0x2001922c*(x**23)+
     L0x20019230*(x**24)+L0x20019234*(x**25)+L0x20019238*(x**26)+
     L0x2001923c*(x**27)+L0x20019240*(x**28)+L0x20019244*(x**29)+
     L0x20019248*(x**30)+L0x2001924c*(x**31)+L0x20019250*(x**32)+
     L0x20019254*(x**33)+L0x20019258*(x**34)+L0x2001925c*(x**35)+
     L0x20019260*(x**36)+L0x20019264*(x**37)+L0x20019268*(x**38)+
     L0x2001926c*(x**39)+L0x20019270*(x**40)+L0x20019274*(x**41)+
     L0x20019278*(x**42)+L0x2001927c*(x**43)+L0x20019280*(x**44)+
     L0x20019284*(x**45)+L0x20019288*(x**46)+L0x2001928c*(x**47)+
     L0x20019290*(x**48)+L0x20019294*(x**49)+L0x20019298*(x**50)+
     L0x2001929c*(x**51)+L0x200192a0*(x**52)+L0x200192a4*(x**53)+
     L0x200192a8*(x**54)+L0x200192ac*(x**55)+L0x200192b0*(x**56)+
     L0x200192b4*(x**57)+L0x200192b8*(x**58)+L0x200192bc*(x**59)+
     L0x200192c0*(x**60)+L0x200192c4*(x**61)+L0x200192c8*(x**62)+
     L0x200192cc*(x**63))
    [ 1043969, x**64 -  554923 ],
eqmod (cinp_poly**2)
    (L0x200192d0*(x** 0)+L0x200192d4*(x** 1)+L0x200192d8*(x** 2)+
     L0x200192dc*(x** 3)+L0x200192e0*(x** 4)+L0x200192e4*(x** 5)+
     L0x200192e8*(x** 6)+L0x200192ec*(x** 7)+L0x200192f0*(x** 8)+
     L0x200192f4*(x** 9)+L0x200192f8*(x**10)+L0x200192fc*(x**11)+
     L0x20019300*(x**12)+L0x20019304*(x**13)+L0x20019308*(x**14)+
     L0x2001930c*(x**15)+L0x20019310*(x**16)+L0x20019314*(x**17)+
     L0x20019318*(x**18)+L0x2001931c*(x**19)+L0x20019320*(x**20)+
     L0x20019324*(x**21)+L0x20019328*(x**22)+L0x2001932c*(x**23)+
     L0x20019330*(x**24)+L0x20019334*(x**25)+L0x20019338*(x**26)+
     L0x2001933c*(x**27)+L0x20019340*(x**28)+L0x20019344*(x**29)+
     L0x20019348*(x**30)+L0x2001934c*(x**31)+L0x20019350*(x**32)+
     L0x20019354*(x**33)+L0x20019358*(x**34)+L0x2001935c*(x**35)+
     L0x20019360*(x**36)+L0x20019364*(x**37)+L0x20019368*(x**38)+
     L0x2001936c*(x**39)+L0x20019370*(x**40)+L0x20019374*(x**41)+
     L0x20019378*(x**42)+L0x2001937c*(x**43)+L0x20019380*(x**44)+
     L0x20019384*(x**45)+L0x20019388*(x**46)+L0x2001938c*(x**47)+
     L0x20019390*(x**48)+L0x20019394*(x**49)+L0x20019398*(x**50)+
     L0x2001939c*(x**51)+L0x200193a0*(x**52)+L0x200193a4*(x**53)+
     L0x200193a8*(x**54)+L0x200193ac*(x**55)+L0x200193b0*(x**56)+
     L0x200193b4*(x**57)+L0x200193b8*(x**58)+L0x200193bc*(x**59)+
     L0x200193c0*(x**60)+L0x200193c4*(x**61)+L0x200193c8*(x**62)+
     L0x200193cc*(x**63))
    [ 1043969, x**64 -  489046 ],
eqmod (cinp_poly**2)
    (L0x200193d0*(x** 0)+L0x200193d4*(x** 1)+L0x200193d8*(x** 2)+
     L0x200193dc*(x** 3)+L0x200193e0*(x** 4)+L0x200193e4*(x** 5)+
     L0x200193e8*(x** 6)+L0x200193ec*(x** 7)+L0x200193f0*(x** 8)+
     L0x200193f4*(x** 9)+L0x200193f8*(x**10)+L0x200193fc*(x**11)+
     L0x20019400*(x**12)+L0x20019404*(x**13)+L0x20019408*(x**14)+
     L0x2001940c*(x**15)+L0x20019410*(x**16)+L0x20019414*(x**17)+
     L0x20019418*(x**18)+L0x2001941c*(x**19)+L0x20019420*(x**20)+
     L0x20019424*(x**21)+L0x20019428*(x**22)+L0x2001942c*(x**23)+
     L0x20019430*(x**24)+L0x20019434*(x**25)+L0x20019438*(x**26)+
     L0x2001943c*(x**27)+L0x20019440*(x**28)+L0x20019444*(x**29)+
     L0x20019448*(x**30)+L0x2001944c*(x**31)+L0x20019450*(x**32)+
     L0x20019454*(x**33)+L0x20019458*(x**34)+L0x2001945c*(x**35)+
     L0x20019460*(x**36)+L0x20019464*(x**37)+L0x20019468*(x**38)+
     L0x2001946c*(x**39)+L0x20019470*(x**40)+L0x20019474*(x**41)+
     L0x20019478*(x**42)+L0x2001947c*(x**43)+L0x20019480*(x**44)+
     L0x20019484*(x**45)+L0x20019488*(x**46)+L0x2001948c*(x**47)+
     L0x20019490*(x**48)+L0x20019494*(x**49)+L0x20019498*(x**50)+
     L0x2001949c*(x**51)+L0x200194a0*(x**52)+L0x200194a4*(x**53)+
     L0x200194a8*(x**54)+L0x200194ac*(x**55)+L0x200194b0*(x**56)+
     L0x200194b4*(x**57)+L0x200194b8*(x**58)+L0x200194bc*(x**59)+
     L0x200194c0*(x**60)+L0x200194c4*(x**61)+L0x200194c8*(x**62)+
     L0x200194cc*(x**63))
    [ 1043969, x**64 -  287998 ],
eqmod (cinp_poly**2)
    (L0x200194d0*(x** 0)+L0x200194d4*(x** 1)+L0x200194d8*(x** 2)+
     L0x200194dc*(x** 3)+L0x200194e0*(x** 4)+L0x200194e4*(x** 5)+
     L0x200194e8*(x** 6)+L0x200194ec*(x** 7)+L0x200194f0*(x** 8)+
     L0x200194f4*(x** 9)+L0x200194f8*(x**10)+L0x200194fc*(x**11)+
     L0x20019500*(x**12)+L0x20019504*(x**13)+L0x20019508*(x**14)+
     L0x2001950c*(x**15)+L0x20019510*(x**16)+L0x20019514*(x**17)+
     L0x20019518*(x**18)+L0x2001951c*(x**19)+L0x20019520*(x**20)+
     L0x20019524*(x**21)+L0x20019528*(x**22)+L0x2001952c*(x**23)+
     L0x20019530*(x**24)+L0x20019534*(x**25)+L0x20019538*(x**26)+
     L0x2001953c*(x**27)+L0x20019540*(x**28)+L0x20019544*(x**29)+
     L0x20019548*(x**30)+L0x2001954c*(x**31)+L0x20019550*(x**32)+
     L0x20019554*(x**33)+L0x20019558*(x**34)+L0x2001955c*(x**35)+
     L0x20019560*(x**36)+L0x20019564*(x**37)+L0x20019568*(x**38)+
     L0x2001956c*(x**39)+L0x20019570*(x**40)+L0x20019574*(x**41)+
     L0x20019578*(x**42)+L0x2001957c*(x**43)+L0x20019580*(x**44)+
     L0x20019584*(x**45)+L0x20019588*(x**46)+L0x2001958c*(x**47)+
     L0x20019590*(x**48)+L0x20019594*(x**49)+L0x20019598*(x**50)+
     L0x2001959c*(x**51)+L0x200195a0*(x**52)+L0x200195a4*(x**53)+
     L0x200195a8*(x**54)+L0x200195ac*(x**55)+L0x200195b0*(x**56)+
     L0x200195b4*(x**57)+L0x200195b8*(x**58)+L0x200195bc*(x**59)+
     L0x200195c0*(x**60)+L0x200195c4*(x**61)+L0x200195c8*(x**62)+
     L0x200195cc*(x**63))
    [ 1043969, x**64 -  755971 ],
eqmod (cinp_poly**2)
    (L0x200195d0*(x** 0)+L0x200195d4*(x** 1)+L0x200195d8*(x** 2)+
     L0x200195dc*(x** 3)+L0x200195e0*(x** 4)+L0x200195e4*(x** 5)+
     L0x200195e8*(x** 6)+L0x200195ec*(x** 7)+L0x200195f0*(x** 8)+
     L0x200195f4*(x** 9)+L0x200195f8*(x**10)+L0x200195fc*(x**11)+
     L0x20019600*(x**12)+L0x20019604*(x**13)+L0x20019608*(x**14)+
     L0x2001960c*(x**15)+L0x20019610*(x**16)+L0x20019614*(x**17)+
     L0x20019618*(x**18)+L0x2001961c*(x**19)+L0x20019620*(x**20)+
     L0x20019624*(x**21)+L0x20019628*(x**22)+L0x2001962c*(x**23)+
     L0x20019630*(x**24)+L0x20019634*(x**25)+L0x20019638*(x**26)+
     L0x2001963c*(x**27)+L0x20019640*(x**28)+L0x20019644*(x**29)+
     L0x20019648*(x**30)+L0x2001964c*(x**31)+L0x20019650*(x**32)+
     L0x20019654*(x**33)+L0x20019658*(x**34)+L0x2001965c*(x**35)+
     L0x20019660*(x**36)+L0x20019664*(x**37)+L0x20019668*(x**38)+
     L0x2001966c*(x**39)+L0x20019670*(x**40)+L0x20019674*(x**41)+
     L0x20019678*(x**42)+L0x2001967c*(x**43)+L0x20019680*(x**44)+
     L0x20019684*(x**45)+L0x20019688*(x**46)+L0x2001968c*(x**47)+
     L0x20019690*(x**48)+L0x20019694*(x**49)+L0x20019698*(x**50)+
     L0x2001969c*(x**51)+L0x200196a0*(x**52)+L0x200196a4*(x**53)+
     L0x200196a8*(x**54)+L0x200196ac*(x**55)+L0x200196b0*(x**56)+
     L0x200196b4*(x**57)+L0x200196b8*(x**58)+L0x200196bc*(x**59)+
     L0x200196c0*(x**60)+L0x200196c4*(x**61)+L0x200196c8*(x**62)+
     L0x200196cc*(x**63))
    [ 1043969, x**64 -  719789 ],
eqmod (cinp_poly**2)
    (L0x200196d0*(x** 0)+L0x200196d4*(x** 1)+L0x200196d8*(x** 2)+
     L0x200196dc*(x** 3)+L0x200196e0*(x** 4)+L0x200196e4*(x** 5)+
     L0x200196e8*(x** 6)+L0x200196ec*(x** 7)+L0x200196f0*(x** 8)+
     L0x200196f4*(x** 9)+L0x200196f8*(x**10)+L0x200196fc*(x**11)+
     L0x20019700*(x**12)+L0x20019704*(x**13)+L0x20019708*(x**14)+
     L0x2001970c*(x**15)+L0x20019710*(x**16)+L0x20019714*(x**17)+
     L0x20019718*(x**18)+L0x2001971c*(x**19)+L0x20019720*(x**20)+
     L0x20019724*(x**21)+L0x20019728*(x**22)+L0x2001972c*(x**23)+
     L0x20019730*(x**24)+L0x20019734*(x**25)+L0x20019738*(x**26)+
     L0x2001973c*(x**27)+L0x20019740*(x**28)+L0x20019744*(x**29)+
     L0x20019748*(x**30)+L0x2001974c*(x**31)+L0x20019750*(x**32)+
     L0x20019754*(x**33)+L0x20019758*(x**34)+L0x2001975c*(x**35)+
     L0x20019760*(x**36)+L0x20019764*(x**37)+L0x20019768*(x**38)+
     L0x2001976c*(x**39)+L0x20019770*(x**40)+L0x20019774*(x**41)+
     L0x20019778*(x**42)+L0x2001977c*(x**43)+L0x20019780*(x**44)+
     L0x20019784*(x**45)+L0x20019788*(x**46)+L0x2001978c*(x**47)+
     L0x20019790*(x**48)+L0x20019794*(x**49)+L0x20019798*(x**50)+
     L0x2001979c*(x**51)+L0x200197a0*(x**52)+L0x200197a4*(x**53)+
     L0x200197a8*(x**54)+L0x200197ac*(x**55)+L0x200197b0*(x**56)+
     L0x200197b4*(x**57)+L0x200197b8*(x**58)+L0x200197bc*(x**59)+
     L0x200197c0*(x**60)+L0x200197c4*(x**61)+L0x200197c8*(x**62)+
     L0x200197cc*(x**63))
    [ 1043969, x**64 -  324180 ],
eqmod (cinp_poly**2)
    (L0x200197d0*(x** 0)+L0x200197d4*(x** 1)+L0x200197d8*(x** 2)+
     L0x200197dc*(x** 3)+L0x200197e0*(x** 4)+L0x200197e4*(x** 5)+
     L0x200197e8*(x** 6)+L0x200197ec*(x** 7)+L0x200197f0*(x** 8)+
     L0x200197f4*(x** 9)+L0x200197f8*(x**10)+L0x200197fc*(x**11)+
     L0x20019800*(x**12)+L0x20019804*(x**13)+L0x20019808*(x**14)+
     L0x2001980c*(x**15)+L0x20019810*(x**16)+L0x20019814*(x**17)+
     L0x20019818*(x**18)+L0x2001981c*(x**19)+L0x20019820*(x**20)+
     L0x20019824*(x**21)+L0x20019828*(x**22)+L0x2001982c*(x**23)+
     L0x20019830*(x**24)+L0x20019834*(x**25)+L0x20019838*(x**26)+
     L0x2001983c*(x**27)+L0x20019840*(x**28)+L0x20019844*(x**29)+
     L0x20019848*(x**30)+L0x2001984c*(x**31)+L0x20019850*(x**32)+
     L0x20019854*(x**33)+L0x20019858*(x**34)+L0x2001985c*(x**35)+
     L0x20019860*(x**36)+L0x20019864*(x**37)+L0x20019868*(x**38)+
     L0x2001986c*(x**39)+L0x20019870*(x**40)+L0x20019874*(x**41)+
     L0x20019878*(x**42)+L0x2001987c*(x**43)+L0x20019880*(x**44)+
     L0x20019884*(x**45)+L0x20019888*(x**46)+L0x2001988c*(x**47)+
     L0x20019890*(x**48)+L0x20019894*(x**49)+L0x20019898*(x**50)+
     L0x2001989c*(x**51)+L0x200198a0*(x**52)+L0x200198a4*(x**53)+
     L0x200198a8*(x**54)+L0x200198ac*(x**55)+L0x200198b0*(x**56)+
     L0x200198b4*(x**57)+L0x200198b8*(x**58)+L0x200198bc*(x**59)+
     L0x200198c0*(x**60)+L0x200198c4*(x**61)+L0x200198c8*(x**62)+
     L0x200198cc*(x**63))
    [ 1043969, x**64 -   29512 ],
eqmod (cinp_poly**2)
    (L0x200198d0*(x** 0)+L0x200198d4*(x** 1)+L0x200198d8*(x** 2)+
     L0x200198dc*(x** 3)+L0x200198e0*(x** 4)+L0x200198e4*(x** 5)+
     L0x200198e8*(x** 6)+L0x200198ec*(x** 7)+L0x200198f0*(x** 8)+
     L0x200198f4*(x** 9)+L0x200198f8*(x**10)+L0x200198fc*(x**11)+
     L0x20019900*(x**12)+L0x20019904*(x**13)+L0x20019908*(x**14)+
     L0x2001990c*(x**15)+L0x20019910*(x**16)+L0x20019914*(x**17)+
     L0x20019918*(x**18)+L0x2001991c*(x**19)+L0x20019920*(x**20)+
     L0x20019924*(x**21)+L0x20019928*(x**22)+L0x2001992c*(x**23)+
     L0x20019930*(x**24)+L0x20019934*(x**25)+L0x20019938*(x**26)+
     L0x2001993c*(x**27)+L0x20019940*(x**28)+L0x20019944*(x**29)+
     L0x20019948*(x**30)+L0x2001994c*(x**31)+L0x20019950*(x**32)+
     L0x20019954*(x**33)+L0x20019958*(x**34)+L0x2001995c*(x**35)+
     L0x20019960*(x**36)+L0x20019964*(x**37)+L0x20019968*(x**38)+
     L0x2001996c*(x**39)+L0x20019970*(x**40)+L0x20019974*(x**41)+
     L0x20019978*(x**42)+L0x2001997c*(x**43)+L0x20019980*(x**44)+
     L0x20019984*(x**45)+L0x20019988*(x**46)+L0x2001998c*(x**47)+
     L0x20019990*(x**48)+L0x20019994*(x**49)+L0x20019998*(x**50)+
     L0x2001999c*(x**51)+L0x200199a0*(x**52)+L0x200199a4*(x**53)+
     L0x200199a8*(x**54)+L0x200199ac*(x**55)+L0x200199b0*(x**56)+
     L0x200199b4*(x**57)+L0x200199b8*(x**58)+L0x200199bc*(x**59)+
     L0x200199c0*(x**60)+L0x200199c4*(x**61)+L0x200199c8*(x**62)+
     L0x200199cc*(x**63))
    [ 1043969, x**64 - 1014457 ],
eqmod (cinp_poly**2)
    (L0x200199d0*(x** 0)+L0x200199d4*(x** 1)+L0x200199d8*(x** 2)+
     L0x200199dc*(x** 3)+L0x200199e0*(x** 4)+L0x200199e4*(x** 5)+
     L0x200199e8*(x** 6)+L0x200199ec*(x** 7)+L0x200199f0*(x** 8)+
     L0x200199f4*(x** 9)+L0x200199f8*(x**10)+L0x200199fc*(x**11)+
     L0x20019a00*(x**12)+L0x20019a04*(x**13)+L0x20019a08*(x**14)+
     L0x20019a0c*(x**15)+L0x20019a10*(x**16)+L0x20019a14*(x**17)+
     L0x20019a18*(x**18)+L0x20019a1c*(x**19)+L0x20019a20*(x**20)+
     L0x20019a24*(x**21)+L0x20019a28*(x**22)+L0x20019a2c*(x**23)+
     L0x20019a30*(x**24)+L0x20019a34*(x**25)+L0x20019a38*(x**26)+
     L0x20019a3c*(x**27)+L0x20019a40*(x**28)+L0x20019a44*(x**29)+
     L0x20019a48*(x**30)+L0x20019a4c*(x**31)+L0x20019a50*(x**32)+
     L0x20019a54*(x**33)+L0x20019a58*(x**34)+L0x20019a5c*(x**35)+
     L0x20019a60*(x**36)+L0x20019a64*(x**37)+L0x20019a68*(x**38)+
     L0x20019a6c*(x**39)+L0x20019a70*(x**40)+L0x20019a74*(x**41)+
     L0x20019a78*(x**42)+L0x20019a7c*(x**43)+L0x20019a80*(x**44)+
     L0x20019a84*(x**45)+L0x20019a88*(x**46)+L0x20019a8c*(x**47)+
     L0x20019a90*(x**48)+L0x20019a94*(x**49)+L0x20019a98*(x**50)+
     L0x20019a9c*(x**51)+L0x20019aa0*(x**52)+L0x20019aa4*(x**53)+
     L0x20019aa8*(x**54)+L0x20019aac*(x**55)+L0x20019ab0*(x**56)+
     L0x20019ab4*(x**57)+L0x20019ab8*(x**58)+L0x20019abc*(x**59)+
     L0x20019ac0*(x**60)+L0x20019ac4*(x**61)+L0x20019ac8*(x**62)+
     L0x20019acc*(x**63))
    [ 1043969, x**64 -  145873 ],
eqmod (cinp_poly**2)
    (L0x20019ad0*(x** 0)+L0x20019ad4*(x** 1)+L0x20019ad8*(x** 2)+
     L0x20019adc*(x** 3)+L0x20019ae0*(x** 4)+L0x20019ae4*(x** 5)+
     L0x20019ae8*(x** 6)+L0x20019aec*(x** 7)+L0x20019af0*(x** 8)+
     L0x20019af4*(x** 9)+L0x20019af8*(x**10)+L0x20019afc*(x**11)+
     L0x20019b00*(x**12)+L0x20019b04*(x**13)+L0x20019b08*(x**14)+
     L0x20019b0c*(x**15)+L0x20019b10*(x**16)+L0x20019b14*(x**17)+
     L0x20019b18*(x**18)+L0x20019b1c*(x**19)+L0x20019b20*(x**20)+
     L0x20019b24*(x**21)+L0x20019b28*(x**22)+L0x20019b2c*(x**23)+
     L0x20019b30*(x**24)+L0x20019b34*(x**25)+L0x20019b38*(x**26)+
     L0x20019b3c*(x**27)+L0x20019b40*(x**28)+L0x20019b44*(x**29)+
     L0x20019b48*(x**30)+L0x20019b4c*(x**31)+L0x20019b50*(x**32)+
     L0x20019b54*(x**33)+L0x20019b58*(x**34)+L0x20019b5c*(x**35)+
     L0x20019b60*(x**36)+L0x20019b64*(x**37)+L0x20019b68*(x**38)+
     L0x20019b6c*(x**39)+L0x20019b70*(x**40)+L0x20019b74*(x**41)+
     L0x20019b78*(x**42)+L0x20019b7c*(x**43)+L0x20019b80*(x**44)+
     L0x20019b84*(x**45)+L0x20019b88*(x**46)+L0x20019b8c*(x**47)+
     L0x20019b90*(x**48)+L0x20019b94*(x**49)+L0x20019b98*(x**50)+
     L0x20019b9c*(x**51)+L0x20019ba0*(x**52)+L0x20019ba4*(x**53)+
     L0x20019ba8*(x**54)+L0x20019bac*(x**55)+L0x20019bb0*(x**56)+
     L0x20019bb4*(x**57)+L0x20019bb8*(x**58)+L0x20019bbc*(x**59)+
     L0x20019bc0*(x**60)+L0x20019bc4*(x**61)+L0x20019bc8*(x**62)+
     L0x20019bcc*(x**63))
    [ 1043969, x**64 -  898096 ],
eqmod (cinp_poly**2)
    (L0x20019bd0*(x** 0)+L0x20019bd4*(x** 1)+L0x20019bd8*(x** 2)+
     L0x20019bdc*(x** 3)+L0x20019be0*(x** 4)+L0x20019be4*(x** 5)+
     L0x20019be8*(x** 6)+L0x20019bec*(x** 7)+L0x20019bf0*(x** 8)+
     L0x20019bf4*(x** 9)+L0x20019bf8*(x**10)+L0x20019bfc*(x**11)+
     L0x20019c00*(x**12)+L0x20019c04*(x**13)+L0x20019c08*(x**14)+
     L0x20019c0c*(x**15)+L0x20019c10*(x**16)+L0x20019c14*(x**17)+
     L0x20019c18*(x**18)+L0x20019c1c*(x**19)+L0x20019c20*(x**20)+
     L0x20019c24*(x**21)+L0x20019c28*(x**22)+L0x20019c2c*(x**23)+
     L0x20019c30*(x**24)+L0x20019c34*(x**25)+L0x20019c38*(x**26)+
     L0x20019c3c*(x**27)+L0x20019c40*(x**28)+L0x20019c44*(x**29)+
     L0x20019c48*(x**30)+L0x20019c4c*(x**31)+L0x20019c50*(x**32)+
     L0x20019c54*(x**33)+L0x20019c58*(x**34)+L0x20019c5c*(x**35)+
     L0x20019c60*(x**36)+L0x20019c64*(x**37)+L0x20019c68*(x**38)+
     L0x20019c6c*(x**39)+L0x20019c70*(x**40)+L0x20019c74*(x**41)+
     L0x20019c78*(x**42)+L0x20019c7c*(x**43)+L0x20019c80*(x**44)+
     L0x20019c84*(x**45)+L0x20019c88*(x**46)+L0x20019c8c*(x**47)+
     L0x20019c90*(x**48)+L0x20019c94*(x**49)+L0x20019c98*(x**50)+
     L0x20019c9c*(x**51)+L0x20019ca0*(x**52)+L0x20019ca4*(x**53)+
     L0x20019ca8*(x**54)+L0x20019cac*(x**55)+L0x20019cb0*(x**56)+
     L0x20019cb4*(x**57)+L0x20019cb8*(x**58)+L0x20019cbc*(x**59)+
     L0x20019cc0*(x**60)+L0x20019cc4*(x**61)+L0x20019cc8*(x**62)+
     L0x20019ccc*(x**63))
    [ 1043969, x**64 -  445347 ],
eqmod (cinp_poly**2)
    (L0x20019cd0*(x** 0)+L0x20019cd4*(x** 1)+L0x20019cd8*(x** 2)+
     L0x20019cdc*(x** 3)+L0x20019ce0*(x** 4)+L0x20019ce4*(x** 5)+
     L0x20019ce8*(x** 6)+L0x20019cec*(x** 7)+L0x20019cf0*(x** 8)+
     L0x20019cf4*(x** 9)+L0x20019cf8*(x**10)+L0x20019cfc*(x**11)+
     L0x20019d00*(x**12)+L0x20019d04*(x**13)+L0x20019d08*(x**14)+
     L0x20019d0c*(x**15)+L0x20019d10*(x**16)+L0x20019d14*(x**17)+
     L0x20019d18*(x**18)+L0x20019d1c*(x**19)+L0x20019d20*(x**20)+
     L0x20019d24*(x**21)+L0x20019d28*(x**22)+L0x20019d2c*(x**23)+
     L0x20019d30*(x**24)+L0x20019d34*(x**25)+L0x20019d38*(x**26)+
     L0x20019d3c*(x**27)+L0x20019d40*(x**28)+L0x20019d44*(x**29)+
     L0x20019d48*(x**30)+L0x20019d4c*(x**31)+L0x20019d50*(x**32)+
     L0x20019d54*(x**33)+L0x20019d58*(x**34)+L0x20019d5c*(x**35)+
     L0x20019d60*(x**36)+L0x20019d64*(x**37)+L0x20019d68*(x**38)+
     L0x20019d6c*(x**39)+L0x20019d70*(x**40)+L0x20019d74*(x**41)+
     L0x20019d78*(x**42)+L0x20019d7c*(x**43)+L0x20019d80*(x**44)+
     L0x20019d84*(x**45)+L0x20019d88*(x**46)+L0x20019d8c*(x**47)+
     L0x20019d90*(x**48)+L0x20019d94*(x**49)+L0x20019d98*(x**50)+
     L0x20019d9c*(x**51)+L0x20019da0*(x**52)+L0x20019da4*(x**53)+
     L0x20019da8*(x**54)+L0x20019dac*(x**55)+L0x20019db0*(x**56)+
     L0x20019db4*(x**57)+L0x20019db8*(x**58)+L0x20019dbc*(x**59)+
     L0x20019dc0*(x**60)+L0x20019dc4*(x**61)+L0x20019dc8*(x**62)+
     L0x20019dcc*(x**63))
    [ 1043969, x**64 -  598622 ],
eqmod (cinp_poly**2)
    (L0x20019dd0*(x** 0)+L0x20019dd4*(x** 1)+L0x20019dd8*(x** 2)+
     L0x20019ddc*(x** 3)+L0x20019de0*(x** 4)+L0x20019de4*(x** 5)+
     L0x20019de8*(x** 6)+L0x20019dec*(x** 7)+L0x20019df0*(x** 8)+
     L0x20019df4*(x** 9)+L0x20019df8*(x**10)+L0x20019dfc*(x**11)+
     L0x20019e00*(x**12)+L0x20019e04*(x**13)+L0x20019e08*(x**14)+
     L0x20019e0c*(x**15)+L0x20019e10*(x**16)+L0x20019e14*(x**17)+
     L0x20019e18*(x**18)+L0x20019e1c*(x**19)+L0x20019e20*(x**20)+
     L0x20019e24*(x**21)+L0x20019e28*(x**22)+L0x20019e2c*(x**23)+
     L0x20019e30*(x**24)+L0x20019e34*(x**25)+L0x20019e38*(x**26)+
     L0x20019e3c*(x**27)+L0x20019e40*(x**28)+L0x20019e44*(x**29)+
     L0x20019e48*(x**30)+L0x20019e4c*(x**31)+L0x20019e50*(x**32)+
     L0x20019e54*(x**33)+L0x20019e58*(x**34)+L0x20019e5c*(x**35)+
     L0x20019e60*(x**36)+L0x20019e64*(x**37)+L0x20019e68*(x**38)+
     L0x20019e6c*(x**39)+L0x20019e70*(x**40)+L0x20019e74*(x**41)+
     L0x20019e78*(x**42)+L0x20019e7c*(x**43)+L0x20019e80*(x**44)+
     L0x20019e84*(x**45)+L0x20019e88*(x**46)+L0x20019e8c*(x**47)+
     L0x20019e90*(x**48)+L0x20019e94*(x**49)+L0x20019e98*(x**50)+
     L0x20019e9c*(x**51)+L0x20019ea0*(x**52)+L0x20019ea4*(x**53)+
     L0x20019ea8*(x**54)+L0x20019eac*(x**55)+L0x20019eb0*(x**56)+
     L0x20019eb4*(x**57)+L0x20019eb8*(x**58)+L0x20019ebc*(x**59)+
     L0x20019ec0*(x**60)+L0x20019ec4*(x**61)+L0x20019ec8*(x**62)+
     L0x20019ecc*(x**63))
    [ 1043969, x**64 -  775725 ],
eqmod (cinp_poly**2)
    (L0x20019ed0*(x** 0)+L0x20019ed4*(x** 1)+L0x20019ed8*(x** 2)+
     L0x20019edc*(x** 3)+L0x20019ee0*(x** 4)+L0x20019ee4*(x** 5)+
     L0x20019ee8*(x** 6)+L0x20019eec*(x** 7)+L0x20019ef0*(x** 8)+
     L0x20019ef4*(x** 9)+L0x20019ef8*(x**10)+L0x20019efc*(x**11)+
     L0x20019f00*(x**12)+L0x20019f04*(x**13)+L0x20019f08*(x**14)+
     L0x20019f0c*(x**15)+L0x20019f10*(x**16)+L0x20019f14*(x**17)+
     L0x20019f18*(x**18)+L0x20019f1c*(x**19)+L0x20019f20*(x**20)+
     L0x20019f24*(x**21)+L0x20019f28*(x**22)+L0x20019f2c*(x**23)+
     L0x20019f30*(x**24)+L0x20019f34*(x**25)+L0x20019f38*(x**26)+
     L0x20019f3c*(x**27)+L0x20019f40*(x**28)+L0x20019f44*(x**29)+
     L0x20019f48*(x**30)+L0x20019f4c*(x**31)+L0x20019f50*(x**32)+
     L0x20019f54*(x**33)+L0x20019f58*(x**34)+L0x20019f5c*(x**35)+
     L0x20019f60*(x**36)+L0x20019f64*(x**37)+L0x20019f68*(x**38)+
     L0x20019f6c*(x**39)+L0x20019f70*(x**40)+L0x20019f74*(x**41)+
     L0x20019f78*(x**42)+L0x20019f7c*(x**43)+L0x20019f80*(x**44)+
     L0x20019f84*(x**45)+L0x20019f88*(x**46)+L0x20019f8c*(x**47)+
     L0x20019f90*(x**48)+L0x20019f94*(x**49)+L0x20019f98*(x**50)+
     L0x20019f9c*(x**51)+L0x20019fa0*(x**52)+L0x20019fa4*(x**53)+
     L0x20019fa8*(x**54)+L0x20019fac*(x**55)+L0x20019fb0*(x**56)+
     L0x20019fb4*(x**57)+L0x20019fb8*(x**58)+L0x20019fbc*(x**59)+
     L0x20019fc0*(x**60)+L0x20019fc4*(x**61)+L0x20019fc8*(x**62)+
     L0x20019fcc*(x**63))
    [ 1043969, x**64 -  268244 ]
] prove with [ cuts [
65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97
] ];

{
and [
eqmod (inp_poly**2) (cinp_poly**2) (2**11),
eqmod (cinp_poly**2)
    (L0x20018fd0*(x** 0)+L0x20018fd4*(x** 1)+L0x20018fd8*(x** 2)+
     L0x20018fdc*(x** 3)+L0x20018fe0*(x** 4)+L0x20018fe4*(x** 5)+
     L0x20018fe8*(x** 6)+L0x20018fec*(x** 7)+L0x20018ff0*(x** 8)+
     L0x20018ff4*(x** 9)+L0x20018ff8*(x**10)+L0x20018ffc*(x**11)+
     L0x20019000*(x**12)+L0x20019004*(x**13)+L0x20019008*(x**14)+
     L0x2001900c*(x**15)+L0x20019010*(x**16)+L0x20019014*(x**17)+
     L0x20019018*(x**18)+L0x2001901c*(x**19)+L0x20019020*(x**20)+
     L0x20019024*(x**21)+L0x20019028*(x**22)+L0x2001902c*(x**23)+
     L0x20019030*(x**24)+L0x20019034*(x**25)+L0x20019038*(x**26)+
     L0x2001903c*(x**27)+L0x20019040*(x**28)+L0x20019044*(x**29)+
     L0x20019048*(x**30)+L0x2001904c*(x**31)+L0x20019050*(x**32)+
     L0x20019054*(x**33)+L0x20019058*(x**34)+L0x2001905c*(x**35)+
     L0x20019060*(x**36)+L0x20019064*(x**37)+L0x20019068*(x**38)+
     L0x2001906c*(x**39)+L0x20019070*(x**40)+L0x20019074*(x**41)+
     L0x20019078*(x**42)+L0x2001907c*(x**43)+L0x20019080*(x**44)+
     L0x20019084*(x**45)+L0x20019088*(x**46)+L0x2001908c*(x**47)+
     L0x20019090*(x**48)+L0x20019094*(x**49)+L0x20019098*(x**50)+
     L0x2001909c*(x**51)+L0x200190a0*(x**52)+L0x200190a4*(x**53)+
     L0x200190a8*(x**54)+L0x200190ac*(x**55)+L0x200190b0*(x**56)+
     L0x200190b4*(x**57)+L0x200190b8*(x**58)+L0x200190bc*(x**59)+
     L0x200190c0*(x**60)+L0x200190c4*(x**61)+L0x200190c8*(x**62)+
     L0x200190cc*(x**63))
    [1043969, x**64 - 1],
eqmod (cinp_poly**2)
    (L0x200190d0*(x** 0)+L0x200190d4*(x** 1)+L0x200190d8*(x** 2)+
     L0x200190dc*(x** 3)+L0x200190e0*(x** 4)+L0x200190e4*(x** 5)+
     L0x200190e8*(x** 6)+L0x200190ec*(x** 7)+L0x200190f0*(x** 8)+
     L0x200190f4*(x** 9)+L0x200190f8*(x**10)+L0x200190fc*(x**11)+
     L0x20019100*(x**12)+L0x20019104*(x**13)+L0x20019108*(x**14)+
     L0x2001910c*(x**15)+L0x20019110*(x**16)+L0x20019114*(x**17)+
     L0x20019118*(x**18)+L0x2001911c*(x**19)+L0x20019120*(x**20)+
     L0x20019124*(x**21)+L0x20019128*(x**22)+L0x2001912c*(x**23)+
     L0x20019130*(x**24)+L0x20019134*(x**25)+L0x20019138*(x**26)+
     L0x2001913c*(x**27)+L0x20019140*(x**28)+L0x20019144*(x**29)+
     L0x20019148*(x**30)+L0x2001914c*(x**31)+L0x20019150*(x**32)+
     L0x20019154*(x**33)+L0x20019158*(x**34)+L0x2001915c*(x**35)+
     L0x20019160*(x**36)+L0x20019164*(x**37)+L0x20019168*(x**38)+
     L0x2001916c*(x**39)+L0x20019170*(x**40)+L0x20019174*(x**41)+
     L0x20019178*(x**42)+L0x2001917c*(x**43)+L0x20019180*(x**44)+
     L0x20019184*(x**45)+L0x20019188*(x**46)+L0x2001918c*(x**47)+
     L0x20019190*(x**48)+L0x20019194*(x**49)+L0x20019198*(x**50)+
     L0x2001919c*(x**51)+L0x200191a0*(x**52)+L0x200191a4*(x**53)+
     L0x200191a8*(x**54)+L0x200191ac*(x**55)+L0x200191b0*(x**56)+
     L0x200191b4*(x**57)+L0x200191b8*(x**58)+L0x200191bc*(x**59)+
     L0x200191c0*(x**60)+L0x200191c4*(x**61)+L0x200191c8*(x**62)+
     L0x200191cc*(x**63))
    [1043969, x**64 - 1043968],
eqmod (cinp_poly**2)
    (L0x200191d0*(x** 0)+L0x200191d4*(x** 1)+L0x200191d8*(x** 2)+
     L0x200191dc*(x** 3)+L0x200191e0*(x** 4)+L0x200191e4*(x** 5)+
     L0x200191e8*(x** 6)+L0x200191ec*(x** 7)+L0x200191f0*(x** 8)+
     L0x200191f4*(x** 9)+L0x200191f8*(x**10)+L0x200191fc*(x**11)+
     L0x20019200*(x**12)+L0x20019204*(x**13)+L0x20019208*(x**14)+
     L0x2001920c*(x**15)+L0x20019210*(x**16)+L0x20019214*(x**17)+
     L0x20019218*(x**18)+L0x2001921c*(x**19)+L0x20019220*(x**20)+
     L0x20019224*(x**21)+L0x20019228*(x**22)+L0x2001922c*(x**23)+
     L0x20019230*(x**24)+L0x20019234*(x**25)+L0x20019238*(x**26)+
     L0x2001923c*(x**27)+L0x20019240*(x**28)+L0x20019244*(x**29)+
     L0x20019248*(x**30)+L0x2001924c*(x**31)+L0x20019250*(x**32)+
     L0x20019254*(x**33)+L0x20019258*(x**34)+L0x2001925c*(x**35)+
     L0x20019260*(x**36)+L0x20019264*(x**37)+L0x20019268*(x**38)+
     L0x2001926c*(x**39)+L0x20019270*(x**40)+L0x20019274*(x**41)+
     L0x20019278*(x**42)+L0x2001927c*(x**43)+L0x20019280*(x**44)+
     L0x20019284*(x**45)+L0x20019288*(x**46)+L0x2001928c*(x**47)+
     L0x20019290*(x**48)+L0x20019294*(x**49)+L0x20019298*(x**50)+
     L0x2001929c*(x**51)+L0x200192a0*(x**52)+L0x200192a4*(x**53)+
     L0x200192a8*(x**54)+L0x200192ac*(x**55)+L0x200192b0*(x**56)+
     L0x200192b4*(x**57)+L0x200192b8*(x**58)+L0x200192bc*(x**59)+
     L0x200192c0*(x**60)+L0x200192c4*(x**61)+L0x200192c8*(x**62)+
     L0x200192cc*(x**63))
    [1043969, x**64 - 554923],
eqmod (cinp_poly**2)
    (L0x200192d0*(x** 0)+L0x200192d4*(x** 1)+L0x200192d8*(x** 2)+
     L0x200192dc*(x** 3)+L0x200192e0*(x** 4)+L0x200192e4*(x** 5)+
     L0x200192e8*(x** 6)+L0x200192ec*(x** 7)+L0x200192f0*(x** 8)+
     L0x200192f4*(x** 9)+L0x200192f8*(x**10)+L0x200192fc*(x**11)+
     L0x20019300*(x**12)+L0x20019304*(x**13)+L0x20019308*(x**14)+
     L0x2001930c*(x**15)+L0x20019310*(x**16)+L0x20019314*(x**17)+
     L0x20019318*(x**18)+L0x2001931c*(x**19)+L0x20019320*(x**20)+
     L0x20019324*(x**21)+L0x20019328*(x**22)+L0x2001932c*(x**23)+
     L0x20019330*(x**24)+L0x20019334*(x**25)+L0x20019338*(x**26)+
     L0x2001933c*(x**27)+L0x20019340*(x**28)+L0x20019344*(x**29)+
     L0x20019348*(x**30)+L0x2001934c*(x**31)+L0x20019350*(x**32)+
     L0x20019354*(x**33)+L0x20019358*(x**34)+L0x2001935c*(x**35)+
     L0x20019360*(x**36)+L0x20019364*(x**37)+L0x20019368*(x**38)+
     L0x2001936c*(x**39)+L0x20019370*(x**40)+L0x20019374*(x**41)+
     L0x20019378*(x**42)+L0x2001937c*(x**43)+L0x20019380*(x**44)+
     L0x20019384*(x**45)+L0x20019388*(x**46)+L0x2001938c*(x**47)+
     L0x20019390*(x**48)+L0x20019394*(x**49)+L0x20019398*(x**50)+
     L0x2001939c*(x**51)+L0x200193a0*(x**52)+L0x200193a4*(x**53)+
     L0x200193a8*(x**54)+L0x200193ac*(x**55)+L0x200193b0*(x**56)+
     L0x200193b4*(x**57)+L0x200193b8*(x**58)+L0x200193bc*(x**59)+
     L0x200193c0*(x**60)+L0x200193c4*(x**61)+L0x200193c8*(x**62)+
     L0x200193cc*(x**63))
    [1043969, x**64 - 489046],
eqmod (cinp_poly**2)
    (L0x200193d0*(x** 0)+L0x200193d4*(x** 1)+L0x200193d8*(x** 2)+
     L0x200193dc*(x** 3)+L0x200193e0*(x** 4)+L0x200193e4*(x** 5)+
     L0x200193e8*(x** 6)+L0x200193ec*(x** 7)+L0x200193f0*(x** 8)+
     L0x200193f4*(x** 9)+L0x200193f8*(x**10)+L0x200193fc*(x**11)+
     L0x20019400*(x**12)+L0x20019404*(x**13)+L0x20019408*(x**14)+
     L0x2001940c*(x**15)+L0x20019410*(x**16)+L0x20019414*(x**17)+
     L0x20019418*(x**18)+L0x2001941c*(x**19)+L0x20019420*(x**20)+
     L0x20019424*(x**21)+L0x20019428*(x**22)+L0x2001942c*(x**23)+
     L0x20019430*(x**24)+L0x20019434*(x**25)+L0x20019438*(x**26)+
     L0x2001943c*(x**27)+L0x20019440*(x**28)+L0x20019444*(x**29)+
     L0x20019448*(x**30)+L0x2001944c*(x**31)+L0x20019450*(x**32)+
     L0x20019454*(x**33)+L0x20019458*(x**34)+L0x2001945c*(x**35)+
     L0x20019460*(x**36)+L0x20019464*(x**37)+L0x20019468*(x**38)+
     L0x2001946c*(x**39)+L0x20019470*(x**40)+L0x20019474*(x**41)+
     L0x20019478*(x**42)+L0x2001947c*(x**43)+L0x20019480*(x**44)+
     L0x20019484*(x**45)+L0x20019488*(x**46)+L0x2001948c*(x**47)+
     L0x20019490*(x**48)+L0x20019494*(x**49)+L0x20019498*(x**50)+
     L0x2001949c*(x**51)+L0x200194a0*(x**52)+L0x200194a4*(x**53)+
     L0x200194a8*(x**54)+L0x200194ac*(x**55)+L0x200194b0*(x**56)+
     L0x200194b4*(x**57)+L0x200194b8*(x**58)+L0x200194bc*(x**59)+
     L0x200194c0*(x**60)+L0x200194c4*(x**61)+L0x200194c8*(x**62)+
     L0x200194cc*(x**63))
    [1043969, x**64 - 287998],
eqmod (cinp_poly**2)
    (L0x200194d0*(x** 0)+L0x200194d4*(x** 1)+L0x200194d8*(x** 2)+
     L0x200194dc*(x** 3)+L0x200194e0*(x** 4)+L0x200194e4*(x** 5)+
     L0x200194e8*(x** 6)+L0x200194ec*(x** 7)+L0x200194f0*(x** 8)+
     L0x200194f4*(x** 9)+L0x200194f8*(x**10)+L0x200194fc*(x**11)+
     L0x20019500*(x**12)+L0x20019504*(x**13)+L0x20019508*(x**14)+
     L0x2001950c*(x**15)+L0x20019510*(x**16)+L0x20019514*(x**17)+
     L0x20019518*(x**18)+L0x2001951c*(x**19)+L0x20019520*(x**20)+
     L0x20019524*(x**21)+L0x20019528*(x**22)+L0x2001952c*(x**23)+
     L0x20019530*(x**24)+L0x20019534*(x**25)+L0x20019538*(x**26)+
     L0x2001953c*(x**27)+L0x20019540*(x**28)+L0x20019544*(x**29)+
     L0x20019548*(x**30)+L0x2001954c*(x**31)+L0x20019550*(x**32)+
     L0x20019554*(x**33)+L0x20019558*(x**34)+L0x2001955c*(x**35)+
     L0x20019560*(x**36)+L0x20019564*(x**37)+L0x20019568*(x**38)+
     L0x2001956c*(x**39)+L0x20019570*(x**40)+L0x20019574*(x**41)+
     L0x20019578*(x**42)+L0x2001957c*(x**43)+L0x20019580*(x**44)+
     L0x20019584*(x**45)+L0x20019588*(x**46)+L0x2001958c*(x**47)+
     L0x20019590*(x**48)+L0x20019594*(x**49)+L0x20019598*(x**50)+
     L0x2001959c*(x**51)+L0x200195a0*(x**52)+L0x200195a4*(x**53)+
     L0x200195a8*(x**54)+L0x200195ac*(x**55)+L0x200195b0*(x**56)+
     L0x200195b4*(x**57)+L0x200195b8*(x**58)+L0x200195bc*(x**59)+
     L0x200195c0*(x**60)+L0x200195c4*(x**61)+L0x200195c8*(x**62)+
     L0x200195cc*(x**63))
    [1043969, x**64 - 755971],
eqmod (cinp_poly**2)
    (L0x200195d0*(x** 0)+L0x200195d4*(x** 1)+L0x200195d8*(x** 2)+
     L0x200195dc*(x** 3)+L0x200195e0*(x** 4)+L0x200195e4*(x** 5)+
     L0x200195e8*(x** 6)+L0x200195ec*(x** 7)+L0x200195f0*(x** 8)+
     L0x200195f4*(x** 9)+L0x200195f8*(x**10)+L0x200195fc*(x**11)+
     L0x20019600*(x**12)+L0x20019604*(x**13)+L0x20019608*(x**14)+
     L0x2001960c*(x**15)+L0x20019610*(x**16)+L0x20019614*(x**17)+
     L0x20019618*(x**18)+L0x2001961c*(x**19)+L0x20019620*(x**20)+
     L0x20019624*(x**21)+L0x20019628*(x**22)+L0x2001962c*(x**23)+
     L0x20019630*(x**24)+L0x20019634*(x**25)+L0x20019638*(x**26)+
     L0x2001963c*(x**27)+L0x20019640*(x**28)+L0x20019644*(x**29)+
     L0x20019648*(x**30)+L0x2001964c*(x**31)+L0x20019650*(x**32)+
     L0x20019654*(x**33)+L0x20019658*(x**34)+L0x2001965c*(x**35)+
     L0x20019660*(x**36)+L0x20019664*(x**37)+L0x20019668*(x**38)+
     L0x2001966c*(x**39)+L0x20019670*(x**40)+L0x20019674*(x**41)+
     L0x20019678*(x**42)+L0x2001967c*(x**43)+L0x20019680*(x**44)+
     L0x20019684*(x**45)+L0x20019688*(x**46)+L0x2001968c*(x**47)+
     L0x20019690*(x**48)+L0x20019694*(x**49)+L0x20019698*(x**50)+
     L0x2001969c*(x**51)+L0x200196a0*(x**52)+L0x200196a4*(x**53)+
     L0x200196a8*(x**54)+L0x200196ac*(x**55)+L0x200196b0*(x**56)+
     L0x200196b4*(x**57)+L0x200196b8*(x**58)+L0x200196bc*(x**59)+
     L0x200196c0*(x**60)+L0x200196c4*(x**61)+L0x200196c8*(x**62)+
     L0x200196cc*(x**63))
    [1043969, x**64 - 719789],
eqmod (cinp_poly**2)
    (L0x200196d0*(x** 0)+L0x200196d4*(x** 1)+L0x200196d8*(x** 2)+
     L0x200196dc*(x** 3)+L0x200196e0*(x** 4)+L0x200196e4*(x** 5)+
     L0x200196e8*(x** 6)+L0x200196ec*(x** 7)+L0x200196f0*(x** 8)+
     L0x200196f4*(x** 9)+L0x200196f8*(x**10)+L0x200196fc*(x**11)+
     L0x20019700*(x**12)+L0x20019704*(x**13)+L0x20019708*(x**14)+
     L0x2001970c*(x**15)+L0x20019710*(x**16)+L0x20019714*(x**17)+
     L0x20019718*(x**18)+L0x2001971c*(x**19)+L0x20019720*(x**20)+
     L0x20019724*(x**21)+L0x20019728*(x**22)+L0x2001972c*(x**23)+
     L0x20019730*(x**24)+L0x20019734*(x**25)+L0x20019738*(x**26)+
     L0x2001973c*(x**27)+L0x20019740*(x**28)+L0x20019744*(x**29)+
     L0x20019748*(x**30)+L0x2001974c*(x**31)+L0x20019750*(x**32)+
     L0x20019754*(x**33)+L0x20019758*(x**34)+L0x2001975c*(x**35)+
     L0x20019760*(x**36)+L0x20019764*(x**37)+L0x20019768*(x**38)+
     L0x2001976c*(x**39)+L0x20019770*(x**40)+L0x20019774*(x**41)+
     L0x20019778*(x**42)+L0x2001977c*(x**43)+L0x20019780*(x**44)+
     L0x20019784*(x**45)+L0x20019788*(x**46)+L0x2001978c*(x**47)+
     L0x20019790*(x**48)+L0x20019794*(x**49)+L0x20019798*(x**50)+
     L0x2001979c*(x**51)+L0x200197a0*(x**52)+L0x200197a4*(x**53)+
     L0x200197a8*(x**54)+L0x200197ac*(x**55)+L0x200197b0*(x**56)+
     L0x200197b4*(x**57)+L0x200197b8*(x**58)+L0x200197bc*(x**59)+
     L0x200197c0*(x**60)+L0x200197c4*(x**61)+L0x200197c8*(x**62)+
     L0x200197cc*(x**63))
    [1043969, x**64 - 324180],
eqmod (cinp_poly**2)
    (L0x200197d0*(x** 0)+L0x200197d4*(x** 1)+L0x200197d8*(x** 2)+
     L0x200197dc*(x** 3)+L0x200197e0*(x** 4)+L0x200197e4*(x** 5)+
     L0x200197e8*(x** 6)+L0x200197ec*(x** 7)+L0x200197f0*(x** 8)+
     L0x200197f4*(x** 9)+L0x200197f8*(x**10)+L0x200197fc*(x**11)+
     L0x20019800*(x**12)+L0x20019804*(x**13)+L0x20019808*(x**14)+
     L0x2001980c*(x**15)+L0x20019810*(x**16)+L0x20019814*(x**17)+
     L0x20019818*(x**18)+L0x2001981c*(x**19)+L0x20019820*(x**20)+
     L0x20019824*(x**21)+L0x20019828*(x**22)+L0x2001982c*(x**23)+
     L0x20019830*(x**24)+L0x20019834*(x**25)+L0x20019838*(x**26)+
     L0x2001983c*(x**27)+L0x20019840*(x**28)+L0x20019844*(x**29)+
     L0x20019848*(x**30)+L0x2001984c*(x**31)+L0x20019850*(x**32)+
     L0x20019854*(x**33)+L0x20019858*(x**34)+L0x2001985c*(x**35)+
     L0x20019860*(x**36)+L0x20019864*(x**37)+L0x20019868*(x**38)+
     L0x2001986c*(x**39)+L0x20019870*(x**40)+L0x20019874*(x**41)+
     L0x20019878*(x**42)+L0x2001987c*(x**43)+L0x20019880*(x**44)+
     L0x20019884*(x**45)+L0x20019888*(x**46)+L0x2001988c*(x**47)+
     L0x20019890*(x**48)+L0x20019894*(x**49)+L0x20019898*(x**50)+
     L0x2001989c*(x**51)+L0x200198a0*(x**52)+L0x200198a4*(x**53)+
     L0x200198a8*(x**54)+L0x200198ac*(x**55)+L0x200198b0*(x**56)+
     L0x200198b4*(x**57)+L0x200198b8*(x**58)+L0x200198bc*(x**59)+
     L0x200198c0*(x**60)+L0x200198c4*(x**61)+L0x200198c8*(x**62)+
     L0x200198cc*(x**63))
    [1043969, x**64 - 29512],
eqmod (cinp_poly**2)
    (L0x200198d0*(x** 0)+L0x200198d4*(x** 1)+L0x200198d8*(x** 2)+
     L0x200198dc*(x** 3)+L0x200198e0*(x** 4)+L0x200198e4*(x** 5)+
     L0x200198e8*(x** 6)+L0x200198ec*(x** 7)+L0x200198f0*(x** 8)+
     L0x200198f4*(x** 9)+L0x200198f8*(x**10)+L0x200198fc*(x**11)+
     L0x20019900*(x**12)+L0x20019904*(x**13)+L0x20019908*(x**14)+
     L0x2001990c*(x**15)+L0x20019910*(x**16)+L0x20019914*(x**17)+
     L0x20019918*(x**18)+L0x2001991c*(x**19)+L0x20019920*(x**20)+
     L0x20019924*(x**21)+L0x20019928*(x**22)+L0x2001992c*(x**23)+
     L0x20019930*(x**24)+L0x20019934*(x**25)+L0x20019938*(x**26)+
     L0x2001993c*(x**27)+L0x20019940*(x**28)+L0x20019944*(x**29)+
     L0x20019948*(x**30)+L0x2001994c*(x**31)+L0x20019950*(x**32)+
     L0x20019954*(x**33)+L0x20019958*(x**34)+L0x2001995c*(x**35)+
     L0x20019960*(x**36)+L0x20019964*(x**37)+L0x20019968*(x**38)+
     L0x2001996c*(x**39)+L0x20019970*(x**40)+L0x20019974*(x**41)+
     L0x20019978*(x**42)+L0x2001997c*(x**43)+L0x20019980*(x**44)+
     L0x20019984*(x**45)+L0x20019988*(x**46)+L0x2001998c*(x**47)+
     L0x20019990*(x**48)+L0x20019994*(x**49)+L0x20019998*(x**50)+
     L0x2001999c*(x**51)+L0x200199a0*(x**52)+L0x200199a4*(x**53)+
     L0x200199a8*(x**54)+L0x200199ac*(x**55)+L0x200199b0*(x**56)+
     L0x200199b4*(x**57)+L0x200199b8*(x**58)+L0x200199bc*(x**59)+
     L0x200199c0*(x**60)+L0x200199c4*(x**61)+L0x200199c8*(x**62)+
     L0x200199cc*(x**63))
    [1043969, x**64 - 1014457],
eqmod (cinp_poly**2)
    (L0x200199d0*(x** 0)+L0x200199d4*(x** 1)+L0x200199d8*(x** 2)+
     L0x200199dc*(x** 3)+L0x200199e0*(x** 4)+L0x200199e4*(x** 5)+
     L0x200199e8*(x** 6)+L0x200199ec*(x** 7)+L0x200199f0*(x** 8)+
     L0x200199f4*(x** 9)+L0x200199f8*(x**10)+L0x200199fc*(x**11)+
     L0x20019a00*(x**12)+L0x20019a04*(x**13)+L0x20019a08*(x**14)+
     L0x20019a0c*(x**15)+L0x20019a10*(x**16)+L0x20019a14*(x**17)+
     L0x20019a18*(x**18)+L0x20019a1c*(x**19)+L0x20019a20*(x**20)+
     L0x20019a24*(x**21)+L0x20019a28*(x**22)+L0x20019a2c*(x**23)+
     L0x20019a30*(x**24)+L0x20019a34*(x**25)+L0x20019a38*(x**26)+
     L0x20019a3c*(x**27)+L0x20019a40*(x**28)+L0x20019a44*(x**29)+
     L0x20019a48*(x**30)+L0x20019a4c*(x**31)+L0x20019a50*(x**32)+
     L0x20019a54*(x**33)+L0x20019a58*(x**34)+L0x20019a5c*(x**35)+
     L0x20019a60*(x**36)+L0x20019a64*(x**37)+L0x20019a68*(x**38)+
     L0x20019a6c*(x**39)+L0x20019a70*(x**40)+L0x20019a74*(x**41)+
     L0x20019a78*(x**42)+L0x20019a7c*(x**43)+L0x20019a80*(x**44)+
     L0x20019a84*(x**45)+L0x20019a88*(x**46)+L0x20019a8c*(x**47)+
     L0x20019a90*(x**48)+L0x20019a94*(x**49)+L0x20019a98*(x**50)+
     L0x20019a9c*(x**51)+L0x20019aa0*(x**52)+L0x20019aa4*(x**53)+
     L0x20019aa8*(x**54)+L0x20019aac*(x**55)+L0x20019ab0*(x**56)+
     L0x20019ab4*(x**57)+L0x20019ab8*(x**58)+L0x20019abc*(x**59)+
     L0x20019ac0*(x**60)+L0x20019ac4*(x**61)+L0x20019ac8*(x**62)+
     L0x20019acc*(x**63))
    [1043969, x**64 - 145873],
eqmod (cinp_poly**2)
    (L0x20019ad0*(x** 0)+L0x20019ad4*(x** 1)+L0x20019ad8*(x** 2)+
     L0x20019adc*(x** 3)+L0x20019ae0*(x** 4)+L0x20019ae4*(x** 5)+
     L0x20019ae8*(x** 6)+L0x20019aec*(x** 7)+L0x20019af0*(x** 8)+
     L0x20019af4*(x** 9)+L0x20019af8*(x**10)+L0x20019afc*(x**11)+
     L0x20019b00*(x**12)+L0x20019b04*(x**13)+L0x20019b08*(x**14)+
     L0x20019b0c*(x**15)+L0x20019b10*(x**16)+L0x20019b14*(x**17)+
     L0x20019b18*(x**18)+L0x20019b1c*(x**19)+L0x20019b20*(x**20)+
     L0x20019b24*(x**21)+L0x20019b28*(x**22)+L0x20019b2c*(x**23)+
     L0x20019b30*(x**24)+L0x20019b34*(x**25)+L0x20019b38*(x**26)+
     L0x20019b3c*(x**27)+L0x20019b40*(x**28)+L0x20019b44*(x**29)+
     L0x20019b48*(x**30)+L0x20019b4c*(x**31)+L0x20019b50*(x**32)+
     L0x20019b54*(x**33)+L0x20019b58*(x**34)+L0x20019b5c*(x**35)+
     L0x20019b60*(x**36)+L0x20019b64*(x**37)+L0x20019b68*(x**38)+
     L0x20019b6c*(x**39)+L0x20019b70*(x**40)+L0x20019b74*(x**41)+
     L0x20019b78*(x**42)+L0x20019b7c*(x**43)+L0x20019b80*(x**44)+
     L0x20019b84*(x**45)+L0x20019b88*(x**46)+L0x20019b8c*(x**47)+
     L0x20019b90*(x**48)+L0x20019b94*(x**49)+L0x20019b98*(x**50)+
     L0x20019b9c*(x**51)+L0x20019ba0*(x**52)+L0x20019ba4*(x**53)+
     L0x20019ba8*(x**54)+L0x20019bac*(x**55)+L0x20019bb0*(x**56)+
     L0x20019bb4*(x**57)+L0x20019bb8*(x**58)+L0x20019bbc*(x**59)+
     L0x20019bc0*(x**60)+L0x20019bc4*(x**61)+L0x20019bc8*(x**62)+
     L0x20019bcc*(x**63))
    [1043969, x**64 - 898096],
eqmod (cinp_poly**2)
    (L0x20019bd0*(x** 0)+L0x20019bd4*(x** 1)+L0x20019bd8*(x** 2)+
     L0x20019bdc*(x** 3)+L0x20019be0*(x** 4)+L0x20019be4*(x** 5)+
     L0x20019be8*(x** 6)+L0x20019bec*(x** 7)+L0x20019bf0*(x** 8)+
     L0x20019bf4*(x** 9)+L0x20019bf8*(x**10)+L0x20019bfc*(x**11)+
     L0x20019c00*(x**12)+L0x20019c04*(x**13)+L0x20019c08*(x**14)+
     L0x20019c0c*(x**15)+L0x20019c10*(x**16)+L0x20019c14*(x**17)+
     L0x20019c18*(x**18)+L0x20019c1c*(x**19)+L0x20019c20*(x**20)+
     L0x20019c24*(x**21)+L0x20019c28*(x**22)+L0x20019c2c*(x**23)+
     L0x20019c30*(x**24)+L0x20019c34*(x**25)+L0x20019c38*(x**26)+
     L0x20019c3c*(x**27)+L0x20019c40*(x**28)+L0x20019c44*(x**29)+
     L0x20019c48*(x**30)+L0x20019c4c*(x**31)+L0x20019c50*(x**32)+
     L0x20019c54*(x**33)+L0x20019c58*(x**34)+L0x20019c5c*(x**35)+
     L0x20019c60*(x**36)+L0x20019c64*(x**37)+L0x20019c68*(x**38)+
     L0x20019c6c*(x**39)+L0x20019c70*(x**40)+L0x20019c74*(x**41)+
     L0x20019c78*(x**42)+L0x20019c7c*(x**43)+L0x20019c80*(x**44)+
     L0x20019c84*(x**45)+L0x20019c88*(x**46)+L0x20019c8c*(x**47)+
     L0x20019c90*(x**48)+L0x20019c94*(x**49)+L0x20019c98*(x**50)+
     L0x20019c9c*(x**51)+L0x20019ca0*(x**52)+L0x20019ca4*(x**53)+
     L0x20019ca8*(x**54)+L0x20019cac*(x**55)+L0x20019cb0*(x**56)+
     L0x20019cb4*(x**57)+L0x20019cb8*(x**58)+L0x20019cbc*(x**59)+
     L0x20019cc0*(x**60)+L0x20019cc4*(x**61)+L0x20019cc8*(x**62)+
     L0x20019ccc*(x**63))
    [1043969, x**64 - 445347],
eqmod (cinp_poly**2)
    (L0x20019cd0*(x** 0)+L0x20019cd4*(x** 1)+L0x20019cd8*(x** 2)+
     L0x20019cdc*(x** 3)+L0x20019ce0*(x** 4)+L0x20019ce4*(x** 5)+
     L0x20019ce8*(x** 6)+L0x20019cec*(x** 7)+L0x20019cf0*(x** 8)+
     L0x20019cf4*(x** 9)+L0x20019cf8*(x**10)+L0x20019cfc*(x**11)+
     L0x20019d00*(x**12)+L0x20019d04*(x**13)+L0x20019d08*(x**14)+
     L0x20019d0c*(x**15)+L0x20019d10*(x**16)+L0x20019d14*(x**17)+
     L0x20019d18*(x**18)+L0x20019d1c*(x**19)+L0x20019d20*(x**20)+
     L0x20019d24*(x**21)+L0x20019d28*(x**22)+L0x20019d2c*(x**23)+
     L0x20019d30*(x**24)+L0x20019d34*(x**25)+L0x20019d38*(x**26)+
     L0x20019d3c*(x**27)+L0x20019d40*(x**28)+L0x20019d44*(x**29)+
     L0x20019d48*(x**30)+L0x20019d4c*(x**31)+L0x20019d50*(x**32)+
     L0x20019d54*(x**33)+L0x20019d58*(x**34)+L0x20019d5c*(x**35)+
     L0x20019d60*(x**36)+L0x20019d64*(x**37)+L0x20019d68*(x**38)+
     L0x20019d6c*(x**39)+L0x20019d70*(x**40)+L0x20019d74*(x**41)+
     L0x20019d78*(x**42)+L0x20019d7c*(x**43)+L0x20019d80*(x**44)+
     L0x20019d84*(x**45)+L0x20019d88*(x**46)+L0x20019d8c*(x**47)+
     L0x20019d90*(x**48)+L0x20019d94*(x**49)+L0x20019d98*(x**50)+
     L0x20019d9c*(x**51)+L0x20019da0*(x**52)+L0x20019da4*(x**53)+
     L0x20019da8*(x**54)+L0x20019dac*(x**55)+L0x20019db0*(x**56)+
     L0x20019db4*(x**57)+L0x20019db8*(x**58)+L0x20019dbc*(x**59)+
     L0x20019dc0*(x**60)+L0x20019dc4*(x**61)+L0x20019dc8*(x**62)+
     L0x20019dcc*(x**63))
    [1043969, x**64 - 598622],
eqmod (cinp_poly**2)
    (L0x20019dd0*(x** 0)+L0x20019dd4*(x** 1)+L0x20019dd8*(x** 2)+
     L0x20019ddc*(x** 3)+L0x20019de0*(x** 4)+L0x20019de4*(x** 5)+
     L0x20019de8*(x** 6)+L0x20019dec*(x** 7)+L0x20019df0*(x** 8)+
     L0x20019df4*(x** 9)+L0x20019df8*(x**10)+L0x20019dfc*(x**11)+
     L0x20019e00*(x**12)+L0x20019e04*(x**13)+L0x20019e08*(x**14)+
     L0x20019e0c*(x**15)+L0x20019e10*(x**16)+L0x20019e14*(x**17)+
     L0x20019e18*(x**18)+L0x20019e1c*(x**19)+L0x20019e20*(x**20)+
     L0x20019e24*(x**21)+L0x20019e28*(x**22)+L0x20019e2c*(x**23)+
     L0x20019e30*(x**24)+L0x20019e34*(x**25)+L0x20019e38*(x**26)+
     L0x20019e3c*(x**27)+L0x20019e40*(x**28)+L0x20019e44*(x**29)+
     L0x20019e48*(x**30)+L0x20019e4c*(x**31)+L0x20019e50*(x**32)+
     L0x20019e54*(x**33)+L0x20019e58*(x**34)+L0x20019e5c*(x**35)+
     L0x20019e60*(x**36)+L0x20019e64*(x**37)+L0x20019e68*(x**38)+
     L0x20019e6c*(x**39)+L0x20019e70*(x**40)+L0x20019e74*(x**41)+
     L0x20019e78*(x**42)+L0x20019e7c*(x**43)+L0x20019e80*(x**44)+
     L0x20019e84*(x**45)+L0x20019e88*(x**46)+L0x20019e8c*(x**47)+
     L0x20019e90*(x**48)+L0x20019e94*(x**49)+L0x20019e98*(x**50)+
     L0x20019e9c*(x**51)+L0x20019ea0*(x**52)+L0x20019ea4*(x**53)+
     L0x20019ea8*(x**54)+L0x20019eac*(x**55)+L0x20019eb0*(x**56)+
     L0x20019eb4*(x**57)+L0x20019eb8*(x**58)+L0x20019ebc*(x**59)+
     L0x20019ec0*(x**60)+L0x20019ec4*(x**61)+L0x20019ec8*(x**62)+
     L0x20019ecc*(x**63))
    [1043969, x**64 - 775725],
eqmod (cinp_poly**2)
    (L0x20019ed0*(x** 0)+L0x20019ed4*(x** 1)+L0x20019ed8*(x** 2)+
     L0x20019edc*(x** 3)+L0x20019ee0*(x** 4)+L0x20019ee4*(x** 5)+
     L0x20019ee8*(x** 6)+L0x20019eec*(x** 7)+L0x20019ef0*(x** 8)+
     L0x20019ef4*(x** 9)+L0x20019ef8*(x**10)+L0x20019efc*(x**11)+
     L0x20019f00*(x**12)+L0x20019f04*(x**13)+L0x20019f08*(x**14)+
     L0x20019f0c*(x**15)+L0x20019f10*(x**16)+L0x20019f14*(x**17)+
     L0x20019f18*(x**18)+L0x20019f1c*(x**19)+L0x20019f20*(x**20)+
     L0x20019f24*(x**21)+L0x20019f28*(x**22)+L0x20019f2c*(x**23)+
     L0x20019f30*(x**24)+L0x20019f34*(x**25)+L0x20019f38*(x**26)+
     L0x20019f3c*(x**27)+L0x20019f40*(x**28)+L0x20019f44*(x**29)+
     L0x20019f48*(x**30)+L0x20019f4c*(x**31)+L0x20019f50*(x**32)+
     L0x20019f54*(x**33)+L0x20019f58*(x**34)+L0x20019f5c*(x**35)+
     L0x20019f60*(x**36)+L0x20019f64*(x**37)+L0x20019f68*(x**38)+
     L0x20019f6c*(x**39)+L0x20019f70*(x**40)+L0x20019f74*(x**41)+
     L0x20019f78*(x**42)+L0x20019f7c*(x**43)+L0x20019f80*(x**44)+
     L0x20019f84*(x**45)+L0x20019f88*(x**46)+L0x20019f8c*(x**47)+
     L0x20019f90*(x**48)+L0x20019f94*(x**49)+L0x20019f98*(x**50)+
     L0x20019f9c*(x**51)+L0x20019fa0*(x**52)+L0x20019fa4*(x**53)+
     L0x20019fa8*(x**54)+L0x20019fac*(x**55)+L0x20019fb0*(x**56)+
     L0x20019fb4*(x**57)+L0x20019fb8*(x**58)+L0x20019fbc*(x**59)+
     L0x20019fc0*(x**60)+L0x20019fc4*(x**61)+L0x20019fc8*(x**62)+
     L0x20019fcc*(x**63))
    [1043969, x**64 - 268244]
] && and [
(-5)@32*1043969@32 <s L0x20018fd0, L0x20018fd0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018fd4, L0x20018fd4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018fd8, L0x20018fd8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018fdc, L0x20018fdc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018fe0, L0x20018fe0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018fe4, L0x20018fe4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018fe8, L0x20018fe8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018fec, L0x20018fec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018ff0, L0x20018ff0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018ff4, L0x20018ff4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018ff8, L0x20018ff8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20018ffc, L0x20018ffc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019000, L0x20019000 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019004, L0x20019004 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019008, L0x20019008 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001900c, L0x2001900c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019010, L0x20019010 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019014, L0x20019014 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019018, L0x20019018 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001901c, L0x2001901c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019020, L0x20019020 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019024, L0x20019024 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019028, L0x20019028 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001902c, L0x2001902c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019030, L0x20019030 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019034, L0x20019034 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019038, L0x20019038 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001903c, L0x2001903c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019040, L0x20019040 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019044, L0x20019044 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019048, L0x20019048 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001904c, L0x2001904c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019050, L0x20019050 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019054, L0x20019054 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019058, L0x20019058 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001905c, L0x2001905c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019060, L0x20019060 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019064, L0x20019064 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019068, L0x20019068 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001906c, L0x2001906c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019070, L0x20019070 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019074, L0x20019074 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019078, L0x20019078 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001907c, L0x2001907c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019080, L0x20019080 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019084, L0x20019084 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019088, L0x20019088 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001908c, L0x2001908c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019090, L0x20019090 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019094, L0x20019094 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019098, L0x20019098 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001909c, L0x2001909c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190a0, L0x200190a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190a4, L0x200190a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190a8, L0x200190a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190ac, L0x200190ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190b0, L0x200190b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190b4, L0x200190b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190b8, L0x200190b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190bc, L0x200190bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190c0, L0x200190c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190c4, L0x200190c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190c8, L0x200190c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190cc, L0x200190cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190d0, L0x200190d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190d4, L0x200190d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190d8, L0x200190d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190dc, L0x200190dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190e0, L0x200190e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190e4, L0x200190e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190e8, L0x200190e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190ec, L0x200190ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190f0, L0x200190f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190f4, L0x200190f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190f8, L0x200190f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200190fc, L0x200190fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019100, L0x20019100 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019104, L0x20019104 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019108, L0x20019108 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001910c, L0x2001910c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019110, L0x20019110 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019114, L0x20019114 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019118, L0x20019118 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001911c, L0x2001911c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019120, L0x20019120 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019124, L0x20019124 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019128, L0x20019128 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001912c, L0x2001912c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019130, L0x20019130 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019134, L0x20019134 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019138, L0x20019138 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001913c, L0x2001913c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019140, L0x20019140 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019144, L0x20019144 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019148, L0x20019148 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001914c, L0x2001914c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019150, L0x20019150 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019154, L0x20019154 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019158, L0x20019158 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001915c, L0x2001915c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019160, L0x20019160 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019164, L0x20019164 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019168, L0x20019168 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001916c, L0x2001916c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019170, L0x20019170 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019174, L0x20019174 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019178, L0x20019178 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001917c, L0x2001917c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019180, L0x20019180 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019184, L0x20019184 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019188, L0x20019188 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001918c, L0x2001918c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019190, L0x20019190 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019194, L0x20019194 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019198, L0x20019198 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001919c, L0x2001919c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191a0, L0x200191a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191a4, L0x200191a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191a8, L0x200191a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191ac, L0x200191ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191b0, L0x200191b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191b4, L0x200191b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191b8, L0x200191b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191bc, L0x200191bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191c0, L0x200191c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191c4, L0x200191c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191c8, L0x200191c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191cc, L0x200191cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191d0, L0x200191d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191d4, L0x200191d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191d8, L0x200191d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191dc, L0x200191dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191e0, L0x200191e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191e4, L0x200191e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191e8, L0x200191e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191ec, L0x200191ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191f0, L0x200191f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191f4, L0x200191f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191f8, L0x200191f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200191fc, L0x200191fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019200, L0x20019200 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019204, L0x20019204 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019208, L0x20019208 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001920c, L0x2001920c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019210, L0x20019210 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019214, L0x20019214 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019218, L0x20019218 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001921c, L0x2001921c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019220, L0x20019220 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019224, L0x20019224 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019228, L0x20019228 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001922c, L0x2001922c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019230, L0x20019230 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019234, L0x20019234 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019238, L0x20019238 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001923c, L0x2001923c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019240, L0x20019240 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019244, L0x20019244 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019248, L0x20019248 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001924c, L0x2001924c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019250, L0x20019250 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019254, L0x20019254 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019258, L0x20019258 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001925c, L0x2001925c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019260, L0x20019260 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019264, L0x20019264 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019268, L0x20019268 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001926c, L0x2001926c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019270, L0x20019270 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019274, L0x20019274 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019278, L0x20019278 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001927c, L0x2001927c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019280, L0x20019280 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019284, L0x20019284 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019288, L0x20019288 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001928c, L0x2001928c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019290, L0x20019290 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019294, L0x20019294 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019298, L0x20019298 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001929c, L0x2001929c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192a0, L0x200192a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192a4, L0x200192a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192a8, L0x200192a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192ac, L0x200192ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192b0, L0x200192b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192b4, L0x200192b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192b8, L0x200192b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192bc, L0x200192bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192c0, L0x200192c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192c4, L0x200192c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192c8, L0x200192c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192cc, L0x200192cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192d0, L0x200192d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192d4, L0x200192d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192d8, L0x200192d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192dc, L0x200192dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192e0, L0x200192e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192e4, L0x200192e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192e8, L0x200192e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192ec, L0x200192ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192f0, L0x200192f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192f4, L0x200192f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192f8, L0x200192f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200192fc, L0x200192fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019300, L0x20019300 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019304, L0x20019304 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019308, L0x20019308 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001930c, L0x2001930c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019310, L0x20019310 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019314, L0x20019314 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019318, L0x20019318 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001931c, L0x2001931c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019320, L0x20019320 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019324, L0x20019324 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019328, L0x20019328 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001932c, L0x2001932c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019330, L0x20019330 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019334, L0x20019334 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019338, L0x20019338 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001933c, L0x2001933c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019340, L0x20019340 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019344, L0x20019344 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019348, L0x20019348 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001934c, L0x2001934c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019350, L0x20019350 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019354, L0x20019354 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019358, L0x20019358 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001935c, L0x2001935c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019360, L0x20019360 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019364, L0x20019364 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019368, L0x20019368 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001936c, L0x2001936c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019370, L0x20019370 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019374, L0x20019374 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019378, L0x20019378 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001937c, L0x2001937c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019380, L0x20019380 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019384, L0x20019384 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019388, L0x20019388 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001938c, L0x2001938c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019390, L0x20019390 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019394, L0x20019394 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019398, L0x20019398 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001939c, L0x2001939c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193a0, L0x200193a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193a4, L0x200193a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193a8, L0x200193a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193ac, L0x200193ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193b0, L0x200193b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193b4, L0x200193b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193b8, L0x200193b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193bc, L0x200193bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193c0, L0x200193c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193c4, L0x200193c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193c8, L0x200193c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193cc, L0x200193cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193d0, L0x200193d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193d4, L0x200193d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193d8, L0x200193d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193dc, L0x200193dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193e0, L0x200193e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193e4, L0x200193e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193e8, L0x200193e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193ec, L0x200193ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193f0, L0x200193f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193f4, L0x200193f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193f8, L0x200193f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200193fc, L0x200193fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019400, L0x20019400 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019404, L0x20019404 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019408, L0x20019408 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001940c, L0x2001940c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019410, L0x20019410 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019414, L0x20019414 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019418, L0x20019418 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001941c, L0x2001941c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019420, L0x20019420 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019424, L0x20019424 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019428, L0x20019428 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001942c, L0x2001942c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019430, L0x20019430 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019434, L0x20019434 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019438, L0x20019438 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001943c, L0x2001943c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019440, L0x20019440 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019444, L0x20019444 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019448, L0x20019448 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001944c, L0x2001944c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019450, L0x20019450 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019454, L0x20019454 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019458, L0x20019458 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001945c, L0x2001945c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019460, L0x20019460 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019464, L0x20019464 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019468, L0x20019468 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001946c, L0x2001946c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019470, L0x20019470 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019474, L0x20019474 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019478, L0x20019478 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001947c, L0x2001947c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019480, L0x20019480 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019484, L0x20019484 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019488, L0x20019488 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001948c, L0x2001948c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019490, L0x20019490 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019494, L0x20019494 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019498, L0x20019498 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001949c, L0x2001949c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194a0, L0x200194a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194a4, L0x200194a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194a8, L0x200194a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194ac, L0x200194ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194b0, L0x200194b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194b4, L0x200194b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194b8, L0x200194b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194bc, L0x200194bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194c0, L0x200194c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194c4, L0x200194c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194c8, L0x200194c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194cc, L0x200194cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194d0, L0x200194d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194d4, L0x200194d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194d8, L0x200194d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194dc, L0x200194dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194e0, L0x200194e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194e4, L0x200194e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194e8, L0x200194e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194ec, L0x200194ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194f0, L0x200194f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194f4, L0x200194f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194f8, L0x200194f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200194fc, L0x200194fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019500, L0x20019500 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019504, L0x20019504 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019508, L0x20019508 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001950c, L0x2001950c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019510, L0x20019510 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019514, L0x20019514 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019518, L0x20019518 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001951c, L0x2001951c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019520, L0x20019520 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019524, L0x20019524 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019528, L0x20019528 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001952c, L0x2001952c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019530, L0x20019530 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019534, L0x20019534 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019538, L0x20019538 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001953c, L0x2001953c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019540, L0x20019540 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019544, L0x20019544 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019548, L0x20019548 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001954c, L0x2001954c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019550, L0x20019550 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019554, L0x20019554 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019558, L0x20019558 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001955c, L0x2001955c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019560, L0x20019560 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019564, L0x20019564 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019568, L0x20019568 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001956c, L0x2001956c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019570, L0x20019570 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019574, L0x20019574 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019578, L0x20019578 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001957c, L0x2001957c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019580, L0x20019580 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019584, L0x20019584 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019588, L0x20019588 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001958c, L0x2001958c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019590, L0x20019590 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019594, L0x20019594 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019598, L0x20019598 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001959c, L0x2001959c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195a0, L0x200195a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195a4, L0x200195a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195a8, L0x200195a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195ac, L0x200195ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195b0, L0x200195b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195b4, L0x200195b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195b8, L0x200195b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195bc, L0x200195bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195c0, L0x200195c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195c4, L0x200195c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195c8, L0x200195c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195cc, L0x200195cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195d0, L0x200195d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195d4, L0x200195d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195d8, L0x200195d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195dc, L0x200195dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195e0, L0x200195e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195e4, L0x200195e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195e8, L0x200195e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195ec, L0x200195ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195f0, L0x200195f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195f4, L0x200195f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195f8, L0x200195f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200195fc, L0x200195fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019600, L0x20019600 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019604, L0x20019604 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019608, L0x20019608 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001960c, L0x2001960c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019610, L0x20019610 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019614, L0x20019614 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019618, L0x20019618 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001961c, L0x2001961c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019620, L0x20019620 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019624, L0x20019624 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019628, L0x20019628 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001962c, L0x2001962c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019630, L0x20019630 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019634, L0x20019634 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019638, L0x20019638 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001963c, L0x2001963c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019640, L0x20019640 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019644, L0x20019644 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019648, L0x20019648 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001964c, L0x2001964c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019650, L0x20019650 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019654, L0x20019654 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019658, L0x20019658 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001965c, L0x2001965c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019660, L0x20019660 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019664, L0x20019664 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019668, L0x20019668 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001966c, L0x2001966c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019670, L0x20019670 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019674, L0x20019674 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019678, L0x20019678 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001967c, L0x2001967c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019680, L0x20019680 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019684, L0x20019684 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019688, L0x20019688 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001968c, L0x2001968c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019690, L0x20019690 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019694, L0x20019694 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019698, L0x20019698 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001969c, L0x2001969c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196a0, L0x200196a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196a4, L0x200196a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196a8, L0x200196a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196ac, L0x200196ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196b0, L0x200196b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196b4, L0x200196b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196b8, L0x200196b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196bc, L0x200196bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196c0, L0x200196c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196c4, L0x200196c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196c8, L0x200196c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196cc, L0x200196cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196d0, L0x200196d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196d4, L0x200196d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196d8, L0x200196d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196dc, L0x200196dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196e0, L0x200196e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196e4, L0x200196e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196e8, L0x200196e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196ec, L0x200196ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196f0, L0x200196f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196f4, L0x200196f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196f8, L0x200196f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200196fc, L0x200196fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019700, L0x20019700 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019704, L0x20019704 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019708, L0x20019708 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001970c, L0x2001970c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019710, L0x20019710 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019714, L0x20019714 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019718, L0x20019718 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001971c, L0x2001971c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019720, L0x20019720 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019724, L0x20019724 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019728, L0x20019728 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001972c, L0x2001972c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019730, L0x20019730 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019734, L0x20019734 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019738, L0x20019738 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001973c, L0x2001973c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019740, L0x20019740 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019744, L0x20019744 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019748, L0x20019748 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001974c, L0x2001974c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019750, L0x20019750 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019754, L0x20019754 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019758, L0x20019758 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001975c, L0x2001975c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019760, L0x20019760 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019764, L0x20019764 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019768, L0x20019768 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001976c, L0x2001976c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019770, L0x20019770 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019774, L0x20019774 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019778, L0x20019778 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001977c, L0x2001977c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019780, L0x20019780 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019784, L0x20019784 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019788, L0x20019788 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001978c, L0x2001978c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019790, L0x20019790 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019794, L0x20019794 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019798, L0x20019798 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001979c, L0x2001979c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197a0, L0x200197a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197a4, L0x200197a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197a8, L0x200197a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197ac, L0x200197ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197b0, L0x200197b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197b4, L0x200197b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197b8, L0x200197b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197bc, L0x200197bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197c0, L0x200197c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197c4, L0x200197c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197c8, L0x200197c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197cc, L0x200197cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197d0, L0x200197d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197d4, L0x200197d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197d8, L0x200197d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197dc, L0x200197dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197e0, L0x200197e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197e4, L0x200197e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197e8, L0x200197e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197ec, L0x200197ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197f0, L0x200197f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197f4, L0x200197f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197f8, L0x200197f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200197fc, L0x200197fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019800, L0x20019800 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019804, L0x20019804 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019808, L0x20019808 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001980c, L0x2001980c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019810, L0x20019810 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019814, L0x20019814 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019818, L0x20019818 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001981c, L0x2001981c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019820, L0x20019820 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019824, L0x20019824 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019828, L0x20019828 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001982c, L0x2001982c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019830, L0x20019830 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019834, L0x20019834 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019838, L0x20019838 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001983c, L0x2001983c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019840, L0x20019840 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019844, L0x20019844 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019848, L0x20019848 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001984c, L0x2001984c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019850, L0x20019850 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019854, L0x20019854 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019858, L0x20019858 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001985c, L0x2001985c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019860, L0x20019860 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019864, L0x20019864 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019868, L0x20019868 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001986c, L0x2001986c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019870, L0x20019870 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019874, L0x20019874 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019878, L0x20019878 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001987c, L0x2001987c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019880, L0x20019880 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019884, L0x20019884 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019888, L0x20019888 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001988c, L0x2001988c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019890, L0x20019890 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019894, L0x20019894 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019898, L0x20019898 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001989c, L0x2001989c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198a0, L0x200198a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198a4, L0x200198a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198a8, L0x200198a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198ac, L0x200198ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198b0, L0x200198b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198b4, L0x200198b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198b8, L0x200198b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198bc, L0x200198bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198c0, L0x200198c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198c4, L0x200198c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198c8, L0x200198c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198cc, L0x200198cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198d0, L0x200198d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198d4, L0x200198d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198d8, L0x200198d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198dc, L0x200198dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198e0, L0x200198e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198e4, L0x200198e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198e8, L0x200198e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198ec, L0x200198ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198f0, L0x200198f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198f4, L0x200198f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198f8, L0x200198f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200198fc, L0x200198fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019900, L0x20019900 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019904, L0x20019904 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019908, L0x20019908 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001990c, L0x2001990c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019910, L0x20019910 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019914, L0x20019914 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019918, L0x20019918 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001991c, L0x2001991c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019920, L0x20019920 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019924, L0x20019924 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019928, L0x20019928 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001992c, L0x2001992c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019930, L0x20019930 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019934, L0x20019934 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019938, L0x20019938 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001993c, L0x2001993c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019940, L0x20019940 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019944, L0x20019944 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019948, L0x20019948 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001994c, L0x2001994c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019950, L0x20019950 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019954, L0x20019954 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019958, L0x20019958 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001995c, L0x2001995c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019960, L0x20019960 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019964, L0x20019964 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019968, L0x20019968 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001996c, L0x2001996c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019970, L0x20019970 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019974, L0x20019974 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019978, L0x20019978 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001997c, L0x2001997c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019980, L0x20019980 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019984, L0x20019984 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019988, L0x20019988 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001998c, L0x2001998c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019990, L0x20019990 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019994, L0x20019994 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019998, L0x20019998 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x2001999c, L0x2001999c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199a0, L0x200199a0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199a4, L0x200199a4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199a8, L0x200199a8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199ac, L0x200199ac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199b0, L0x200199b0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199b4, L0x200199b4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199b8, L0x200199b8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199bc, L0x200199bc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199c0, L0x200199c0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199c4, L0x200199c4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199c8, L0x200199c8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199cc, L0x200199cc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199d0, L0x200199d0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199d4, L0x200199d4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199d8, L0x200199d8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199dc, L0x200199dc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199e0, L0x200199e0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199e4, L0x200199e4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199e8, L0x200199e8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199ec, L0x200199ec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199f0, L0x200199f0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199f4, L0x200199f4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199f8, L0x200199f8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x200199fc, L0x200199fc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a00, L0x20019a00 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a04, L0x20019a04 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a08, L0x20019a08 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a0c, L0x20019a0c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a10, L0x20019a10 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a14, L0x20019a14 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a18, L0x20019a18 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a1c, L0x20019a1c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a20, L0x20019a20 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a24, L0x20019a24 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a28, L0x20019a28 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a2c, L0x20019a2c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a30, L0x20019a30 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a34, L0x20019a34 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a38, L0x20019a38 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a3c, L0x20019a3c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a40, L0x20019a40 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a44, L0x20019a44 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a48, L0x20019a48 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a4c, L0x20019a4c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a50, L0x20019a50 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a54, L0x20019a54 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a58, L0x20019a58 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a5c, L0x20019a5c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a60, L0x20019a60 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a64, L0x20019a64 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a68, L0x20019a68 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a6c, L0x20019a6c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a70, L0x20019a70 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a74, L0x20019a74 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a78, L0x20019a78 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a7c, L0x20019a7c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a80, L0x20019a80 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a84, L0x20019a84 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a88, L0x20019a88 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a8c, L0x20019a8c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a90, L0x20019a90 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a94, L0x20019a94 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a98, L0x20019a98 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019a9c, L0x20019a9c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019aa0, L0x20019aa0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019aa4, L0x20019aa4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019aa8, L0x20019aa8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019aac, L0x20019aac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ab0, L0x20019ab0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ab4, L0x20019ab4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ab8, L0x20019ab8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019abc, L0x20019abc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ac0, L0x20019ac0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ac4, L0x20019ac4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ac8, L0x20019ac8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019acc, L0x20019acc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ad0, L0x20019ad0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ad4, L0x20019ad4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ad8, L0x20019ad8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019adc, L0x20019adc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ae0, L0x20019ae0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ae4, L0x20019ae4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ae8, L0x20019ae8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019aec, L0x20019aec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019af0, L0x20019af0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019af4, L0x20019af4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019af8, L0x20019af8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019afc, L0x20019afc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b00, L0x20019b00 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b04, L0x20019b04 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b08, L0x20019b08 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b0c, L0x20019b0c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b10, L0x20019b10 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b14, L0x20019b14 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b18, L0x20019b18 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b1c, L0x20019b1c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b20, L0x20019b20 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b24, L0x20019b24 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b28, L0x20019b28 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b2c, L0x20019b2c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b30, L0x20019b30 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b34, L0x20019b34 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b38, L0x20019b38 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b3c, L0x20019b3c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b40, L0x20019b40 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b44, L0x20019b44 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b48, L0x20019b48 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b4c, L0x20019b4c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b50, L0x20019b50 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b54, L0x20019b54 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b58, L0x20019b58 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b5c, L0x20019b5c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b60, L0x20019b60 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b64, L0x20019b64 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b68, L0x20019b68 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b6c, L0x20019b6c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b70, L0x20019b70 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b74, L0x20019b74 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b78, L0x20019b78 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b7c, L0x20019b7c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b80, L0x20019b80 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b84, L0x20019b84 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b88, L0x20019b88 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b8c, L0x20019b8c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b90, L0x20019b90 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b94, L0x20019b94 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b98, L0x20019b98 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019b9c, L0x20019b9c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ba0, L0x20019ba0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ba4, L0x20019ba4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ba8, L0x20019ba8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bac, L0x20019bac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bb0, L0x20019bb0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bb4, L0x20019bb4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bb8, L0x20019bb8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bbc, L0x20019bbc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bc0, L0x20019bc0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bc4, L0x20019bc4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bc8, L0x20019bc8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bcc, L0x20019bcc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bd0, L0x20019bd0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bd4, L0x20019bd4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bd8, L0x20019bd8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bdc, L0x20019bdc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019be0, L0x20019be0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019be4, L0x20019be4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019be8, L0x20019be8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bec, L0x20019bec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bf0, L0x20019bf0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bf4, L0x20019bf4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bf8, L0x20019bf8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019bfc, L0x20019bfc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c00, L0x20019c00 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c04, L0x20019c04 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c08, L0x20019c08 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c0c, L0x20019c0c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c10, L0x20019c10 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c14, L0x20019c14 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c18, L0x20019c18 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c1c, L0x20019c1c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c20, L0x20019c20 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c24, L0x20019c24 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c28, L0x20019c28 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c2c, L0x20019c2c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c30, L0x20019c30 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c34, L0x20019c34 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c38, L0x20019c38 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c3c, L0x20019c3c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c40, L0x20019c40 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c44, L0x20019c44 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c48, L0x20019c48 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c4c, L0x20019c4c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c50, L0x20019c50 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c54, L0x20019c54 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c58, L0x20019c58 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c5c, L0x20019c5c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c60, L0x20019c60 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c64, L0x20019c64 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c68, L0x20019c68 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c6c, L0x20019c6c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c70, L0x20019c70 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c74, L0x20019c74 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c78, L0x20019c78 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c7c, L0x20019c7c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c80, L0x20019c80 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c84, L0x20019c84 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c88, L0x20019c88 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c8c, L0x20019c8c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c90, L0x20019c90 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c94, L0x20019c94 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c98, L0x20019c98 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019c9c, L0x20019c9c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ca0, L0x20019ca0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ca4, L0x20019ca4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ca8, L0x20019ca8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cac, L0x20019cac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cb0, L0x20019cb0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cb4, L0x20019cb4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cb8, L0x20019cb8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cbc, L0x20019cbc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cc0, L0x20019cc0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cc4, L0x20019cc4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cc8, L0x20019cc8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ccc, L0x20019ccc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cd0, L0x20019cd0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cd4, L0x20019cd4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cd8, L0x20019cd8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cdc, L0x20019cdc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ce0, L0x20019ce0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ce4, L0x20019ce4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ce8, L0x20019ce8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cec, L0x20019cec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cf0, L0x20019cf0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cf4, L0x20019cf4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cf8, L0x20019cf8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019cfc, L0x20019cfc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d00, L0x20019d00 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d04, L0x20019d04 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d08, L0x20019d08 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d0c, L0x20019d0c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d10, L0x20019d10 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d14, L0x20019d14 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d18, L0x20019d18 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d1c, L0x20019d1c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d20, L0x20019d20 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d24, L0x20019d24 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d28, L0x20019d28 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d2c, L0x20019d2c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d30, L0x20019d30 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d34, L0x20019d34 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d38, L0x20019d38 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d3c, L0x20019d3c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d40, L0x20019d40 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d44, L0x20019d44 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d48, L0x20019d48 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d4c, L0x20019d4c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d50, L0x20019d50 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d54, L0x20019d54 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d58, L0x20019d58 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d5c, L0x20019d5c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d60, L0x20019d60 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d64, L0x20019d64 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d68, L0x20019d68 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d6c, L0x20019d6c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d70, L0x20019d70 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d74, L0x20019d74 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d78, L0x20019d78 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d7c, L0x20019d7c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d80, L0x20019d80 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d84, L0x20019d84 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d88, L0x20019d88 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d8c, L0x20019d8c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d90, L0x20019d90 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d94, L0x20019d94 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d98, L0x20019d98 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019d9c, L0x20019d9c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019da0, L0x20019da0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019da4, L0x20019da4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019da8, L0x20019da8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dac, L0x20019dac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019db0, L0x20019db0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019db4, L0x20019db4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019db8, L0x20019db8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dbc, L0x20019dbc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dc0, L0x20019dc0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dc4, L0x20019dc4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dc8, L0x20019dc8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dcc, L0x20019dcc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dd0, L0x20019dd0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dd4, L0x20019dd4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dd8, L0x20019dd8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ddc, L0x20019ddc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019de0, L0x20019de0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019de4, L0x20019de4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019de8, L0x20019de8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dec, L0x20019dec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019df0, L0x20019df0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019df4, L0x20019df4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019df8, L0x20019df8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019dfc, L0x20019dfc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e00, L0x20019e00 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e04, L0x20019e04 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e08, L0x20019e08 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e0c, L0x20019e0c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e10, L0x20019e10 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e14, L0x20019e14 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e18, L0x20019e18 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e1c, L0x20019e1c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e20, L0x20019e20 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e24, L0x20019e24 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e28, L0x20019e28 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e2c, L0x20019e2c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e30, L0x20019e30 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e34, L0x20019e34 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e38, L0x20019e38 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e3c, L0x20019e3c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e40, L0x20019e40 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e44, L0x20019e44 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e48, L0x20019e48 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e4c, L0x20019e4c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e50, L0x20019e50 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e54, L0x20019e54 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e58, L0x20019e58 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e5c, L0x20019e5c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e60, L0x20019e60 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e64, L0x20019e64 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e68, L0x20019e68 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e6c, L0x20019e6c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e70, L0x20019e70 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e74, L0x20019e74 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e78, L0x20019e78 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e7c, L0x20019e7c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e80, L0x20019e80 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e84, L0x20019e84 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e88, L0x20019e88 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e8c, L0x20019e8c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e90, L0x20019e90 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e94, L0x20019e94 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e98, L0x20019e98 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019e9c, L0x20019e9c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ea0, L0x20019ea0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ea4, L0x20019ea4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ea8, L0x20019ea8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019eac, L0x20019eac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019eb0, L0x20019eb0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019eb4, L0x20019eb4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019eb8, L0x20019eb8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ebc, L0x20019ebc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ec0, L0x20019ec0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ec4, L0x20019ec4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ec8, L0x20019ec8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ecc, L0x20019ecc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ed0, L0x20019ed0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ed4, L0x20019ed4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ed8, L0x20019ed8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019edc, L0x20019edc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ee0, L0x20019ee0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ee4, L0x20019ee4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ee8, L0x20019ee8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019eec, L0x20019eec <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ef0, L0x20019ef0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ef4, L0x20019ef4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019ef8, L0x20019ef8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019efc, L0x20019efc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f00, L0x20019f00 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f04, L0x20019f04 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f08, L0x20019f08 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f0c, L0x20019f0c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f10, L0x20019f10 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f14, L0x20019f14 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f18, L0x20019f18 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f1c, L0x20019f1c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f20, L0x20019f20 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f24, L0x20019f24 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f28, L0x20019f28 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f2c, L0x20019f2c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f30, L0x20019f30 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f34, L0x20019f34 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f38, L0x20019f38 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f3c, L0x20019f3c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f40, L0x20019f40 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f44, L0x20019f44 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f48, L0x20019f48 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f4c, L0x20019f4c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f50, L0x20019f50 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f54, L0x20019f54 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f58, L0x20019f58 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f5c, L0x20019f5c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f60, L0x20019f60 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f64, L0x20019f64 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f68, L0x20019f68 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f6c, L0x20019f6c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f70, L0x20019f70 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f74, L0x20019f74 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f78, L0x20019f78 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f7c, L0x20019f7c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f80, L0x20019f80 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f84, L0x20019f84 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f88, L0x20019f88 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f8c, L0x20019f8c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f90, L0x20019f90 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f94, L0x20019f94 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f98, L0x20019f98 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019f9c, L0x20019f9c <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fa0, L0x20019fa0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fa4, L0x20019fa4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fa8, L0x20019fa8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fac, L0x20019fac <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fb0, L0x20019fb0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fb4, L0x20019fb4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fb8, L0x20019fb8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fbc, L0x20019fbc <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fc0, L0x20019fc0 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fc4, L0x20019fc4 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fc8, L0x20019fc8 <s 5@32*1043969@32,
(-5)@32*1043969@32 <s L0x20019fcc, L0x20019fcc <s 5@32*1043969@32
]
}

